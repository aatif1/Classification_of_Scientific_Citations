<?xml version="1.0" encoding="iso-8859-1"?>
<acldoc acl_id="P04-1054">
	

	<s id="1">
		 Dependency Tree Kernels for Relation Extraction Aron Culotta University of Massachusetts Amherst , MA 01002 USA culotta@cs.umass.edu Jeffrey Sorensen IBM T.J. Watson Research Center Yorktown Heights , NY 10598 USA sorenj@us.ibm.com Abstract We extend previous work on tree kernels to estimate the similarity between the dependency trees of sentences . 
	</s>
	

	<s id="2">
		 Using this kernel within a Support Vector Machine , we detect and classify relations between entities in the Automatic Content Extraction ( ACE ) corpus of news articles . 
	</s>
	

	<s id="3">
		 We examine the utility of different features such as Wordnet hypernyms , parts of speech , and entity types , and find that the dependency tree kernel achieves a 20 % F1 improvement over a “bag-of-words” kernel . 
	</s>
	

	<s id="4">
		 1 Introduction The ability to detect complex patterns in data is limited by the complexity of the data’s representation . 
	</s>
	

	<s id="5">
		 In the case of text , a more structured data source ( e.g. a relational database ) allows richer queries than does an unstructured data source ( e.g. a collection of news articles ) . 
	</s>
	

	<s id="6">
		 For example , current web search engines would not perform well on the query , “list all California-based CEOs who have social ties with a United States Senator.” Only a structured representation of the data can effectively provide such a list . 
	</s>
	

	<s id="7">
		 The goal of Information Extraction ( IE ) is to discover relevant segments of information in a data stream that will be useful for structuring the data . 
	</s>
	

	<s id="8">
		 In the case of text , this usually amounts to finding mentions of interesting entities and the relations that join them , transforming a large corpus of unstructured text into a relational database with entries such as those in Table 1 . 
	</s>
	

	<s id="9">
		 IE is commonly viewed as a three stage process : first , an entity tagger detects all mentions of interest ; second , coreference resolution resolves disparate mentions of the same entity ; third , a relation extractor finds relations between these entities . 
	</s>
	

	<s id="10">
		 Entity tagging has been thoroughly addressed by many statistical machine learning techniques , obtaining greater than 90 % F1 on many datasets 
		<ref citStr="Tjong Kim Sang and De Meulder , 2003" id="1" label="CEPF" position="2215">
			( Tjong Kim Sang and De Meulder , 2003 )
		</ref>
		 . 
	</s>
	

	<s id="11">
		 Coreference resolution is an active area of research not investigated here ( Pa- Entity Type Location Apple Organization Organization Cupertino , CA Redmond , WA Microsoft Table 1 : An example of extracted fields sula et al. , 2002 ; McCallum and Wellner , 2003 ) . 
	</s>
	

	<s id="12">
		 We describe a relation extraction technique based on kernel methods . 
	</s>
	

	<s id="13">
		 Kernel methods are non- parametric density estimation techniques that compute a kernel function between data instances , where a kernel function can be thought of as a similarity measure . 
	</s>
	

	<s id="14">
		 Given a set of labeled instances , kernel methods determine the label of a novel instance by comparing it to the labeled training instances using this kernel function . 
	</s>
	

	<s id="15">
		 Nearest neighbor classification and support-vector machines ( SVMs ) are two popular examples of kernel methods 
		<ref citStr="Fukunaga , 1990" id="2" label="CEPF" position="3069">
			( Fukunaga , 1990 
		</ref>
		<ref citStr="Cortes and Vapnik , 1995" id="3" label="CEPF" position="3087">
			; Cortes and Vapnik , 1995 )
		</ref>
		 . 
	</s>
	

	<s id="16">
		 An advantage of kernel methods is that they can search a feature space much larger than could be represented by a feature extraction-based approach . 
	</s>
	

	<s id="17">
		 This is possible because the kernel function can explore an implicit feature space when calculating the similarity between two instances , as described in the Section 3 . 
	</s>
	

	<s id="18">
		 Working in such a large feature space can lead to over-fitting in many machine learning algorithms . 
	</s>
	

	<s id="19">
		 To address this problem , we apply SVMs to the task of relation extraction . 
	</s>
	

	<s id="20">
		 SVMs find a boundary between instances of different classes such that the distance between the boundary and the nearest instances is maximized . 
	</s>
	

	<s id="21">
		 This characteristic , in addition to empirical validation , indicates that SVMs are particularly robust to over-fitting . 
	</s>
	

	<s id="22">
		 Here we are interested in detecting and classifying instances of relations , where a relation is some meaningful connection between two entities ( Table 2 ) . 
	</s>
	

	<s id="23">
		 We represent each relation instance as an augmented dependency tree . 
	</s>
	

	<s id="24">
		 A dependency tree represents the grammatical dependencies in a sentence ; we augment this tree with features for each node AT NEAR PART ROLE SOCIAL Based-In Located Residence Relative-location Part-of Affiliate , Founder Associate , Grandparent Subsidiary Citizen-of , Management Parent , Sibling Other Client , Member Spouse , Other-professional Owner , Other , Staff Other-relative , Other-personal Table 2 : Relation types and subtypes . 
	</s>
	

	<s id="25">
		 ( e.g. part of speech ) We choose this representation because we hypothesize that instances containing similar relations will share similar substructures in their dependency trees . 
	</s>
	

	<s id="26">
		 The task of the kernel function is to find these similarities . 
	</s>
	

	<s id="27">
		 We define a tree kernel over dependency trees and incorporate this kernel within an SVM to extract relations from newswire documents . 
	</s>
	

	<s id="28">
		 The tree kernel approach consistently outperforms the bag-ofwords kernel , suggesting that this highly-structured representation of sentences is more informative for detecting and distinguishing relations . 
	</s>
	

	<s id="29">
		 2 Related Work Kernel methods 
		<ref citStr="Vapnik , 1998" id="4" label="CEPF" position="5298">
			( Vapnik , 1998 
		</ref>
		<ref citStr="Cristianini and Shawe-Taylor , 2000" id="5" label="CEPF" position="5314">
			; Cristianini and Shawe-Taylor , 2000 )
		</ref>
		 have become increasingly popular because of their ability to map arbitrary objects to a Euclidian feature space . 
	</s>
	

	<s id="30">
		 
		<ref citStr="Haussler ( 1999 )" id="6" label="CEPF" position="5494">
			Haussler ( 1999 )
		</ref>
		 describes a framework for calculating kernels over discrete structures such as strings and trees . 
	</s>
	

	<s id="31">
		 String kernels for text classification are explored in 
		<ref citStr="Lodhi et al . ( 2000 )" id="7" label="CEPF" position="5680">
			Lodhi et al . ( 2000 )
		</ref>
		 , and tree kernel variants are described in 
		<ref citStr="Zelenko et al. , 2003" id="8" label="CEPF" position="5725">
			( Zelenko et al. , 2003 
		</ref>
		<ref citStr="Collins and Duffy , 2002" id="9" label="CEPF" position="5749">
			; Collins and Duffy , 2002 
		</ref>
		<ref citStr="Cumby and Roth , 2003" id="10" label="CEPF" position="5776">
			; Cumby and Roth , 2003 )
		</ref>
		 . 
	</s>
	

	<s id="32">
		 Our algorithm is similar to that described by 
		<ref citStr="Zelenko et al . ( 2003 )" id="11" label="CERF" position="5883">
			Zelenko et al . ( 2003 )
		</ref>
		 . 
	</s>
	

	<s id="33">
		 Our contributions are a richer sentence representation , a more general framework to allow feature weighting , as well as the use of composite kernels to reduce kernel sparsity . 
	</s>
	

	<s id="34">
		 
		<ref citStr="Brin ( 1998 )" id="12" label="CEPN" position="6096">
			Brin ( 1998 )
		</ref>
		 and 
		<ref citStr="Agichtein and Gravano ( 2000 )" id="13" label="CEPN" position="6131">
			Agichtein and Gravano ( 2000 )
		</ref>
		 apply pattern matching and wrapper techniques for relation extraction , but these approaches do not scale well to fastly evolving corpora . 
	</s>
	

	<s id="35">
		 
		<ref citStr="Miller et al . ( 2000 )" id="14" label="CEPF" position="6304">
			Miller et al . ( 2000 )
		</ref>
		 propose an integrated statistical parsing technique that augments parse trees with semantic labels denoting entity and relation types . 
	</s>
	

	<s id="36">
		 Whereas 
		<ref citStr="Miller et al . ( 2000 )" id="15" label="CEPF" position="6481">
			Miller et al . ( 2000 )
		</ref>
		 use a generative model to produce parse information as well as relation information , we hypothesize that a technique discriminatively trained to classify relations will achieve better performance . 
	</s>
	

	<s id="37">
		 Also , 
		<ref citStr="Roth and Yih ( 2002 )" id="16" label="CEPF" position="6718">
			Roth and Yih ( 2002 )
		</ref>
		 learn a Bayesian network to tag entities and their relations simultaneously . 
	</s>
	

	<s id="38">
		 We experiment with a more challenging set of relation types and a larger corpus . 
	</s>
	

	<s id="39">
		 3 Kernel Methods In traditional machine learning , we are provided a set of training instances S = { x1 ... xN } , where each instance xZ is represented by some d- dimensional feature vector . 
	</s>
	

	<s id="40">
		 Much time is spent on the task of feature engineering – searching for the optimal feature set either manually by consulting domain experts or automatically through feature induction and selection 
		<ref citStr="Scott and Matwin , 1999" id="17" label="CEPF" position="7323">
			( Scott and Matwin , 1999 )
		</ref>
		 . 
	</s>
	

	<s id="41">
		 For example , in entity detection the original instance representation is generally a word vector corresponding to a sentence . 
	</s>
	

	<s id="42">
		 Feature extraction and induction may result in features such as part-ofspeech , word n-grams , character n-grams , capitalization , and conjunctions of these features . 
	</s>
	

	<s id="43">
		 In the case of more structured objects , such as parse trees , features may include some description of the object’s structure , such as “has an NP-VP subtree.” Kernel methods can be particularly effective at reducing the feature engineering burden for structured objects . 
	</s>
	

	<s id="44">
		 By calculating the similarity between two objects , kernel methods can employ dynamic programming solutions to efficiently enumerate over substructures that would be too costly to explicitly include as features . 
	</s>
	

	<s id="45">
		 Formally , a kernel function K is a mapping K : X x X ^ [ 0 , oc ] from instance space X to a similarity score K(x , y ) = PZ OZ(x)OZ(y) = O(x) · O(y) . 
	</s>
	

	<s id="46">
		 Here , OZ(x) is some feature function over the instance x . 
	</s>
	

	<s id="47">
		 The kernel function must be symmetric [ K(x , y ) = K(y , x ) ] and positivesemidefinite . 
	</s>
	

	<s id="48">
		 By positive-semidefinite , we require that the if x 1 , ... , xn E X , then the n x n matrix G defined by GZj = K(xZ , xj ) is positive semi- definite . 
	</s>
	

	<s id="49">
		 It has been shown that any function that takes the dot product of feature vectors is a kernel function 
		<ref citStr="Haussler , 1999" id="18" label="CEPF" position="8774">
			( Haussler , 1999 )
		</ref>
		 . 
	</s>
	

	<s id="50">
		 A simple kernel function takes the dot product of the vector representation of instances being compared . 
	</s>
	

	<s id="51">
		 For example , in document classification , each document can be represented by a binary vector , where each element corresponds to the presence or absence of a particular word in that document . 
	</s>
	

	<s id="52">
		 Here , OZ(x) = 1 if word i occurs in document x . 
	</s>
	

	<s id="53">
		 Thus , the kernel function K(x , y ) returns the num- ber of words in common between x and y . 
	</s>
	

	<s id="54">
		 We refer to this kernel as the “bag-of-words” kernel , since it ignores word order . 
	</s>
	

	<s id="55">
		 When instances are more structured , as in the case of dependency trees , more complex kernels become necessary . 
	</s>
	

	<s id="56">
		 
		<ref citStr="Haussler ( 1999 )" id="19" label="CEPF" position="9504">
			Haussler ( 1999 )
		</ref>
		 describes convolution kernels , which find the similarity between two structures by summing the similarity of their substructures . 
	</s>
	

	<s id="57">
		 As an example , consider a kernel over strings . 
	</s>
	

	<s id="58">
		 To determine the similarity between two strings , string kernels 
		<ref citStr="Lodhi et al. , 2000" id="20" label="CEPF" position="9792">
			( Lodhi et al. , 2000 )
		</ref>
		 count the number of common subsequences in the two strings , and weight these matches by their length . 
	</s>
	

	<s id="59">
		 Thus , Oi ( x ) is the number of times string x contains the subsequence referenced by i . 
	</s>
	

	<s id="60">
		 These matches can be found efficiently through a dynamic program , allowing string kernels to examine long-range features that would be computationally infeasible in a feature-based method . 
	</s>
	

	<s id="61">
		 Given a training set S = { xs ... xN } , kernel methods compute the Gram matrix G such that Gib = K(xi,xb) . 
	</s>
	

	<s id="62">
		 Given G , the classifier finds a hyperplane which separates instances of different classes . 
	</s>
	

	<s id="63">
		 To classify an unseen instance x , the classifier first projects x into the feature space defined by the kernel function . 
	</s>
	

	<s id="64">
		 Classification then consists of determining on which side of the separating hyper- plane x lies . 
	</s>
	

	<s id="65">
		 A support vector machine ( SVM ) is a type of classifier that formulates the task of finding the separating hyperplane as the solution to a quadratic programming problem 
		<ref citStr="Cristianini and Shawe-Taylor , 2000" id="21" label="CEPF" position="10874">
			( Cristianini and Shawe-Taylor , 2000 )
		</ref>
		 . 
	</s>
	

	<s id="66">
		 Support vector machines attempt to find a hyperplane that not only separates the classes but also maximizes the margin between them . 
	</s>
	

	<s id="67">
		 The hope is that this will lead to better generalization performance on unseen instances . 
	</s>
	

	<s id="68">
		 4 Augmented Dependency Trees Our task is to detect and classify relations between entities in text . 
	</s>
	

	<s id="69">
		 We assume that entity tagging has been performed ; so to generate potential relation instances , we iterate over all pairs of entities occurring in the same sentence . 
	</s>
	

	<s id="70">
		 For each entity pair , we create an augmented dependency tree ( described below ) representing this instance . 
	</s>
	

	<s id="71">
		 Given a labeled training set of potential relations , we define a tree kernel over dependency trees which we then use in an SVM to classify test instances . 
	</s>
	

	<s id="72">
		 A dependency tree is a representation that denotes grammatical relations between words in a sentence ( Figure 1 ) . 
	</s>
	

	<s id="73">
		 A set of rules maps a parse tree to a dependency tree . 
	</s>
	

	<s id="74">
		 For example , subjects are dependent on their verbs and adjectives are dependent Figure 1 : A dependency tree for the sentence Troops advanced near Tikrit . 
	</s>
	

	<s id="75">
		 Feature Example word troops , Tikrit part-of-speech ( 24 values ) NN , NNP general-pos ( 5 values ) noun , verb , adj chunk-tag NP , VP , ADJP entity-type person , geo-political-entity entity-level name , nominal , pronoun Wordnet hypernyms social group , city relation-argument ARG A , ARG B Table 3 : List of features assigned to each node in the dependency tree . 
	</s>
	

	<s id="76">
		 on the nouns they modify . 
	</s>
	

	<s id="77">
		 Note that for the purposes of this paper , we do not consider the link labels ( e.g. “object” , “subject” ) ; instead we use only the dependency structure . 
	</s>
	

	<s id="78">
		 To generate the parse tree of each sentence , we use MXPOST , a maximum entropy statistical parser1 ; we then convert this parse tree to a dependency tree . 
	</s>
	

	<s id="79">
		 Note that the left-to-right ordering of the sentence is maintained in the dependency tree only among siblings ( i.e. the dependency tree does not specify an order to traverse the tree to recover the original sentence ) . 
	</s>
	

	<s id="80">
		 For each pair of entities in a sentence , we find the smallest common subtree in the dependency tree that includes both entities . 
	</s>
	

	<s id="81">
		 We choose to use this subtree instead of the entire tree to reduce noise and emphasize the local characteristics of relations . 
	</s>
	

	<s id="82">
		 We then augment each node of the tree with a feature vector ( Table 3 ) . 
	</s>
	

	<s id="83">
		 The relation-argument feature specifies whether an entity is the first or second argument in a relation . 
	</s>
	

	<s id="84">
		 This is required to learn asymmetric relations ( e.g. X OWNS Y ) . 
	</s>
	

	<s id="85">
		 Formally , a relation instance is a dependency tree 1http://www.cis.upenn.edu/˜adwait/statnlp.html Troops t 1 t2 advanced t0 near Tikrit t3 T with nodes It0 ... tn } . 
	</s>
	

	<s id="86">
		 The features of node ti are given by 0(ti) = Iv1 ... vd } . 
	</s>
	

	<s id="87">
		 We refer to the jth child of node ti as ti[j] , and we denote the set of all children of node ti as ti[c] . 
	</s>
	

	<s id="88">
		 We reference a subset j of children of ti by ti [ j ] C_ ti [ c ] . 
	</s>
	

	<s id="89">
		 Finally , we refer to the parent of node ti as ti.p . 
	</s>
	

	<s id="90">
		 From the example in Figure 1 , t0 [ 1 ] = t2 , t0[I0,1}]= It1 , t2 } , and t1.p = t0 . 
	</s>
	

	<s id="91">
		 5 Tree kernels for dependency trees We now define a kernel function for dependency trees . 
	</s>
	

	<s id="92">
		 The tree kernel is a function K(T1 , T2 ) that returns a normalized , symmetric similarity score in the range ( 0 , 1 ) for two trees T1 and T2 . 
	</s>
	

	<s id="93">
		 We define a slightly more general version of the kernel described by 
		<ref citStr="Zelenko et al . ( 2003 )" id="22" label="CERF" position="14535">
			Zelenko et al . ( 2003 )
		</ref>
		 . 
	</s>
	

	<s id="94">
		 We first define two functions over the features of tree nodes : a matching function m(ti , tj ) E I0 , 1 } and a similarity function s(ti , tj ) E ( 0 , oc ] . 
	</s>
	

	<s id="95">
		 Let the feature vector 0(ti) = Iv1 ... vd } consist of two possibly overlapping subsets 0m(ti) C_ 0(ti) and 03(ti) C_ 0(ti) . 
	</s>
	

	<s id="96">
		 We use 0m(ti) in the matching function and 03(ti) in the similarity function . 
	</s>
	

	<s id="97">
		 We define ~ m(ti , tj 1 if 0m ( ti ) = 0m(tj) ) = 0 otherwise and s(ti , tj ) = X X C(vq , vr ) vy ^Os ( ti ) vr^Os(tj) where C(vq , vr ) is some compatibility function between two feature values . 
	</s>
	

	<s id="98">
		 For example , in the simplest case where ~ C(vq , vr ) = 1 if vq = vr 0 otherwise s(ti , tj ) returns the number of feature values in common between feature vectors 03 ( ti ) and 03 ( tj ) . 
	</s>
	

	<s id="99">
		 We can think of the distinction between functions m(ti , tj ) and s(ti , tj ) as a way to discretize the similarity between two nodes . 
	</s>
	

	<s id="100">
		 If 0m(ti) =~ 0m(tj) , then we declare the two nodes completely dissimilar . 
	</s>
	

	<s id="101">
		 However , if 0m(ti) = 0m(tj) , then we proceed to compute the similarity s(ti , tj ) . 
	</s>
	

	<s id="102">
		 Thus , restricting nodes by m(ti , tj ) is a way to prune the search space of matching subtrees , as shown below . 
	</s>
	

	<s id="103">
		 For two dependency trees T1 , T2 , with root nodes r1 and r2 , we define the tree kernel K(T1 , T2 ) as 0 if m(r1 , r2 ) = 0 s(r1 , r2)+ Kc(r1 [ c ] , r2 [ c ] ) otherwise where Kc is a kernel function over children . 
	</s>
	

	<s id="104">
		 Let a and b be sequences of indices such that a is a sequence a1 &lt; a2 &lt; ... &lt; an , and likewise for b . 
	</s>
	

	<s id="105">
		 Let d(a) = an ^ a1 + 1 and l(a) be the length of a . 
	</s>
	

	<s id="106">
		 Then we have Kc ( ti [ c ] , tj [ c ] ) = X Ad(a)Ad(b)K ( ti [ a ] , tj [ b ] ) a,b,l(a)=l(b) The constant 0 &lt; A &lt; 1 is a decay factor that penalizes matching subsequences that are spread out within the child sequences . 
	</s>
	

	<s id="107">
		 See 
		<ref citStr="Zelenko et al . ( 2003 )" id="23" label="CEPF" position="16471">
			Zelenko et al . ( 2003 )
		</ref>
		 for a proof that K is kernel function . 
	</s>
	

	<s id="108">
		 Intuitively , whenever we find a pair of matching nodes , we search for all matching subsequences of the children of each node . 
	</s>
	

	<s id="109">
		 A matching subsequence of children is a sequence of children a and b such that m(ai , bi ) = 1 ( bi &lt; n ) . 
	</s>
	

	<s id="110">
		 For each matching pair of nodes ( ai , bi ) in a matching subsequence , we accumulate the result of the similarity function s(ai , bj ) and then recursively search for matching subsequences of their children ai [ c ] , bj [ c ] . 
	</s>
	

	<s id="111">
		 We implement two types of tree kernels . 
	</s>
	

	<s id="112">
		 A contiguous kernel only matches children subsequences that are uninterrupted by non-matching nodes . 
	</s>
	

	<s id="113">
		 Therefore , d(a) = l(a) . 
	</s>
	

	<s id="114">
		 A sparse tree kernel , by contrast , allows non-matching nodes within matching subsequences . 
	</s>
	

	<s id="115">
		 Figure 2 shows two relation instances , where each node contains the original text plus the features used for the matching function , 0m ( ti ) = Igeneralpos , entity-type , relation-argument } . 
	</s>
	

	<s id="116">
		 ( “NA” denotes the feature is not present for this node . 
	</s>
	

	<s id="117">
		 ) The contiguous kernel matches the following substructures : It0 [ 0 ] , u0 [ 0 ] } , It0 [ 2 ] , u0 [ 1 ] } , It3 [ 0 ] , u2 [ 0 ] } . 
	</s>
	

	<s id="118">
		 Because the sparse kernel allows non-contiguous matching sequences , it matches an additional substructure It0 [ 0 , * , 2 ] , u0 [ 0 , *,1 ] } , where ( * ) indicates an arbitrary number of non-matching nodes . 
	</s>
	

	<s id="119">
		 
		<ref citStr="Zelenko et al . ( 2003 )" id="24" label="CEPF" position="17982">
			Zelenko et al . ( 2003 )
		</ref>
		 have shown the contiguous kernel to be computable in O(mn) and the sparse kernel in O(mn3) , where m and n are the number of children in trees T1 and T2 respectively . 
	</s>
	

	<s id="120">
		 6 Experiments We extract relations from the Automatic Content Extraction ( ACE ) corpus provided by the National Institute for Standards and Technology ( NIST ) . 
	</s>
	

	<s id="121">
		 The follows : K(T1,T2) = ^ ^^ ^^ Figure 2 : Two instances of the NEAR relation . 
	</s>
	

	<s id="122">
		 data consists of about 800 annotated text documents gathered from various newspapers and broadcasts . 
	</s>
	

	<s id="123">
		 Five entities have been annotated ( PERSON , ORGANIZATION , GEO-POLITICAL ENTITY , LOCATION , FACILITY ) , along with 24 types of relations ( Table 2 ) . 
	</s>
	

	<s id="124">
		 As noted from the distribution of relationship types in the training data ( Figure 3 ) , data imbalance and sparsity are potential problems . 
	</s>
	

	<s id="125">
		 In addition to the contiguous and sparse tree kernels , we also implement a bag-of-words kernel , which treats the tree as a vector of features over nodes , disregarding any structural information . 
	</s>
	

	<s id="126">
		 We also create composite kernels by combining the sparse and contiguous kernels with the bagof-words kernel . 
	</s>
	

	<s id="127">
		 
		<ref citStr="Joachims et al . ( 2001 )" id="25" label="CEPF" position="19199">
			Joachims et al . ( 2001 )
		</ref>
		 have shown that given two kernels K1 , K2 , the composite kernel K12 ( Xi , Xj ) = K1 ( Xi , Xj ) + K2 ( Xi , Xj ) is also a kernel . 
	</s>
	

	<s id="128">
		 We find that this composite kernel improves performance when the Gram matrix G is sparse ( i.e. our instances are far apart in the kernel space ) . 
	</s>
	

	<s id="129">
		 The features used to represent each node are shown in Table 3 . 
	</s>
	

	<s id="130">
		 After initial experimentation , the set of features we use in the matching func- tion is ^m(ti) = { general-pos , entity-type , relation- argument } , and the similarity function examines the Figure 3 : Distribution over relation types in training data . 
	</s>
	

	<s id="131">
		 remaining features . 
	</s>
	

	<s id="132">
		 In our experiments we tested the following five kernels : K0 = K1 = K2 = K3 = K4 = We also experimented with the function C(vQ , vr ) , the compatibility function between two feature values . 
	</s>
	

	<s id="133">
		 For example , we can increase the importance of two nodes having the same Wordnet hypernym2 . 
	</s>
	

	<s id="134">
		 If vQ , vr are hypernym features , then we can define ~ a if vQ = vr C(vQ , vr ) = 0 otherwise When a &gt; 1 , we increase the similarity of nodes that share a hypernym . 
	</s>
	

	<s id="135">
		 We tested a number of weighting schemes , but did not obtain a set of weights that produced consistent significant improvements . 
	</s>
	

	<s id="136">
		 See Section 8 for alternate approaches to setting C. 2http://www.cogsci.princeton.edu/˜wn/ ARG _A person forces noun ARG A person troops noun t 1 u 1 NA NA moved verb quickly adverb NA NA NA NA advanced verb t t2 t3 0 u 0 Baghdad noun geopolitical ARG B geopolitical ARG B toward prep NA NA Tikrit noun near prep NA NA t 4 u 2 u 3 sparse kernel contiguous kernel bag-of-words kernel K0 + K2 K1 + K2 Avg . 
	</s>
	

	<s id="137">
		 Prec . 
	</s>
	

	<s id="138">
		 Avg . 
	</s>
	

	<s id="139">
		 Rec . 
	</s>
	

	<s id="140">
		 Avg . 
	</s>
	

	<s id="141">
		 F 1 K1 69.6 25.3 36.8 K2 47.0 10.0 14.2 K3 68.9 24.3 35.5 K4 70.3 26.3 38.0 Table 4 : Kernel performance comparison . 
	</s>
	

	<s id="142">
		 Table 4 shows the results of each kernel within an SVM . 
	</s>
	

	<s id="143">
		 ( We augment the LibSVM3 implementation to include our dependency tree kernel . 
	</s>
	

	<s id="144">
		 ) Note that , although training was done over all 24 relation subtypes , we evaluate only over the 5 high-level relation types . 
	</s>
	

	<s id="145">
		 Thus , classifying a RESIDENCE relation as a LOCATED relation is deemed correct4 . 
	</s>
	

	<s id="146">
		 Note also that K0 is not included in Table 4 because of burdensome computational time . 
	</s>
	

	<s id="147">
		 Table 4 shows that precision is adequate , but recall is low . 
	</s>
	

	<s id="148">
		 This is a result of the aforementioned class imbalance – very few of the training examples are relations , so the classifier is less likely to identify a testing instances as a relation . 
	</s>
	

	<s id="149">
		 Because we treat every pair of mentions in a sentence as a possible relation , our training set contains fewer than 15 % positive relation instances . 
	</s>
	

	<s id="150">
		 To remedy this , we retrain each SVMs for a binary classification task . 
	</s>
	

	<s id="151">
		 Here , we detect , but do not classify , relations . 
	</s>
	

	<s id="152">
		 This allows us to combine all positive relation instances into one class , which provides us more training samples to estimate the class boundary . 
	</s>
	

	<s id="153">
		 We then threshold our output to achieve an optimal operating point . 
	</s>
	

	<s id="154">
		 As seen in Table 5 , this method of relation detection outperforms that of the multi-class classifier . 
	</s>
	

	<s id="155">
		 We then use these binary classifiers in a cascading scheme as follows : First , we use the binary SVM to detect possible relations . 
	</s>
	

	<s id="156">
		 Then , we use the SVM trained only on positive relation instances to classify each predicted relation . 
	</s>
	

	<s id="157">
		 These results are shown in Table 6 . 
	</s>
	

	<s id="158">
		 The first result of interest is that the sparse tree kernel , K0 , does not perform as well as the contiguous tree kernel , K1 . 
	</s>
	

	<s id="159">
		 Suspecting that noise was introduced by the non-matching nodes allowed in the sparse tree kernel , we performed the experiment with different values for the decay factor A = { .9,.5 ,. 1 } , but obtained no improvement . 
	</s>
	

	<s id="160">
		 The second result of interest is that all tree kernels outperform the bag-of-words kernel , K2 , most noticeably in recall performance , implying that the 3http://www.csie.ntu.edu.tw/˜cj lin/libsvm/ 4This is to compensate for the small amount of training data for many classes . 
	</s>
	

	<s id="161">
		 Prec . 
	</s>
	

	<s id="162">
		 Rec . 
	</s>
	

	<s id="163">
		 F 1 K0 – – – K0 ( B ) 83.4 45.5 58.8 K1 91.4 37.1 52.8 K1 ( B ) 84.7 49.3 62.3 K2 92.7 10.6 19.0 K2 ( B ) 72.5 40.2 51.7 K3 91.3 35.1 50.8 K3 ( B ) 80.1 49.9 61.5 K4 91.8 37.5 53.3 K4 ( B ) 81.2 51.8 63.2 Table 5 : Relation detection performance . 
	</s>
	

	<s id="164">
		 ( B ) denotes binary classification . 
	</s>
	

	<s id="165">
		 D C Avg . 
	</s>
	

	<s id="166">
		 Prec . 
	</s>
	

	<s id="167">
		 Avg . 
	</s>
	

	<s id="168">
		 Rec . 
	</s>
	

	<s id="169">
		 Avg . 
	</s>
	

	<s id="170">
		 F1 K0 K0 66.0 29.0 40.1 K1 K1 66.6 32.4 43.5 K2 K2 62.5 27.7 38.1 K3 K3 67.5 34.3 45.3 K4 K4 67.1 35.0 45.8 K1 K4 67.4 33.9 45.0 K4 K1 65.3 32.5 43.3 Table 6 : Results on the cascading classification . 
	</s>
	

	<s id="171">
		 D and C denote the kernel used for relation detection and classification , respectively . 
	</s>
	

	<s id="172">
		 structural information the tree kernel provides is extremely useful for relation detection . 
	</s>
	

	<s id="173">
		 Note that the average results reported here are representative of the performance per relation , except for the NEAR relation , which had slightly lower results overall due to its infrequency in training . 
	</s>
	

	<s id="174">
		 7 Conclusions We have shown that using a dependency tree kernel for relation extraction provides a vast improvement over a bag-of-words kernel . 
	</s>
	

	<s id="175">
		 While the dependency tree kernel appears to perform well at the task of classifying relations , recall is still relatively low . 
	</s>
	

	<s id="176">
		 Detecting relations is a difficult task for a kernel method because the set of all non-relation instances is extremely heterogeneous , and is therefore difficult to characterize with a similarity metric . 
	</s>
	

	<s id="177">
		 An improved system might use a different method to detect candidate relations and then use this kernel method to classify the relations . 
	</s>
	

	<s id="178">
		 8 Future Work The most immediate extension is to automatically learn the feature compatibility function C(vq , vr ) . 
	</s>
	

	<s id="179">
		 A first approach might use tf-idf to weight each feature . 
	</s>
	

	<s id="180">
		 Another approach might be to calculate the information gain for each feature and use that as its weight . 
	</s>
	

	<s id="181">
		 A more complex system might learn a weight for each pair of features ; however this seems computationally infeasible for large numbers of features . 
	</s>
	

	<s id="182">
		 One could also perform latent semantic indexing to collapse feature values into similar “categories” — for example , the words “football” and “baseball” might fall into the same category . 
	</s>
	

	<s id="183">
		 Here , C(vQ , vr ) might return a1 if vQ = vr , and a2 if vQ and vr are in the same category , where a1 &gt; a2 &gt; 0 . 
	</s>
	

	<s id="184">
		 Any method that provides a “soft” match between feature values will sharpen the granularity of the kernel and enhance its modeling power . 
	</s>
	

	<s id="185">
		 Further investigation is also needed to understand why the sparse kernel performs worse than the contiguous kernel . 
	</s>
	

	<s id="186">
		 These results contradict those given in 
		<ref citStr="Zelenko et al . ( 2003 )" id="26" label="CJPF" position="26296">
			Zelenko et al . ( 2003 )
		</ref>
		 , where the sparse kernel achieves 2-3 % better F1 performance than the contiguous kernel . 
	</s>
	

	<s id="187">
		 It is worthwhile to characterize relation types that are better captured by the sparse kernel , and to determine when using the sparse kernel is worth the increased computational burden . 
	</s>
	

	<s id="188">
		 References Eugene Agichtein and Luis Gravano . 
	</s>
	

	<s id="189">
		 2000. Snowball : Extracting relations from large plain-text collections . 
	</s>
	

	<s id="190">
		 In Proceedings of the Fifth ACMInternational Conference on Digital Libraries . 
	</s>
	

	<s id="191">
		 Sergey Brin . 
	</s>
	

	<s id="192">
		 1998. Extracting patterns and relations from the world wide web. . 
	</s>
	

	<s id="193">
		 In WebDB Workshop at 6th International Conference on Extending Database Technology , EDBT’98 . 
	</s>
	

	<s id="194">
		 M. Collins and N. Duffy . 
	</s>
	

	<s id="195">
		 2002. Convolution kernels for natural language . 
	</s>
	

	<s id="196">
		 In T. G. Dietterich , S. Becker , and Z. Ghahramani , editors , Advances in Neural Information Processing Systems 14 , Cambridge , MA . 
	</s>
	

	<s id="197">
		 MIT Press . 
	</s>
	

	<s id="198">
		 Corinna Cortes and Vladimir Vapnik . 
	</s>
	

	<s id="199">
		 1995. Support-vector networks . 
	</s>
	

	<s id="200">
		 Machine Learning , 20(3):273–297 . 
	</s>
	

	<s id="201">
		 N. Cristianini and J. Shawe-Taylor . 
	</s>
	

	<s id="202">
		 2000. An introduction to support vector machines . 
	</s>
	

	<s id="203">
		 Cambridge University Press . 
	</s>
	

	<s id="204">
		 Chad M. Cumby and Dan Roth . 
	</s>
	

	<s id="205">
		 2003. On kernel methods for relational learning . 
	</s>
	

	<s id="206">
		 In Tom Fawcett and Nina Mishra , editors , Machine Learning , Proceedings of the Twentieth International Conference ( ICML 2003 ) , August 21-24 , 2003 , Washington , DC , USA . 
	</s>
	

	<s id="207">
		 AAAI Press . 
	</s>
	

	<s id="208">
		 K. Fukunaga . 
	</s>
	

	<s id="209">
		 1990. Introduction to Statistical Pat- tern Recognition . 
	</s>
	

	<s id="210">
		 Academic Press , second edition . 
	</s>
	

	<s id="211">
		 D. Haussler . 
	</s>
	

	<s id="212">
		 1999. Convolution kernels on discrete structures . 
	</s>
	

	<s id="213">
		 Technical Report UCS-CRL-99- 10 , University of California , Santa Cruz . 
	</s>
	

	<s id="214">
		 Thorsten Joachims , Nello Cristianini , and John Shawe-Taylor . 
	</s>
	

	<s id="215">
		 2001. Composite kernels for hypertext categorisation . 
	</s>
	

	<s id="216">
		 In Carla Brodley and Andrea Danyluk , editors , Proceedings of ICML01 , 18th International Conference on Machine Learning , pages 250–257 , Williams College , US . 
	</s>
	

	<s id="217">
		 Morgan Kaufmann Publishers , San Francisco , US . 
	</s>
	

	<s id="218">
		 Huma Lodhi , John Shawe-Taylor , Nello Cristianini , and Christopher J. C. H. Watkins . 
	</s>
	

	<s id="219">
		 2000. Text classification using string kernels . 
	</s>
	

	<s id="220">
		 In NIPS , pages 563–569 . 
	</s>
	

	<s id="221">
		 A. McCallum and B. Wellner . 
	</s>
	

	<s id="222">
		 2003. Toward conditional models of identity uncertainty with application to proper noun coreference . 
	</s>
	

	<s id="223">
		 In IJCAI Workshop on Information Integration on the Web. S. Miller , H. Fox , L. Ramshaw , and R. Weischedel . 
	</s>
	

	<s id="224">
		 2000. A novel use of statistical parsing to extract information from text . 
	</s>
	

	<s id="225">
		 In 6th Applied Natural Language Processing Conference . 
	</s>
	

	<s id="226">
		 H. Pasula , B. Marthi , B. Milch , S. Russell , and I. Shpitser . 
	</s>
	

	<s id="227">
		 2002. Identity uncertainty and citation matching . 
	</s>
	

	<s id="228">
		 Dan Roth and Wen-tau Yih . 
	</s>
	

	<s id="229">
		 2002. Probabilistic reasoning for entity and relation recognition . 
	</s>
	

	<s id="230">
		 In 19th International Conference on Computational Linguistics . 
	</s>
	

	<s id="231">
		 Sam Scott and Stan Matwin . 
	</s>
	

	<s id="232">
		 1999. Feature engineering for text classification . 
	</s>
	

	<s id="233">
		 In Proceedings of ICML-99 , 16th International Conference on Machine Learning . 
	</s>
	

	<s id="234">
		 Erik F. Tjong Kim Sang and Fien De Meulder . 
	</s>
	

	<s id="235">
		 2003. Introduction to the CoNLL-2003 shared task : Language-independent named entity recognition . 
	</s>
	

	<s id="236">
		 In Walter Daelemans and Miles Osborne , editors , Proceedings of CoNLL-2003 , pages 142– 147 . 
	</s>
	

	<s id="237">
		 Edmonton , Canada . 
	</s>
	

	<s id="238">
		 Vladimir Vapnik . 
	</s>
	

	<s id="239">
		 1998. Statistical Learning Theory . 
	</s>
	

	<s id="240">
		 Whiley , Chichester , GB . 
	</s>
	

	<s id="241">
		 D. Zelenko , C. Aone , and A. Richardella. 2003 . 
	</s>
	

	<s id="242">
		 Kernel methods for relation extraction . 
	</s>
	

	<s id="243">
		 Journal ofMachine Learning Research , pages 1083– 1106. 
	</s>
	


</acldoc>
