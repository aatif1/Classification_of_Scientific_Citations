<?xml version="1.0" encoding="iso-8859-1"?>
<acldoc acl_id="P04-3034">
	

	<s id="1">
		 Fragments and Text Categorization Jan Bla^tik and Eva Mrikovi and Lubo^s Popelinsk´y Knowledge Discovery Lab Faculty of Informatics , Masaryk University 602 00 Brno , Czech Republic xblatak , glum , popel @fi.muni.cz Abstract We introduce two novel methods of text categorization in which documents are split into fragments . 
	</s>
	

	<s id="2">
		 We conducted experiments on English , French and Czech . 
	</s>
	

	<s id="3">
		 In all cases , the problems referred to a binary document classification . 
	</s>
	

	<s id="4">
		 We find that both methods increase the accuracy of text categorization . 
	</s>
	

	<s id="5">
		 For the Naive Bayes classifier this increase is significant . 
	</s>
	

	<s id="6">
		 1 Motivation In the process of automatic classifying documents into several predefined classes – text categorization 
		<ref citStr="Sebastiani , 2002" id="1" label="CEPF" position="783">
			( Sebastiani , 2002 )
		</ref>
		 – text documents are usually seen as sets or bags of all the words that have appeared in a document , maybe after removing words in a stop-list . 
	</s>
	

	<s id="7">
		 In this paper we describe a novel approach to text categorization in which each documents is first split into subparts , called fragments . 
	</s>
	

	<s id="8">
		 Each fragment is consequently seen as a new document which shares the same label with its source document . 
	</s>
	

	<s id="9">
		 We introduce two variants of this approach – skip - tail and fragments . 
	</s>
	

	<s id="10">
		 Both of these methods are briefly described below . 
	</s>
	

	<s id="11">
		 We demonstrate the increased accuracy that we observed . 
	</s>
	

	<s id="12">
		 1.1 Skipping the tail of a document The first method uses only the first sentences of a document and is henceforth referred to as skip - tail . 
	</s>
	

	<s id="13">
		 The idea behind this approach is that the beginning of each document contains enough information for the classification . 
	</s>
	

	<s id="14">
		 In the process of learning , each document is first replaced by its initial part . 
	</s>
	

	<s id="15">
		 The learning algorithm then uses only these initial fragments as learning ( test ) examples . 
	</s>
	

	<s id="16">
		 We also sought the minimum length of initial fragments that preserve the accuracy of the classification . 
	</s>
	

	<s id="17">
		 1.2 Splitting a document into fragments The second method splits the documents into fragments which are classified independently of each others . 
	</s>
	

	<s id="18">
		 This method is henceforth referred to as fragments . 
	</s>
	

	<s id="19">
		 Initially , the classifier is used to generate a model from these fragments . 
	</s>
	

	<s id="20">
		 Subsequently , the model is utilized to classify unseen documents ( test set ) which have also been split into fragments . 
	</s>
	

	<s id="21">
		 2 Data We conducted experiments using English , French and Czech documents . 
	</s>
	

	<s id="22">
		 In all cases , the problems referred to a binary document classification . 
	</s>
	

	<s id="23">
		 The main characteristics of the data are in Table 1 . 
	</s>
	

	<s id="24">
		 Three kinds of English documents were used : 20 Newsgroups1 ( 202 randomly chosen documents from each class were used . 
	</s>
	

	<s id="25">
		 The mail header was removed so that the text contained only the body of the message and in some cases , replies ) Reuters-21578 , Distribution 1.02 ( only documents from money- fx , money- supply , trade classified into a single class were chosen ) . 
	</s>
	

	<s id="26">
		 All documents marked as BRIEF and UNPROC were removed . 
	</s>
	

	<s id="27">
		 The classification tasks involved money- f x+money -supply vs. trade , money- fx vs. money-supply , money- fx vs. trade and money- supply vs. trade . 
	</s>
	

	<s id="28">
		 MEDLINE data3 ( 235 abstracts of medical papers that concerned gynecology and assisted reproduction ) n docs ave sdev 20 Newsgroups 138 4040 15.79 5.99 Reuters-21578 4 1022 11.03 2.02 Medline 1 235 12.54 0.22 French cooking 36 1370 9.41 1.24 Czech newspaper 15 2545 22.04 4.22 Table 1 : Data ( n=number of classification tasks , docs=number of documents , ave =average number of sentences per document , sdev =standard deviation ) 1http://www.ai.mit.edu/-jrennie/ 20Newsgroups/ 2http://www.research.att.com/-lewis 3http://www.fi.muni.cz/-zizka/medocs The French documents contained French recipes . 
	</s>
	

	<s id="29">
		 Examples of the classification tasks are Accompagnements vs. Cremes , Cremes vs. Pates-PainsCrepes , Desserts vs. Douceurs , Entrees vs. PlatsChauds and Pates-Pains-Crepes vs. . 
	</s>
	

	<s id="30">
		 Sauces , among others . 
	</s>
	

	<s id="31">
		 We also used both methods for classifying Czech documents . 
	</s>
	

	<s id="32">
		 The data involved fifteen classification tasks . 
	</s>
	

	<s id="33">
		 The articles used had been taken from Czech newspapers . 
	</s>
	

	<s id="34">
		 Six tasks concerned authorship recognition , the other seven to find a document source – either a newspaper or a particular page ( or column ) . 
	</s>
	

	<s id="35">
		 Topic recognition was the goal of two tasks . 
	</s>
	

	<s id="36">
		 The structure of the rest of this paper is as follows . 
	</s>
	

	<s id="37">
		 The method for computing the classification of the whole document from classifying fragments ( fragments method ) is described in Section 3 . 
	</s>
	

	<s id="38">
		 Experimental settings are introduced in Section 4 . 
	</s>
	

	<s id="39">
		 Section 5 presents the main results . 
	</s>
	

	<s id="40">
		 We conclude with an overview of related works and with directions for potential future research in Sections 6 and 7 . 
	</s>
	

	<s id="41">
		 3 Classification by means of fragments of documents The class of the whole document is determined as follows . 
	</s>
	

	<s id="42">
		 Let us take a document which consists of fragments , ... , such that and . 
	</s>
	

	<s id="43">
		 The value of depends on the length of the document and on the number of sentences in the fragments . 
	</s>
	

	<s id="44">
		 Let , and denotes the set of possible classes . 
	</s>
	

	<s id="45">
		 We than use the learned model to assign a class to each of the fragments . 
	</s>
	

	<s id="46">
		 Let be the confidence of the classification fragment into the class . 
	</s>
	

	<s id="47">
		 This confidence measure is computed as an estimated probability of the predicted class . 
	</s>
	

	<s id="48">
		 Then for each fragment classified to the class we define . 
	</s>
	

	<s id="49">
		 The confidence of the classification of the whole document into is computed as follows Finally , the class which is assigned to a docu- ment is computed according to the following definition : In other words , a document is classified to a , which was assigned to the most fragments from ( the most frequent class ) . 
	</s>
	

	<s id="50">
		 If there are two classes with the same cardinality , the confidence measure is employed . 
	</s>
	

	<s id="51">
		 We also tested another method that exploited the confidence of classification but the results were not satisfactory . 
	</s>
	

	<s id="52">
		 4 Experiments For feature ( i.e. significant word ) selection , we tested four methods 
		<ref citStr="Forman , 2002" id="2" label="CEPF" position="6314">
			( Forman , 2002 
		</ref>
		<ref citStr="Yang and Liu , 1999" id="3" label="CEPF" position="6330">
			; Yang and Liu , 1999 )
		</ref>
		 – Chi-Squared ( chi ) , Information Gain ( ig ) , F -measure ( f1 ) and Probability Ratio ( pr ) . 
	</s>
	

	<s id="53">
		 Eventually , we chose ig because it yielded the best results . 
	</s>
	

	<s id="54">
		 We utilized three learning algorithms from the Weka4 system – the decision tree learner J48 , the Naive Bayes , the SVM Sequential Minimal Optimization ( SMO ) . 
	</s>
	

	<s id="55">
		 All the algorithms were used with default settings . 
	</s>
	

	<s id="56">
		 The entire documents have been split to fragments containing 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 20 , 25 , 30 , and 40 sentences . 
	</s>
	

	<s id="57">
		 For the skip - tail classification which uses only the beginnings of documents we also employed these values . 
	</s>
	

	<s id="58">
		 As an evaluation criterion we used the accuracy defined as the percentage of correctly classified documents from the test set . 
	</s>
	

	<s id="59">
		 All the results have been obtained by a 10-fold cross validation . 
	</s>
	

	<s id="60">
		 5 Results 5.1 General We observed that for both skip-tail and fragments there is always a consistent size of fragments for which the accuracy increased . 
	</s>
	

	<s id="61">
		 It is the most important result . 
	</s>
	

	<s id="62">
		 More details can be found in the next two paragraphs . 
	</s>
	

	<s id="63">
		 Among the learning algorithms , the highest accuracy was achieved for all the three languages with the Naive Bayes . 
	</s>
	

	<s id="64">
		 It is surprising because for full versions of documents it was the SMO algorithm that was even slightly better than the Naive Bayes in terms of accuracy . 
	</s>
	

	<s id="65">
		 On the other hand , the highest impact was observed for J48 . 
	</s>
	

	<s id="66">
		 Thus , for instance for Czech , it was observed for fragments that the accuracy was higher for 14 out of 15 tasks when J48 had been used , and for 12 out of 15 in the case of the Naive Bayes and the Support Vector Machines . 
	</s>
	

	<s id="67">
		 However , the performance of J48 was far inferior to that of the other algorithms . 
	</s>
	

	<s id="68">
		 In only three tasks J48 for and . 
	</s>
	

	<s id="69">
		 4http://www.cs.waikato.ac.nz/ml/weka resulted in a higher accuracy than the Naive Bayes and the Support Vector Machines . 
	</s>
	

	<s id="70">
		 The similar situation appeared for English and French . 
	</s>
	

	<s id="71">
		 5.2 skip-tail skip-tail method was successful for all the three languages ( see Table 2 ) . 
	</s>
	

	<s id="72">
		 It results in increased accuracy even for a very small initial fragment . 
	</s>
	

	<s id="73">
		 In Figure 1 there are results for skip- tail and initial fragments of the length from 40 % up to 100 % of the average length of documents in the learning set . 
	</s>
	

	<s id="74">
		 92.5 92 91.5 91 skip-tail(fr) full(fr) skip-tail(eng) full(eng) 40 50 60 70 80 90 100 lentgh of the fragment n NB stail lngth incr English 143 90.96 92.04 1.3 ++105 French 36 92.04 92.56 0.9 +25 Czech 15 79.51 81.13 0.9 +12 Table 2 : Results for skip-tail and the Naive Bayes ( n=number of classification tasks , NB=average of error rates for full documents , stail=average of error rates for skip-tail , lngth=optimal length of the fragment , incr=number of tasks with the increase of accuracy : + , ++ means significant on level 95 % resp 99 % , the sign test . 
	</s>
	

	<s id="75">
		 ) For example , for English , taking only the first 40 % of sentences in a document results in a slightly increased accuracy . 
	</s>
	

	<s id="76">
		 Figure 2 displays the relative increase of accuracy for fragments of the length up to 40 sentences for different learning algorithms for English . 
	</s>
	

	<s id="77">
		 It is important to stress that even for the initial fragment of the length of 5 sentences , the accuracy is the same as for full documents . 
	</s>
	

	<s id="78">
		 When the initial fragment is longer the classification accuracy further increase until the length of 12 sentences . 
	</s>
	

	<s id="79">
		 We observed similar behaviour for skip-tail when employed on other languages , and also for the fragments method . 
	</s>
	

	<s id="80">
		 5.3 fragments This method was successful for classifying English and Czech documents ( significant on level 99 % for English and 95 % for Czech ) . 
	</s>
	

	<s id="81">
		 In the case of French cooking recipes , a small , but not significant impact has been observed , too . 
	</s>
	

	<s id="82">
		 This may have been caused by the special format of recipes . 
	</s>
	

	<s id="83">
		 n NB frag lngth incr English 143 91.12 93.21 1.1 ++96 French 36 92.04 92.27 1.0 19 Czech 15 82.36 84.07 1.0 +12 Table 3 : Results for fragments ( for the description see Table 2 ) Figure 1 : skip-tail , Naive Bayes . 
	</s>
	

	<s id="84">
		 ( lentgh of the fragment = percentage of the average document length ) 5 0 -5 -10 -15 -20 -25 -30 -35 NaiveBayes-bm SMO-bm J48-bm 0 5 10 15 20 25 30 35 40 no . 
	</s>
	

	<s id="85">
		 of senteces Figure 2 : Relative increase of accuracy : English , skip-tail 5.4 Optimal length of fragments We also looked for the optimal length of fragments . 
	</s>
	

	<s id="86">
		 We found that for the lengths of fragments for the range about the average document length ( in the learning set ) , the accuracy increased for the significant number of the data sets ( the sign test 95 % ) . 
	</s>
	

	<s id="87">
		 It holds for skip-tail and for all languages . 
	</s>
	

	<s id="88">
		 and for English and Czech in the case of fragments . 
	</s>
	

	<s id="89">
		 However , an increase of accuracy is observed even for 60 % of the average length ( see Fig . 
	</s>
	

	<s id="90">
		 1 ) . 
	</s>
	

	<s id="91">
		 Moreover , for the average length this increase is significant for Czech at a level 95 % ( t-test ) . 
	</s>
	

	<s id="92">
		 6 Discussion and related work Two possible reasons may result in an accuracy increase for skip- tail . 
	</s>
	

	<s id="93">
		 As a rule , the beginning of a document contains the most relevant information . 
	</s>
	

	<s id="94">
		 The concluding part , on the other hand , often includes the author’s interpretation and cross- reference to other documents which can cause confusion . 
	</s>
	

	<s id="95">
		 However , these statements are yet to be verified . 
	</s>
	

	<s id="96">
		 Additional information , namely lexical or syntactic , may result in even higher accuracy of classification . 
	</s>
	

	<s id="97">
		 We performed several experiments for Czech . 
	</s>
	

	<s id="98">
		 We observed that adding noun , verb and prepositional phrases led to a small increase in the accuracy but that increase was not significant . 
	</s>
	

	<s id="99">
		 Other kinds of fragments should be checked , for instance intersecting fragments or sliding fragments . 
	</s>
	

	<s id="100">
		 So far we have ignored the structure of the documents ( titles , splitting into paragraphs ) and focused only on plain text . 
	</s>
	

	<s id="101">
		 In the next stage , we will apply these methods to classifying HTML and XML documents . 
	</s>
	

	<s id="102">
		 Larkey 
		<ref citStr="Larkey , 1999" id="4" label="CEPF" position="12673">
			( Larkey , 1999 )
		</ref>
		 employed a method similar to skip-tail for classifying patent documents . 
	</s>
	

	<s id="103">
		 He exploited the structure of documents – the title , the abstract , and the first twenty lines of the summary – assigning different weights to each part . 
	</s>
	

	<s id="104">
		 We showed that this approach can be used even for non-structured texts like newspaper articles . 
	</s>
	

	<s id="105">
		 Tombros et al . 
	</s>
	

	<s id="106">
		 
		<ref citStr="Tombros et al. , 2003" id="5" label="CEPF" position="13080">
			( Tombros et al. , 2003 )
		</ref>
		 combined text summarization when clustering so called top-ranking sentences ( TRS ) . 
	</s>
	

	<s id="107">
		 It will be interesting to check how fragments are related to the TRS . 
	</s>
	

	<s id="108">
		 7 Conclusion We have introduced two methods – skip - tail and fragments –utilized for document categorization which are based on splitting documents into its subparts . 
	</s>
	

	<s id="109">
		 We observed that both methods resulted in significant increase of accuracy . 
	</s>
	

	<s id="110">
		 We also tested a method which exploited only the most confident fragments . 
	</s>
	

	<s id="111">
		 However , this did not result in any accuracy increase . 
	</s>
	

	<s id="112">
		 However , use of the most confident fragments for text summarization should also be checked . 
	</s>
	

	<s id="113">
		 8 Acknowledgements We thank James Mayfield , James Thomas and Martin Dvo^rik for their assistance . 
	</s>
	

	<s id="114">
		 This work has been partially supported by the Czech Ministry of Education under the Grant No. 143300003 . 
	</s>
	

	<s id="115">
		 References G. Forman . 
	</s>
	

	<s id="116">
		 2002. Choose your words carefully . 
	</s>
	

	<s id="117">
		 In T. Elomaa , H. Mannila , and H. Toivonen , editors , Proceedings of the 6th Eur . 
	</s>
	

	<s id="118">
		 Conf . 
	</s>
	

	<s id="119">
		 on Principles Data Mining and Knowledge Discovery ( PKDD ) , Helsinki , 2002 , LNCS vol. 2431 , pages 150–162 . 
	</s>
	

	<s id="120">
		 Springer Verlag . 
	</s>
	

	<s id="121">
		 L. S. Larkey . 
	</s>
	

	<s id="122">
		 1999. A patent search and classification system . 
	</s>
	

	<s id="123">
		 In Proceedings of the fourth ACM conference on Digital libraries , pages 179–187 . 
	</s>
	

	<s id="124">
		 ACM Press . 
	</s>
	

	<s id="125">
		 F. Sebastiani . 
	</s>
	

	<s id="126">
		 2002. Machine learning in automated text categorization . 
	</s>
	

	<s id="127">
		 ACM Comput . 
	</s>
	

	<s id="128">
		 Surv. , 34(1):1–47 . 
	</s>
	

	<s id="129">
		 A. Tombros , J. M. Jose , and I. Ruthven . 
	</s>
	

	<s id="130">
		 2003. Clustering top-ranking sentences for information access . 
	</s>
	

	<s id="131">
		 In T. Koch and I. Sølvberg , editors , Proceedings of the 7 European Conference on Research and Advanced Technology for Digital Libraries ( ECDL ) , Trondheim 2003 , LNCS vl. 2769 , pages 523–528 . 
	</s>
	

	<s id="132">
		 Springer Verlag . 
	</s>
	

	<s id="133">
		 Y. Yang and X. Liu . 
	</s>
	

	<s id="134">
		 1999. A re-examination of text categorization methods . 
	</s>
	

	<s id="135">
		 In Proceedings of the 22 annual international ACM SIGIR conference on Research and development in information retrieval , pages 42–49 . 
	</s>
	

	<s id="136">
		 ACM Press . 
	</s>
	


</acldoc>
