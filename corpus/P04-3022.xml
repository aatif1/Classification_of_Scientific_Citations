<?xml version="1.0" encoding="iso-8859-1"?>
<acldoc acl_id="P04-3022">
	

	<s id="1">
		 Combining Lexical , Syntactic , and Semantic Features with Maximum Entropy Models for Extracting Relations Nanda Kambhatla IBM T. J. Watson Research Center 1101 Kitchawan Road Route 134 Yorktown Heights , NY 10598 nanda@us.ibm.com Abstract Extracting semantic relationships between entities is challenging because of a paucity of annotated data and the errors induced by entity detection modules . 
	</s>
	

	<s id="2">
		 We employ Maximum Entropy models to combine diverse lexical , syntactic and semantic features derived from the text . 
	</s>
	

	<s id="3">
		 Our system obtained competitive results in the Automatic Content Extraction ( ACE ) evaluation . 
	</s>
	

	<s id="4">
		 Here we present our general approach and describe our ACE results . 
	</s>
	

	<s id="5">
		 1 Introduction Extraction of semantic relationships between entities can be very useful for applications such as biography extraction and question answering , e.g. to answer queries such as “Where is the Taj Mahal?” . 
	</s>
	

	<s id="6">
		 Several prior approaches to relation extraction have focused on using syntactic parse trees . 
	</s>
	

	<s id="7">
		 For the Template Relations task of MUC-7 , BBN researchers 
		<ref citStr="Miller et al. , 2000" id="1" label="CEPF" position="1137">
			( Miller et al. , 2000 )
		</ref>
		 augmented syntactic parse trees with semantic information corresponding to entities and relations and built generative models for the augmented trees . 
	</s>
	

	<s id="8">
		 More recently , 
		<ref citStr="Zelenko et al. , 2003" id="2" label="CEPF" position="1340">
			( Zelenko et al. , 2003 )
		</ref>
		 have proposed extracting relations by computing kernel functions between parse trees and 
		<ref citStr="Culotta and Sorensen , 2004" id="3" label="CEPF" position="1461">
			( Culotta and Sorensen , 2004 )
		</ref>
		 have extended this work to estimate kernel functions between augmented dependency trees . 
	</s>
	

	<s id="9">
		 We build Maximum Entropy models for extracting relations that combine diverse lexical , syntactic and semantic features . 
	</s>
	

	<s id="10">
		 Our results indicate that using a variety of information sources can result in improved recall and overall F measure . 
	</s>
	

	<s id="11">
		 Our approach can easily scale to include more features from a multitude of sources–e.g . 
	</s>
	

	<s id="12">
		 WordNet , gazatteers , output of other semantic taggers etc.–that can be brought to bear on this task . 
	</s>
	

	<s id="13">
		 In this paper , we present our general approach , describe the features we currently use and show the results of our participation in the ACE evaluation . 
	</s>
	

	<s id="14">
		 Automatic Content Extraction ( ACE , 2004 ) is an evaluation conducted by NIST to measure Entity Detection and Tracking ( EDT ) and relation detection and characterization ( RDC ) . 
	</s>
	

	<s id="15">
		 The EDT task entails the detection of mentions of entities and chaining them together by identifying their coreference . 
	</s>
	

	<s id="16">
		 In ACE vocabulary , entities are objects , mentions are references to them , and relations are explicitly or implicitly stated relationships among entities . 
	</s>
	

	<s id="17">
		 Entities can be of five types : persons , organizations , locations , facilities , and geo-political entities ( geographically defined regions that define a political boundary , e.g. countries , cities , etc. ) . 
	</s>
	

	<s id="18">
		 Mentions have levels : they can be names , nominal expressions or pronouns . 
	</s>
	

	<s id="19">
		 The RDC task detects implicit and explicit rela- tions ' between entities identified by the EDT task . 
	</s>
	

	<s id="20">
		 Here is an example : The American Medical Association voted yesterday to install the heir apparent as its president-elect , rejecting a strong , upstart challenge by a District doctor who argued that the nation’s largest physicians’ group needs stronger ethics and new leadership . 
	</s>
	

	<s id="21">
		 In electing Thomas R. Reardon , an Oregon general practitioner who had been the chairman of its board , ... 
	</s>
	

	<s id="22">
		 In this fragment , all the underlined phrases are mentions referring to the American Medical Association , or to Thomas R. Reardon or the board ( an organization ) of the American Medical Association . 
	</s>
	

	<s id="23">
		 Moreover , there is an explicit management relation between chairman and board , which are references to Thomas R. Reardon and the board of the American Medical Association respectively . 
	</s>
	

	<s id="24">
		 Relation extraction is hard , since successful extraction implies correctly detecting both the argument mentions , correctly chaining these mentions to their re- ' Explict relations occur in text with explicit evidence sug- gesting the relationship . 
	</s>
	

	<s id="25">
		 Implicit relations need not have explicit supporting evidence in text , though they should be evident from a reading of the document . 
	</s>
	

	<s id="26">
		 Type Subtype Count AT based-In 496 located 2879 residence 395 NEAR relative-location 288 PART other 6 part-Of 1178 subsidiary 366 ROLE affiliate -partner 219 citizen-Of 450 client 159 founder 37 general-staff 1507 management 1559 member 1404 other 174 owner 274 SOCIAL associate 119 grandparent 10 other -personal 108 other -professional 415 other-relative 86 parent 149 sibling 23 spouse 89 Table 1 : The list of relation types and subtypes used in the ACE 2003 evaluation . 
	</s>
	

	<s id="27">
		 spective entities , and correctly determining the type of relation that holds between them . 
	</s>
	

	<s id="28">
		 This paper focuses on the relation extraction component of our ACE system . 
	</s>
	

	<s id="29">
		 The reader is referred to 
		<ref citStr="Florian et al. , 2004" id="4" label="CEPF" position="5025">
			( Florian et al. , 2004 
		</ref>
		<ref citStr="Ittycheriah et al. , 2003" id="5" label="CEPF" position="5049">
			; Ittycheriah et al. , 2003 
		</ref>
		<ref citStr="Luo et al. , 2004" id="6" label="CEPF" position="5077">
			; Luo et al. , 2004 )
		</ref>
		 for more details of our mention detection and mention chaining modules . 
	</s>
	

	<s id="30">
		 In the next section , we describe our extraction system . 
	</s>
	

	<s id="31">
		 We present results in section 3 , and we conclude after making some general observations in section 4 . 
	</s>
	

	<s id="32">
		 2 Maximum Entropy models for extracting relations We built Maximum Entropy models for predicting the type of relation ( if any ) between every pair of mentions within each sentence . 
	</s>
	

	<s id="33">
		 We only model explicit relations , because of poor inter-annotator agreement in the annotation of implicit relations . 
	</s>
	

	<s id="34">
		 Table 1 lists the types and subtypes of relations for the ACE RDC task , along with their frequency of occurence in the ACE training data2 . 
	</s>
	

	<s id="35">
		 Note that only 6 of these 24 relation types are symmetric : “relative-location” , “associate” , “other-relative” , “other-professional” , “sibling” , and “spouse” . 
	</s>
	

	<s id="36">
		 We only model the relation subtypes , after making them unique by concatenating the type where appropriate ( e.g. “OTHER” became “OTHER-PART” and “OTHER-ROLE” ) . 
	</s>
	

	<s id="37">
		 We explicitly model the argument order of mentions . 
	</s>
	

	<s id="38">
		 Thus , when comparing mentions and , we distinguish between the case where -citizen-Of- and -citizen-Of- . 
	</s>
	

	<s id="39">
		 We thus model the extraction as a classification problem with 49 classes , two for each relation subtype and a “NONE” class for the case where the two mentions are not related . 
	</s>
	

	<s id="40">
		 For each pair of mentions , we compute several feature streams shown below . 
	</s>
	

	<s id="41">
		 All the syntactic features are derived from the syntactic parse tree and the dependency tree that we compute using a statistical parser trained on the PennTree Bank using the Maximum Entropy framework 
		<ref citStr="Ratnaparkhi , 1999" id="7" label="CEPF" position="6871">
			( Ratnaparkhi , 1999 )
		</ref>
		 . 
	</s>
	

	<s id="42">
		 The feature streams are : Words The words of both the mentions and all the words in between . 
	</s>
	

	<s id="43">
		 Entity Type The entity type ( one of PERSON , ORGANIZATION , LOCATION , FACILITY , Geo-Political Entity or GPE ) of both the mentions . 
	</s>
	

	<s id="44">
		 Mention Level The mention level ( one of NAME , NOMINAL , PRONOUN ) of both the mentions . 
	</s>
	

	<s id="45">
		 Overlap The number of words ( if any ) separating the two mentions , the number of other mentions in between , flags indicating whether the two mentions are in the same noun phrase , verb phrase or prepositional phrase . 
	</s>
	

	<s id="46">
		 Dependency The words and part-of-speech and chunk labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic parse tree . 
	</s>
	

	<s id="47">
		 Parse Tree The path of non-terminals ( removing duplicates ) connecting the two mentions in the parse tree , and the path annotated with head words . 
	</s>
	

	<s id="48">
		 Here is an example . 
	</s>
	

	<s id="49">
		 For the sentence fragment , been the chairman of its board ... the corresponding syntactic parse tree is shown in Figure 1 and the dependency tree is shown in Figure 2 . 
	</s>
	

	<s id="50">
		 For the pair of mentions chairman and board , the feature streams are shown below . 
	</s>
	

	<s id="51">
		 Words 2The reader is referred to 
		<ref citStr="Strassel et al. , 2003" id="8" label="CEPF" position="8159">
			( Strassel et al. , 2003 )
		</ref>
		 or LDC’s web site for more details of the data. , , , . 
	</s>
	

	<s id="52">
		 Figure 1 : The syntactic parse tree for the fragment “chairman of its board” . 
	</s>
	

	<s id="53">
		 Figure 2 : The dependency tree for the fragment “chairman of its board” . 
	</s>
	

	<s id="54">
		 Entity Type ( for “chairman” ) , ( for “board” ) . 
	</s>
	

	<s id="55">
		 Mention Level . 
	</s>
	

	<s id="56">
		 Overlap one-mention-in-between ( the word “its” ) , two-words-apart , in-same-noun-phrase . 
	</s>
	

	<s id="57">
		 Dependency ( word on which is depedent ) , ( POS of word on which is dependent ) , ( chunk label of word on which is de- pendent ) , Parse Tree PERSON-NP-PP-ORGANIZATION , PERSON-NP-PP : of-ORGANIZATION ( both derived from the path shown in bold in Figure 1 ) . 
	</s>
	

	<s id="58">
		 We trained Maximum Entropy models using features derived from the feature streams described above . 
	</s>
	

	<s id="59">
		 3 Experimental results We divided the ACE training data provided by LDC into separate training and development sets . 
	</s>
	

	<s id="60">
		 The training set contained around 300K words , and 9752 instances of relations and the development set contained around 46K words , and 1679 instances of relations . 
	</s>
	

	<s id="61">
		 Features P R F Value Words 81.9 17.4 28.6 8.0 + Entity Type 71.1 27.5 39.6 19.3 + Mention Level 71.6 28.6 40.9 20.2 + Overlap 61.4 38.8 47.6 34.7 + Dependency 63.4 44.3 52.1 40.2 + Parse Tree 63.5 45.2 52.8 40.9 Table 2 : The Precision , Recall , F-measure and the ACE Value on the development set with true mentions and entities . 
	</s>
	

	<s id="62">
		 We report results in two ways . 
	</s>
	

	<s id="63">
		 To isolate the perfomance of relation extraction , we measure the performance of relation extraction models on “true” mentions with “true” chaining ( i.e. as annotated by LDC annotators ) . 
	</s>
	

	<s id="64">
		 We also measured performance of models run on the deficient output of mention detection and mention chaining modules . 
	</s>
	

	<s id="65">
		 We report both the F-measure ' and the ACE value of relation extraction . 
	</s>
	

	<s id="66">
		 The ACE value is a NIST metric that assigns 0 % value for a system which produces no output and 100 % value for a system that extracts all the relations and produces no false alarms . 
	</s>
	

	<s id="67">
		 We count the misses ; the true relations not extracted by the system , and the false alarms ; the spurious relations extracted by the system , and obtain the ACE value by subtracting from 1.0 , the normalized weighted cost of the misses and false alarms . 
	</s>
	

	<s id="68">
		 The ACE value counts each relation only once , even if it was expressed many times in a document in different ways . 
	</s>
	

	<s id="69">
		 The reader is referred to the ACE web site ( ACE , 2004 ) for more details . 
	</s>
	

	<s id="70">
		 We built several models to compare the relative utility of the feature streams described in the previous section . 
	</s>
	

	<s id="71">
		 Table 2 shows the results we obtained when running on “truth” for the development set and Table 3 shows the results we obtained when running on the output of mention detection and mention chaining modules . 
	</s>
	

	<s id="72">
		 Note that a model trained with only words as features obtains a very high precision and a very low recall . 
	</s>
	

	<s id="73">
		 For example , for the mention pair his and wife with no words in between , the lexical features together with the fact that there are no words in between is sufficient ( though not necessary ) to extract the relationship between the two entities . 
	</s>
	

	<s id="74">
		 The addition of entity types , mention levels and especially , the word proximity features ( “overlap” ) boosts the recall at the expense of the very 3 The F-measure is the harmonic mean of the precision , defined as the percentage of extracted relations that are valid , and the recall , defined as the percentage of valid relations that are extracted . 
	</s>
	

	<s id="75">
		 NP NN DT NN IN PRP PP NP NP been the chairman of its board ... ... VBN DT NN IN PRP NN been the chairman of its board ... ... , , , , m1-m2-dependent-in-second-level(number of links traversed in dependency tree to go from one mention to another in Figure 2 ) . 
	</s>
	

	<s id="76">
		 Features P R F Value Words 58.4 11.1 18.6 5.9 + Entity Type 43.6 14.0 21.1 12.5 + Mention Level 43.6 14.5 21.7 13.4 + Overlap 35.6 17.6 23.5 21.0 + Dependency 35.0 19.1 24.7 24.6 + Parse Tree 35.5 19.8 25.4 25.2 Table 3 : The Precision , Recall , F-measure , and ACE Value on the development set with system output mentions and entities . 
	</s>
	

	<s id="77">
		 Eval Value F Value F Set ( T ) ( T ) ( S ) ( S ) Feb’02 31.3 52.4 17.3 24.9 Sept’03 39.4 55.2 18.3 23.6 Table 4 : The F-measure and ACE Value for the test sets with true ( T ) and system output ( S ) mentions and entities . 
	</s>
	

	<s id="78">
		 high precision . 
	</s>
	

	<s id="79">
		 Adding the parse tree and dependency tree based features gives us our best result by exploiting the consistent syntactic patterns exhibited between mentions for some relations . 
	</s>
	

	<s id="80">
		 Note that the trends of contributions from different feature streams is consistent for the “truth” and system output runs . 
	</s>
	

	<s id="81">
		 As expected , the numbers are significantly lower for the system output runs due to errors made by the mention detection and mention chaining modules . 
	</s>
	

	<s id="82">
		 We ran the best model on the official ACE Feb’2002 and ACE Sept’2003 evaluation sets . 
	</s>
	

	<s id="83">
		 We obtained competitive results shown in Table 4 . 
	</s>
	

	<s id="84">
		 The rules of the ACE evaluation prohibit us from disclosing our final ranking and the results of other participants . 
	</s>
	

	<s id="85">
		 4 Discussion We have presented a statistical approach for extracting relations where we combine diverse lexical , syntactic , and semantic features . 
	</s>
	

	<s id="86">
		 We obtained competitive results on the ACE RDC task . 
	</s>
	

	<s id="87">
		 Several previous relation extraction systems have focused almost exclusively on syntactic parse trees . 
	</s>
	

	<s id="88">
		 We believe our approach of combining many kinds of evidence can potentially scale better to problems ( like ACE ) , where we have a lot of relation types with relatively small amounts of annotated data . 
	</s>
	

	<s id="89">
		 Our system certainly benefits from features derived from parse trees , but it is not inextricably linked to them . 
	</s>
	

	<s id="90">
		 Even using very simple lexical features , we obtained high precision extractors that can poten tially be used to annotate large amounts of unlabeled data for semi-supervised or unsupervised learning , without having to parse the entire data . 
	</s>
	

	<s id="91">
		 We obtained our best results when we combined a variety of features . 
	</s>
	

	<s id="92">
		 Acknowledgements We thank Salim Roukos for several invaluable suggestions and the entire ACE team at IBM for help with various components , feature suggestions and guidance . 
	</s>
	

	<s id="93">
		 References ACE . 
	</s>
	

	<s id="94">
		 2004. The nist ace evaluation website . 
	</s>
	

	<s id="95">
		 http://www.nist.gov/speech/tests/ace/ . 
	</s>
	

	<s id="96">
		 Aron Culotta and Jeffrey Sorensen . 
	</s>
	

	<s id="97">
		 2004. Dependency tree kernels for relation extraction . 
	</s>
	

	<s id="98">
		 In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics , Barcelona , Spain , July 21–July 26 . 
	</s>
	

	<s id="99">
		 Radu Florian , Hany Hassan , Hongyan Jing , Nanda Kambhatla , Xiaqiang Luo , Nicolas Nicolov , and Salim Roukos . 
	</s>
	

	<s id="100">
		 2004. A statistical model for multilingual entity detection and tracking . 
	</s>
	

	<s id="101">
		 In Proceedings of the Human Language Technologies Conference ( HLTNAACL’04 ) , Boston , Mass. , May 27 – June 1 . 
	</s>
	

	<s id="102">
		 Abraham Ittycheriah , Lucian Lita , Nanda Kambhatla , Nicolas Nicolov , Salim Roukos , and Margo Stys . 
	</s>
	

	<s id="103">
		 2003. Identifying and tracking entity mentions in a maximum entropy framework . 
	</s>
	

	<s id="104">
		 In Proceedings of the Human Language Technologies Conference ( HLTNAACL’03 ) , pages 40–42 , Edmonton , Canada , May 27 – June 1 . 
	</s>
	

	<s id="105">
		 Xiaoqiang Luo , Abraham Ittycheriah , Hongyan Jing , Nanda Kambhatla , and Salim Roukos . 
	</s>
	

	<s id="106">
		 2004. A mention-synchronous coreference resolution algorithm based on the bell tree . 
	</s>
	

	<s id="107">
		 In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics , Barcelona , Spain , July 21–July 26 . 
	</s>
	

	<s id="108">
		 Scott Miller , Heidi Fox , Lance Ramshaw , and Ralph Weischedel . 
	</s>
	

	<s id="109">
		 2000. A novel use of statistical parsing to extract information from text . 
	</s>
	

	<s id="110">
		 In 1 st Meeting of the North American Chapter of the Association for Computational Linguistics , pages 226–233 , Seattle , Washington , April 29–May 4 . 
	</s>
	

	<s id="111">
		 Adwait Ratnaparkhi . 
	</s>
	

	<s id="112">
		 1999. Learning to parse natural language with maximum entropy . 
	</s>
	

	<s id="113">
		 Machine Learning ( Special Issue on Natural Language Learning ) , 34(1- 3):151–176 . 
	</s>
	

	<s id="114">
		 Stephanie Strassel , Alexis Mitchell , and Shudong Huang . 
	</s>
	

	<s id="115">
		 2003 . 
	</s>
	

	<s id="116">
		 Multilingual resources for entity detection . 
	</s>
	

	<s id="117">
		 In Proceedings of the ACL 2003 Workshop on Multilingual Resources for Entity Detection . 
	</s>
	

	<s id="118">
		 Dmitry Zelenko , Chinatsu Aone , and Anthony Richardella. 2003 . 
	</s>
	

	<s id="119">
		 Kernel methods for relation extraction . 
	</s>
	

	<s id="120">
		 Journal of Machine Learning Research , 3:1083–1106 . 
	</s>
	


</acldoc>
