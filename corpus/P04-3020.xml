<?xml version="1.0" encoding="iso-8859-1"?>
<acldoc acl_id="P04-3020">
	

	<s id="1">
		 Graph-based Ranking Algorithms for Sentence Extraction , Applied to Text Summarization Rada Mihalcea Department of Computer Science University of North Texas rada@cs.unt.edu Abstract This paper presents an innovative unsupervised method for automatic sentence extraction using graph- based ranking algorithms . 
	</s>
	

	<s id="2">
		 We evaluate the method in the context of a text summarization task , and show that the results obtained compare favorably with previously published results on established benchmarks . 
	</s>
	

	<s id="3">
		 1 Introduction Graph-based ranking algorithms , such as Kleinberg’s HITS algorithm 
		<ref citStr="Kleinberg , 1999" id="1" label="CEPF" position="622">
			( Kleinberg , 1999 )
		</ref>
		 or Google’s PageRank 
		<ref citStr="Brin and Page , 1998" id="2" label="CEPF" position="669">
			( Brin and Page , 1998 )
		</ref>
		 , have been traditionally and successfully used in citation analysis , social networks , and the analysis of the link-structure of the World Wide Web. . 
	</s>
	

	<s id="4">
		 In short , a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph , by taking into account global information recursively computed from the entire graph , rather than relying only on local vertex-specific information . 
	</s>
	

	<s id="5">
		 A similar line of thinking can be applied to lexical or semantic graphs extracted from natural language documents , resulting in a graph-based ranking model called TextRank 
		<ref citStr="Mihalcea and Tarau , 2004" id="3" label="CEPF" position="1305">
			( Mihalcea and Tarau , 2004 )
		</ref>
		 , which can be used for a variety of natural language processing applications where knowledge drawn from an entire text is used in making local ranking/selection decisions . 
	</s>
	

	<s id="6">
		 Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases , to extractive summarization and word sense disambiguation 
		<ref citStr="Mihalcea et al. , 2004" id="4" label="CEPF" position="1682">
			( Mihalcea et al. , 2004 )
		</ref>
		 . 
	</s>
	

	<s id="7">
		 In this paper , we investigate a range of graph- based ranking algorithms , and evaluate their application to automatic unsupervised sentence extraction in the context of a text summarization task . 
	</s>
	

	<s id="8">
		 We show that the results obtained with this new unsupervised method are competitive with previously developed state-of-the-art systems . 
	</s>
	

	<s id="9">
		 2 Graph-Based Ranking Algorithms Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph , based on information drawn from the graph structure . 
	</s>
	

	<s id="10">
		 In this section , we present three graph-based ranking algorithms – previously found to be successful on a range of ranking problems . 
	</s>
	

	<s id="11">
		 We also show how these algorithms can be adapted to undirected or weighted graphs , which are particularly useful in the context of text-based ranking applications . 
	</s>
	

	<s id="12">
		 Let G = ( V , E ) be a directed graph with the set of vertices V and set of edges E , where E is a subset of V x V . 
	</s>
	

	<s id="13">
		 For a given vertex Vi , let In(Vi) be the set of vertices that point to it ( predecessors ) , and let Out(Vi) be the set of vertices that vertex Vi points to ( successors ) . 
	</s>
	

	<s id="14">
		 2.1 HITS HITS ( Hyperlinked Induced Topic Search ) 
		<ref citStr="Klein- berg , 1999" id="5" label="CEPF" position="2956">
			( Klein- berg , 1999 )
		</ref>
		 is an iterative algorithm that was designed for ranking Web pages according to their degree of “authority” . 
	</s>
	

	<s id="15">
		 The HITS algorithm makes a distinction between “authorities” ( pages with a large number of incoming links ) and “hubs” ( pages with a large number of outgoing links ) . 
	</s>
	

	<s id="16">
		 For each vertex , HITS produces two sets of scores – an “authority” score , and a “hub” score : HITS,(Vi) = E HITSH(Vj) ( 1 ) VjEIn(Vi) HITSH(Vi) = E HITS,(Vj) ( 2 ) Vj EOut(Vi) 2.2 Positional Power Function Introduced by 
		<ref citStr="Herings et al. , 2001" id="6" label="CEPF" position="3512">
			( Herings et al. , 2001 )
		</ref>
		 , the positional power function is a ranking algorithm that determines the score of a vertex as a function that combines both the number of its successors , and the score of its successors . 
	</s>
	

	<s id="17">
		 E POSP(Vi) = ICI The counterpart of thepositional power function is the positional weakness function , defined as : POSW(Vi) = IVI 1 E ( 1+POSW(Vj)) ( 4 ) VjEIn(Vi) ( 1+POSP(Vj)) ( 3 ) Vj EOut(Vi) 2.3 PageRank PageRank 
		<ref citStr="Brin and Page , 1998" id="7" label="CEPF" position="3956">
			( Brin and Page , 1998 )
		</ref>
		 is perhaps one of the most popular ranking algorithms , and was designed as a method for Web link analysis . 
	</s>
	

	<s id="18">
		 Unlike other ranking algorithms , PageRank integrates the impact of both incoming and outgoing links into one single model , and therefore it produces only one set of scores : HITSWA ( Vi ) = ~ wjiHITSWH(Vj) ( 6 ) Vj EIn(Vi) HITSWH(Vi) = 11 wij HITSWA ( Vj ) ( 7 ) Vj EOut(Vi) ~ ( 1+wijPOSWP ( Vj ) ) ( 8 ) POSWP ( Vi ) = 1 |V | Vj EOut(Vi) PR(Vi) = ( 1— d ) + d * E PR(Vj) ( 5 ) ~ ( 1 + wjiPOSWW ( Vj ) ) ( 9 ) VjEIn(Vi) |Out(Vj)| POSWW ( Vi ) = 1 |V | Vj EIn(Vi) where d is a parameter that is set between 0 and 1 1 . 
	</s>
	

	<s id="19">
		 For each of these algorithms , starting from arbitrary values assigned to each node in the graph , the computation iterates until convergence below a given threshold is achieved . 
	</s>
	

	<s id="20">
		 After running the algorithm , a score is associated with each vertex , which represents the “importance” or “power” of that vertex within the graph . 
	</s>
	

	<s id="21">
		 Notice that the final values are not affected by the choice of the initial value , only the number of iterations to convergence may be different . 
	</s>
	

	<s id="22">
		 2.4 Undirected Graphs Although traditionally applied on directed graphs , recursive graph-based ranking algorithms can be also applied to undirected graphs , in which case the out- degree of a vertex is equal to the in-degree of the vertex . 
	</s>
	

	<s id="23">
		 For loosely connected graphs , with the number of edges proportional with the number of vertices , undirected graphs tend to have more gradual convergence curves . 
	</s>
	

	<s id="24">
		 As the connectivity of the graph increases ( i.e. larger number of edges ) , convergence is usually achieved after fewer iterations , and the convergence curves for directed and undirected graphs practically overlap . 
	</s>
	

	<s id="25">
		 2.5 Weighted Graphs In the context of Web surfing or citation analysis , it is unusual for a vertex to include multiple or partial links to another vertex , and hence the original definition for graph-based ranking algorithms is assuming unweighted graphs . 
	</s>
	

	<s id="26">
		 However , in our TextRank model the graphs are build from natural language texts , and may include multiple or partial links between the units ( vertices ) that are extracted from text . 
	</s>
	

	<s id="27">
		 It may be therefore useful to indicate and incorporate into the model the “strength” of the connection between two vertices Vi and Vg as a weight wig added to the corresponding edge that connects the two vertices . 
	</s>
	

	<s id="28">
		 Consequently , we introduce new formulae for graph-based ranking that take into account edge weights when computing the score associated with a vertex in the graph . 
	</s>
	

	<s id="29">
		 ' The factor d is usually set at 0.85 
		<ref citStr="Brin and Page , 1998" id="8" label="CEPF" position="6690">
			( Brin and Page , 1998 )
		</ref>
		 , and this is the value we are also using in our implementation . 
	</s>
	

	<s id="30">
		 PRW(Vi) = ( 1 — d ) + d * E Vj EIn(Vi) While the final vertex scores ( and therefore rankings ) for weighted graphs differ significantly as compared to their unweighted alternatives , the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs . 
	</s>
	

	<s id="31">
		 3 Sentence Extraction To enable the application of graph-based ranking algorithms to natural language texts , TextRank starts by building a graph that represents the text , and interconnects words or other text entities with meaningful relations . 
	</s>
	

	<s id="32">
		 For the task of sentence extraction , the goal is to rank entire sentences , and therefore a vertex is added to the graph for each sentence in the text . 
	</s>
	

	<s id="33">
		 To establish connections ( edges ) between sentences , we are defining a “similarity” relation , where “similarity” is measured as a function of content overlap . 
	</s>
	

	<s id="34">
		 Such a relation between two sentences can be seen as a process of “recommendation” : a sentence that addresses certain concepts in a text , gives the reader a “recommendation” to refer to other sentences in the text that address the same concepts , and therefore a link can be drawn between any two such sentences that share common content . 
	</s>
	

	<s id="35">
		 The overlap of two sentences can be determined simply as the number of common tokens between the lexical representations of the two sentences , or it can be run through syntactic filters , which only count words of a certain syntactic category . 
	</s>
	

	<s id="36">
		 Moreover , to avoid promoting long sentences , we are using a normalization factor , and divide the content overlap of two sentences with the length of each sentence . 
	</s>
	

	<s id="37">
		 Formally , given two sentences Si and Sg , with a sentence being represented by the set of Ni words that appear in the sentence : Si = Wi~ , Wi~ , ... , WiNi , the similarity of Si and Sg is defined as : Similarity ( Si,Sg ) = Ws(ISiI)+lo ( IsjI , ) The resulting graph is highly connected , with a weight associated with each edge , indicating the PRW ( Vj ) E wkj Vk EOut(Vj ) wji ( 10 ) strength of the connections between various sentence pairs in the text2 . 
	</s>
	

	<s id="38">
		 The text is therefore represented as a weighted graph , and consequently we are using the weighted graph-based ranking formulae introduced in Section 2.5 . 
	</s>
	

	<s id="39">
		 The graph can be represented as : ( a ) simple undirected graph ; ( b ) directed weighted graph with the orientation of edges set from a sentence to sentences that follow in the text ( directed forward ) ; or ( c ) directed weighted graph with the orientation of edges set from a sentence to previous sentences in the text ( directed backward ) . 
	</s>
	

	<s id="40">
		 After the ranking algorithm is run on the graph , sentences are sorted in reversed order of their score , and the top ranked sentences are selected for inclusion in the summary . 
	</s>
	

	<s id="41">
		 Figure 1 shows a text sample , and the associated weighted graph constructed for this text . 
	</s>
	

	<s id="42">
		 The figure also shows sample weights attached to the edges connected to vertex 93 , and the final score computed for each vertex , using the PR formula , applied on an undirected graph . 
	</s>
	

	<s id="43">
		 The sentences with the highest rank are selected for inclusion in the abstract . 
	</s>
	

	<s id="44">
		 For this sample article , sentences with id-s 9 , 15 , 16 , 18 are extracted , resulting in a summary of about 100 words , which according to automatic evaluation measures , is ranked the second among summaries produced by 15 other systems ( see Section 4 for evaluation methodology ) . 
	</s>
	

	<s id="45">
		 4 Evaluation The TextRank sentence extraction algorithm is evaluated in the context of a single-document summarization task , using 567 news articles provided during the Document Understanding Evaluations 2002 ( DUC , 2002 ) . 
	</s>
	

	<s id="46">
		 For each article , TextRank generates a 100-words summary — the task undertaken by other systems participating in this single document summarization task . 
	</s>
	

	<s id="47">
		 For evaluation , we are using the ROUGE evaluation toolkit , which is a method based on Ngram statistics , found to be highly correlated with human evaluations 
		<ref citStr="Lin and Hovy , 2003a" id="9" label="CEPF" position="10932">
			( Lin and Hovy , 2003a )
		</ref>
		 . 
	</s>
	

	<s id="48">
		 Two manually produced reference summaries are provided , and used in the evaluation process4 . 
	</s>
	

	<s id="49">
		 2In single documents , sentences with highly similar content are very rarely if at all encountered , and therefore sentence redundancy does not have a significant impact on the summarization of individual texts . 
	</s>
	

	<s id="50">
		 This may not be however the case with multiple document summarization , where a redundancy removal technique – such as a maximum threshold imposed on the sentence similarity – needs to be implemented . 
	</s>
	

	<s id="51">
		 3Weights are listed to the right or above the edge they correspond to . 
	</s>
	

	<s id="52">
		 Similar weights are computed for each edge in the graph , but are not displayed due to space restrictions . 
	</s>
	

	<s id="53">
		 4The evaluation is done using the Ngram(1,1) setting of ROUGE , which was found to have the highest correlation with human judgments , at a confidence level of 95 % . 
	</s>
	

	<s id="54">
		 Only the first 100 words in each summary are considered . 
	</s>
	

	<s id="55">
		 Figure 1 : Sample graph build for sentence extraction from a newspaper article . 
	</s>
	

	<s id="56">
		 We evaluate the summaries produced by TextRank using each of the three graph-based ranking algorithms described in Section 2 . 
	</s>
	

	<s id="57">
		 Table 1 shows the results obtained with each algorithm , when using graphs that are : ( a ) undirected , ( b ) directed forward , or ( c ) directed backward . 
	</s>
	

	<s id="58">
		 For a comparative evaluation , Table 2 shows the results obtained on this data set by the top 5 ( out of 15 ) performing systems participating in the single document summarization task at DUC 2002 ( DUC , 2002 ) . 
	</s>
	

	<s id="59">
		 It also lists the baseline performance , computed for 100-word summaries generated by taking the first sentences in each article . 
	</s>
	

	<s id="60">
		 Discussion . 
	</s>
	

	<s id="61">
		 The TextRank approach to sentence extraction succeeds in identifying the most important sentences in a text based on information exclusively 3 : BC^HurricaineGilbert , 09^11 339 4 : BC^Hurricaine Gilbert , 0348 5 : Hurricaine Gilbert heads toward Dominican Coast 6 : By Ruddy Gonzalez 7 : Associated Press Writer 8 : Santo Domingo , Dominican Republic ( AP ) 9 : Hurricaine Gilbert Swept towrd the Dominican Republic Sunday , and the Civil Defense alerted its heavily populated south coast to prepare for high winds , heavy rains , and high seas . 
	</s>
	

	<s id="62">
		 10 : The storm was approaching from the southeast with sustained winds of 75 mph gusting to 92 mph. 11 : &quot; There is no need for alarm , &quot; Civil Defense Director Eugenio Cabral said in a television alert shortly after midnight Saturday . 
	</s>
	

	<s id="63">
		 12 : Cabral said residents of the province of Barahona should closely follow Gilbert’s movement . 
	</s>
	

	<s id="64">
		 13 : An estimated 100,000 people live in the province , including 70,000 in the city of Barahona , about 125 miles west of Santo Domingo . 
	</s>
	

	<s id="65">
		 14. Tropical storm Gilbert formed in the eastern Carribean and strenghtened into a hurricaine Saturday night . 
	</s>
	

	<s id="66">
		 15 : The National Hurricaine Center in Miami reported its position at 2 a.m. Sunday at latitude 16.1 north , longitude 67.5 west , about 140 miles south of Ponce , Puerto Rico , and 200 miles southeast of Santo Domingo . 
	</s>
	

	<s id="67">
		 16 : The National Weather Service in San Juan , Puerto Rico , said Gilbert was moving westard at 15 mph with a &quot; broad area of cloudiness and heavy weather &quot; rotating around the center of the storm . 
	</s>
	

	<s id="68">
		 17. The weather service issued a flash flood watch for Puerto Rico and the Virgin Islands until at least 6 p.m. Sunday . 
	</s>
	

	<s id="69">
		 18 : Strong winds associated with the Gilbert brought coastal flooding , strong southeast winds , and up to 12 feet to Puerto Rico’s south coast . 
	</s>
	

	<s id="70">
		 19 : There were no reports on casualties . 
	</s>
	

	<s id="71">
		 20 : San Juan , on the north coast , had heavy rains and gusts Saturday , but they subsided during the night . 
	</s>
	

	<s id="72">
		 21 : On Saturday , Hurricane Florence was downgraded to a tropical storm , and its remnants pushed inland from the U.S. Gulf Coast . 
	</s>
	

	<s id="73">
		 22 : Residents returned home , happy to find little damage from 90 mph winds and sheets of rain . 
	</s>
	

	<s id="74">
		 23 : Florence , the sixth named storm of the 1988 Atlantic storm season , was the second hurricane . 
	</s>
	

	<s id="75">
		 24 : The first , Debby , reached minimal hurricane strength briefly before hitting the Mexican coast last month . 
	</s>
	

	<s id="76">
		 6 [ 0.15 ] [ 1.02 ] 21 [ 0.84 ] 20 [0.15]19 [ 1.58 ] 18 [ 0.70 ] 23 [ 0.80 ] 5 [ 1.20 ] 22 [ 0.70 ] 12 [ 0.93 ] [ 0.76 ] 7 [ 0.15 ] 8 [ 0.70 ] 0.35 9 [ 1.83 ] 0.15 0.2910 [ 0.99 ] 11 [ 0.56 ] 0.19 0.15 0.55 0.30 0.59 0.15 0.27 0.16 0.14 0.15 17 0.15 [0.50]24 4 [ 0.71 ] 15 14 13 [ 1.36 ] [ 1.09 ] 16 [ 1.65 ] Algorithm Graph Undirected Dir . 
	</s>
	

	<s id="77">
		 forward Dir . 
	</s>
	

	<s id="78">
		 backward HITSA 0.4912 0.4584 0.5023 HITSH 0.4912 0.5023 0.4584 POSP 0.4878 0.4538 0.3910 POSW 0.4878 0.3910 0.4538 PageRank 0.4904 0.4202 0.5008 Table 1 : Results for text summarization using Text- Rank sentence extraction . 
	</s>
	

	<s id="79">
		 Graph-based ranking algorithms : HITS , Positional Function , PageRank . 
	</s>
	

	<s id="80">
		 Graphs : undirected , directed forward , directed backward . 
	</s>
	

	<s id="81">
		 Top 5 systems ( DUC , 2002 ) Baseline S27 S31 S28 S21 S29 0.5011 0.4914 0.4890 0.4869 0.4681 0.4799 Table 2 : Results for single document summarization for top 5 ( out of 15 ) DUC 2002 systems , and baseline . 
	</s>
	

	<s id="82">
		 drawn from the text itself . 
	</s>
	

	<s id="83">
		 Unlike other supervised systems , which attempt to learn what makes a good summary by training on collections of summaries built for other articles , TextRank is fully unsupervised , and relies only on the given text to derive an extractive summary . 
	</s>
	

	<s id="84">
		 Among all algorithms , the HITSA and PageRank algorithms provide the best performance , at par with the best performing system from DUC 20025 . 
	</s>
	

	<s id="85">
		 This proves that graph-based ranking algorithms , previously found successful in Web link analysis , can be turned into a state-of-the-art tool for sentence extraction when applied to graphs extracted from texts . 
	</s>
	

	<s id="86">
		 Notice that TextRank goes beyond the sentence “connectivity” in a text . 
	</s>
	

	<s id="87">
		 For instance , sentence 15 in the example provided in Figure 1 would not be identified as “important” based on the number of connections it has with other vertices in the graph6 , but it is identified as “important” by TextRank ( and by humans – according to the reference summaries for this text ) . 
	</s>
	

	<s id="88">
		 Another important advantage of TextRank is that it gives a ranking over all sentences in a text – which means that it can be easily adapted to extracting very short summaries , or longer more explicative summaries , consisting of more than 100 words . 
	</s>
	

	<s id="89">
		 5 Related Work Sentence extraction is considered to be an important first step for automatic text summarization . 
	</s>
	

	<s id="90">
		 As a consequence , there is a large body of work on algorithms 5Notice that rows two and four in Table 1 are in fact redundant , since the ` hub” ( ` weakness” ) variations of the HITS ( Positional ) algorithms can be derived from their ` authority” ( ` power” ) counterparts by reversing the edge orientation in the graphs . 
	</s>
	

	<s id="91">
		 6Only seven edges are incident with vertex 15 , less than e.g. eleven edges incident with vertex 14 – not selected as ` important” by TextRank . 
	</s>
	

	<s id="92">
		 for sentence extraction undertaken as part of the DUC evaluation exercises . 
	</s>
	

	<s id="93">
		 Previous approaches include supervised learning 
		<ref citStr="Teufel and Moens , 1997" id="10" label="CEPF" position="18355">
			( Teufel and Moens , 1997 )
		</ref>
		 , vectorial similarity computed between an initial abstract and sentences in the given document , or intra-document similarities 
		<ref citStr="Salton et al. , 1997" id="11" label="CEPF" position="18509">
			( Salton et al. , 1997 )
		</ref>
		 . 
	</s>
	

	<s id="94">
		 It is also notable the study reported in 
		<ref citStr="Lin and Hovy , 2003b" id="12" label="CEPF" position="18586">
			( Lin and Hovy , 2003b )
		</ref>
		 discussing the usefulness and limitations of automatic sentence extraction for summarization , which emphasizes the need of accurate tools for sentence extraction , as an integral part of automatic summarization systems . 
	</s>
	

	<s id="95">
		 6 Conclusions Intuitively , TextRank works well because it does not only rely on the local context of a text unit ( vertex ) , but rather it takes into account information recursively drawn from the entire text ( graph ) . 
	</s>
	

	<s id="96">
		 Through the graphs it builds on texts , TextRank identifies connections between various entities in a text , and implements the concept of recommendation . 
	</s>
	

	<s id="97">
		 A text unit recommends other related text units , and the strength of the recommendation is recursively computed based on the importance of the units making the recommendation . 
	</s>
	

	<s id="98">
		 In the process of identifying important sentences in a text , a sentence recommends another sentence that addresses similar concepts as being useful for the overall understanding of the text . 
	</s>
	

	<s id="99">
		 Sentences that are highly recommended by other sentences are likely to be more informative for the given text , and will be therefore given a higher score . 
	</s>
	

	<s id="100">
		 An important aspect of TextRank is that it does not require deep linguistic knowledge , nor domain or language specific annotated corpora , which makes it highly portable to other domains , genres , or languages . 
	</s>
	

	<s id="101">
		 References S. Brin and L. Page . 
	</s>
	

	<s id="102">
		 1998. The anatomy of a large-scale hypertextual Web search engine . 
	</s>
	

	<s id="103">
		 Computer Networks and ISDN Systems , 30(1–7) . 
	</s>
	

	<s id="104">
		 DUC . 
	</s>
	

	<s id="105">
		 2002. Document understanding conference 2002. http://www- nlpir.nist.gov/projects/duc/ . 
	</s>
	

	<s id="106">
		 P.J. Herings , G. van der Laan , and D. Talman . 
	</s>
	

	<s id="107">
		 2001. Measuring the power of nodes in digraphs . 
	</s>
	

	<s id="108">
		 Technical report , Tinbergen Institute . 
	</s>
	

	<s id="109">
		 J.M. Kleinberg . 
	</s>
	

	<s id="110">
		 1999 . 
	</s>
	

	<s id="111">
		 Authoritative sources in a hyperlinked environ- ment.Journal of the ACM , 46(5):604–632 . 
	</s>
	

	<s id="112">
		 C.Y. Lin and E.H. Hovy . 
	</s>
	

	<s id="113">
		 2003 a . 
	</s>
	

	<s id="114">
		 Automatic evaluation of summaries using n-gram co-occurrence statistics . 
	</s>
	

	<s id="115">
		 In Proceedings of Human Language Technology Conference ( HLT-NAACL 2003 ) , Edmonton , Canada , May . 
	</s>
	

	<s id="116">
		 C.Y. Lin and E.H. Hovy . 
	</s>
	

	<s id="117">
		 2003b . 
	</s>
	

	<s id="118">
		 The potential and limitations of sentence extraction for summarization . 
	</s>
	

	<s id="119">
		 In Proceedings of the HLT/NAACL Workshop on Automatic Summarization , Edmonton , Canada , May . 
	</s>
	

	<s id="120">
		 R. Mihalcea and P. Tarau . 
	</s>
	

	<s id="121">
		 2004. TextRank – bringing order into texts . 
	</s>
	

	<s id="122">
		 R. Mihalcea , P. Tarau , and E. Figa . 
	</s>
	

	<s id="123">
		 2004. PageRank on semantic networks , with application to word sense disambiguation . 
	</s>
	

	<s id="124">
		 In Proceedings of the 20st International Conference on Computational Linguistics ( COLING 2004 ) , Geneva , Switzerland , August . 
	</s>
	

	<s id="125">
		 G. Salton , A. Singhal , M. Mitra , and C. Buckley . 
	</s>
	

	<s id="126">
		 1997. Automatic text structuring and summarization . 
	</s>
	

	<s id="127">
		 Information Processing and Management , 2(32) . 
	</s>
	

	<s id="128">
		 S. Teufel and M. Moens . 
	</s>
	

	<s id="129">
		 1997. Sentence extraction as a classification task . 
	</s>
	

	<s id="130">
		 In ACL/EACL workshop on ”Intelligent and scalable Text summarization ” , pages 58–65 , Madrid , Spain . 
	</s>
	


</acldoc>
