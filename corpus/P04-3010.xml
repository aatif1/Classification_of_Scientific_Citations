<?xml version="1.0" encoding="iso-8859-1"?>
<acldoc acl_id="P04-3010">
	

	<s id="1">
		 Part-of-Speech Tagging Considering Surface Form for an Agglutinative Language Do-Gil Lee and Hae-Chang Rim Dept. of Computer Science &amp; Engineering Korea University 1 , 5-ka , Anam-dong , Seongbuk-ku Seoul 136-701 , Korea { dglee , rim}@nlp.korea.ac.kr Abstract The previous probabilistic part-of-speech tagging models for agglutinative languages have considered only lexical forms of morphemes , not surface forms of words . 
	</s>
	

	<s id="2">
		 This causes an inaccurate calculation of the probability . 
	</s>
	

	<s id="3">
		 The proposed model is based on the observation that when there exist words ( surface forms ) that share the same lexical forms , the probabilities to appear are different from each other . 
	</s>
	

	<s id="4">
		 Also , it is designed to consider lexical form of word . 
	</s>
	

	<s id="5">
		 By experiments , we show that the proposed model outperforms the bigram Hidden Markov model (HMM)-based tagging model . 
	</s>
	

	<s id="6">
		 based tagging model . 
	</s>
	

	<s id="7">
		 2 Korean POS tagging model In this section , we first describe the standard morpheme-unit tagging model and point out a mistake of this model . 
	</s>
	

	<s id="8">
		 Then , we describe the proposed model . 
	</s>
	

	<s id="9">
		 2.1 Standard morpheme-unit model This section describes the HMM-based morpheme- unit model . 
	</s>
	

	<s id="10">
		 The morpheme-unit POS tagging model is to find the most likely sequence of morphemes M and corresponding POS tags T for a given sentence W , as follows 
		<ref citStr="Kim et al. , 1998" id="1" label="CEPF" position="1391">
			( Kim et al. , 1998 
		</ref>
		<ref citStr="Lee et al. , 2000" id="2" label="CEPF" position="1411">
			; Lee et al. , 2000 )
		</ref>
		 : 1 Introduction F ( W ) def argmax P(M,TIW) M,T Part-of-speech ( POS ) tagging is a job to assign a proper POS tag to each linguistic unit such as word for a given sentence . 
	</s>
	

	<s id="11">
		 In English POS tagging , word is used as a linguistic unit . 
	</s>
	

	<s id="12">
		 However , the number of possible words in agglutinative languages such as Korean is almost infinite because words can be freely formed by gluing morphemes together . 
	</s>
	

	<s id="13">
		 Therefore , morpheme-unit tagging is preferred and more suitable in such languages than word-unit tagging . 
	</s>
	

	<s id="14">
		 Figure 1 shows an example of morpheme structure of a sentence , where the bold lines indicate the most likely morpheme-POS sequence . 
	</s>
	

	<s id="15">
		 A solid line represents a transition between two morphemes across a word boundary and a dotted line represents a transition between two morphemes in a word . 
	</s>
	

	<s id="16">
		 The previous probabilistic POS models for agglutinative languages have considered only lexical forms of morphemes , not surface forms of words . 
	</s>
	

	<s id="17">
		 This causes an inaccurate calculation of the probability . 
	</s>
	

	<s id="18">
		 The proposed model is based on the observation that when there exist words ( surface forms ) that share the same lexical forms , the probabilities to appear are different from each other . 
	</s>
	

	<s id="19">
		 Also , it is designed to consider lexical form of word . 
	</s>
	

	<s id="20">
		 By experiments , we show that the proposed model outperforms the bigram Hidden Markov model (HMM)- =argmax P(ml,u , tl,u I Wl,n ) ( 1 ) ml,u,tl,u ~argmax P(ml,u , tl,u ) ( 2 ) ml,u,tl,u In the equation , u(&gt;= n ) denotes the number of morphemes in the sentence . 
	</s>
	

	<s id="21">
		 A sequence of W = Wl,n = WlW2 • • • Wn is a sentence of n words , and a sequence of M = ml,u = mlm2 • • • mu and a sequence of T = tl,u = tlt2 • • • tu denote a sequence of u lexical forms of morphemes and a sequence of u morpheme categories ( POS tags ) , respectively . 
	</s>
	

	<s id="22">
		 To simplify Equation 2 , a Markov assumption is usually used as follows : P(ti I ti-l,p)P(ti I mi ) ( 3 ) where , to is a pseudo tag which denotes the beginning of word and is also written as BOW . 
	</s>
	

	<s id="23">
		 p denotes a type of transition from the previous tag to the current tag . 
	</s>
	

	<s id="24">
		 It has a binary value according to the type of the transition ( either intra-word or inter- word transition ) . 
	</s>
	

	<s id="25">
		 As can be seen , the word1 sequence Wl,n is discarded in Equation 2 . 
	</s>
	

	<s id="26">
		 This leads to an inaccurate 1A word is a surface form . 
	</s>
	

	<s id="27">
		 F(W) ~f argmax ml,u,tl,u ~u i=l BOS na/NNP na/VV na/VX nal/VV neun/PX neun/EFD n-da/EFC n-da/EFF hag-gyo/NNC e/PA ga/VV ga/VX gal/VV EOS Figure 1 : Morpheme structure of the sentence “na-neun hag-gyo-e gan-da” ( I go to school ) calculation of the probability . 
	</s>
	

	<s id="28">
		 A lexical form of a word can be mapped to more than one surface word . 
	</s>
	

	<s id="29">
		 In this case , although the different surface forms are given , if they have the same lexical form , then the probabilities will be the same . 
	</s>
	

	<s id="30">
		 For example , a lexical form mong-go/nc+leul/jc2 , can be mapped from two surface forms mong-gol and mong-go-leul . 
	</s>
	

	<s id="31">
		 By applying Equation 1 and Equation 2 to both words , the following equations can be derived : P(mong-go , nc , leul , jc I mong-gol ) ti P(mong-go , nc , leul , jc ) ( 4 ) P ( mong-go , nc , leul , j c I mong-go-leul ) ti P(mong-go , nc , leul , jc ) ( 5 ) As a result , we can acquire the following equation from Equation 4 and Equation 5 : P( mong-go , nc , leul , jc I mong-gol ) = P(mong-go , nc , leul , jc I mong-go-leul ) ( 6 ) That is , they assume that probabilities of the results that have the same lexical form are the same . 
	</s>
	

	<s id="32">
		 However , we can easily show that Equation 6 is mistaken : Actually , P(mong-go , nc , leul , jcI mong-go-leul ) = 1 and P(mong-gol , ncI mong-gol ) =~ 0 . 
	</s>
	

	<s id="33">
		 Hence , P(mong-go , nc , leul , jc I mong-gol ) &lt; P(mong-go , nc , leul , jc I mong-go-leul ) . 
	</s>
	

	<s id="34">
		 To overcome the disadvantage , we propose a new tagging model that can consider the surface form . 
	</s>
	

	<s id="35">
		 2.2 The proposed model This section describes the proposed model . 
	</s>
	

	<s id="36">
		 To simplify the notation , we introduce a variable R , which means a tagging result of a given sentence and consists of M and T. F ( W ) def argmax P(M,TIW) ( 7 ) M,T =argmax P(RIW) ( 8 ) R 2mong-go means Mongolia , nc is a common noun , and jc is a objective case postposition . 
	</s>
	

	<s id="37">
		 The probability P ( R I W ) is given as follows : P(R I W ) = P(r1,n I w1,n ) ( 9 ) P(ri I w1,n , r1,i-1 ) ( 10 ) P(ri I wi , ri-1 ) ( 11 ) where , ri denotes the tagging result of ith word ( wi ) , and ro denotes a pseudo variable to indicate the beginning of word . 
	</s>
	

	<s id="38">
		 Equation 9 becomes Equation 10 by the chain rule . 
	</s>
	

	<s id="39">
		 To be a more tractable form , Equation 10 is simplified by a Markov assumption as Equation 11 . 
	</s>
	

	<s id="40">
		 The probability P(ri I wi , ri-1 ) cannot be calculated directly , so it is derived as follows : P(wi , ri-1 , ri ) P ( ri Iwi , ri-1 ) =12 P(wi , ri-1 ) P(wi)P(ri I wi)P(ri-1 I ri ) ( 13 ) ti P(wi)P(ri-1) = P(riIwi)P(ri-1 I ri ) ( 14 ) P(ri-1) = P(riIwi) P(ri-1 , ri ) ( 15 ) P(ri-1)P(ri ) Equation 12 is derived by Bayes rule , Equation 13 by a chain rule and an independence assumption , and Equation 15 by Bayes rule . 
	</s>
	

	<s id="41">
		 In Equation 15 , we call the left term “morphological analysis model” and right one “transition model” . 
	</s>
	

	<s id="42">
		 The morphological analysis model P(ri I wi ) can be implemented in a morphological analyzer . 
	</s>
	

	<s id="43">
		 If a morphological analyzer can provide the probability , then the tagger can use the values as they are . 
	</s>
	

	<s id="44">
		 Actually , we use the probability that a morphological analyzer , ProKOMA 
		<ref citStr="Lee and Rim , 2004" id="3" label="OERF" position="7127">
			( Lee and Rim , 2004 )
		</ref>
		 produces . 
	</s>
	

	<s id="45">
		 Although it is not necessary to discuss the morphological analysis model in detail , we should note that surface forms are considered here . 
	</s>
	

	<s id="46">
		 The transition model is a form of point-wise mutual information . 
	</s>
	

	<s id="47">
		 = ~n i=1 ~n ti i=1 P(1i-1~~1 ) ' i-1 ' 1( i ' 1r ) ( 16 ) P(1i-1 ' 1 i-P~~i ' T ) P(m1,j1'ti1,3- . 
	</s>
	

	<s id="48">
		 )P(mi1,k' ti1,k ) where , a superscript i in mi1 k and ti1 k denotes the position of the word in a sentence . 
	</s>
	

	<s id="49">
		 The denominator means a joint probability that the morphemes and the tags in a word appear together , and the numerator means a joint probability that all the morphemes and the tags between two words appear together . 
	</s>
	

	<s id="50">
		 Due to the sparse data problem , they cannot also be calculated directly from the test data . 
	</s>
	

	<s id="51">
		 By a Markov assumption , the denominator and the numerator can be broken down into Equation 18 and Equation 19 , respectively . 
	</s>
	

	<s id="52">
		 where , Pinter ( ti1 I ti~ 1 ) means a transition probability between the last morpheme of the (i~1)th word and the first morpheme of the ith word . 
	</s>
	

	<s id="53">
		 By applying Equation 18 and Equation 19 to Equation 17 , we obtain the following equation : Pinter ( ti1 Itj 1 ) P ( ti1IBOW ) ( 20 ) For a given sentence , Figure 2 shows the bigram HMM-based tagging model , and Figure 3 the proposed model . 
	</s>
	

	<s id="54">
		 The main difference between the two models is the proposed model considers surface forms but the HMM does not . 
	</s>
	

	<s id="55">
		 3 Experiments For evaluation , two data sets are used : ETRI POS tagged corpus and KAIST POS tagged corpus . 
	</s>
	

	<s id="56">
		 We divided the test data into ten parts . 
	</s>
	

	<s id="57">
		 The performances of the model are measured by averaging over the ten test sets in the 10-fold cross-validation experiment . 
	</s>
	

	<s id="58">
		 Table 1 shows the summary of the corpora . 
	</s>
	

	<s id="59">
		 Table 1 : Summary of the data Generally , POS tagging goes through the following steps : First , run a morphological analyzer , where it generates all the possible interpretations for a given input text . 
	</s>
	

	<s id="60">
		 Then , a POS tagger takes the results as input and chooses the most likely one among them . 
	</s>
	

	<s id="61">
		 Therefore , the performance of the tagger depends on that of the preceding morphological analyzer . 
	</s>
	

	<s id="62">
		 If the morphological analyzer does not generate the exact result , the tagger has no chance to select the correct one , thus an answer inclusion rate of the morphological analyzer becomes the upper bound of the tagger . 
	</s>
	

	<s id="63">
		 The previous works preprocessed the dictionary to include all the exact answers in the morphological analyzer’s results . 
	</s>
	

	<s id="64">
		 However , this evaluation method is inappropriate to the real application in the strict sense . 
	</s>
	

	<s id="65">
		 In this experiment , we present the accuracy of the morphological analyzer instead of preprocessing the dictionary . 
	</s>
	

	<s id="66">
		 ProKOMA’s results with the test data are listed in Table 2 . 
	</s>
	

	<s id="67">
		 Table 2 : Morphological analyzer’s results with the test data Corpus ETRI KAIST Answer inclusion rate ( % ) 95.82 95.95 Average # of results per word 2.16 1.81 1-best accuracy ( % ) 88.31 90.12 In the table , 1-best accuracy is defined as the number of words whose result with the highest probability is matched to the gold standard over the entire words in the test data . 
	</s>
	

	<s id="68">
		 This can also be a tagging model that does not consider any outer context . 
	</s>
	

	<s id="69">
		 To compare the proposed model with the standard model , the results of the two models are given in Table 3 . 
	</s>
	

	<s id="70">
		 As can be seen , our model outperforms the HMM model . 
	</s>
	

	<s id="71">
		 Moreover , the HMM model is even worse than the ProKOMA’s 1-best accuracy . 
	</s>
	

	<s id="72">
		 This tells that the standard HMM by itself is not a good model for agglutinative languages . 
	</s>
	

	<s id="73">
		 4 Conclusion We have presented a new POS tagging model that can consider the surface form for Korean , which Corpus ETRI KAIST Total # of words 288,291 175,468 Total # of sentences 27,855 16,193 # of tags 27 54 P(ri-1 ' ri ) P(ri-1 )P(ri) P(m1,j1' 1J 'mi1,k'ti1,k ) 17 ) P(m1,k,t1,k) _ ~k P(ti I ti-1)P(mi I ti ) ( 18 ) i=1 ~j(P(tl-1 tl=i ) 1 \P(ml-1 ti-1)l ~-1 X Pinter(ti1 I tj 1 ) X P(mi1 ti1 ) ~X P((Tni tI ti ) ) ) ( 19 ) m-2( m m P(m i-1 i-1 i i 1,9 ' t1,j ' m1,k' t1,k ) k P(ri-1 , ri ) P(ri-1)P(ri) na neun hag-gyo e ga n-da BOS NNP PX NNC PA VV EFF EOS Figure 2 : Lattice of the bigram HMM-based model na-neun hag-gyo-e gan-da BOS na/NNP+neun/PX hag-gyo/NNC+e/PA ga/VV+n-da/EFF EOS Figure 3 : Lattice of the proposed model Table 3 : Tagging accuracies ( % ) of the standard HMM and the proposed model Corpus ETRI KAIST The standard HMM The proposed model 87.47 89.83 90.66 92.01 is an agglutinative language . 
	</s>
	

	<s id="74">
		 Although the model leaves much room for improvement , it outperforms the HMM based model according to the experimental results . 
	</s>
	

	<s id="75">
		 Acknowledgement This work was supported by Korea Research Foundation Grant ( KRF-2003-041-D20485 ) References J.-D. Kim , S.-Z. Lee , and H.-C. Rim . 
	</s>
	

	<s id="76">
		 1998. A morpheme-unit POS tagging model considering word-spacing . 
	</s>
	

	<s id="77">
		 In Proceedings of the 1998 Conference on Hangul and Korean Information Processing , pages 3–8 . 
	</s>
	

	<s id="78">
		 D.-G. Lee and H.-C. Rim . 
	</s>
	

	<s id="79">
		 2004. ProKOMA : A probabilistic Korean morphological analyzer . 
	</s>
	

	<s id="80">
		 Technical Report KU-NLP-04-01 , Department of Computer Science and Engineering , Korea University . 
	</s>
	

	<s id="81">
		 S.-Z. Lee , Jun’ichi Tsujii , and H.-C. Rim . 
	</s>
	

	<s id="82">
		 2000. Hidden markov model-based Korean part-ofspeech tagging considering high agglutinativity , word-spacing , and lexical correlativity . 
	</s>
	

	<s id="83">
		 In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics . 
	</s>
	


</acldoc>
