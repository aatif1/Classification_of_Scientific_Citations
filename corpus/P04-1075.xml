<?xml version="1.0" encoding="iso-8859-1"?>
<acldoc acl_id="P04-1075">
	

	<s id="1">
		 Multi-Criteria- based Active Learning for Named Entity Recognition Dan Shen†‡1 Jie Zhang†‡ Jian Su† Guodong Zhou† Chew-Lim Tan‡ † Institute for Infocomm Technology ‡ Department of Computer Science 21 Heng Mui Keng Terrace National University of Singapore Singapore 119613 3 Science Drive 2 , Singapore 117543 {shendan,zhangjie,sujian,zhougd}@i2r.a-star.edu.sg {shendan,zhangjie,tancl}@comp.nus.edu.sg Abstract In this paper , we propose a multi-criteriabased active learning approach and effectively apply it to named entity recognition . 
	</s>
	

	<s id="2">
		 Active learning targets to minimize the human annotation efforts by selecting examples for labeling . 
	</s>
	

	<s id="3">
		 To maximize the contribution of the selected examples , we consider the multiple criteria : informativeness , representativeness and diversity and propose measures to quantify them . 
	</s>
	

	<s id="4">
		 More comprehensively , we incorporate all the criteria using two selection strategies , both of which result in less labeling cost than single-criterion-based method . 
	</s>
	

	<s id="5">
		 The results of the named entity recognition in both MUC-6 and GENIA show that the labeling cost can be reduced by at least 80 % without degrading the performance . 
	</s>
	

	<s id="6">
		 1 Introduction In the machine learning approaches of natural language processing ( NLP ) , models are generally trained on large annotated corpus . 
	</s>
	

	<s id="7">
		 However , annotating such corpus is expensive and time- consuming , which makes it difficult to adapt an existing model to a new domain . 
	</s>
	

	<s id="8">
		 In order to overcome this difficulty , active learning ( sample selection ) has been studied in more and more NLP applications such as POS tagging 
		<ref citStr="Engelson and Dagan 1999" id="1" label="CEPF" position="1693">
			( Engelson and Dagan 1999 )
		</ref>
		 , information extraction 
		<ref citStr="Thompson et al . 1999" id="2" label="CEPF" position="1744">
			( Thompson et al . 1999 )
		</ref>
		 , text classification 
		<ref citStr="Lewis and Catlett 1994" id="3" label="CEPF" position="1767">
			( Lewis and Catlett 1994 
		</ref>
		<ref citStr="McCallum and Nigam 1998" id="4" label="CEPF" position="1792">
			; McCallum and Nigam 1998 
		</ref>
		<ref citStr="Schohn and Cohn 2000" id="5" label="CEPF" position="1818">
			; Schohn and Cohn 2000 
		</ref>
		<ref citStr="Tong and Koller 2000" id="6" label="CEPF" position="1841">
			; Tong and Koller 2000 
		</ref>
		<ref citStr="Brinker 2003" id="7" label="CEPF" position="1864">
			; Brinker 2003 )
		</ref>
		 , statistical parsing 
		<ref citStr="Thompson et al . 1999" id="8" label="CEPF" position="1903">
			( Thompson et al . 1999 
		</ref>
		<ref citStr="Tang et al . 2002" id="9" label="CEPF" position="1927">
			; Tang et al . 2002 
		</ref>
		<ref citStr="Steedman et al . 2003" id="10" label="CEPF" position="1947">
			; Steedman et al . 2003 )
		</ref>
		 , noun phrase chunking 
		<ref citStr="Ngai and Yarowsky 2000" id="11" label="CEPF" position="2022">
			( Ngai and Yarowsky 2000 )
		</ref>
		 , etc. . 
	</s>
	

	<s id="9">
		 Active learning is based on the assumption that a small number of annotated examples and a large number of unannotated examples are available . 
	</s>
	

	<s id="10">
		 This assumption is valid in most NLP tasks . 
	</s>
	

	<s id="11">
		 Different from supervised learning in which the entire corpus are labeled manually , active learning is to select the most useful example for labeling and add the labeled example to training set to retrain model . 
	</s>
	

	<s id="12">
		 This procedure is repeated until the model achieves a certain level of performance . 
	</s>
	

	<s id="13">
		 Practically , a batch of examples are selected at a time , called batched- based sample selection 
		<ref citStr="Lewis and Catlett 1994" id="12" label="CEPF" position="2689">
			( Lewis and Catlett 1994 )
		</ref>
		 since it is time consuming to retrain the model if only one new example is added to the training set . 
	</s>
	

	<s id="14">
		 Many existing work in the area focus on two approaches : certainty-based methods 
		<ref citStr="Thompson et al . 1999" id="13" label="CEPF" position="2883">
			( Thompson et al . 1999 
		</ref>
		<ref citStr="Tang et al . 2002" id="14" label="CEPF" position="2907">
			; Tang et al . 2002 
		</ref>
		<ref citStr="Schohn and Cohn 2000" id="15" label="CEPF" position="2927">
			; Schohn and Cohn 2000 
		</ref>
		<ref citStr="Tong and Koller 2000" id="16" label="CEPF" position="2950">
			; Tong and Koller 2000 
		</ref>
		<ref citStr="Brinker 2003" id="17" label="CEPF" position="2973">
			; Brinker 2003 )
		</ref>
		 and committee-based methods 
		<ref citStr="McCallum and Nigam 1998" id="18" label="CEPF" position="3018">
			( McCallum and Nigam 1998 
		</ref>
		<ref citStr="Engelson and Dagan 1999" id="19" label="CEPF" position="3044">
			; Engelson and Dagan 1999 
		</ref>
		<ref citStr="Ngai and Yarowsky 2000" id="20" label="CEPF" position="3070">
			; Ngai and Yarowsky 2000 )
		</ref>
		 to select the most informative examples for which the current model are most uncertain . 
	</s>
	

	<s id="15">
		 Being the first piece of work on active learning for name entity recognition ( NER ) task , we target to minimize the human annotation efforts yet still reaching the same level of performance as a supervised learning approach . 
	</s>
	

	<s id="16">
		 For this purpose , we make a more comprehensive consideration on the contribution of individual examples , and more importantly maximizing the contribution of a batch based on three criteria : informativeness , representativeness and diversity . 
	</s>
	

	<s id="17">
		 First , we propose three scoring functions to quantify the informativeness of an example , which can be used to select the most uncertain examples . 
	</s>
	

	<s id="18">
		 Second , the representativeness measure is further proposed to choose the examples representing the majority . 
	</s>
	

	<s id="19">
		 Third , we propose two diversity considerations ( global and local ) to avoid repetition among the examples of a batch . 
	</s>
	

	<s id="20">
		 Finally , two combination strategies with the above three criteria are proposed to reach the maximum effectiveness on active learning for NER . 
	</s>
	

	<s id="21">
		 1 Current address of the first author : Universität des Saarlandes , Computational Linguistics Dept. , 66041 Saarbrü cken , Germany dshen@coli.uni-sb.de We build our NER model using Support Vector Machines ( SVM ) . 
	</s>
	

	<s id="22">
		 The experiment shows that our active learning methods achieve a promising result in this NER task . 
	</s>
	

	<s id="23">
		 The results in both MUC6 and GENIA show that the amount of the labeled training data can be reduced by at least 80 % without degrading the quality of the named entity recognizer . 
	</s>
	

	<s id="24">
		 The contributions not only come from the above measures , but also the two sample selection strategies which effectively incorporate informativeness , representativeness and diversity criteria . 
	</s>
	

	<s id="25">
		 To our knowledge , it is the first work on considering the three criteria all together for active learning . 
	</s>
	

	<s id="26">
		 Furthermore , such measures and strategies can be easily adapted to other active learning tasks as well . 
	</s>
	

	<s id="27">
		 2 Multi-criteria for NER Active Learning Support Vector Machines ( SVM ) is a powerful machine learning method , which has been applied successfully in NER tasks , such as 
		<ref citStr="Kazama et al . 2002" id="21" label="CEPF" position="5382">
			( Kazama et al . 2002 
		</ref>
		<ref citStr="Lee et al . 2003" id="22" label="CEPF" position="5404">
			; Lee et al . 2003 )
		</ref>
		 . 
	</s>
	

	<s id="28">
		 In this paper , we apply active learning methods to a simple and effective SVM model to recognize one class of names at a time , such as protein names , person names , etc. . 
	</s>
	

	<s id="29">
		 In NER , SVM is to classify a word into positive class “1” indicating that the word is a part of an entity , or negative class “-1” indicating that the word is not a part of an entity . 
	</s>
	

	<s id="30">
		 Each word in SVM is represented as a high-dimensional feature vector including surface word information , orthographic features , POS feature and semantic trigger features 
		<ref citStr="Shen et al . 2003" id="23" label="CEPF" position="6012">
			( Shen et al . 2003 )
		</ref>
		 . 
	</s>
	

	<s id="31">
		 The semantic trigger features consist of some special head nouns for an entity class which is supplied by users . 
	</s>
	

	<s id="32">
		 Furthermore , a window ( size = 7 ) , which represents the local context of the target word w , is also used to classify w . 
	</s>
	

	<s id="33">
		 However , for active learning in NER , it is not reasonable to select a single word without context for human to label . 
	</s>
	

	<s id="34">
		 Even if we require human to label a single word , he has to make an addition effort to refer to the context of the word . 
	</s>
	

	<s id="35">
		 In our active learning process , we select a word sequence which consists of a machine-annotated named entity and its context rather than a single word . 
	</s>
	

	<s id="36">
		 Therefore , all of the measures we propose for active learning should be applied to the machine- annotated named entities and we have to further study how to extend the measures for words to named entities . 
	</s>
	

	<s id="37">
		 Thus , the active learning in SVMbased NER will be more complex than that in simple classification tasks , such as text classification on which most SVM active learning works are conducted 
		<ref citStr="Schohn and Cohn 2000" id="24" label="CEPF" position="7111">
			( Schohn and Cohn 2000 
		</ref>
		<ref citStr="Tong and Koller 2000" id="25" label="CEPF" position="7134">
			; Tong and Koller 2000 
		</ref>
		<ref citStr="Brinker 2003" id="26" label="CEPF" position="7157">
			; Brinker 2003 )
		</ref>
		 . 
	</s>
	

	<s id="38">
		 In the next part , we will introduce informativeness , representativeness and diversity measures for the SVM-based NER . 
	</s>
	

	<s id="39">
		 2.1 Informativeness The basic idea of informativeness criterion is similar to certainty-based sample selection methods , which have been used in many previous works . 
	</s>
	

	<s id="40">
		 In our task , we use a distance-based measure to evaluate the informativeness of a word and extend it to the measure of an entity using three scoring functions . 
	</s>
	

	<s id="41">
		 We prefer the examples with high informative degree for which the current model are most uncertain . 
	</s>
	

	<s id="42">
		 2.1.1 Informativeness Measure for Word In the simplest linear form , training SVM is to find a hyperplane that can separate the positive and negative examples in training set with maximum margin . 
	</s>
	

	<s id="43">
		 The margin is defined by the distance of the hyperplane to the nearest of the positive and negative examples . 
	</s>
	

	<s id="44">
		 The training examples which are closest to the hyperplane are called support vectors . 
	</s>
	

	<s id="45">
		 In SVM , only the support vectors are useful for the classification , which is different from statistical models . 
	</s>
	

	<s id="46">
		 SVM training is to get these support vectors and their weights from training set by solving quadratic programming problem . 
	</s>
	

	<s id="47">
		 The support vectors can later be used to classify the test data . 
	</s>
	

	<s id="48">
		 Intuitively , we consider the informativeness of an example as how it can make effect on the support vectors by adding it to training set . 
	</s>
	

	<s id="49">
		 An example may be informative for the learner if the distance of its feature vector to the hyperplane is less than that of the support vectors to the hyper- plane ( equal to 1 ) . 
	</s>
	

	<s id="50">
		 This intuition is also justified by 
		<ref citStr="Schohn and Cohn 2000" id="27" label="CEPF" position="8900">
			( Schohn and Cohn 2000 
		</ref>
		<ref citStr="Tong and Koller 2000" id="28" label="CEPF" position="8923">
			; Tong and Koller 2000 )
		</ref>
		 based on a version space analysis . 
	</s>
	

	<s id="51">
		 They state that labeling an example that lies on or close to the hyperplane is guaranteed to have an effect on the solution . 
	</s>
	

	<s id="52">
		 In our task , we use the distance to measure the informativeness of an example . 
	</s>
	

	<s id="53">
		 The distance of a word’s feature vector to the hyperplane is computed as follows : where w is the feature vector of the word , ai , yi , si corresponds to the weight , the class and the feature vector of the ith support vector respectively . 
	</s>
	

	<s id="54">
		 N is the number of the support vectors in current model . 
	</s>
	

	<s id="55">
		 We select the example with minimal Dist , which indicates that it comes closest to the hyper- plane in feature space . 
	</s>
	

	<s id="56">
		 This example is considered most informative for current model . 
	</s>
	

	<s id="57">
		 2.1.2 Informativeness Measure for Named Entity N = ^ Dist(w) a iyik(si,w)+b = 1 i Based on the above informativeness measure for a word , we compute the overall informativeness degree of a named entity NE . 
	</s>
	

	<s id="58">
		 In this paper , we propose three scoring functions as follows . 
	</s>
	

	<s id="59">
		 Let NE = w1...wN in which wi is the feature vector of the ith word of NE . 
	</s>
	

	<s id="60">
		 • Info_Avg : The informativeness of NE is scored by the average distance of the words in NE to the hyperplane . 
	</s>
	

	<s id="61">
		 Info(NE) = 1^ ^ Dist(wi) wi^ NE where , wi is the feature vector of the ith word in NE . 
	</s>
	

	<s id="62">
		 • Info_Min : The informativeness of NE is scored by the minimal distance of the words in NE . 
	</s>
	

	<s id="63">
		 Info(NE) = 1^ Min{Dist(wi)} wi ^ NE • Info_S/N : If the distance of a word to the hy- perplane is less than a threshold a ( = 1 in our task ) , the word is considered with short distance . 
	</s>
	

	<s id="64">
		 Then , we compute the proportion of the number of words with short distance to the to- tal number of words in the named entity and use this proportion to quantify the informativeness of the named entity . 
	</s>
	

	<s id="65">
		 NUM ( Dist ( wi ) &lt; a ) NE N In Section 4.3 , we will evaluate the effectiveness of these scoring functions . 
	</s>
	

	<s id="66">
		 2.2 Representativeness In addition to the most informative example , we also prefer the most representative example . 
	</s>
	

	<s id="67">
		 The representativeness of an example can be evaluated based on how many examples there are similar or near to it . 
	</s>
	

	<s id="68">
		 So , the examples with high representative degree are less likely to be an outlier . 
	</s>
	

	<s id="69">
		 Adding them to the training set will have effect on a large number of unlabeled examples . 
	</s>
	

	<s id="70">
		 There are only a few works considering this selection criterion 
		<ref citStr="McCallum and Nigam 1998" id="29" label="CEPF" position="11479">
			( McCallum and Nigam 1998 
		</ref>
		<ref citStr="Tang et al . 2002" id="30" label="CEPF" position="11505">
			; Tang et al . 2002 )
		</ref>
		 and both of them are specific to their tasks , viz . 
	</s>
	

	<s id="71">
		 text classification and statistical parsing . 
	</s>
	

	<s id="72">
		 In this section , we compute the similarity between words using a general vector-based measure , extend this measure to named entity level using dynamic time warping algorithm and quantify the representativeness of a named entity by its density . 
	</s>
	

	<s id="73">
		 2.2.1 Similarity Measure between Words In general vector space model , the similarity between two vectors may be measured by computing the cosine value of the angle between them . 
	</s>
	

	<s id="74">
		 The smaller the angle is , the more similar between the vectors are . 
	</s>
	

	<s id="75">
		 This measure , called cosine-similarity measure , has been widely used in information retrieval tasks 
		<ref citStr="Baeza -Yates and Ribeiro-Neto 1999" id="31" label="CEPF" position="12308">
			( Baeza -Yates and Ribeiro-Neto 1999 )
		</ref>
		 . 
	</s>
	

	<s id="76">
		 In our task , we also use it to quantify the similarity between two words . 
	</s>
	

	<s id="77">
		 Particularly , the calculation in SVM need be projected to a higher dimensional space by using a certain kernel function K ( w i , w j ) . 
	</s>
	

	<s id="78">
		 Therefore , we adapt the cosine-similarity measure to SVM as follows : k(wi , wj ) (wi,wj)=k(wi,wi)k(wj,wj) where , wi and wj are the feature vectors of the words i and j . 
	</s>
	

	<s id="79">
		 This calculation is also supported by 
		<ref citStr="Brinker 2003" id="32" label="CEPF" position="12788">
			( Brinker 2003)
		</ref>
		’s work . 
	</s>
	

	<s id="80">
		 Furthermore , if we use the linear kernel k(wi , wj ) = wi ^ w j , the measure is the same as the traditional cosine similarity meas- ure cosq = wi ^ wj and may be regarded as a wi ^ wj general vector-based similarity measure . 
	</s>
	

	<s id="81">
		 2.2.2 Similarity Measure between Named Entities In this part , we compute the similarity between two machine-annotated named entities given the similarities between words . 
	</s>
	

	<s id="82">
		 Regarding an entity as a word sequence , this work is analogous to the alignment of two sequences . 
	</s>
	

	<s id="83">
		 We employ the dynamic time warping ( DTW ) algorithm 
		<ref citStr="Rabiner et al . 1978" id="33" label="CERF" position="13413">
			( Rabiner et al . 1978 )
		</ref>
		 to find an optimal alignment between the words in the sequences which maximize the accumulated similarity degree between the sequences . 
	</s>
	

	<s id="84">
		 Here , we adapt it to our task . 
	</s>
	

	<s id="85">
		 A sketch of the modified algorithm is as follows . 
	</s>
	

	<s id="86">
		 Let NE1 = w11w12 ... w1n...w1N , ( n = 1 , ... , N ) and NE2 = w21w22 ... w2m...w2M , ( m = 1 , ... , M ) denote two word sequences to be matched . 
	</s>
	

	<s id="87">
		 NE1 and NE2 consist of M and N words respectively . 
	</s>
	

	<s id="88">
		 NE1 ( n ) = w1n and NE2(m) = w2m . 
	</s>
	

	<s id="89">
		 A similarity value Sim(w1n , w2m ) has been known for every pair of words ( w1n , w2m ) within NE1 and NE2 . 
	</s>
	

	<s id="90">
		 The goal of DTW is to find a path , m = map(n) , which map n onto the corresponding m such that the accumulated similarity Sim* along the path is maximized . 
	</s>
	

	<s id="91">
		 N Sim*= Max { ^ Sim ( NE1(n) , NE2 ( map(n)) } { map(n) } n=1 A dynamic programming method is used to determine the optimum path map(n) . 
	</s>
	

	<s id="92">
		 The accumulated similarity SimA to any grid point ( n , m ) can be recursively calculated as SimA (n,m)= Sim (w1n,w2m)+MaxSimA(n^1,q) q^m Finally , Sim* = Sim A(N,M ) Certainly , the overall similarity measure Sim* has to be normalized as longer sequences normally give higher similarity value . 
	</s>
	

	<s id="93">
		 So , the similarity between two sequences NE1 and NE2 is calculated as Info ( NE ) = wi^ Sim 2.2.3 Representativeness Measure for Named Entity Given a set of machine-annotated named entities NESet = { NE1 , ... , NEN } , the representativeness of a named entity NE ; in NESet is quantified by its density . 
	</s>
	

	<s id="94">
		 The density of NE ; is defined as the average similarity between NE ; and all the other entities NEj in NESet as follows . 
	</s>
	

	<s id="95">
		 ( NE ; , NEj ) Dens;ty ( NE ; ) = j^ ; N^1 If NE ; has the largest density among all the entities in NESet , it can be regarded as the centroid of NE- Set and also the most representative examples in NESet . 
	</s>
	

	<s id="96">
		 2.3 Diversity Diversity criterion is to maximize the training utility of a batch . 
	</s>
	

	<s id="97">
		 We prefer the batch in which the examples have high variance to each other . 
	</s>
	

	<s id="98">
		 For example , given the batch size 5 , we try not to select five repetitious examples at a time . 
	</s>
	

	<s id="99">
		 To our knowledge , there is only one work 
		<ref citStr="Brinker 2003" id="34" label="CEPF" position="15669">
			( Brinker 2003 )
		</ref>
		 exploring this criterion . 
	</s>
	

	<s id="100">
		 In our task , we propose two methods : local and global , to make the examples diverse enough in a batch . 
	</s>
	

	<s id="101">
		 2.3.1 Global Consideration For a global consideration , we cluster all named entities in NESet based on the similarity measure proposed in Section 2.2.2 . 
	</s>
	

	<s id="102">
		 The named entities in the same cluster may be considered similar to each other , so we will select the named entities from different clusters at one time . 
	</s>
	

	<s id="103">
		 We employ a K- means clustering algorithm 
		<ref citStr="Jelinek 1997" id="35" label="CEPF" position="16209">
			( Jelinek 1997 )
		</ref>
		 , which is shown in Figure 1 . 
	</s>
	

	<s id="104">
		 Given : NESet = { NE1 , ... , NEN } Suppose : The number of clusters is K Initialization : Randomly equally partition { NE1 , ... , NEN } into K initial clusters Cj ( j = 1 , ... , K ) . 
	</s>
	

	<s id="105">
		 Loop until the number of changes for the centroids of all clusters is less than a threshold • Find the centroid of each cluster Cj ( j = 1 , ... , K ) . 
	</s>
	

	<s id="106">
		 NECent j = arg max( ^ S;m(NE ; , NE ) ) NE^ Cj NE;^ Cj • Repartition { NE1 , ... , NEN } into K clusters . 
	</s>
	

	<s id="107">
		 NE ; will be assigned to Cluster Cj if S;m(NE;,NECentj)^ S;m(NE;,NECentw),w ^ j Figure 1 : Global Consideration for Diversity : K- Means Clustering algorithm In each round , we need to compute the pair- wise similarities within each cluster to get the centroid of the cluster . 
	</s>
	

	<s id="108">
		 And then , we need to compute the similarities between each example and all centroids to repartition the examples . 
	</s>
	

	<s id="109">
		 So , the algorithm is time-consuming . 
	</s>
	

	<s id="110">
		 Based on the assumption that N examples are uniformly distributed between the K clusters , the time complexity of the algorithm is about O(N2/K+NK) 
		<ref citStr="Tang et al . 2002" id="36" label="CEPF" position="17355">
			( Tang et al . 2002 )
		</ref>
		 . 
	</s>
	

	<s id="111">
		 In one of our experiments , the size of the NESet ( N ) is around 17000 and K is equal to 50 , so the time complexity is about O(106) . 
	</s>
	

	<s id="112">
		 For efficiency , we may filter the entities in NESet before clustering them , which will be further discussed in Section 3 . 
	</s>
	

	<s id="113">
		 2.3.2 Local Consideration When selecting a machine-annotated named entity , we compare it with all previously selected named entities in the current batch . 
	</s>
	

	<s id="114">
		 If the similarity between them is above a threshold 8 , this example cannot be allowed to add into the batch . 
	</s>
	

	<s id="115">
		 The order of selecting examples is based on some measure , such as informativeness measure , representativeness measure or their combination . 
	</s>
	

	<s id="116">
		 This local selection method is shown in Figure 2 . 
	</s>
	

	<s id="117">
		 In this way , we avoid selecting too similar examples ( similarity value ^ 8 ) in a batch . 
	</s>
	

	<s id="118">
		 The threshold 8 may be the average similarity between the examples in NESet . 
	</s>
	

	<s id="119">
		 Given : NESet = { NE1 , ... , NEN } BatchSet with the maximal size K. Initialization : BatchSet = empty Loop until BatchSet is full • Select NE ; based on some measure from NESet . 
	</s>
	

	<s id="120">
		 • RepeatFlag = false ; • Loop from j = 1 to CurrentSize(BatchSet) If S;m ( NE;,NEj ) ^ b Then RepeatFlag = true ; Stop the Loop ; • If RepeatFlag == false Then add NE ; into BatchSet • remove NE ; from NESet Figure 2 : Local Consideration for Diversity This consideration only requires O(NK+K2) computational time . 
	</s>
	

	<s id="121">
		 In one of our experiments ( N ˜ 17000 and K = 50 ) , the time complexity is about O(105) . 
	</s>
	

	<s id="122">
		 It is more efficient than clustering algorithm described in Section 2.3.1 . 
	</s>
	

	<s id="123">
		 3 Sample Selection strategies In this section , we will study how to combine and strike a proper balance between these criteria , viz . 
	</s>
	

	<s id="124">
		 informativeness , representativeness and diversity , S;m(NE„ NE2 ) = S;m Max(N , M ) ^ S;m to reach the maximum effectiveness on NER active learning . 
	</s>
	

	<s id="125">
		 We build two strategies to combine the measures proposed above . 
	</s>
	

	<s id="126">
		 These strategies are based on the varying priorities of the criteria and the varying degrees to satisfy the criteria . 
	</s>
	

	<s id="127">
		 • Strategy 1 : We first consider the informativeness criterion . 
	</s>
	

	<s id="128">
		 We choose m examples with the most informativeness score from NESet to an intermediate set called INTERSet . 
	</s>
	

	<s id="129">
		 By this preselecting , we make the selection process faster in the later steps since the size of INTERSet is much smaller than that of NESet . 
	</s>
	

	<s id="130">
		 Then we cluster the examples in INTERSet and choose the centroid of each cluster into a batch called BatchSet . 
	</s>
	

	<s id="131">
		 The centroid of a cluster is the most representative example in that cluster since it has the largest density . 
	</s>
	

	<s id="132">
		 Furthermore , the examples in different clusters may be considered diverse to each other . 
	</s>
	

	<s id="133">
		 By this means , we consider representativeness and diversity criteria at the same time . 
	</s>
	

	<s id="134">
		 This strategy is shown in Figure 3 . 
	</s>
	

	<s id="135">
		 One limitation of this strategy is that clustering result may not reflect the distrib ution of whole sample space since we only cluster on INTERSet for efficiency . 
	</s>
	

	<s id="136">
		 The other is that since the representativeness of an example is only evaluated on a cluster . 
	</s>
	

	<s id="137">
		 If the cluster size is too small , the most representative example in this cluster may not be representative in the whole sample space . 
	</s>
	

	<s id="138">
		 Given : NESet = { NE1 , ... , NEN } BatchSet with the maximal size K. INTERSet with the maximal size M Steps : • BatchSet = ^ • INTERSet = ^ • Select M entities with most Info score from NESet to INTERSet . 
	</s>
	

	<s id="139">
		 • Cluster the entities in INTERSet into K clusters • Add the centroid entity of each cluster to BatchSet Figure 3 : Sample Selection Strategy 1 • Strategy 2 : ( Figure 4 ) We combine the infor- mativeness and representativeness criteria using the functio l Info(NE) + ( 1^ l )Density(NEi) , in which the Info and Density value of NEi are normalized first . 
	</s>
	

	<s id="140">
		 The individual importance of each criterion in this function is adjusted by the trade- off parameter l ( 0 ^ l ^1 ) ( set to 0.6 in our experiment ) . 
	</s>
	

	<s id="141">
		 First , we select a candidate example NEi with the maximum value of this function from NESet . 
	</s>
	

	<s id="142">
		 Second , we consider diversity criterion using the local method in Section 3.3.2 . 
	</s>
	

	<s id="143">
		 We add the candidate example NEi to a batch only if NEi is different enough from any previously selected example in the batch . 
	</s>
	

	<s id="144">
		 The threshold ß is set to the average pair-wise similarity of the entities in NE- Set . 
	</s>
	

	<s id="145">
		 Given : NESet = { NE1 , ... , NEN } BatchSet with the maximal size K. Initialization : BatchSet = ^ Loop until BatchSet is full • Select NEi which have the maximum value for the combination function between Info score and Density socre from NESet . 
	</s>
	

	<s id="146">
		 NEi = argMax(l Info (NEi)+ ( 1^ l )Density(NEi)) NEi^ NESet • RepeatFlag = false ; • Loop from j = 1 to CurrentSize(BatchSet) If Sim ( NEi , NEj ) ^ b Then RepeatFlag = true ; Stop the Loop ; • If RepeatFlag == false Then add NEi into BatchSet • remove NEi from NESet Figure 4 : Sample Selection Strategy 2 4 Experimental Results and Analysis 4.1 Experiment Settings In order to evaluate the effectiveness of our selection strategies , we apply them to recognize protein ( PRT ) names in biomedical domain using GENIA corpus V 1.1 
		<ref citStr="Ohta et al . 2002" id="37" label="OEPF" position="22794">
			( Ohta et al . 2002 )
		</ref>
		 and person ( PER ) , location ( LOC ) , organization ( ORG ) names in newswire domain using MUC-6 corpus . 
	</s>
	

	<s id="147">
		 First , we randomly split the whole corpus into three parts : an initial training set to build an initial model , a test set to evaluate the performance of the model and an unlabeled set to select examples . 
	</s>
	

	<s id="148">
		 The size of each data set is shown in Table 1 . 
	</s>
	

	<s id="149">
		 Then , iteratively , we select a batch of examples following the selection strategies proposed , require human experts to label them and add them into the training set . 
	</s>
	

	<s id="150">
		 The batch size K = 50 in GENIA and 10 in MUC-6 . 
	</s>
	

	<s id="151">
		 Each example is defined as a machine-recognized named entity and its context words ( previous 3 words and next 3 words ) . 
	</s>
	

	<s id="152">
		 Domain Class Corpus Initial Training Set Test Set Unlabeled Set Biomedical PRT GENIA1.1 10 sent . 
	</s>
	

	<s id="153">
		 ( 277 words ) 900 sent . 
	</s>
	

	<s id="154">
		 ( 26K words ) 8004 sent . 
	</s>
	

	<s id="155">
		 ( 223K words ) Newswire PER MUC-6 5 sent . 
	</s>
	

	<s id="156">
		 ( 131 words ) 602 sent . 
	</s>
	

	<s id="157">
		 ( 14K words ) 7809 sent . 
	</s>
	

	<s id="158">
		 ( 157K words ) LOC 5 sent . 
	</s>
	

	<s id="159">
		 ( 130 words ) 7809 sent . 
	</s>
	

	<s id="160">
		 ( 157K words ) ORG 5 sent . 
	</s>
	

	<s id="161">
		 ( 113 words ) 7809 sent . 
	</s>
	

	<s id="162">
		 ( 157K words ) Table 1 : Experiment settings for active learning using GENIA1 . 
	</s>
	

	<s id="163">
		 1(PRT) and MUC-6(PER,LOC,ORG) The goal of our work is to minimize the human annotation effort to learn a named entity recognizer with the same performance level as supervised learning . 
	</s>
	

	<s id="164">
		 The performance of our model is evaluated using “precision/recall/F-measure” . 
	</s>
	

	<s id="165">
		 4.2 Overall Result in GENIA and MUC-6 In this section , we evaluate our selection strategies by comparing them with a random selection method , in which a batch of examples is randomly selected iteratively , on GENIA and MUC-6 corpus . 
	</s>
	

	<s id="166">
		 Table 2 shows the amount of training data needed to achieve the performance of supervised learning using various selection methods , viz . 
	</s>
	

	<s id="167">
		 Random , Strategy1 and Strategy2 . 
	</s>
	

	<s id="168">
		 In GENIA , we find : • The model achieves 63.3 F-measure using 223K words in the supervised learning . 
	</s>
	

	<s id="169">
		 • The best performer is Strategy2 ( 31K words ) , requiring less than 40 % of the training data that Random ( 83K words ) does and 14 % of the training data that the supervised learning does . 
	</s>
	

	<s id="170">
		 • Strategy1 ( 40K words ) performs slightly worse than Strategy2 , requiring 9K more words . 
	</s>
	

	<s id="171">
		 It is probably because Strategy1 cannot avoid selecting outliers if a cluster is too small . 
	</s>
	

	<s id="172">
		 • Random ( 83K words ) requires about 37 % of the training data that the supervised learning does . 
	</s>
	

	<s id="173">
		 It indicates that only the words in and around a named entity are useful for classification and the words far from the named entity may not be helpful . 
	</s>
	

	<s id="174">
		 Class Supervised Random Strategy1 Strategy2 PRT 223K ( F=63.3 ) 83K 40K 31K PER 157K ( F=90.4 ) 11.5K 4.2K 3.5K LOC 157K ( F=73.5 ) 13.6K 3.5K 2.1K ORG 157K ( F=86.0 ) 20.2K 9.5K 7.8K Table 2 : Overall Result in GENIA and MUC-6 Furthermore , when we apply our model to newswire domain ( MUC-6 ) to recognize person , location and organization names , Strategy1 and Strategy2 show a more promising result by comparing with the supervised learning and Random , as shown in Table 2 . 
	</s>
	

	<s id="175">
		 On average , about 95 % of the data can be reduced to achieve the same performance with the supervised learning in MUC-6 . 
	</s>
	

	<s id="176">
		 It is probably because NER in the newswire domain is much simpler than that in the biomedical domain 
		<ref citStr="Shen et al . 2003" id="38" label="CEPF" position="26343">
			( Shen et al . 2003 )
		</ref>
		 and named entities are less and distributed much sparser in the newswire texts than in the biomedical texts . 
	</s>
	

	<s id="177">
		 4.3 Effectiveness of Informativeness-based Selection Method In this section , we investigate the effectiveness of informativeness criterion in NER task . 
	</s>
	

	<s id="178">
		 Figure 5 shows a plot of training data size versus F-measure achieved by the informativeness-based measures in Section 3.1.2 : Info_Avg , Info_Min and Info_S/N as well as Random . 
	</s>
	

	<s id="179">
		 We make the comparisons in GENIA corpus . 
	</s>
	

	<s id="180">
		 In Figure 5 , the horizontal line is the performance level ( 63.3 F-measure ) achieved by supervised learning ( 223K words ) . 
	</s>
	

	<s id="181">
		 We find that the three informativeness-based measures perform similarly and each of them outperforms Random . 
	</s>
	

	<s id="182">
		 Table 3 highlights the various data sizes to achieve the peak performance using these selection methods . 
	</s>
	

	<s id="183">
		 We find that Random ( 83K words ) on average requires over 1.5 times as much as data to achieve the same performance as the informativeness-based selection methods ( 52K words ) . 
	</s>
	

	<s id="184">
		 Figure 5 : Active learning curves : effectiveness of the three informativeness-criterion-based selections comparing with the Random selection . 
	</s>
	

	<s id="185">
		 Supervised Random Info_Avg Info_Min Info_ S/N 223K 83K 52.0K 51.9K 52.3K Table 3 : Training data sizes for various selection methods to achieve the same performance level as the supervised learning 4.4 Effectiveness of Two Sample Selection Strategies In addition to the informativeness criterion , we further incorporate representativeness and diversity criteria into active learning using two strategies described in Section 3 . 
	</s>
	

	<s id="186">
		 Comparing the two strategies with the best result of the single-criterionbased selection methods Info_Min , we are to justify that representativeness and diversity are also important factors for active learning . 
	</s>
	

	<s id="187">
		 Figure 6 shows the learning curves for the various methods : Strategy1 , Strategy2 and Info_Min . 
	</s>
	

	<s id="188">
		 In the beginning iterations ( F-measure &lt; 60 ) , the three methods performed similarly . 
	</s>
	

	<s id="189">
		 But with the larger training set , the efficiencies of Stratety1 and Strategy2 begin to be evident . 
	</s>
	

	<s id="190">
		 Table 4 highlights the final result of the three methods . 
	</s>
	

	<s id="191">
		 In order to reach the performance of supervised learning , Strategy1 ( 40K words ) and Strategyy2 ( 31K words ) require about 80 % and 60 % of the data that Info_Min ( 51.9K ) does . 
	</s>
	

	<s id="192">
		 So we believe the effective combinations of informativeness , representativeness and diversity will help to learn the NER model more quickly and cost less in annotation . 
	</s>
	

	<s id="193">
		 0 20 40 60 K words 80 0.65 0.55 F 0.5 0.6 Supervised Random Info_Min Info_S/N Info_Avg 0 20 40 60 K words Figure 6 : Active learning curves : effectiveness of the two multi-criteria-based selection strategies comparing with the informativeness-criterion-based selection ( Info_Min ) . 
	</s>
	

	<s id="194">
		 Info_Min Strategy 1 Strategy2 51.9K 40K 31K Table 4 : Comparisons of training data sizes for the multicriteria-based selection strategies and the informativenesscriterion-based selection ( Info_Min ) to achieve the same performance level as the supervised learning . 
	</s>
	

	<s id="195">
		 5 Related Work Since there is no study on active learning for NER task previously , we only introduce general active learning methods here . 
	</s>
	

	<s id="196">
		 Many existing active learning methods are to select the most uncertain examples using various measures 
		<ref citStr="Thompson et al . 1999" id="39" label="CEPN" position="29820">
			( Thompson et al . 1999 
		</ref>
		<ref citStr="Schohn and Cohn 2000" id="40" label="CEPN" position="29844">
			; Schohn and Cohn 2000 
		</ref>
		<ref citStr="Tong and Koller 2000" id="41" label="CEPN" position="29867">
			; Tong and Koller 2000 
		</ref>
		<ref citStr="Engelson and Dagan 1999" id="42" label="CEPN" position="29890">
			; Engelson and Dagan 1999 
		</ref>
		<ref citStr="Ngai and Yarowsky 2000" id="43" label="CEPN" position="29916">
			; Ngai and Yarowsky 2000 )
		</ref>
		 . 
	</s>
	

	<s id="197">
		 Our informativeness-based measure is similar to these works . 
	</s>
	

	<s id="198">
		 However these works just follow a single criterion . 
	</s>
	

	<s id="199">
		 
		<ref citStr="McCallum and Nigam 1998" id="44" label="CEPF" position="30087">
			( McCallum and Nigam 1998 
		</ref>
		<ref citStr="Tang et al . 2002" id="45" label="CEPF" position="30113">
			; Tang et al . 2002 )
		</ref>
		 are the only two works considering the representativeness criterion in active learning . 
	</s>
	

	<s id="200">
		 
		<ref citStr="Tang et al . 2002" id="46" label="CJPF" position="30254">
			( Tang et al . 2002 )
		</ref>
		 use the density information to weight the selected examples while we use it to select examples . 
	</s>
	

	<s id="201">
		 Moreover , the representativeness measure we use is relatively general and easy to adapt to other tasks , in which the example selected is a sequence of words , such as text chunking , POS tagging , etc. . 
	</s>
	

	<s id="202">
		 On the other hand , 
		<ref citStr="Brinker 2003" id="47" label="CEPF" position="30612">
			( Brinker 2003 )
		</ref>
		 first incorporate diversity in active learning for text classification . 
	</s>
	

	<s id="203">
		 Their work is similar to our local consideration in Section 2.3.2 . 
	</s>
	

	<s id="204">
		 However , he didn’t further explore how to avoid selecting outliers to a batch . 
	</s>
	

	<s id="205">
		 So far , we haven’t found any previous work integrating the informativeness , representativeness and diversity all together . 
	</s>
	

	<s id="206">
		 6 Conclusion and Future Work In this paper , we study the active learning in a more complex NLP task , named entity recognition . 
	</s>
	

	<s id="207">
		 We propose a multi-criteria-based approach to select examples based on their informativeness , representativeness and diversity , which are incorporated all together by two strategies ( local and global ) . 
	</s>
	

	<s id="208">
		 Experiments show that , in both MUC6 and GENIA , both of the two strategies combining the three criteria outperform the single criterion ( informativeness ) . 
	</s>
	

	<s id="209">
		 The labeling cost can be significantly reduced by at least 80 % comparing with the supervised learning . 
	</s>
	

	<s id="210">
		 To our best knowledge , this is not only the first work to report the empirical results of active learning for NER , but also the first work to incorporate the three criteria all together for selecting examples . 
	</s>
	

	<s id="211">
		 Although the current experiment results are very promising , some parameters in our experi- ment , such as the batch size K and the l in the function of strategy 2 , are decided by our experience in the domain . 
	</s>
	

	<s id="212">
		 In practical application , the optimal value of these parameters should be decided automatically based on the training process . 
	</s>
	

	<s id="213">
		 Furthermore , we will study how to overcome the limitation of the strategy 1 discussed in Section 3 by using more effective clustering algorithm . 
	</s>
	

	<s id="214">
		 Another interesting work is to study when to stop active learning . 
	</s>
	

	<s id="215">
		 References R. Baeza-Yates and B. Ribeiro-Neto . 
	</s>
	

	<s id="216">
		 1999. Modern Information Retrieval . 
	</s>
	

	<s id="217">
		 ISBN 0-201-39829-X. K. Brinker . 
	</s>
	

	<s id="218">
		 2003 . 
	</s>
	

	<s id="219">
		 Incorporating Diversity in Active Learning with Support Vector Machines . 
	</s>
	

	<s id="220">
		 In Proceedings of ICML , 2003 . 
	</s>
	

	<s id="221">
		 S. A. Engelson and I. Dagan . 
	</s>
	

	<s id="222">
		 1999. Committee- Based Sample Selection for Probabilistic Classifiers . 
	</s>
	

	<s id="223">
		 Journal of Artifical Intelligence Research . 
	</s>
	

	<s id="224">
		 F. Jelinek . 
	</s>
	

	<s id="225">
		 1997. Statistical Methods for Speech Recognition . 
	</s>
	

	<s id="226">
		 MIT Press . 
	</s>
	

	<s id="227">
		 J. Kazama , T. Makino , Y. Ohta and J. Tsujii . 
	</s>
	

	<s id="228">
		 2002 . 
	</s>
	

	<s id="229">
		 Tuning Support Vector Machines for Biomedical Named Entity Recognition . 
	</s>
	

	<s id="230">
		 In Proceedings of the ACL2002 Workshop on NLP in Biomedicine . 
	</s>
	

	<s id="231">
		 K. J. Lee , Y. S. Hwang and H. C. Rim . 
	</s>
	

	<s id="232">
		 2003 . 
	</s>
	

	<s id="233">
		 Two- Phase Biomedical NE Recognition based on SVMs . 
	</s>
	

	<s id="234">
		 In Proceedings of the ACL2003 Work- shop on NLP in Biomedicine . 
	</s>
	

	<s id="235">
		 D. D. Lewis and J. Catlett . 
	</s>
	

	<s id="236">
		 1994. Heterogeneous Uncertainty Sampling for Supervised Learning . 
	</s>
	

	<s id="237">
		 In Proceedings of ICML , 1994 . 
	</s>
	

	<s id="238">
		 A. McCallum and K. Nigam . 
	</s>
	

	<s id="239">
		 1998. Employing EM in Pool-Based Active Learning for Text Classification . 
	</s>
	

	<s id="240">
		 In Proceedings of ICML , 1998 . 
	</s>
	

	<s id="241">
		 G. Ngai and D. Yarowsky . 
	</s>
	

	<s id="242">
		 2000. Rule Writing or Annotation : Cost-efficient Resource Usage for Base Noun Phrase Chunking . 
	</s>
	

	<s id="243">
		 In Proceedings of ACL , 2000 . 
	</s>
	

	<s id="244">
		 0.65 F 0.55 0.6 0.5 Supervised Info_Min Strategy1 Strategy2 T. Ohta , Y. Tateisi , J. Kim , H. Mima and J. Tsujii . 
	</s>
	

	<s id="245">
		 2002. The GENIA corpus : An annotated research abstract corpus in molecular biology domain . 
	</s>
	

	<s id="246">
		 In Proceedings of HLT 2002 . 
	</s>
	

	<s id="247">
		 L. R. Rabiner , A. E. Rosenberg and S. E. Levinson . 
	</s>
	

	<s id="248">
		 1978. Considerations in Dynamic Time Warping Algorithms for Discrete Word Recognition . 
	</s>
	

	<s id="249">
		 In Proceedings of IEEE Transactions on acoustics , speech and signal processing . 
	</s>
	

	<s id="250">
		 Vol. ASSP-26 , NO.6 . 
	</s>
	

	<s id="251">
		 D. Schohn and D. Cohn . 
	</s>
	

	<s id="252">
		 2000. Less is More : Active Learning with Support Vector Machines . 
	</s>
	

	<s id="253">
		 In Proceedings of the 17th International Conference on Machine Learning . 
	</s>
	

	<s id="254">
		 D. Shen , J. Zhang , G. D. Zhou , J. Su and C. L. Tan . 
	</s>
	

	<s id="255">
		 2003. Effective Adaptation of a Hidden Markov Model-based Named Entity Recognizer for Biomedical Domain . 
	</s>
	

	<s id="256">
		 In Proceedings of the ACL2003 Workshop on NLP in Biomedicine . 
	</s>
	

	<s id="257">
		 M. Steedman , R. Hwa , S. Clark , M. Osborne , A. Sarkar , J. Hockenmaier , P. Ruhlen , S. Baker and J. Crim . 
	</s>
	

	<s id="258">
		 2003 . 
	</s>
	

	<s id="259">
		 Example Selection for Bootstrapping Statistical Parsers . 
	</s>
	

	<s id="260">
		 In Proceedings of HLTNAACL , 2003 . 
	</s>
	

	<s id="261">
		 M. Tang , X. Luo and S. Roukos . 
	</s>
	

	<s id="262">
		 2002. Active Learning for Statistical Natural Language Parsing . 
	</s>
	

	<s id="263">
		 In Proceedings of the ACL 2002 . 
	</s>
	

	<s id="264">
		 C. A. Thompson , M. E. Califf and R. J. Mooney . 
	</s>
	

	<s id="265">
		 1999. Active Learning for Natural Language Parsing and Information Extraction . 
	</s>
	

	<s id="266">
		 In Proceedings of ICML 1999 . 
	</s>
	

	<s id="267">
		 S. Tong and D. Koller . 
	</s>
	

	<s id="268">
		 2000. Support Vector Machine Active Learning with Applications to Text Classification . 
	</s>
	

	<s id="269">
		 Journal of Machine Learning Research . 
	</s>
	

	<s id="270">
		 V. Vapnik . 
	</s>
	

	<s id="271">
		 1998. Statistical learning theory . 
	</s>
	

	<s id="272">
		 N.Y.:John Wiley . 
	</s>
	


</acldoc>
