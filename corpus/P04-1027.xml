<?xml version="1.0" encoding="iso-8859-1"?>
<acldoc acl_id="P04-1027">
	

	<s id="1">
		 An Empirical Study of Information Synthesis Tasks Enrique Amig´o Julio Gonzalo Victor Peinado Anselmo Pe˜nas Felisa Verdejo Departamento de Lenguajes y Sistemas Inform´aticos Universidad Nacional de Educaci´on a Distancia c/Juan del Rosal , 16 - 28040 Madrid - Spain {enrique,julio,victor,anselmo,felisa}@lsi.uned.es Abstract This paper describes an empirical study of the “Information Synthesis” task , defined as the process of ( given a complex information need ) extracting , organizing and inter-relating the pieces of information contained in a set of relevant documents , in order to obtain a comprehensive , non redundant report that satisfies the information need . 
	</s>
	

	<s id="2">
		 Two main results are presented : a ) the creation of an Information Synthesis testbed with 72 reports manually generated by nine subjects for eight complex topics with 100 relevant documents each ; and b ) an empirical comparison of similarity metrics between reports , under the hypothesis that the best metric is the one that best distinguishes between manual and automatically generated reports . 
	</s>
	

	<s id="3">
		 A metric based on key concepts overlap gives better results than metrics based on n-gram overlap ( such as ROUGE ) or sentence overlap . 
	</s>
	

	<s id="4">
		 1 Introduction A classical Information Retrieval ( IR ) system helps the user finding relevant documents in a given text collection . 
	</s>
	

	<s id="5">
		 In most occasions , however , this is only the first step towards fulfilling an information need . 
	</s>
	

	<s id="6">
		 The next steps consist of extracting , organizing and relating the relevant pieces of information , in order to obtain a comprehensive , non redundant report that satisfies the information need . 
	</s>
	

	<s id="7">
		 In this paper , we will refer to this process as Information Synthesis . 
	</s>
	

	<s id="8">
		 It is normally understood as an ( intellectually challenging ) human task , and per- haps the Google Answer Service ' is the best gen- eral purpose illustration of how it works . 
	</s>
	

	<s id="9">
		 In this service , users send complex queries which cannot be answered simply by inspecting the first two or three documents returned by a search engine . 
	</s>
	

	<s id="10">
		 These are a couple of real , representative examples : a ) I’m looking for information concerning the history of text compression both before and with computers . 
	</s>
	

	<s id="11">
		 lhttp://answers.google.com b ) Provide an analysis on the future of web browsers , if any . 
	</s>
	

	<s id="12">
		 Answers to such complex information needs are provided by experts which , commonly , search the Internet , select the best sources , and assemble the most relevant pieces of information into a report , organizing the most important facts and providing additional web hyperlinks for further reading . 
	</s>
	

	<s id="13">
		 This Information Synthesis task is understood , in Google Answers , as a human task for which a search engine only provides the initial starting point . 
	</s>
	

	<s id="14">
		 Our midterm goal is to develop computer assistants that help users to accomplish Information Synthesis tasks . 
	</s>
	

	<s id="15">
		 From a Computational Linguistics point of view , Information Synthesis can be seen as a kind of topic-oriented , informative multi-document summarization , where the goal is to produce a single text as a compressed version of a set of documents with a minimum loss of relevant information . 
	</s>
	

	<s id="16">
		 Unlike indicative summaries ( which help to determine whether a document is relevant to a particular topic ) , informative summaries must be helpful to answer , for instance , factual questions about the topic . 
	</s>
	

	<s id="17">
		 In the remainder of the paper , we will use the term “reports” to refer to the summaries produced in an Information Synthesis task , in order to distinguish them from other kinds of summaries . 
	</s>
	

	<s id="18">
		 Topic-oriented multi-document summarization has already been studied in other evaluation initiatives which provide testbeds to compare alternative approaches 
		<ref citStr="Over , 2003" id="1" label="CEPN" position="3888">
			( Over , 2003 
		</ref>
		<ref citStr="Goldstein et al. , 2000" id="2" label="CEPN" position="3902">
			; Goldstein et al. , 2000 
		</ref>
		<ref citStr="Radev et al. , 2000" id="3" label="CEPN" position="3928">
			; Radev et al. , 2000 )
		</ref>
		 . 
	</s>
	

	<s id="19">
		 Unfortunately , those studies have been restricted to very small summaries ( around 100 words ) and small document sets ( 10- 20 documents ) . 
	</s>
	

	<s id="20">
		 These are relevant summarization tasks , but hardly representative of the Information Synthesis problem we are focusing on . 
	</s>
	

	<s id="21">
		 The first goal of our work has been , therefore , to create a suitable testbed that permits qualitative and quantitative studies on the information synthesis task . 
	</s>
	

	<s id="22">
		 Section 2 describes the creation of such a testbed , which includes the manual generation of 72 reports by nine different subjects across 8 complex topics with 100 relevant documents per topic . 
	</s>
	

	<s id="23">
		 Using this testbed , our second goal has been to compare alternative similarity metrics for the Information Synthesis task . 
	</s>
	

	<s id="24">
		 A good similarity metric provides a way of evaluating Information Synthesis systems ( comparing their output with manually generated reports ) , and should also shed some light on the common properties of manually generated reports . 
	</s>
	

	<s id="25">
		 Our working hypothesis is that the best metric will best distinguish between manual and automatically generated reports . 
	</s>
	

	<s id="26">
		 We have compared several similarity metrics , including a few baseline measures ( based on document , sentence and vocabulary overlap ) and a state- of-the-art measure to evaluate summarization systems , ROUGE 
		<ref citStr="Lin and Hovy , 2003" id="4" label="OEPF" position="5368">
			( Lin and Hovy , 2003 )
		</ref>
		 . 
	</s>
	

	<s id="27">
		 We also introduce another proximity measure based on key concept overlap , which turns out to be substantially better than ROUGE for a relevant class of topics . 
	</s>
	

	<s id="28">
		 Section 3 describes these metrics and the experimental design to compare them ; in Section 4 , we analyze the outcome of the experiment , and Section 5 discusses related work . 
	</s>
	

	<s id="29">
		 Finally , Section 6 draws the main conclusions of this work . 
	</s>
	

	<s id="30">
		 2 Creation of an Information Synthesis testbed We refer to Information Synthesis as the process of generating a topic-oriented report from a nontrivial amount of relevant , possibly interrelated documents . 
	</s>
	

	<s id="31">
		 The first goal of our work is the generation of a testbed ( ISCORPUS ) with manually produced reports that serve as a starting point for further empirical studies and evaluation of information synthesis systems . 
	</s>
	

	<s id="32">
		 This section describes how this testbed has been built . 
	</s>
	

	<s id="33">
		 2.1 Document collection and topic set The testbed must have a certain number of features which , altogether , differentiate the task from current multi-document summarization evaluations : Complex information needs . 
	</s>
	

	<s id="34">
		 Being Information Synthesis a step which immediately follows a document retrieval process , it seems natural to start with standard IR topics as used in evaluation conferences such as TREC2 , CLEF3 or NTCIR4 . 
	</s>
	

	<s id="35">
		 The title/description/narrative topics commonly used in such evaluation exercises are specially well suited for an Information Synthesis task : they are complex 2http://trec.nist.gov 3http://www.clef-campaign.org 4http://research.nii.ac.jp/ntcir/ and well defined , unlike , for instance , typical web queries . 
	</s>
	

	<s id="36">
		 We have selected the Spanish CLEF 2001-2003 news collection testbed 
		<ref citStr="Peters et al. , 2002" id="5" label="CEPF" position="7170">
			( Peters et al. , 2002 )
		</ref>
		 , because Spanish is the native language of the subjects recruited for the manual generation of reports . 
	</s>
	

	<s id="37">
		 Out of the CLEF topic set , we have chosen the eight topics with the largest number of documents manually judged as relevant from the assessment pools . 
	</s>
	

	<s id="38">
		 We have slightly reworded the topics to change the document retrieval focus ( “Find documents that...” ) into an information synthesis wording ( “Generate a report about...” ) . 
	</s>
	

	<s id="39">
		 Table 1 shows the eight selected topics . 
	</s>
	

	<s id="40">
		 C042 : Generate a report about the invasion of Haiti by UN/US soldiers . 
	</s>
	

	<s id="41">
		 C045 : Generate a report about the main negotiators of the Middle East peace treaty between Israel and Jordan , giving detailed information on the treaty . 
	</s>
	

	<s id="42">
		 C47 : What are the reasons for the military intervention of Russia in Chechnya ? 
	</s>
	

	<s id="43">
		 C48 : Reasons for the withdrawal of United Nations ( UN ) peace- keeping forces from Bosnia . 
	</s>
	

	<s id="44">
		 C050 : Generate a report about the uprising of Indians in Chiapas ( Mexico ) . 
	</s>
	

	<s id="45">
		 C085 : Generate a report about the operation “Turquoise” , the French humanitarian program in Rwanda . 
	</s>
	

	<s id="46">
		 C056 : Generate a report about campaigns against racism in Europe . 
	</s>
	

	<s id="47">
		 C080 : Generate a report about hunger strikes attempted in order to attract attention to a cause . 
	</s>
	

	<s id="48">
		 Table 1 : Topic set This set of eight CLEF topics has two differentiated subsets : in a majority of cases ( first six topics ) , it is necessary to study how a situation evolves in time ; the importance of every event related to the topic can only be established in relation with the others . 
	</s>
	

	<s id="49">
		 The invasion of Haiti by UN and USA troops ( C042 ) is an example of such a topic . 
	</s>
	

	<s id="50">
		 We will refer to them as “Topic Tracking” ( TT ) reports , because they resemble the kind of topics used in such task . 
	</s>
	

	<s id="51">
		 The last two questions ( 56 and 80 ) , however , resemble Information Extraction tasks : essentially , the user has to detect and describe instances of a generic event ( cases of hunger strikes and campaigns against racism in Europe ) ; hence we will refer to them as “IE” reports . 
	</s>
	

	<s id="52">
		 Topic tracking reports need a more elaborated treatment of the information in the documents , and therefore are more interesting from the point of view of Information Synthesis . 
	</s>
	

	<s id="53">
		 We have , however , decided to keep the two IE topics ; first , because they also reflect a realistic synthesis task ; and second , because they can provide contrastive information as compared to TT reports . 
	</s>
	

	<s id="54">
		 Large document sets . 
	</s>
	

	<s id="55">
		 All the selected CLEF topics have more than one hundred documents judged as relevant by the CLEF assessors . 
	</s>
	

	<s id="56">
		 For homogeneity , we have restricted the task to the first 100 documents for each topic ( using a chronological order ) . 
	</s>
	

	<s id="57">
		 Complex reports . 
	</s>
	

	<s id="58">
		 The elaboration of a comprehensive report requires more space than is allowed in current multi-document summarization experiences . 
	</s>
	

	<s id="59">
		 We have established a maximum of fifty sentences per summary , i.e. , half a sentence per document . 
	</s>
	

	<s id="60">
		 This limit satisfies three conditions : a ) it is large enough to contain the essential information about the topic , b ) it requires a substantial compression effort from the user , and c ) it avoids defaulting to a “first sentence” strategy by lazy ( or tired ) users , because this strategy would double the maximum size allowed . 
	</s>
	

	<s id="61">
		 We decided that the report generation would be an extractive task , which consists of selecting sentences from the documents . 
	</s>
	

	<s id="62">
		 Obviously , a realistic information synthesis process also involves rewriting and elaboration of the texts contained in the documents . 
	</s>
	

	<s id="63">
		 Keeping the task extractive has , however , two major advantages : first , it permits a direct comparison to automatic systems , which will typically be extractive ; and second , it is a simpler task which produces less fatigue . 
	</s>
	

	<s id="64">
		 2.2 Generation of manual reports Nine subjects between 25 and 35 years-old were recruited for the manual generation of reports . 
	</s>
	

	<s id="65">
		 All of them self-reported university degrees and a large experience using search engines and performing information searches . 
	</s>
	

	<s id="66">
		 All subjects were given an in-place detailed description of the task in order to minimize divergent interpretations . 
	</s>
	

	<s id="67">
		 They were told that , in a first step , they had to generate reports with a maximum of information about every topic within the fifty sentence space limit . 
	</s>
	

	<s id="68">
		 In a second step , which would take place six months afterwards , they would be examined from each of the eight topics . 
	</s>
	

	<s id="69">
		 The only documentation allowed during the exam would be the reports generated in the first phase of the experiment . 
	</s>
	

	<s id="70">
		 Subjects scoring best would be rewarded . 
	</s>
	

	<s id="71">
		 These instructions had two practical effects : first , the competitive setup was an extra motivation for achieving better results . 
	</s>
	

	<s id="72">
		 And second , users tried to take advantage of all available space , and thus most reports were close to the fifty sentences limit . 
	</s>
	

	<s id="73">
		 The time limit per topic was set to 30 minutes , which is tight for the information synthesis task , but prevents the effects of fatigue . 
	</s>
	

	<s id="74">
		 We implemented an interface to facilitate the generation of extractive reports . 
	</s>
	

	<s id="75">
		 The system displays a list with the titles of relevant documents in chronological order . 
	</s>
	

	<s id="76">
		 Clicking on a title displays the full document , where the user can select any sentence(s) and add them to the final report . 
	</s>
	

	<s id="77">
		 A different frame displays the selected sentences ( also in chronological order ) , together with one bar indicating the remaining time and another bar indicating the remaining space . 
	</s>
	

	<s id="78">
		 The 50 sentence limit can be temporarily exceeded and , when the 30 minute limit has been reached , the user can still remove sentences from the report until the sentence limit is reached back . 
	</s>
	

	<s id="79">
		 2.3 Questionnaires After summarizing every topic , the following questionnaire was filled in by every user : • Who are the main people involved in the topic ? 
	</s>
	

	<s id="80">
		 • What are the main organizations participating in the topic ? 
	</s>
	

	<s id="81">
		 • What are the key factors in the topic ? 
	</s>
	

	<s id="82">
		 Users provided free-text answers to these questions , with their freshly generated summary at hand . 
	</s>
	

	<s id="83">
		 We did not provide any suggestions or constraints at this point , except that a maximum of eight slots were available per question ( i.e. a maximum of 8X3 = 24 key concepts per topic , per user ) . 
	</s>
	

	<s id="84">
		 This is , for instance , the answer of one user for the topic 42 about the invasion of Haiti by UN and USA troops in 1994 : People Organizations Jean Bertrand Aristide ONU ( UN ) Clinton EEUU ( USA ) Raoul Cedras OEA ( OAS ) Philippe Biambi Michel Josep Francois Factors militares golpistas ( coup attempting soldiers ) golpe militar ( coup attempt ) restaurar la democracia ( reinstatement of democracy ) Finally , a single list of key concepts is generated for each topic , joining all the different answers . 
	</s>
	

	<s id="85">
		 Redundant concepts ( e.g. “war” and “conflict” ) were inspected and collapsed by hand . 
	</s>
	

	<s id="86">
		 These lists of key concepts constitute the gold standard for the similarity metric described in Section 3.2.5 . 
	</s>
	

	<s id="87">
		 Besides identifying key concepts , users also filled in the following questionnaire : • Were you familiarized with the topic ? 
	</s>
	

	<s id="88">
		 • Was it hard for you to elaborate the report ? 
	</s>
	

	<s id="89">
		 • Did you miss the possibility of introducing annotations or rewriting parts of the report by hand ? 
	</s>
	

	<s id="90">
		 • Do you consider that you generated a good report ? 
	</s>
	

	<s id="91">
		 • Are you tired ? 
	</s>
	

	<s id="92">
		 Out of the answers provided by users , the most remarkable facts are that : • only in 6 % of the cases the user missed “a lot” the possibility of rewriting/adding comments to the topic . 
	</s>
	

	<s id="93">
		 The fact that reports are made extractively did not seem to be a significant problem for our users . 
	</s>
	

	<s id="94">
		 • in 73 % of the cases , the user was quite or very satisfied about his summary . 
	</s>
	

	<s id="95">
		 These are indications that the practical constraints imposed on the task ( time limit and extractive nature of the summaries ) do not necessarily compromise the representativeness of the testbed . 
	</s>
	

	<s id="96">
		 The time limit is very tight , but the temporal arrangement of documents and their highly redundant nature facilitates skipping repetitive material ( some pieces of news are discarded just by looking at the title , without examining the content ) . 
	</s>
	

	<s id="97">
		 2.4 Generation of baseline reports We have automatically generated baseline reports in two steps : • For every topic , we have produced 30 tentative baseline reports using DUC style criteria : – 18 summaries consist only of picking the first sentence out of each document in 18 different document subsets . 
	</s>
	

	<s id="98">
		 The subsets are formed using different strategies , e.g. the most relevant documents for the query ( according to the Inquery search engine ) , one document per day , the first or last 50 documents in chronological order , etc. – The other 12 summaries consist of a ) picking the first n sentences out of a set of selected documents ( with different values for n and different sets of documents ) and b ) taking the full content of a few documents . 
	</s>
	

	<s id="99">
		 In both cases , document sets are formed with similar criteria as above . 
	</s>
	

	<s id="100">
		 • Out of these 30 baseline reports , we have selected the 10 reports which have the highest sentence overlap with the manual summaries . 
	</s>
	

	<s id="101">
		 The second step increases the quality of the baselines , making the task of differentiating manual and baseline reports more challenging . 
	</s>
	

	<s id="102">
		 3 Comparison of similarity metrics Formal aspects of a summary ( or report ) , such as legibility , grammatical correctness , informativeness , etc. , can only be evaluated manually . 
	</s>
	

	<s id="103">
		 However , automatic evaluation metrics can play a useful role in the evaluation of how well the information from the original sources is preserved 
		<ref citStr="Mani , 2001" id="6" label="CEPF" position="17319">
			( Mani , 2001 )
		</ref>
		 . 
	</s>
	

	<s id="104">
		 Previous studies have shown that it is feasible to evaluate the output of summarization systems automatically 
		<ref citStr="Lin and Hovy , 2003" id="7" label="CEPF" position="17464">
			( Lin and Hovy , 2003 )
		</ref>
		 . 
	</s>
	

	<s id="105">
		 The process is based in similarity metrics between texts . 
	</s>
	

	<s id="106">
		 The first step is to establish a ( manual ) reference summary , and then the automatically generated summaries are ranked according to their similarity to the reference summary . 
	</s>
	

	<s id="107">
		 The challenge is , then , to define an appropriate proximity metric for reports generated in the information synthesis task . 
	</s>
	

	<s id="108">
		 3.1 How to compare similarity metrics without human judgments ? 
	</s>
	

	<s id="109">
		 The QARLA estimation In tasks such as Machine Translation and Summarization , the quality of a proximity metric is measured in terms of the correlation between the ranking produced by the metric , and a reference ranking produced by human judges . 
	</s>
	

	<s id="110">
		 An optimal similarity metric should produce the same ranking as human judges . 
	</s>
	

	<s id="111">
		 In our case , acquiring human judgments about the quality of the baseline reports is too costly , and probably cannot be done reliably : a fine-grained evaluation of 50-sentence reports summarizing sets of 100 documents is a very complex task , which would probably produce different rankings from different judges . 
	</s>
	

	<s id="112">
		 We believe there is a cheaper and more robust way of comparing similarity metrics without using human assessments . 
	</s>
	

	<s id="113">
		 We assume a simple hypothesis : the best metric should be the one that best discriminates between manual and automatically generated reports . 
	</s>
	

	<s id="114">
		 In other words , a similarity metric that cannot distinguish manual and automatic reports cannot be a good metric . 
	</s>
	

	<s id="115">
		 Then , all we need is an estimation of how well a similarity metric separates manual and automatic reports . 
	</s>
	

	<s id="116">
		 We propose to use the probability that , given any manual report MTe f , any other manual report M is closer to MTe f than any other automatic report A : QARLA(sim) = P(sim(M , MTe f ) &gt; sim(A , MTef ) ) where M , MTe f E M , A E A where M is the set of manually generated reports , A is the set of automatically generated reports , and “sim” is the similarity metric being evaluated . 
	</s>
	

	<s id="117">
		 We refer to this value as the QARLA5 estimation . 
	</s>
	

	<s id="118">
		 QARLA has two interesting features : • No human assessments are needed to compute QARLA. . 
	</s>
	

	<s id="119">
		 Only a set of manually produced summaries and a set of automatic summaries , for each topic considered . 
	</s>
	

	<s id="120">
		 This reduces the cost of creating the testbed and , in addition , eliminates the possible bias introduced by human judges . 
	</s>
	

	<s id="121">
		 • It is easy to collect enough data to achieve statistically significant results . 
	</s>
	

	<s id="122">
		 For instance , our testbed provides 720 combinations per topic to estimate QARLA probability ( we have nine manual plus ten automatic summaries per topic ) . 
	</s>
	

	<s id="123">
		 A good QARLA value does not guarantee that a similarity metric will produce the same rankings as human judges , but a good similarity metric must have a good QARLA value : it is unlikely that a measure that cannot distinguish between manual and automatic summaries can still produce high- quality rankings of automatic summaries by comparison to manual reference summaries . 
	</s>
	

	<s id="124">
		 3.2 Similarity metrics We have compared five different metrics using the QARLA estimation . 
	</s>
	

	<s id="125">
		 The first three are meant as baselines ; the fourth is the standard similarity metric used to evaluate summaries ( ROUGE ) ; and the last one , introduced in this paper , is based on the overlapping of key concepts . 
	</s>
	

	<s id="126">
		 3.2.1 Baseline 1 : Document co-selection metric The following metric estimates the similarity of two reports from the set of documents which are represented in both reports ( i.e. at least one sentence in each report belongs to the document ) . 
	</s>
	

	<s id="127">
		 DocSim(M ,. , M ) _ IDoc(~) n Doc(M) I where Mr is the reference report , M a second re- port and Doc(Mr) , Doc(M) are the documents to which the sentences in Mr , M belong to. 5Quality criterion for reports evaluation metrics 3.2.2 Baselines 2 and 3 : Sentence co-selection The more sentences in common between two reports , the more similar their content will be . 
	</s>
	

	<s id="128">
		 We can measure Recall ( how many sentences from the reference report are also in the contrastive report ) and Precision ( how many sentences from the contrastive report are also in the reference report ) : SentenceSimR(M ,. , M ) _ IS(M,.) n S(M)I IS(M,.)I SentenceSimP(M ,. , M ) _ IS(M,.) n S(M)I IS(M)I where S(Mr) , S(M) are the sets of sentences in the reports Mr ( reference ) and M ( contrastive ) . 
	</s>
	

	<s id="129">
		 3.2.3 Baseline 4 : Perplexity A language model is a probability distribution over word sequences obtained from some training corpora ( see e.g. 
		<ref citStr="Manning and Schutze , 1999" id="8" label="CEPF" position="22129">
			( Manning and Schutze , 1999 )
		</ref>
		 ) . 
	</s>
	

	<s id="130">
		 Perplexity is a measure of the degree of surprise of a text or corpus given a language model . 
	</s>
	

	<s id="131">
		 In our case , we build a language model LM(Mr) for the refer- ence report Mr , and measure the perplexity of the contrastive report M as compared to that language model : PerplexitySim(M ,. , M ) _ 1 Perp(LM(M,.) , M ) We have used the Good-Turing discount algorithm to compute the language models 
		<ref citStr="Clarkson and Rosenfeld , 1997" id="9" label="CERF" position="22578">
			( Clarkson and Rosenfeld , 1997 )
		</ref>
		 . 
	</s>
	

	<s id="132">
		 Note that this is also a baseline metric , because it only measures whether the content of the contrastive report is compatible with the reference report , but it does not consider the coverage : a single sentence from the reference report will have a low perplexity , even if it covers only a small fraction of the whole report . 
	</s>
	

	<s id="133">
		 This problem is mitigated by the fact that we are comparing reports of approximately the same size and without repeated sentences . 
	</s>
	

	<s id="134">
		 3.2.4 ROUGE metric The distance between two summaries can be established as a function of their vocabulary ( unigrams ) and how this vocabulary is used ( n-grams ) . 
	</s>
	

	<s id="135">
		 From this point of view , some of the measures used in the evaluation of Machine Translation systems , such as BLEU 
		<ref citStr="Papineni et al. , 2002" id="10" label="CEPF" position="23388">
			( Papineni et al. , 2002 )
		</ref>
		 , have been imported into the summarization task . 
	</s>
	

	<s id="136">
		 BLEU is based in the precision and n-gram co-ocurrence between an automatic translation and a reference manual translation . 
	</s>
	

	<s id="137">
		 
		<ref citStr="Lin and Hovy , 2003" id="11" label="CEPN" position="23606">
			( Lin and Hovy , 2003 )
		</ref>
		 tried to apply BLEU as a measure to evaluate summaries , but the results were not as good as in Machine Translation . 
	</s>
	

	<s id="138">
		 Indeed , some of the characteristics that define a good translation are not related with the features of a good summary ; then Lin and Hovy proposed a recall- based variation of BLEU , known as ROUGE . 
	</s>
	

	<s id="139">
		 The idea is the same : the quality of a proposed summary can be calculated as a function of the n-grams in common between the units of a model summary . 
	</s>
	

	<s id="140">
		 The units can be sentences or discourse units : ECE{MU} En-gramEC Count where MU is the set of model units , Count n is the maximum number of n-grams co-ocurring in a peer summary and a model unit , and Count is the number of n-grams in the model unit . 
	</s>
	

	<s id="141">
		 It has been established that unigram and bigram based metrics permit to create a ranking of automatic summaries better ( more similar to a human-produced ranking ) than n-grams with n &gt; 2 . 
	</s>
	

	<s id="142">
		 For our experiment , we have only considered unigrams ( lemmatized words , excluding stop words ) , which gives good results with standard summaries 
		<ref citStr="Lin and Hovy , 2003" id="12" label="CEPF" position="24744">
			( Lin and Hovy , 2003 )
		</ref>
		 . 
	</s>
	

	<s id="143">
		 3.2.5 Key concepts metric Two summaries generated by different subjects may differ in the documents that contribute to the summary , in the sentences that are chosen , and even in the information that they provide . 
	</s>
	

	<s id="144">
		 In our Information Synthesis settings , where topics are complex and the number of documents to summarize is large , it is likely to expect that similarity measures based on document , sentence or n-gram overlap do not give large similarity values between pairs of manually generated summaries . 
	</s>
	

	<s id="145">
		 Our hypothesis is that two manual reports , even if they differ in their information content , will have the same ( or very similar ) key concepts ; if this is true , comparing the key concepts of two reports can be a better similarity measure than the previous ones . 
	</s>
	

	<s id="146">
		 In order to measure the overlap of key concepts between two reports , we create a vector ~kc for every report , such that every element in the vector represents the frequency of a key concept in the report in relation to the size of the report : kc(M)Z = freq(CZ , M ) Iwords(M)I being f req(CZ , M ) the number of times the key concept CZ appears in the report M , and Iwords ( M ) I the number of words in the report . 
	</s>
	

	<s id="147">
		 The key concept similarity NICOS ( Nuclear Informative Concept Similarity ) between two reports M and Mr can then be defined as the inverse of the Euclidean distance between their associated concept vectors : I ~kc(Mr) — ~kc(M) I In our experiment , the dimensions of kc vectors correspond to the list of key concepts provided by our test subjects ( see Section 2.3 ) . 
	</s>
	

	<s id="148">
		 This list is our gold standard for every topic . 
	</s>
	

	<s id="149">
		 4 Experimental results Figure 1 shows , for every topic ( horizontal axis ) , the QARLA estimation obtained for each similarity metric , i.e. , the probability of a manual report being closer to other manual report than to an automatic report . 
	</s>
	

	<s id="150">
		 Table 2 shows the average QARLA measure across all topics . 
	</s>
	

	<s id="151">
		 Metric TT topics IE topics Perplexity 0.19 0.60 DocSim 0.20 0.34 SentenceSimR 0.29 0.52 SentenceSimP 0.38 0.57 ROUGE 0.54 0.53 NICOS 0.77 0.52 Table 2 : Average QARLA For the six TT topics , the key concept similarity NICOS performs 43 % better than ROUGE , and all baselines give poor results ( all their QARLA probabilities are below chance , QARLA &lt; 0.5 ) . 
	</s>
	

	<s id="152">
		 A non- parametric Wilcoxon sign test confirms that the difference between NICOS and ROUGE is highly significant ( p &lt; 0.005 ) . 
	</s>
	

	<s id="153">
		 This is an indication that the Information Synthesis task , as we have defined it , should not be studied as a standard summarization problem . 
	</s>
	

	<s id="154">
		 It also confirms our hypothesis that key concepts tend to be stable across different users , and may help to generate the reports . 
	</s>
	

	<s id="155">
		 The behavior of the two Information Extraction ( IE ) topics is substantially different from TT topics . 
	</s>
	

	<s id="156">
		 While the ROUGE measure remains stable ( 0.53 versus 0.54 ) , the key concept similarity is much worse with IE topics ( 0.52 versus 0.77 ) . 
	</s>
	

	<s id="157">
		 On the other hand , all baselines improve , and some of them ( SentenceSim precision and perplexity ) give better results than both ROUGE and NICOS . 
	</s>
	

	<s id="158">
		 Of course , no reliable conclusion can be obtained from only two IE topics . 
	</s>
	

	<s id="159">
		 But the observed differences suggest that TT and IE may need different approaches , both to the automatic generation of reports and to their evaluation . 
	</s>
	

	<s id="160">
		 ROUGE _ E CE{MU} En-gramEC Countm , 1 NICOS(M , Mr ) = Figure 1 : Comparison of similarity metrics by topic One possible reason for this different behavior is that IE topics do not have a set of consistent key concepts ; every case of a hunger strike , for instance , involves different people , organizations and places . 
	</s>
	

	<s id="161">
		 The average number of different key concepts is 18.7 for TT topics and 28.5 for IE topics , a differ- ence that reveals less agreement between subjects , supporting this argument . 
	</s>
	

	<s id="162">
		 5 Related work Besides the measures included in our experiment , there are other criteria to compare summaries which could as well be tested for Information Synthesis : Annotation of relevant sentences in a corpus . 
	</s>
	

	<s id="163">
		 
		<ref citStr="Khandelwal et al. , 2001" id="13" label="CEPF" position="29009">
			( Khandelwal et al. , 2001 )
		</ref>
		 propose a task , called “Temporal Summarization” , that combines summarization and topic tracking . 
	</s>
	

	<s id="164">
		 The paper describes the creation of an evaluation corpus in which the most relevant sentences in a set of related news were annotated . 
	</s>
	

	<s id="165">
		 Summaries are evaluated with a measure called “novel recall” , based in sentences selected by a summarization system and sentences manually associated to events in the corpus . 
	</s>
	

	<s id="166">
		 The agreement rate between subjects in the identification of key events and the sentence annotation does not correspond with the agreement between reports that we have obtained in our experiments . 
	</s>
	

	<s id="167">
		 There are , at least , two reasons to explain this : • 
		<ref citStr="Khandelwal et al. , 2001" id="14" label="CEPF" position="29745">
			( Khandelwal et al. , 2001 )
		</ref>
		 work on an average of 43 documents , half the size of the topics in our corpus . 
	</s>
	

	<s id="168">
		 • Although there are topics in both experiments , the information needs in our testbed are more complex ( e.g. motivations for the invasion of Chechnya ) Factoids . 
	</s>
	

	<s id="169">
		 One of the problems in the evaluation of summaries is the versatility of human language . 
	</s>
	

	<s id="170">
		 Two different summaries may contain the same information . 
	</s>
	

	<s id="171">
		 In 
		<ref citStr="Halteren and Teufel , 2003" id="15" label="CEPF" position="30211">
			( Halteren and Teufel , 2003 )
		</ref>
		 , the content of summaries is manually represented , decomposing sentences in factoids or simple facts . 
	</s>
	

	<s id="172">
		 They also annotate the composition , generalization and implication relations between extracted factoids . 
	</s>
	

	<s id="173">
		 The resulting measure is different from unigram based similarity . 
	</s>
	

	<s id="174">
		 The main problem of factoids , as compared to other metrics , is that they require a costly manual processing of the summaries to be evaluated . 
	</s>
	

	<s id="175">
		 6 Conclusions In this paper , we have reported an empirical study of the “Information Synthesis” task , defined as the process of ( given a complex information need ) extracting , organizing and relating the pieces of information contained in a set of relevant documents , in order to obtain a comprehensive , non redundant report that satisfies the information need . 
	</s>
	

	<s id="176">
		 We have obtained two main results : • The creation of an Information Synthesis testbed ( ISCORPUS ) with 72 reports manually generated by 9 subjects for 8 complex topics with 100 relevant documents each . 
	</s>
	

	<s id="177">
		 • The empirical comparison of candidate metrics to estimate the similarity between reports . 
	</s>
	

	<s id="178">
		 Our empirical comparison uses a quantitative criterion ( the QARLA estimation ) based on the hypothesis that a good similarity metric will be able to distinguish between manual and automatic reports . 
	</s>
	

	<s id="179">
		 According to this measure , we have found evidence that the Information Synthesis task is not a standard multi-document summarization problem : state-of- the-art similarity metrics for summaries do not perform equally well with the reports in our testbed . 
	</s>
	

	<s id="180">
		 Our most interesting finding is that manually generated reports tend to have the same key concepts : a similarity metric based on overlapping key concepts ( NICOS ) gives significantly better results than metrics based on language models , n-gram coocurrence and sentence overlapping . 
	</s>
	

	<s id="181">
		 This is an indication that detecting relevant key concepts is a promising strategy in the process of generating reports . 
	</s>
	

	<s id="182">
		 Our results , however , has also some intrinsic limitations . 
	</s>
	

	<s id="183">
		 Firstly , manually generated summaries are extractive , which is good for comparison purposes , but does not faithfully reflect a natural process of human information synthesis . 
	</s>
	

	<s id="184">
		 Another weakness is the maximum time allowed per report : 30 minutes seems too little to examine 100 documents and extract a decent report , but allowing more time would have caused an excessive fatigue to users . 
	</s>
	

	<s id="185">
		 Our volunteers , however , reported a medium to high satisfaction with the results of their work , and in some occasions finished their task without reaching the time limit . 
	</s>
	

	<s id="186">
		 ISCORPUS is available at : http://nlp.uned.es/ISCORPUS Acknowledgments This research has been partially supported by a grant of the Spanish Government , project HERMES ( TIC-2000-0335-C03-01 ) . 
	</s>
	

	<s id="187">
		 We are indebted to E. Hovy for his comments on an earlier version of this paper , and C. Y. Lin for his assistance with the ROUGE measure . 
	</s>
	

	<s id="188">
		 Thanks also to our volunteers for their valuable cooperation . 
	</s>
	

	<s id="189">
		 References P. Clarkson and R. Rosenfeld . 
	</s>
	

	<s id="190">
		 1997. Statistical language modeling using the CMU-Cambridge toolkit . 
	</s>
	

	<s id="191">
		 In Proceeding of Eurospeech ’97 , Rhodes , Greece . 
	</s>
	

	<s id="192">
		 J. Goldstein , V. O. Mittal , J. G. Carbonell , and J. P. Callan . 
	</s>
	

	<s id="193">
		 2000. Creating and Evaluating Multi-Document Sentence Extract Summaries . 
	</s>
	

	<s id="194">
		 In Proceedings of Ninth International Conferences on Information Knowledge Management ( CIKM´00 ) , pages 165–172 , McLean , VA . 
	</s>
	

	<s id="195">
		 H. V. Halteren and S. Teufel . 
	</s>
	

	<s id="196">
		 2003. Examining the Consensus between Human Summaries : Initial Experiments with Factoids Analysis . 
	</s>
	

	<s id="197">
		 In HLT/NAACL-2003 Workshop on Automatic Summarization , Edmonton , Canada . 
	</s>
	

	<s id="198">
		 V. Khandelwal , R. Gupta , and J. Allan . 
	</s>
	

	<s id="199">
		 2001. An Evaluation Corpus for Temporal Summarization . 
	</s>
	

	<s id="200">
		 In Proceedings of the First International Conference on Human Language Technology Research ( HLT 2001 ) , Tolouse , France . 
	</s>
	

	<s id="201">
		 C. Lin and E. H. Hovy . 
	</s>
	

	<s id="202">
		 2003. Automatic Evaluation of Summaries Using N-gram Co-ocurrence Statistics . 
	</s>
	

	<s id="203">
		 In Proceeding of the 2003 Language Technology Conference ( HLT-NAACL 2003 ) , Edmonton , Canada . 
	</s>
	

	<s id="204">
		 I. Mani . 
	</s>
	

	<s id="205">
		 2001. Automatic Summarization , volume 3 of Natural Language Processing . 
	</s>
	

	<s id="206">
		 John Benjamins Publishing Company , Amsterdam/Philadelphia . 
	</s>
	

	<s id="207">
		 C. D. Manning and H. Schutze . 
	</s>
	

	<s id="208">
		 1999. Foundations of statistical natural language processing . 
	</s>
	

	<s id="209">
		 MIT Press , Cambridge Mass. P. . 
	</s>
	

	<s id="210">
		 Over . 
	</s>
	

	<s id="211">
		 2003. Introduction to DUC-2003 : An Intrinsic Evaluation of Generic News Text Summarization Systems . 
	</s>
	

	<s id="212">
		 In Proceedings of Workshop on Automatic Summarization ( DUC 2003 ) . 
	</s>
	

	<s id="213">
		 K. Papineni , S. Roukos , T. Ward , and W. Zhu . 
	</s>
	

	<s id="214">
		 2002. Bleu : a method for automatic evaluation of machine translation . 
	</s>
	

	<s id="215">
		 In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics ( ACL ) , pages 311– 318 , Philadelphia . 
	</s>
	

	<s id="216">
		 C. Peters , M. Braschler , J. Gonzalo , and M. Kluck , editors . 
	</s>
	

	<s id="217">
		 2002. Evaluation of Cross-Language Information Retrieval Systems , volume 2406 of Lecture Notes in Computer Science . 
	</s>
	

	<s id="218">
		 SpringerVerlag , Berlin-Heidelberg-New York . 
	</s>
	

	<s id="219">
		 D. R. Radev , J. Hongyan , and M. Budzikowska . 
	</s>
	

	<s id="220">
		 2000. Centroid-Based Summarization of Multiple Documents : Sentence Extraction , Utility- Based Evaluation , and User Studies . 
	</s>
	

	<s id="221">
		 In Proceedings of the Workshop on Automatic Summarization at the 6th Applied Natural Language Processing Conference and the 1st Conference of the North American Chapter of the Association for Computational Linguistics , Seattle , WA , April . 
	</s>
	


</acldoc>
