<?xml version="1.0" encoding="iso-8859-1"?>
<acldoc acl_id="P04-3009">
	

	<s id="1">
		 Wide Coverage Symbolic Surface Realization Charles B. Callaway Istituto per la Ricerca Scientifica e Tecnologica Istituto Trentino di Cultura , Italy ( ITC-irst ) callaway@itc.it Abstract Recent evaluation techniques applied to corpus- based systems have been introduced that can predict quantitatively how well surface realizers will generate unseen sentences in isolation . 
	</s>
	

	<s id="2">
		 We introduce a similar method for determining the coverage on the Fuf/Surge symbolic surface realizer , report that its coverage and accuracy on the Penn TreeBank is higher than that of a similar statistics-based generator , describe several benefits that can be used in other areas of computational linguistics , and present an updated version of Surge for use in the NLG community . 
	</s>
	

	<s id="3">
		 1 Introduction Surface realization is the process of converting the semantic and syntactic representation of a sentence or series of sentences into the text , or surface form , of a particular language 
		<ref citStr="Elhadad , 1991" id="1" label="CEPF" position="986">
			( Elhadad , 1991 
		</ref>
		<ref citStr="Bateman , 1995" id="2" label="CEPF" position="1003">
			; Bateman , 1995 )
		</ref>
		 . 
	</s>
	

	<s id="4">
		 Most surface realizers have been symbolic , grammar-based systems using syntactic linguistic theories like HPSG . 
	</s>
	

	<s id="5">
		 These systems were often developed as either proof-of-concept implementations or to support larger end-to-end NLG systems which have produced limited amounts of domain-specific texts . 
	</s>
	

	<s id="6">
		 As such , determining the generic coverage of a language has been substituted by the goal of producing the necessary syntactic coverage for a particular project . 
	</s>
	

	<s id="7">
		 As described in 
		<ref citStr="LangkildeGeary , 2002" id="3" label="CEPF" position="1563">
			( LangkildeGeary , 2002 )
		</ref>
		 , the result has been the use of regression testing with hand-picked examples rather than broad evaluations of linguistic competence . 
	</s>
	

	<s id="8">
		 Instead , large syntactically annotated corpora such as the Penn TreeBank 
		<ref citStr="Marcus et al. , 1993" id="4" label="CEPF" position="1806">
			( Marcus et al. , 1993 )
		</ref>
		 have allowed statistically based systems to produce large quantities of sentences and then more objectively determine generation coverage with automatic evaluation measures . 
	</s>
	

	<s id="9">
		 We conducted a similar corpus-based experiment 
		<ref citStr="Callaway , 2003" id="5" label="CEPF" position="2057">
			( Callaway , 2003 )
		</ref>
		 with the FUF/SURGE symbolic surface realizer 
		<ref citStr="Elhadad , 1991" id="6" label="CEPF" position="2121">
			( Elhadad , 1991 )
		</ref>
		 . 
	</s>
	

	<s id="10">
		 We describe a direct comparison with HALOGEN 
		<ref citStr="Langkilde-Geary , 2002" id="7" label="OJPN" position="2204">
			( Langkilde-Geary , 2002 )
		</ref>
		 using Section 23 of the TreeBank , showing that the symbolic approach improves upon the statistical system in both coverage and accuracy . 
	</s>
	

	<s id="11">
		 We also present a longitudinal comparison of two versions of FUF/SURGE showing a significant improvement in its coverage and accuracy after new grammar and morphology rules were added . 
	</s>
	

	<s id="12">
		 This improved version of SURGE is available for use in the NLG community . 
	</s>
	

	<s id="13">
		 2 Related Work in Wide Coverage Generation Verifying wide coverage generation depends on ( 1 ) a large , well structured corpus , ( 2 ) a transformation algorithm that converts annotated sentences into the surface realizer 's expected input form , ( 3 ) the surface realizer itself , and ( 4 ) an automatic metric for determining the accuracy of the generated sentences . 
	</s>
	

	<s id="14">
		 Large , well structured , syntactically marked corpora such as the Penn TreeBank 
		<ref citStr="Marcus et al. , 1993" id="8" label="CEPF" position="3118">
			( Marcus et al. , 1993 )
		</ref>
		 can provide a source of example sentences , while automatic metrics like simple string accuracy are capable of giving a fast , rough estimate of quality for individual sentences . 
	</s>
	

	<s id="15">
		 Realization of text from corpora has been approached in several ways . 
	</s>
	

	<s id="16">
		 In the case of Ratnaparkhi 's generator for flight information in the air travel domain 
		<ref citStr="Ratnaparkhi , 2000" id="9" label="CEPF" position="3498">
			( Ratnaparkhi , 2000 )
		</ref>
		 , the transformation algorithm is trivial as the generator uses the corpus itself ( annotated with semantic information such as destination or flight number ) as input to a surface realizer with an n-gram model of the domain , along with a maximum entropy probability model for selecting when to use which phrase . 
	</s>
	

	<s id="17">
		 FERGUS 
		<ref citStr="Bangalore and Rambow , 2000" id="10" label="CEPF" position="3861">
			( Bangalore and Rambow , 2000 )
		</ref>
		 used the Penn TreeBank as a corpus , requiring a more substantial transformation algorithm since it requires a lexical predicate-argument structure instead of the TreeBank 's representation . 
	</s>
	

	<s id="18">
		 The system uses an underlying tree- ( S ( NP-SBJ ( ( cat clause ) ( NP ( JJ overall ) ( process ( ( type ascriptive ) ( tense past ) ) ) ( NNS sales ) ) ) ( participants ( VP ( VBD were ) ( ( carrier ( ( cat common ) ( lex &quot; sale &quot; ) ( number plural ) ( ADJP-PRD ( describer ( ( cat adj ) ( lex &quot; overall &quot; ) ) ) ) ) ( RB roughly ) ( attribute ( ( cat ap ) ( lex &quot; flat &quot; ) ( JJ flat ) ) ) ) ( modifier ( ( cat adv ) ( lex &quot; roughly &quot; ) ) ) ) ) ) ) Figure 1 : A Penn TreeBank Sentence and Corresponding SUrGE Input Representation based syntactic model to generate a set of possible candidate realizations , and then chooses the best candidate with a trigram model of the Treebank text . 
	</s>
	

	<s id="19">
		 An evaluation of three versions of FErGUS on randomly chosen Wall Street Journal sentences of the TreeBank showed simple string accuracy up to 58.9 % . 
	</s>
	

	<s id="20">
		 Finally , Langkilde 's work on HALOGEN 
		<ref citStr="Langkilde-Geary , 2002" id="11" label="CEPF" position="4985">
			( Langkilde-Geary , 2002 )
		</ref>
		 uses a rewriting algorithm to convert the syntactically annotated sentences from the TreeBank into a semantic input notation via rewrite rules . 
	</s>
	

	<s id="21">
		 The system uses the transformed semantic input to create millions of possible realizations ( most of which are grammatical but unwieldy ) in a lattice structure and then also uses n-grams to select the most probable as its output sentence . 
	</s>
	

	<s id="22">
		 Langkilde evaluated the system using the standard train-and-test methodology with Section 23 of the TreeBank as the unseen set . 
	</s>
	

	<s id="23">
		 These systems represent a statistical approach to wide coverage realization , turning to automatic methods to evaluate coverage and quality based on corpus statistics . 
	</s>
	

	<s id="24">
		 However , a symbolic realizer can use the same evaluation technique if a method exists to transform the corpus annotation into the realizer 's input representation . 
	</s>
	

	<s id="25">
		 Thus symbolic realizers can also use the same types of evaluations employed by the parsing and MT communities , allowing for meaningful comparisons of their performance on metrics such as coverage and accuracy . 
	</s>
	

	<s id="26">
		 3 The Penn TreeBank The Penn TreeBank 
		<ref citStr="Marcus et al. , 1993" id="12" label="OEPF" position="6164">
			( Marcus et al. , 1993 )
		</ref>
		 is a large set of sentences bracketed for syntactic dependency and part of speech , covering almost 5 million words of text . 
	</s>
	

	<s id="27">
		 The corpus is divided into 24 sections , with each section having on average 2000 sentences . 
	</s>
	

	<s id="28">
		 The representation of an example sentence is shown at the left of Figure 1 . 
	</s>
	

	<s id="29">
		 In general , many sentences contained in the TreeBank are not typical of those produced by current NLG systems . 
	</s>
	

	<s id="30">
		 For instance , newspaper text requires extensive quoting for conveying dialogue , special formatting for stock reports , and methods for dealing with contractions . 
	</s>
	

	<s id="31">
		 These types of constructions are not available in current general purpose , rule-based generators : • Direct and indirect quotations from re- porters ' interviews 
		<ref citStr="Callaway , 2001" id="13" label="CEPF" position="6968">
			( Callaway , 2001 )
		</ref>
		 : &quot; It 's turning out to be a real blockbuster , &quot; Mr. Sweig said . 
	</s>
	

	<s id="32">
		 • Incomplete quotations : Then retailers &quot; will probably push them out altogether , &quot; he says . 
	</s>
	

	<s id="33">
		 • Simple lists of facts from stock reports : 8 13/16 % high , 8 1/2 % low , 8 5/8 % near closing bid , 8 3/4 % offered . 
	</s>
	

	<s id="34">
		 • Both formal and informal language : You 've either got a chair or you do n't . 
	</s>
	

	<s id="35">
		 • A variety of punctuation mixed with text : $ 55,730,000 of school financing bonds , 1989 Series B ( 1987 resolution ) . 
	</s>
	

	<s id="36">
		 • Combinations of infrequent syntactic rules : Then how should we think about service ? 
	</s>
	

	<s id="37">
		 • Irregular and rare words : &quot; I was upset with Roger , I fumpered and schmumpered , &quot; says Mr. Peters . 
	</s>
	

	<s id="38">
		 By adding rules for these phenomena , NLG realizers can significantly increase their coverage . 
	</s>
	

	<s id="39">
		 For instance , approximately 15 % of Penn TreeBank sentences contain either direct , indirect or incomplete written dialogue . 
	</s>
	

	<s id="40">
		 Thus for a newspaper domain , excluding dialogue from the grammar greatly limits potential coverage . 
	</s>
	

	<s id="41">
		 Furthermore , using a corpus for testing a surface realizer is akin to having a very large regression test set , with the added benefit of being able to robustly generate real-world sentences . 
	</s>
	

	<s id="42">
		 In order to compare a symbolic surface realizer with its statistical counterparts , we tested an enhanced version of an off-the-shelf symbolic generation system , the FUF/SURGE 
		<ref citStr="Elhadad , 1991" id="14" label="CERF" position="8469">
			( Elhadad , 1991 )
		</ref>
		 surface realizer . 
	</s>
	

	<s id="43">
		 To obtain a meaningful comparison , we utilized the same approach as Realizer Sentences Coverage Matches Covered Matches Total Matches Accuracy SURGE 2.2 2416 48.1 % 102 8.8 % 4.2 % 0.8542 SURGE+ 2416 98.9 % 1474 61.7 % 61.0 % 0.9483 HalOGEN 2416 83.3 % 1157 57.5 % 47.9 % 0.9450 Table 1 : Comparing two SURGE versions with HALOGEN [ Langkilde 20021 . 
	</s>
	

	<s id="44">
		 HALOGEN , treating Section 23 of the Treebank as an unseen test set . 
	</s>
	

	<s id="45">
		 We created an analogous transformation algorithm 
		<ref citStr="Callaway , 2003" id="15" label="CERF" position="9006">
			( Callaway , 2003 )
		</ref>
		 to convert TreeBank sentences into the SURGE representation ( Figure 1 ) , which are then given to the symbolic surface realizer , allowing us to measure both coverage and accuracy . 
	</s>
	

	<s id="46">
		 4 Coverage and Accuracy Evaluation Of the three statistical systems presented above , only 
		<ref citStr="Langkilde-Geary , 2002" id="16" label="CEPF" position="9316">
			( Langkilde-Geary , 2002 )
		</ref>
		 used a standard , recoverable method for replicating the generation experiment . 
	</s>
	

	<s id="47">
		 Because of the sheer number of sentences ( 2416 ) , and to enable a direct comparison with HALOGEN , we similarly used the simple string accuracy 
		<ref citStr="Doddington , 2002" id="17" label="CEPF" position="9574">
			( Doddington , 2002 )
		</ref>
		 , where the smallest number of Adds , Deletions , and Insertions were used to calculate accuracy : 1 - ( A + D + I ) / #Characters . 
	</s>
	

	<s id="48">
		 Unlike typical statistical and machine learning experiments , the grammar was &quot; trained &quot; by hand , though the evaluation of the resulting sentences was performed automatically . 
	</s>
	

	<s id="49">
		 This resulted in numerous generalized syntactic and morphology rules being added to the SURGE grammar , as well as specialized rules pertaining to specific domain elements from the texts . 
	</s>
	

	<s id="50">
		 Table 1 shows a comparative coverage and accuracy analysis of three surface realizers on Section 23 of the Penn TreeBank : the original SURGE 2.2 distribution , our modified version of SURGE , and the HALOGEN system described in 
		<ref citStr="Langkilde-Geary , 2002" id="18" label="CERF" position="10358">
			( Langkilde-Geary , 2002 )
		</ref>
		 . 
	</s>
	

	<s id="51">
		 The surface realizers are measured in terms of : • Coverage : The number of sentences for which the realizer returned a recognizable string rather than failure or an error . 
	</s>
	

	<s id="52">
		 • Matches : The number of identical sen- tences ( including punctuation/capitals ) . 
	</s>
	

	<s id="53">
		 • Percent of covered matches : How often the realizer returned a sentence match given that a sentence is produced . 
	</s>
	

	<s id="54">
		 • Percent of matches for all sentences : A measure of matches from all inputs , which penalizes systems that improve accuracy at the expense of coverage ( Matches / 2416 , or Coverage * Covered Matches ) . 
	</s>
	

	<s id="55">
		 • Accuracy : The aggregate simple string accuracy score for all covered sentences ( as opposed to the entire sentence set ) . 
	</s>
	

	<s id="56">
		 The first thing to note is the drastic improvement between the two versions of SURGE . 
	</s>
	

	<s id="57">
		 As the analysis in Section 3 showed , studying the elements of a particular domain are very important in determining what parts of a grammar should be improved . 
	</s>
	

	<s id="58">
		 For instance , the TreeBank contains many constructions which are not handled by SURGE 2.2 , such as quotations , which account for 15 % of the sentences . 
	</s>
	

	<s id="59">
		 When SURGE 2.2 encounters a quotation , it fails to produce a text string , accounting for a large chunk of the sentences not covered ( 51.9 % compared to 1.1 % for our enhanced version of SURGE ) . 
	</s>
	

	<s id="60">
		 Additionally , a number of morphology enhancements , such as contractions and punctuation placement contributed to the much higher percentage of exact matches . 
	</s>
	

	<s id="61">
		 While some of these are domain-specific , many are broader generalizations which although useful , were not included in the original grammar because they were not encountered in previous domains or arose only in complex sentences . 
	</s>
	

	<s id="62">
		 On all four measures the enhanced version of SURGE performed much better than the statistical approach to surface realization embodied in HALOGEN . 
	</s>
	

	<s id="63">
		 The accuracy measure is especially surprising given that statistical and machine learning approaches employ maximization algorithms to ensure that grammar rules are chosen to get the highest possible accuracy . 
	</s>
	

	<s id="64">
		 However , given that the difference in accuracy from Surge 2.2 is relatively small while its quality is obviously poor , using such accuracy measures alone is a bad way to compare surface realizers . 
	</s>
	

	<s id="65">
		 Finally , the coverage difference between the enhanced version of SURGE and that of HALOGEN is especially striking . 
	</s>
	

	<s id="66">
		 Some explanations may be that statistical systems are not yet capable of handling certain linguistic phenomena like long-distance dependencies ( due to n-gram ap- proaches ) , or given that statistical systems are typically robust and very unlikely to produce no output , that there were problems in the transformation algorithm that converted individual sentence representations from the corpus . 
	</s>
	

	<s id="67">
		 5 Additional Benefits The evaluation approach presented here has other advantages besides calculating the coverage and accuracy of a grammar . 
	</s>
	

	<s id="68">
		 For instance , in realizers where linguists must add new lexical resources by hand , such a system allows them to generate text by first creating sample sentences in the more familiar TreeBank notation . 
	</s>
	

	<s id="69">
		 Sentences could also be directly generated by feeding an example text to a parser capable of producing TreeBank structures . 
	</s>
	

	<s id="70">
		 This would be especially useful in new domains to quickly see what new specialized syntax they might need . 
	</s>
	

	<s id="71">
		 Additionally , the transformation program can be used as an error-checker to assist in annotating sentences in a new corpus . 
	</s>
	

	<s id="72">
		 Rules could be ( and have been ) added alongside the normal transformation rules that detect when errors are encountered , categorize them , and make them available to the corpus creator for correction . 
	</s>
	

	<s id="73">
		 This can extend beyond the syntax level , detecting even morphology errors such as incorrect verbs , typos , or dialect differences . 
	</s>
	

	<s id="74">
		 Finally , such an approach can help test parsing systems without the need for the time- consuming process of annotating corpora in the first place . 
	</s>
	

	<s id="75">
		 If a parser creates a TreeBank representation for a sentence , the generation system can then attempt to regenerate that same sentence automatically . 
	</s>
	

	<s id="76">
		 Exact matches are highly likely to have been correctly parsed , and more time can be spent locating and resolving parses that returned very low accuracy scores . 
	</s>
	

	<s id="77">
		 6 Conclusions and Future Work Recent statistical systems for generation have focused on surface realizers , offering robustness , wide coverage , and domain- and language- independence given certain resources . 
	</s>
	

	<s id="78">
		 This paper represents the analogous effort for a symbolic generation system using an enhanced version of the FUF/SURGE systemic realizer . 
	</s>
	

	<s id="79">
		 We presented a grammatical coverage and accuracy experiment showing the symbolic system had a much higher level of coverage of English and better accuracy as represented by the Penn TreeBank . 
	</s>
	

	<s id="80">
		 The improved SURGE grammar , version 2.4 , will be made freely available to the NLG community . 
	</s>
	

	<s id="81">
		 While we feel that both coverage and accuracy could be improved even more , additional gains would not imply a substantial improvement in the quality of the grammar itself . 
	</s>
	

	<s id="82">
		 The reason is that most problems affecting accuracy lie in transforming the TreeBank representation as opposed to the grammar , which has remained relatively stable . 
	</s>
	

	<s id="83">
		 References S. Bangalore and O. Rambow . 
	</s>
	

	<s id="84">
		 2000. Exploiting a probabilistic hierarchical model for generation . 
	</s>
	

	<s id="85">
		 In COLING-2000 : Proceedings of the 18th International Conference on Computational Linguistics , Saarbruecken , Germany . 
	</s>
	

	<s id="86">
		 John A. Bateman . 
	</s>
	

	<s id="87">
		 1995. KPML : The KOMETpenman ( multilingual ) development environment . 
	</s>
	

	<s id="88">
		 Technical Report Release 0.8 , Institut f^ur Integrierte Publikations- und Informationssysteme ( IPSI ) , GMD , Darmstadt . 
	</s>
	

	<s id="89">
		 Charles Callaway . 
	</s>
	

	<s id="90">
		 2001. A computational feature analysis for multilingual character-tocharacter dialogue . 
	</s>
	

	<s id="91">
		 In Proceedings of the Second International Conference on Intelligent Text Processing and Computational Linguistics , pages 251264 , Mexico City , Mexico . 
	</s>
	

	<s id="92">
		 Charles B. Callaway . 
	</s>
	

	<s id="93">
		 2003. Evaluating coverage for large symbolic NLG grammars . 
	</s>
	

	<s id="94">
		 In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence , pages 811817 , Acapulco , Mexico , August . 
	</s>
	

	<s id="95">
		 George Doddington . 
	</s>
	

	<s id="96">
		 2002. Automatic evaluation of machine translation quality using n- gram co-occurrence statistics . 
	</s>
	

	<s id="97">
		 In Proceedings of the 2002 Conference on Human Language Technology , San Diego , CA , March . 
	</s>
	

	<s id="98">
		 Michael Elhadad . 
	</s>
	

	<s id="99">
		 1991. FUF : The universal unifier user manual version 5.0 . 
	</s>
	

	<s id="100">
		 Technical Report CUCS-038-91 , Dept. of Computer Science , Columbia University . 
	</s>
	

	<s id="101">
		 Irene Langkilde-Geary . 
	</s>
	

	<s id="102">
		 2002. An empirical verification of coverage and correctness for a general-purpose sentence generator . 
	</s>
	

	<s id="103">
		 In Second International Natural Language Generation Conference , Harriman , NY , July . 
	</s>
	

	<s id="104">
		 M. Marcus , B. Santorini , and M. Marcinkiewicz . 
	</s>
	

	<s id="105">
		 1993. Building a large annotated corpus of English : The PennTreeBank . 
	</s>
	

	<s id="106">
		 Computational Linguistics , 26(2) . 
	</s>
	

	<s id="107">
		 Adwait Ratnaparkhi . 
	</s>
	

	<s id="108">
		 2000. Trainable methods for surface natural language generation . 
	</s>
	

	<s id="109">
		 In Proceedings of the First North American Conference of the ACL , Seattle , WA , May . 
	</s>
	


</acldoc>
