<?xml version="1.0" encoding="iso-8859-1"?>
<acldoc acl_id="P04-3006">
	

	<s id="1">
		 An Automatic Filter for Non-Parallel Texts Chris Pike Computer Science Department New York University 715 Broadway , 7th Floor New York , NY 10003 USA lastname @cs.nyu.edu I. Dan Melamed Computer Science Department New York University 715 Broadway , 7th Floor New York , NY 10013 USA lastname @cs.nyu.edu Abstract Numerous cross-lingual applications , including state-of-the-art machine translation systems , require parallel texts aligned at the sentence level . 
	</s>
	

	<s id="2">
		 However , collections of such texts are often polluted by pairs of texts that are comparable but not parallel . 
	</s>
	

	<s id="3">
		 Bitext maps can help to discriminate between parallel and comparable texts . 
	</s>
	

	<s id="4">
		 Bitext mapping algorithms use a larger set of document features than competing approaches to this task , resulting in higher accuracy . 
	</s>
	

	<s id="5">
		 In addition , good bitext mapping algorithms are not limited to documents with structural mark-up such as web pages . 
	</s>
	

	<s id="6">
		 The task of filtering non-parallel text pairs represents a new application of bitext mapping algorithms . 
	</s>
	

	<s id="7">
		 1 Introduction In June 2003 , the U.S. government organized a “Surprise Language Exercise” for the NLP community . 
	</s>
	

	<s id="8">
		 The goal was to build the best possible language technologies for a “surprise” language in just one month 
		<ref citStr="Oard , 2003" id="1" label="CEPF" position="1321">
			( Oard , 2003 )
		</ref>
		 . 
	</s>
	

	<s id="9">
		 One of the main technologies pursued was machine translation ( MT ) . 
	</s>
	

	<s id="10">
		 Statistical MT ( SMT ) systems were the most successful in this scenario , because their construction typically requires less time than other approaches . 
	</s>
	

	<s id="11">
		 On the other hand , SMT systems require large quantities of parallel text as training data . 
	</s>
	

	<s id="12">
		 A significant collection of parallel text was obtained for this purpose from multiple sources . 
	</s>
	

	<s id="13">
		 SMT systems were built and tested ; results were reported . 
	</s>
	

	<s id="14">
		 Much later we were surprised to discover that a significant portion of the training data was not parallel text ! 
	</s>
	

	<s id="15">
		 Some of the document pairs were on the same topic but not translations of each other . 
	</s>
	

	<s id="16">
		 For today’s sentence-based SMT systems , this kind of data is noise . 
	</s>
	

	<s id="17">
		 How much better would the results have been if the noisy training data were automatically filtered out ? 
	</s>
	

	<s id="18">
		 This question is becoming more important as SMT systems increase their reliance on automatically collected parallel texts . 
	</s>
	

	<s id="19">
		 There is abundant literature on aligning parallel texts at the sentence level . 
	</s>
	

	<s id="20">
		 To the best of our knowledge , all published methods happily misalign nonparallel inputs , without so much as a warning . 
	</s>
	

	<s id="21">
		 There is also some recent work on distinguishing parallel texts from pairs of unrelated texts 
		<ref citStr="Resnik and Smith , 2003" id="2" label="CEPF" position="2738">
			( Resnik and Smith , 2003 )
		</ref>
		 . 
	</s>
	

	<s id="22">
		 In this paper , we propose a solution to the more difficult problem of distinguishing parallel texts from texts that are comparable but not parallel . 
	</s>
	

	<s id="23">
		 Definitions of “comparable texts” vary in the literature . 
	</s>
	

	<s id="24">
		 Here we adopt a definition that is most suitable for filtering SMT training data : Two texts are “comparable” if they are not alignable at approximately the sentence level . 
	</s>
	

	<s id="25">
		 This definition is also suitable for other applications of parallel texts , such as machine-assisted translation and computer- assisted foreign language learning . 
	</s>
	

	<s id="26">
		 
		<ref citStr="Resnik and Smith ( 2003 )" id="3" label="CEPF" position="3363">
			Resnik and Smith ( 2003 )
		</ref>
		 suggested three approaches to filtering non-parallel texts : STRAND , tsim , and a combination of the two . 
	</s>
	

	<s id="27">
		 STRAND relies on mark-up within a document to reveal the document’s structure . 
	</s>
	

	<s id="28">
		 STRAND then predicts that documents with the same structure are parallel . 
	</s>
	

	<s id="29">
		 Tsim uses a machine-readable bilingual dictionary to find word-to-word matches between two halves of a bitext . 
	</s>
	

	<s id="30">
		 It then computes a similarity score based on the maximum cardinality bipartite matching between the two halves . 
	</s>
	

	<s id="31">
		 We chose to compare our method with tsim because we were interested in an approach that works with both marked up and plain text documents . 
	</s>
	

	<s id="32">
		 2 A Modification to SIMR Our work is based on a modification of the SIMR bitext mapping algorithm 
		<ref citStr="Melamed , 1999" id="4" label="CERF" position="4164">
			( Melamed , 1999 )
		</ref>
		 . 
	</s>
	

	<s id="33">
		 The SIMR algorithm attempts to construct a piecewise linear approximation to the True Bitext Map ( TBM ) of a bitext by greedily searching for small chains of points of correspondence . 
	</s>
	

	<s id="34">
		 Each chain forms one section of the approximation . 
	</s>
	

	<s id="35">
		 SIMR uses a two- phase approach to generating chains . 
	</s>
	

	<s id="36">
		 First , it generates a set of potential points of correspondence within a search rectangle . 
	</s>
	

	<s id="37">
		 Next , it searches the Figure 1 : On the left is part of a bitext map generated by SIMR for non-parallel texts . 
	</s>
	

	<s id="38">
		 On the right is part of a bitext map for parallel texts . 
	</s>
	

	<s id="39">
		 points of correspondence for chains whose points meet requirements for linearity , injectivity , and maximum angle deviation . 
	</s>
	

	<s id="40">
		 If no such chain is found , the search rectangle is expanded and the search repeats . 
	</s>
	

	<s id="41">
		 Our method of detecting translations is based on the premise that SIMR will find fewer points of correspondence in comparable texts than it will in parallel texts . 
	</s>
	

	<s id="42">
		 This is because points of correspondence are more likely to occur in closely corresponding locations in the two halves of a bitext than in two documents that are merely comparable . 
	</s>
	

	<s id="43">
		 Therefore , the bitext map of parallel texts will usually be much denser than the bitext map of comparable texts . 
	</s>
	

	<s id="44">
		 Figure 1 above contrasts the bitext maps output by SIMR for non-parallel and parallel texts . 
	</s>
	

	<s id="45">
		 To maximize the percentage of correctly classified document pairs , we need to maximize the difference between the map densities of parallel and comparable texts . 
	</s>
	

	<s id="46">
		 SIMR’s built in restrictions on the chains it will accept severely limit the number of points of correspondence SIMR accepts from most non-parallel texts . 
	</s>
	

	<s id="47">
		 Despite this SIMR still generated bitext maps for some non-parallel documents that had densities very close to the densities of parallel documents . 
	</s>
	

	<s id="48">
		 Chains of spurious points tended to form over a longer section of the bitext than correct chains . 
	</s>
	

	<s id="49">
		 Therefore we introduced an additional parameter that limited the length of chains that SIMR would accept . 
	</s>
	

	<s id="50">
		 This modification of SIMR is called SIMR-cl . 
	</s>
	

	<s id="51">
		 Chains are not perfectly linear . 
	</s>
	

	<s id="52">
		 Therefore we cannot calculate chain length by simply taking the distance between the first and last points in the chain . 
	</s>
	

	<s id="53">
		 Instead we find the smallest possible rectangle for which all points in the chain are interior points . 
	</s>
	

	<s id="54">
		 We then calculate the length of the chain as the distance from the lower left corner to the upper right hand corner of the rectangle . 
	</s>
	

	<s id="55">
		 When SIMR finds an acceptable chain the search rectangle is moved so that the point on the lower left is no longer included in the search . 
	</s>
	

	<s id="56">
		 As a result , when SIMR is finding a large number of chains , the length of those chains will remain relatively short . 
	</s>
	

	<s id="57">
		 Therefore , in parallel texts SIMR will find many chains and limiting the chain length will have a minimal effect on the number of chains SIMR will find . 
	</s>
	

	<s id="58">
		 On a non-parallel text , however , SIMR will find fewer sets of points of correspondence meeting the criteria for a chain . 
	</s>
	

	<s id="59">
		 The result is longer chains , which can be filtered by our new parameter . 
	</s>
	

	<s id="60">
		 E.g. , the non-parallel bitext map in Figure 1 , which was created without the chain length parameter , has on average 630 characters between points . 
	</s>
	

	<s id="61">
		 In contrast , running SIMR on the same pair of nonparallel documents with a maximum chain length of 700 yielded only 22 points of correspondence , or 3032 characters between points on average . 
	</s>
	

	<s id="62">
		 3 Training Training SIMR-cl , much like SIMR , requires a state space search algorithm , and an objective function to evaluate the current state . 
	</s>
	

	<s id="63">
		 We chose to use simulated annealing to perform our state space search . 
	</s>
	

	<s id="64">
		 The first step in training is to generate a set of parameter values that make up the current state . 
	</s>
	

	<s id="65">
		 SIMR-cl uses the standard SIMR parameters plus the additional chain length parameter discussed above . 
	</s>
	

	<s id="66">
		 Once the current state is set SIMR-cl generates a bitext map and calculates the density of the map . 
	</s>
	

	<s id="67">
		 The bitext map density is defined as the number of points in the bitext map divided by the length of the main diagonal of the bitext space . 
	</s>
	

	<s id="68">
		 We call this the SIMR-cl score . 
	</s>
	

	<s id="69">
		 Our objective function seeks to drive the parameters to a state where we can select a single threshold value that will classify all candidate bitexts in the development set correctly . 
	</s>
	

	<s id="70">
		 That is , all parallel texts should have a SIMR-cl score greater than the threshold , and all non-parallel texts should have a SIMR-cl score less than the threshold . 
	</s>
	

	<s id="71">
		 We cannot achieve this by simply measuring the percentage of correctly classified candidate text pairs , because any given change to the parameters is not likely to change the classification of any candidate bitexts . 
	</s>
	

	<s id="72">
		 In order to measure the amount of error we borrowed the concept of margin slack from the support vector machines literature . 
	</s>
	

	<s id="73">
		 For simplicity we used a margin of zero , which reduces the margin slack of a SIMR-cl score to the difference between the threshold density , and the density of a misclassified candidate pair . 
	</s>
	

	<s id="74">
		 Any correctly classified candidate pair is defined to have a margin slack of zero . 
	</s>
	

	<s id="75">
		 From there we defined our objective as minimizing the sum of the margin slack of all candidate pairs . 
	</s>
	

	<s id="76">
		 All that is left at this point is to select an optimal threshold . 
	</s>
	

	<s id="77">
		 We performed a line search for the best possible threshold for each parameter set . 
	</s>
	

	<s id="78">
		 4 Experiments In our first two experiments we limited the points of correspondence to orthographic cognates . 
	</s>
	

	<s id="79">
		 We used the Longest Common Subsequence Ratio ( LCSR ) to measure similarity 
		<ref citStr="Melamed , 1995" id="5" label="CEPF" position="10122">
			( Melamed , 1995 )
		</ref>
		 . 
	</s>
	

	<s id="80">
		 The LCSR ratio is the length of the longest common subsequence of two tokens , divided by the length of the longer token . 
	</s>
	

	<s id="81">
		 In our English-Hindi experiments we used an English-Hindi dictionary because the languages are written in different character sets , limiting the effectiveness of orthographic cognates . 
	</s>
	

	<s id="82">
		 4.1 STRAND data Before evaluating our approach on the more difficult task of discriminating parallel texts from comparable texts , we compared it to previous approaches on the easier task of discriminating parallel texts from unrelated texts . 
	</s>
	

	<s id="83">
		 For this purpose , we used the STRAND corpus , which consists of 326 candidate bitexts in French and English1 
		<ref citStr="Resnik and Smith , 2003" id="6" label="OEPF" position="10852">
			( Resnik and Smith , 2003 )
		</ref>
		 . 
	</s>
	

	<s id="84">
		 As a precursor to generating a bitext map of a candidate pair we tokenized the STRAND documents and generated the axis files required by SIMR-cl . 
	</s>
	

	<s id="85">
		 We attempted several schemes on training data and found that generating one token per HTML tag gave us the best results . 
	</s>
	

	<s id="86">
		 While the end performance of the two approaches was comparable , we did find that tsim had an advantage over SIMR-cl in training . 
	</s>
	

	<s id="87">
		 
		<ref citStr="Resnik and Smith ( 2003 )" id="7" label="CEPF" position="11316">
			Resnik and Smith ( 2003 )
		</ref>
		 trained tsim using 32 of the 326 available STRAND candidate pairs to achieve their published result . 
	</s>
	

	<s id="88">
		 We repeated their experiments using 1/4 of the available candidate pairs for training and found no improvement , indicating that tsim can be optimally trained using a small development set . 
	</s>
	

	<s id="89">
		 By contrast , using 32 training instances , SIMRcl achieved only 86 % agreement with the human judges , compared to tsim’s 96 % . 
	</s>
	

	<s id="90">
		 When trained with 1/4 of the candidate pairs , SIMR-cl achieved 96 % accuracy . 
	</s>
	

	<s id="91">
		 4.2 Filtering of Comparable Texts We were unable to find a suitable corpus containing both parallel and comparable texts . 
	</s>
	

	<s id="92">
		 Expert opinion suggests that no such corpora are publicly available 2 . 
	</s>
	

	<s id="93">
		 Therefore we proceeded by simulation . 
	</s>
	

	<s id="94">
		 We constructed 3 sets of two corpora from the Romanian/English Multext-East 1984 corpus ( Tufis , 1We removed all document pairs which were not in French/English . 
	</s>
	

	<s id="95">
		 2Doug Oard , personal cummunication , 2004. text length 164 820 1640 tsim 66 % 66 % 66 % SIMR-cl 90 % 96.5 % 98.5 % Table 1 : Percentage of documents correctly classified by tsim and SIMR-cl on parallel and comparable corpora with texts of varying lengths , by average number of words in the English text . 
	</s>
	

	<s id="96">
		 1999 ) . 
	</s>
	

	<s id="97">
		 We constructed parallel texts by breaking the corpus into aligned chunks of 10 , 50 , and 100 segments . 
	</s>
	

	<s id="98">
		 We then simulated comparable texts by pairing non-aligned , consecutive chunks of the same length . 
	</s>
	

	<s id="99">
		 We chose to use consecutive chunks because there is a better chance for overlap between words in adjacent segments than in segments far apart . 
	</s>
	

	<s id="100">
		 After breaking the corpus into chunks , 1/3 of the chunks were used as a training set and the remaining 2/3 were used as a test set . 
	</s>
	

	<s id="101">
		 We had 63 training and 130 test pairs of size 100 , 126 training and 259 test pairs of size 50 , and 642 training and 1285 test pairs of size 10 . 
	</s>
	

	<s id="102">
		 On average each English segment was 16 words in length . 
	</s>
	

	<s id="103">
		 Since a Romanian/English bilingual dictionary was not readily available , we created a dictionary for tsim by searching all aligned segments for cognates . 
	</s>
	

	<s id="104">
		 We then performed the same optimization process for tsim and SIMR-cl using documents containing 10 , 50 , and 100 segments . 
	</s>
	

	<s id="105">
		 After performing our optimizations , we found that the LCSR parameters optimized for tsim generated a dictionary containing 3380 pairs . 
	</s>
	

	<s id="106">
		 Using this parameter set , tsim correctly classified 66 % of the documents in the 1984 corpus . 
	</s>
	

	<s id="107">
		 The accuracy was the same for all bitext lengths . 
	</s>
	

	<s id="108">
		 Much like tsim , we found that for SIMR-cl the optimal parameter set was independent of the length of the bitexts being compared . 
	</s>
	

	<s id="109">
		 SIMR-cl did however perform better on longer texts . 
	</s>
	

	<s id="110">
		 Regardless , SIMR-cl outperformed tsim on all text lengths , as shown in table 1 . 
	</s>
	

	<s id="111">
		 4.3 The Surprise Language Data Encouraged by our success on French/English and on Romanian/English , we applied our method to the Hindi/English data used during the surprise language exercise . 
	</s>
	

	<s id="112">
		 We did not have Hindi/English bitexts that were reliably classified as parallel or not , so we could not optimize SIMR-cl’s parameters specifically for this language pair . 
	</s>
	

	<s id="113">
		 However , we were interested in determining how sensitive the parameters were to changes in the input language pair and text genre . 
	</s>
	

	<s id="114">
		 So we simply reused the param- eters that were found to be optimal on the Romanian/English 1984 corpus . 
	</s>
	

	<s id="115">
		 With these parameters , we ran SIMR-cl on just over half of the Hindi/English collection , the part that was collected from Indian government web pages . 
	</s>
	

	<s id="116">
		 Our method classified 6 of the document pairs as non-parallel . 
	</s>
	

	<s id="117">
		 Some of these 6 document pairs were relatively long , together they accounted for 7 % of the English word count in this part of the collection . 
	</s>
	

	<s id="118">
		 We asked a Hindi speaker to compare the Hindi and English text in each of these 6 document pairs . 
	</s>
	

	<s id="119">
		 For each text pair , we asked our informant : 1 . 
	</s>
	

	<s id="120">
		 Do the texts express the same ideas ? 
	</s>
	

	<s id="121">
		 If yes , was one of the texts probably written as a translation of the other ? 
	</s>
	

	<s id="122">
		 If yes , was the translation done roughly at the sentence level ? 
	</s>
	

	<s id="123">
		 The informant decided that in all 6 cases , the pair of texts expressed the same ideas . 
	</s>
	

	<s id="124">
		 However in 4 of the pairs , the two texts were probably written independently , rather than one as a translation of the other . 
	</s>
	

	<s id="125">
		 In the remaining two texts , the informant found large omissions on the English side , larger than what typical alignment algorithms can handle . 
	</s>
	

	<s id="126">
		 In these latter two documents , our Hindi informant also discovered an interesting phenomenon that we were not expecting — the sections that were translated were summarized to some degree . 
	</s>
	

	<s id="127">
		 I.e. , even in sections where the order of ideas was largely the same in the two languages , the English wording was much more terse ( the informant said ”compressed” ) , and omitted many details . 
	</s>
	

	<s id="128">
		 In summary , our method achieved 100 % precision in filtering out document pairs that were comparable but not parallel . 
	</s>
	

	<s id="129">
		 We then asked our informant to examine 3 document pairs that our method accepted as parallel . 
	</s>
	

	<s id="130">
		 After a cursory inspection , the informant answered yes to all 3 questions above for each of these pairs . 
	</s>
	

	<s id="131">
		 Unfortunately , it would have been very time-consuming to evaluate recall rigourously , because it would entail exhaustive reading of pairs of documents in parallel , to ensure that there were no non-parallel segments . 
	</s>
	

	<s id="132">
		 5 Conclusions We have shown that SIMR-cl , a modified version of the SIMR bitext mapping algorithm , can reliably discriminate between parallel and comparable texts . 
	</s>
	

	<s id="133">
		 We have demonstrated that SIMR-cl is effective on three language pairs , including two where no bilingual dictionary was available . 
	</s>
	

	<s id="134">
		 In addition , we have presented tentative evidence that the parameters of SIMR-cl are not very sensitive to particular language pairs or text genres on this task . 
	</s>
	

	<s id="135">
		 Our results suggest several new avenues for future research . 
	</s>
	

	<s id="136">
		 First , it would be useful to combine our method for filtering out non-parallel texts with methods for detecting omissions in translations 
		<ref citStr="Melamed , 1996" id="8" label="CEPF" position="17776">
			( Melamed , 1996 )
		</ref>
		 . 
	</s>
	

	<s id="137">
		 Some of the translations found on the web today might be made more literal by deleting the untranslated parts . 
	</s>
	

	<s id="138">
		 Second , we seem to have discovered the existence of training data for a machine learning approach to translation with summarization . 
	</s>
	

	<s id="139">
		 Third , our results suggest that the density of a bitext map is highly correlated with its accuracy , and that this correlation is largely invariant across language pairs and text genres . 
	</s>
	

	<s id="140">
		 If this is true , then it should be possible to train bitext mapping algorithms without any hand-aligned training data , by using map density as the objective function instead of RMS error . 
	</s>
	

	<s id="141">
		 Acknowledgements Thanks to Philip Resnik and Noah Smith for sharing STRAND data , human judgements , and tsim scores . 
	</s>
	

	<s id="142">
		 Thanks also to Noah Smith for providing a tsim implementation . 
	</s>
	

	<s id="143">
		 This research was sponsored by the DARPA TIDES program , by an NSF CAREER award , and by an equipment gift from Sun Microsystems . 
	</s>
	

	<s id="144">
		 References I. Dan Melamed . 
	</s>
	

	<s id="145">
		 1995. Automatic evaluation and uniform filter cascades for inducing n-best translation lexicons . 
	</s>
	

	<s id="146">
		 In Proceedings of the 3rd ACL Workshop on Very Large Corpora ( WVLC ) , Cambridge , Massachusetts . 
	</s>
	

	<s id="147">
		 I. Dan Melamed . 
	</s>
	

	<s id="148">
		 1996. Automatic detection of omissions in translations . 
	</s>
	

	<s id="149">
		 In Proceedings of the International Conference on Computational Linguistics ( COLING ) 1996 , pages 764–769 , Copenhagen , Denmark , August . 
	</s>
	

	<s id="150">
		 I. Dan Melamed . 
	</s>
	

	<s id="151">
		 1999. Bitext maps and alignment via pattern recognition . 
	</s>
	

	<s id="152">
		 Computational Linguistics , 25(1):107–139 , March . 
	</s>
	

	<s id="153">
		 D. Oard . 
	</s>
	

	<s id="154">
		 2003. The surprise language excercises . 
	</s>
	

	<s id="155">
		 In ACM Transactions on Asian Language Information Processing ( TALIP ) , pages 79–84 , New York , NY , June . 
	</s>
	

	<s id="156">
		 P. Resnik and N. A. Smith . 
	</s>
	

	<s id="157">
		 2003. The web as a parallel corpus . 
	</s>
	

	<s id="158">
		 Computational Linguistics , pages 349–380 , September . 
	</s>
	

	<s id="159">
		 D. Tufis . 
	</s>
	

	<s id="160">
		 1999. Multext-east 1984 corpus . 
	</s>
	

	<s id="161">
		 http://nl.ijs.si/ME/ . 
	</s>
	


</acldoc>
