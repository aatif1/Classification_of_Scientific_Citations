<?xml version="1.0" encoding="iso-8859-1"?>
<acldoc acl_id="P04-1040">
	

	<s id="1">
		 Enriching the Output of a Parser Using Memory-Based Learning Valentin Jijkoun and Maarten de Rijke Informatics Institute , University of Amsterdam jijkoun , mdr @science.uva.nl Abstract We describe a method for enriching the output of a parser with information available in a corpus . 
	</s>
	

	<s id="2">
		 The method is based on graph rewriting using memory- based learning , applied to dependency structures . 
	</s>
	

	<s id="3">
		 This general framework allows us to accurately recover both grammatical and semantic information as well as non-local dependencies . 
	</s>
	

	<s id="4">
		 It also facilitates dependency-based evaluation of phrase structure parsers . 
	</s>
	

	<s id="5">
		 Our method is largely independent of the choice of parser and corpus , and shows state of the art performance . 
	</s>
	

	<s id="6">
		 1 Introduction We describe a method to automatically enrich the output of parsers with information that is present in existing treebanks but usually not produced by the parsers themselves . 
	</s>
	

	<s id="7">
		 Our motivation is two-fold . 
	</s>
	

	<s id="8">
		 First and most important , for applications requiring information extraction or semantic interpretation of text , it is desirable to have parsers produce grammatically and semantically rich output . 
	</s>
	

	<s id="9">
		 Second , to facilitate dependency-based comparison and evaluation of different parsers , their outputs may need to be transformed into specific rich dependency formalisms . 
	</s>
	

	<s id="10">
		 The method allows us to automatically transform the output of a parser into structures as they are annotated in a dependency treebank . 
	</s>
	

	<s id="11">
		 For a phrase structure parser , we first convert the produced phrase structures into dependency graphs in a straightforward way , and then apply a sequence of graph transformations : changing dependency labels , adding new nodes , and adding new dependencies . 
	</s>
	

	<s id="12">
		 A memory-based learner trained on a dependency corpus is used to detect which modifications should be performed . 
	</s>
	

	<s id="13">
		 For a dependency corpus derived from the Penn Treebank and the parsers we considered , these transformations correspond to adding Penn functional tags ( e.g. , -SBJ , -TMP , -LOC ) , empty nodes ( e.g. , NP PRO ) and non-local dependencies ( controlled traces , WH extraction , etc. ) . 
	</s>
	

	<s id="14">
		 For these specific sub-tasks our method achieves state of the art performance . 
	</s>
	

	<s id="15">
		 The evaluation of the transformed output of the parsers of 
		<ref citStr="Charniak ( 2000 )" id="1" label="OEPF" position="2389">
			Charniak ( 2000 )
		</ref>
		 and 
		<ref citStr="Collins ( 1999 )" id="2" label="OEPF" position="2410">
			Collins ( 1999 )
		</ref>
		 gives 90 % unlabelled and 84 % labelled accuracy with respect to dependencies , when measured against a dependency corpus derived from the Penn Treebank . 
	</s>
	

	<s id="16">
		 The paper is organized as follows . 
	</s>
	

	<s id="17">
		 After providing some background and motivation in Section 2 , we give the general overview of our method in Section 3 . 
	</s>
	

	<s id="18">
		 In Sections 4 through 8 , we describe all stages of the transformation process , providing evaluation results and comparing our methods to earlier work . 
	</s>
	

	<s id="19">
		 We discuss the results in Section 9 . 
	</s>
	

	<s id="20">
		 2 Background and Motivation State of the art statistical parsers , e.g. , parsers trained on the Penn Treebank , produce syntactic parse trees with bare phrase labels , such as NP , PP , S , although the training corpora are usually much richer and often contain additional grammatical and semantic information ( distinguishing various modifiers , complements , subjects , objects , etc. ) , including non-local dependencies , i.e. , relations between phrases not adjacent in the parse tree . 
	</s>
	

	<s id="21">
		 While this information may be explicitly annotated in a treebank , it is rarely used or delivered by parsers.l The reason is that bringing in more information of this type usually makes the underlying parsing model more complicated : more parameters need to be estimated and independence assumptions may no longer hold . 
	</s>
	

	<s id="22">
		 
		<ref citStr="Klein and Manning ( 2003 )" id="3" label="CEPF" position="3817">
			Klein and Manning ( 2003 )
		</ref>
		 , for example , mention that using functional tags of the Penn Treebank ( temporal , location , subject , predicate , etc. ) with a simple unlexicalized PCFG generally had a negative effect on the parser’s performance . 
	</s>
	

	<s id="23">
		 Currently , there are no parsers trained on the Penn Treebank that use the structure of the treebank in full and that are thus ' Some notable exceptions are the CCG parser described in 
		<ref citStr="Hockenmaier , 2003" id="4" label="OEPF" position="4255">
			( Hockenmaier , 2003 )
		</ref>
		 , which incorporates non-local dependencies into the parser’s statistical model , and the parser of 
		<ref citStr="Collins ( 1999 )" id="5" label="OEPF" position="4373">
			Collins ( 1999 )
		</ref>
		 , which uses WH traces and argument/modifier distinctions . 
	</s>
	

	<s id="24">
		 capable of producing syntactic structures containing all or nearly all of the information annotated in the corpus . 
	</s>
	

	<s id="25">
		 In recent years there has been a growing interest in getting more information from parsers than just bare phrase trees . 
	</s>
	

	<s id="26">
		 
		<ref citStr="Blaheta and Charniak ( 2000 )" id="6" label="CEPF" position="4727">
			Blaheta and Charniak ( 2000 )
		</ref>
		 presented the first method for assigning Penn functional tags to constituents identified by a parser . 
	</s>
	

	<s id="27">
		 Pattern-matching approaches were used in 
		<ref citStr="Johnson , 2002" id="7" label="CEPF" position="4899">
			( Johnson , 2002 )
		</ref>
		 and 
		<ref citStr="Jijkoun , 2003" id="8" label="CEPF" position="4922">
			( Jijkoun , 2003 )
		</ref>
		 to recover non-local dependencies in phrase trees . 
	</s>
	

	<s id="28">
		 Furthermore , experiments described in 
		<ref citStr="Dienes and Dubey , 2003" id="9" label="CEPF" position="5050">
			( Dienes and Dubey , 2003 )
		</ref>
		 show that the latter task can be successfully addressed by shallow preprocessing methods . 
	</s>
	

	<s id="29">
		 3 An Overview of the Method In this section we give a high-level overview of our method for transforming a parser’s output and describe the different steps of the process . 
	</s>
	

	<s id="30">
		 In the experiments we used the parsers described in 
		<ref citStr="Charniak , 2000" id="10" label="OERF" position="5405">
			( Charniak , 2000 )
		</ref>
		 and 
		<ref citStr="Collins , 1999" id="11" label="OERF" position="5428">
			( Collins , 1999 )
		</ref>
		 . 
	</s>
	

	<s id="31">
		 For Collins’ parser the text was first POS-tagged using Ratnaparkhi’s maximum enthropy tagger . 
	</s>
	

	<s id="32">
		 The training phase of the method consists in learning which transformations need to be applied to the output of a parser to make it as similar to the treebank data as possible . 
	</s>
	

	<s id="33">
		 As a preliminary step ( Step 0 ) , we convert the WSJ2 to a dependency corpus without losing the annotated information ( functional tags , empty nodes , non-local dependencies ) . 
	</s>
	

	<s id="34">
		 The same conversion is applied to the output of the parsers we consider . 
	</s>
	

	<s id="35">
		 The details of the conversion process are described in Section 4 below . 
	</s>
	

	<s id="36">
		 The training then proceeds by comparing graphs derived from a parser’s output with the graphs from the dependency corpus , detecting various mismatches , such as incorrect arc labels and missing nodes or arcs . 
	</s>
	

	<s id="37">
		 Then the following steps are taken to fix the mismatches : Step 1 : changing arc labels Step 2 : adding new nodes Step 3 : adding new arcs Obviously , other modifications are possible , such as deleting arcs or moving arcs from one node to another . 
	</s>
	

	<s id="38">
		 We leave these for future work , though , and focus on the three transformations mentioned above . 
	</s>
	

	<s id="39">
		 The dependency corpus was split into training ( WSJ sections 02–21 ) , development ( sections 00– 2Thoughout the paper WSJ refers to the Penn Treebank II Wall Street Journal corpus . 
	</s>
	

	<s id="40">
		 01 ) and test ( section 23 ) corpora . 
	</s>
	

	<s id="41">
		 For each of the steps 1 , 2 and 3 we proceed as follows : 1. compare the training corpus to the output of the parser on the strings of the corpus , after applying the transformations of the previous steps 2. identify possible beneficial transformations ( which arc labels need to be changed or where new nodes or arcs need to be added ) 3. train a memory-based classifier to predict possible transformations given their context ( i.e. , information about the local structure of the dependency graph around possible application sites ) . 
	</s>
	

	<s id="42">
		 While the definitions of the context and application site and the graph modifications are different for the three steps , the general structure of the method remains the same at each stage . 
	</s>
	

	<s id="43">
		 Sections 6 , 7 and 8 describe the steps in detail . 
	</s>
	

	<s id="44">
		 In the application phase of the method , we proceed similarly . 
	</s>
	

	<s id="45">
		 First , the output of the parser is converted to dependency graphs , and then the learners trained during the steps 1 , 2 and 3 are applied in sequence to perform the graph transformations . 
	</s>
	

	<s id="46">
		 Apart from the conversion from phrase structures to dependency graphs and the extraction of some linguistic features for the learning , our method does not use any information about the details of the tree- bank annotation or the parser’s output : it works with arbitrary labelled directed graphs . 
	</s>
	

	<s id="47">
		 4 Step 0 : From Constituents to Dependencies To convert phrase trees to dependency structures , we followed the commonly used scheme 
		<ref citStr="Collins , 1999" id="12" label="CERF" position="8458">
			( Collins , 1999 )
		</ref>
		 . 
	</s>
	

	<s id="48">
		 The conversion routine,3 described below , is applied both to the original WSJ structures and the output of the parsers , though the former provides more information ( e.g. , traces ) which is used by the conversion routine if available . 
	</s>
	

	<s id="49">
		 First , for the treebank data , all traces are resolved and corresponding empty nodes are replaced with links to target constituents , so that syntactic trees become directed acyclic graphs . 
	</s>
	

	<s id="50">
		 Second , for each constituent we detect its head daughters ( more than one in the case of conjunction ) and identify lexical heads . 
	</s>
	

	<s id="51">
		 Then , for each constituent we output new dependencies between its lexical head and the lexical heads of its non-head daughters . 
	</s>
	

	<s id="52">
		 The label of every new dependency is the constituent’s phrase 3Our converter is available at http : //www. science . 
	</s>
	

	<s id="53">
		 uva.nl/˜jijkoun/software . 
	</s>
	

	<s id="54">
		 *^1 ( a ) to seek NP seats S planned this month NP^SBJ VP S NP^SBJ^1 directors NP^TMP VP S NP directors VP NP planned S VP to seek NP seats ( b ) this month planned Figure 1 : Example of ( a ) the Penn Treebank WSJ annotation , ( b ) the output of Charniak’s parser , and the results of the conversion to dependency structures of ( c ) the Penn tree and of ( d ) the parser’s output planned SINP VPIS directors SINP VPITO VPINP month seek seats NPIDT to this S I NP-SBJ VPIS directors seek VPINP seats month VPITO NPIDT to this S I NP-TMP ( d ) ( c ) S I NP-SBJ label , stripped of all functional tags and coindexing marks , conjoined with the label of the non-head daughter , with its functional tags but without coindexing marks . 
	</s>
	

	<s id="55">
		 Figure 1 shows an example of the original Penn annotation ( a ) , the output of Charniak’s parser ( b ) and the results of our conversion of these trees to dependency structures ( c and d ) . 
	</s>
	

	<s id="56">
		 The interpretation of the dependency labels is straightforward : e.g. , the label S NP-TMP corresponds to a sentence ( S ) being modified by a temporal noun phrase ( NP-TMP ) . 
	</s>
	

	<s id="57">
		 The core of the conversion routine is the selection of head daughters of the constituents . 
	</s>
	

	<s id="58">
		 Following 
		<ref citStr="Collins , 1999" id="13" label="CERN" position="10625">
			( Collins , 1999 )
		</ref>
		 , we used a head table , but extended it with a set of additional rules , based on constituent labels , POS tags or , sometimes actual words , to account for situations where the head table alone gave unsatisfactory results . 
	</s>
	

	<s id="59">
		 The most notable extension is our handling of conjunctions , which are often left relatively flat in WSJ and , as a result , in a parser’s output : we used simple pattern-based heuristics to detect conjuncts and mark all conjuncts as heads of a conjunction . 
	</s>
	

	<s id="60">
		 After the conversion , every resulting dependency structure is modified deterministically : auxiliary verbs ( be , do , have ) become dependents of corresponding main verbs ( similar to modal verbs , which are handled by the head table ) ; to fix a WSJ inconsistency , we move the -LGS tag ( indicating logical subject of passive in a by-phrase ) from the PP to its child NP . 
	</s>
	

	<s id="61">
		 5 Dependency-based Evaluation of Parsers After the original WSJ structures and the parsers’ outputs have been converted to dependency structures , we evaluate the performance of the parsers against the dependency corpus . 
	</s>
	

	<s id="62">
		 We use the standard precision/recall measures over sets of dependencies ( excluding punctuation marks , as usual ) and evaluate Collins’ and Charniak’s parsers on WSJ section 23 in three settings : on unlabelled dependencies ; on labelled dependencies with only bare labels ( all functional tags discarded ) ; on labelled dependencies with functional tags . 
	</s>
	

	<s id="63">
		 Notice that since neither Collins’ nor Charniak’s parser outputs WSJ functional labels , all dependencies with functional labels in the gold parse will be judged incorrect in the third setting . 
	</s>
	

	<s id="64">
		 The evaluation results are shown in Table 1 , in the row “step 0”.4 As explained above , the low numbers for the dependency evaluation with functional tags are expected , because the two parsers were not intended to produce functional labels . 
	</s>
	

	<s id="65">
		 Interestingly , the ranking of the two parsers is different for the dependency-based evaluation than for PARSEVAL : Charniak’s parser obtains a higher PARSEVAL score than Collins’ ( 89.0 % vs. 88.2 % ) , 4For meaningful comparison , the Collins’ tags -A and -g are removed in this evaluation . 
	</s>
	

	<s id="66">
		 Evaluation Parser unlabelled P labelled f with func. tags P R f P R f R after conversion Charniak 89.9 83.9 86.8 85.9 80.1 82.9 68.0 63.5 65.7 ( step 0 , Section 4 ) Collins 90.4 83.7 87.0 86.7 80.3 83.4 68.4 63.4 65.8 after relabelling Charniak 89.9 83.9 86.8 86.3 80.5 83.3 83.8 78.2 80.9 ( step 1 , Section 6 ) Collins 90.4 83.7 87.0 87.0 80.6 83.7 84.6 78.4 81.4 after adding nodes Charniak 90.1 85.4 87.7 86.5 82.0 84.2 84.1 79.8 81.9 ( step 2 , Section 7 ) Collins 90.6 85.3 87.9 87.2 82.1 84.6 84.9 79.9 82.3 after adding arcs Charniak 90.0 89.7 89.8 86.5 86.2 86.4 84.2 83.9 84.0 ( step 3 , Section 8 ) Collins 90.4 89.4 89.9 87.1 86.2 86.6 84.9 83.9 84.4 Table 1 : Dependency-based evaluation of the parsers after different transformation steps but slightly lower f-score on dependencies without functional tags ( 82.9 % vs. 83.4 % ) . 
	</s>
	

	<s id="67">
		 To summarize the evaluation scores at this stage , both parsers perform with f-score around 87 % on unlabelled dependencies . 
	</s>
	

	<s id="68">
		 When evaluating on bare dependency labels ( i.e. , disregarding functional tags ) the performance drops to 83 % . 
	</s>
	

	<s id="69">
		 The new errors that appear when taking labels into account come from different sources : incorrect POS tags ( NN vs. VBG ) , different degrees of flatness of analyses in gold and test parses ( JJ vs. ADJP , or CD vs. QP ) and inconsistencies in the Penn annotation ( VP vs. RRC ) . 
	</s>
	

	<s id="70">
		 Finally , the performance goes down to around 66 % when taking into account functional tags , which are not produced by the parsers at all . 
	</s>
	

	<s id="71">
		 6 Step 1 : Changing Dependency Labels Intuitively , it seems that the 66 % performance on labels with functional tags is an underestimation , because much of the missing information is easily recoverable . 
	</s>
	

	<s id="72">
		 E.g. , one can think of simple heuristics to distinguish subject NPs , temporal PPs , etc. , thus introducing functional labels and improving the scores . 
	</s>
	

	<s id="73">
		 Developing such heuristics would be a very time consuming and ad hoc process : e.g. , Collins’ -A and -g tags may give useful clues for this labelling , but they are not available in the output of other parsers . 
	</s>
	

	<s id="74">
		 As an alternative to hard- coded heuristics , 
		<ref citStr="Blaheta and Charniak ( 2000 )" id="14" label="CEPF" position="15114">
			Blaheta and Charniak ( 2000 )
		</ref>
		 proposed to recover the Penn functional tags automatically . 
	</s>
	

	<s id="75">
		 On the Penn Treebank , they trained a statistical model that , given a constituent in a parsed sentence and its context ( parent , grandparent , head words thereof etc. ) , predicted the functional label , possibly empty . 
	</s>
	

	<s id="76">
		 The method gave impressive performance , with 98.64 % accuracy on all constituents and 87.28 % f-score for non-empty functional labels , when applied to constituents correctly identified by Charniak’s parser . 
	</s>
	

	<s id="77">
		 If we extrapolate these re- sults to labelled PARSEVAL with functional labels , the method would give around 87.8 % performance ( 98.64 % of the “usual” 89 % ) for Charniak’s parser . 
	</s>
	

	<s id="78">
		 Adding functional labels can be viewed as a relabelling task : we need to change the labels produced by a parser . 
	</s>
	

	<s id="79">
		 We considered this more general task , and used a different approach , taking dependency graphs as input . 
	</s>
	

	<s id="80">
		 We first parsed the training part of our dependency tree- bank ( sections 02–21 ) and identified possible relabellings by comparing dependencies output by a parser to dependencies from the treebank . 
	</s>
	

	<s id="81">
		 E.g. , for Collins’ parser the most frequent relabellings were SNP SNP-SBJ , PP NP-A PP NP , VP NP-A VP NP , S NP-A S NP-SBJ and VP PP VP PP-CLR . 
	</s>
	

	<s id="82">
		 In total , around 30 % of all the parser’s dependencies had different labels in the treebank . 
	</s>
	

	<s id="83">
		 We then learned a mapping from the parser’s labels to those in the dependency corpus , using TiMBL , a memory-based classifier 
		<ref citStr="Daelemans et al. , 2003" id="15" label="OERF" position="16700">
			( Daelemans et al. , 2003 )
		</ref>
		 . 
	</s>
	

	<s id="84">
		 The features used for the relabelling were similar to those used by Blaheta and Charniak , but redefined for dependency structures . 
	</s>
	

	<s id="85">
		 For each dependency we included : the head ( ) and dependent ( ) , their POS tags ; the leftmost dependent of and its POS ; the head of ( ) , its POS and the label of the dependency the closest left and right siblings of ( dependents of ) and their POS tags ; the label of the dependency ( ) as derived from the parser’s output . 
	</s>
	

	<s id="86">
		 When included in feature vectors , all dependency labels were split at ‘ ’ , e.g. , the label S NP-A resulted in two features : S and NP-A . 
	</s>
	

	<s id="87">
		 Testing was done as follows . 
	</s>
	

	<s id="88">
		 The test corpus ( section 23 ) was also parsed , and for each dependency a feature vector was formed and given to ; TiMBL to correct the dependency label . 
	</s>
	

	<s id="89">
		 After this transformation the outputs of the parsers were evaluated , as before , on dependencies in the three settings . 
	</s>
	

	<s id="90">
		 The results of the evaluation are shown in Table 1 ( the row marked “step 1” ) . 
	</s>
	

	<s id="91">
		 Let us take a closer look at the evaluation results . 
	</s>
	

	<s id="92">
		 Obviously , relabelling does not change the unlabelled scores . 
	</s>
	

	<s id="93">
		 The 1 % improvement for evaluation on bare labels suggests that our approach is capable not only of adding functional tags , but can also correct the parser’s phrase labels and partof-speech tags : for Collins’ parser the most frequent correct changes not involving functional labels were NP NN NP JJ and NP JJ NP VBN , fixing POS tagging errors . 
	</s>
	

	<s id="94">
		 A very substantial increase of the labelled score ( from 66 % to 81 % ) , which is only 6 % lower than unlabelled score , clearly indicates that , although the parsers do not produce functional labels , this information is to a large extent implicitly present in trees and can be recovered . 
	</s>
	

	<s id="95">
		 6.1 Comparison to Earlier Work One effect of the relabelling procedure described above is the recovery of Penn functional tags . 
	</s>
	

	<s id="96">
		 Thus , it is informative to compare our results with those reported in 
		<ref citStr="Blaheta and Charniak , 2000" id="16" label="CEPF" position="18809">
			( Blaheta and Charniak , 2000 )
		</ref>
		 for this same task . 
	</s>
	

	<s id="97">
		 Blaheta and Charniak measured tagging accuracy and precision/recall for functional tag identification only for constituents correctly identified by the parser ( i.e. , having the correct span and nonterminal label ) . 
	</s>
	

	<s id="98">
		 Since our method uses the dependency formalism , to make a meaningful comparison we need to model the notion of a constituent being correctly found by a parser . 
	</s>
	

	<s id="99">
		 For a word we say that the constituent corresponding to its maximal projection is correctly identified if there exists , the head of , and for the dependency the right part of its label ( e.g. , NP-SBJ for S NP-SBJ ) is a nonterminal ( i.e. , not a POS tag ) and matches the right part of the label in the gold dependency structure , after stripping functional tags . 
	</s>
	

	<s id="100">
		 Thus , the constituent’s label and headword should be correct , but not necessarily the span . 
	</s>
	

	<s id="101">
		 Moreover , 2.5 % of all constituents with functional labels ( 246 out of 9928 in section 23 ) are not maximal projections . 
	</s>
	

	<s id="102">
		 Since our method ignores functional tags of such constituents ( these tags disappear after the conversion of phrase structures to dependency graphs ) , we consider them as errors , i.e. , reducing our recall value . 
	</s>
	

	<s id="103">
		 Below , the tagging accuracy , precision and recall are evaluated on constituents correctly identified by Charniak’s parser for section 23 . 
	</s>
	

	<s id="104">
		 Method Accuracy P R f Blaheta This paper 98.6 87.2 87.4 87.3 94.7 90.2 86.9 88.5 The difference in the accuracy is due to two reasons . 
	</s>
	

	<s id="105">
		 First , because of the different definition of a correctly identified constituent in the parser’s output , we apply our method to a greater portion of all labels produced by the parser ( 95 % vs. 89 % reported in 
		<ref citStr="Blaheta and Charniak , 2000" id="17" label="OJPF" position="20619">
			( Blaheta and Charniak , 2000 )
		</ref>
		 ) . 
	</s>
	

	<s id="106">
		 This might make the task for out system more difficult . 
	</s>
	

	<s id="107">
		 And second , whereas 22 % of all constituents in section 23 have a functional tag , 36 % of the maximal projections have one . 
	</s>
	

	<s id="108">
		 Since we apply our method only to labels of maximal projections , this means that our accuracy baseline ( i.e. , never assign any tag ) is lower . 
	</s>
	

	<s id="109">
		 7 Step 2 : Adding Missing Nodes As the row labelled “step 1” in Table 1 indicates , for both parsers the recall is relatively low ( 6 % lower than the precision ) : while the WSJ trees , and hence the derived dependency structures , contain non-local dependencies and empty nodes , the parsers simply do not provide this information . 
	</s>
	

	<s id="110">
		 To make up for this , we considered two further tranformations of the output of the parsers : adding new nodes ( corresponding to empty nodes in WSJ ) , and adding new labelled arcs . 
	</s>
	

	<s id="111">
		 This section describes the former modification and Section 8 the latter . 
	</s>
	

	<s id="112">
		 As described in Section 4 , when converting WSJ trees to dependency structures , traces are resolved , their empty nodes removed and new dependencies introduced . 
	</s>
	

	<s id="113">
		 Of the remaining empty nodes ( i.e. , non-traces ) , the most frequent in WSJ are : NP PRO , empty units , empty complementizers , empty relative pronouns . 
	</s>
	

	<s id="114">
		 To add missing empty nodes to dependency graphs , we compared the output of the parsers on the strings of the training corpus after steps 0 and 1 ( conversion to dependencies and relabelling ) to the structures in the corpus itself . 
	</s>
	

	<s id="115">
		 We trained a classifier which , for every word in the parser’s output , had to decide whether an empty node should be added as a new dependent of the word , and what its symbol ( ‘*’ , ‘*U*’ or ‘0’ in WSJ ) , POS tag ( always -NONE- in WSJ ) and the label of the new dependency ( e.g. , ‘S NP-SBJ’ for NP PRO and ‘VP SBAR’ for empty complementizers ) should be . 
	</s>
	

	<s id="116">
		 This decision is conditioned on the word itself and its context . 
	</s>
	

	<s id="117">
		 The features used were : the word and its POS tag , whether the word has any subject and object dependents , and whether it is the head of a finite verb group ; the same information for the word’s head ( if any ) and also the label of the corresponding dependency ; the same information for the rightmost and leftmost dependents of the word ( if exist ) along with their dependency labels . 
	</s>
	

	<s id="118">
		 In total , we extracted 23 symbolic features for every word in the corpus . 
	</s>
	

	<s id="119">
		 TiMBL was trained on sections 02–21 and applied to the output of the parsers ( after steps 0 and 1 ) on the test corpus ( section 23 ) , producing a list of empty nodes to be inserted in the dependency graphs . 
	</s>
	

	<s id="120">
		 After insertion of the empty nodes , the resulting structures were evaluated against section 23 of the gold dependency treebank . 
	</s>
	

	<s id="121">
		 The results are shown in Table 1 ( the row “step 2” ) . 
	</s>
	

	<s id="122">
		 For both parsers the insertion of empty nodes improves the recall by 1.5 % , resulting in a 1 % increase of the f-score . 
	</s>
	

	<s id="123">
		 7.1 Comparison to Earlier Work A procedure for empty node recovery was first described in 
		<ref citStr="Johnson , 2002" id="18" label="CEPF" position="23804">
			( Johnson , 2002 )
		</ref>
		 , along with an evaluation criterion : an empty node is correct if its category and position in the sentence are correct . 
	</s>
	

	<s id="124">
		 Since our method works with dependency structures , not phrase trees , we adopt a different but comparable criterion : an empty node should be attached as a dependent to the correct word , and with the correct dependency label . 
	</s>
	

	<s id="125">
		 Unlike the first metric , our correctness criterion also requires that possible attachment ambiguities are resolved correctly ( e.g. , as in the number of reports 0 they sent , where the empty relative pronoun may be attached either to number or to reports ) . 
	</s>
	

	<s id="126">
		 For this task , the best published results ( using Johnson’s metric ) were reported by 
		<ref citStr="Dienes and Dubey ( 2003 )" id="19" label="CEPF" position="24558">
			Dienes and Dubey ( 2003 )
		</ref>
		 , who used shallow tagging to insert empty elements . 
	</s>
	

	<s id="127">
		 Below we give the comparison to our method . 
	</s>
	

	<s id="128">
		 Notice that this evaluation does not include traces ( i.e. , empty elements with antecedents ) : recovery of traces is described in Section 8 . 
	</s>
	

	<s id="129">
		 This paper Dienes&amp;Dubey P R f P R f PRO-NP 73.1 63.89 68.1 68.7 70.4 69.5 COMP-SBAR 82.6 83.1 82.8 93.8 78.6 85.5 COMP-VRHNP 65.3 40.0 49.6 67.2 38.3 48.8 UNIT 95.4 91.8 93.6 99.1 92.5 95.7 For comparison we use the notation of Dienes and Dubey : PRO-NP for uncontrolled PROs ( nodes ‘*’ in the WSJ ) , COMP-SBAR for empty complementizers ( nodes ‘0’ with dependency label VP SBAR ) , COMP-WHNP for empty relative pronouns ( nodes ‘0’ with dependency label X SBAR , where X VP ) and UNIT for empty units ( nodes ‘*U*’ ) . 
	</s>
	

	<s id="130">
		 It is interesting to see that for empty nodes except for UNIT both methods have their advantages , showing better precision or better recall . 
	</s>
	

	<s id="131">
		 Yet shallow tagging clearly performs better for UNIT . 
	</s>
	

	<s id="132">
		 8 Step 3 : Adding Missing Dependencies We now get to the third and final step of our transformation method : adding missing arcs to dependency graphs . 
	</s>
	

	<s id="133">
		 The parsers we considered do not explicitly provide information about non-local dependencies ( control , WH-extraction ) present in the treebank . 
	</s>
	

	<s id="134">
		 Moreover , newly inserted empty nodes ( step 2 , Section 7 ) might also need more links to the rest of a sentence ( e.g. , the inserted empty complementizers ) . 
	</s>
	

	<s id="135">
		 In this section we describe the insertion of missing dependencies . 
	</s>
	

	<s id="136">
		 
		<ref citStr="Johnson ( 2002 )" id="20" label="CEPF" position="26169">
			Johnson ( 2002 )
		</ref>
		 was the first to address recovery of non-local dependencies in a parser’s output . 
	</s>
	

	<s id="137">
		 He proposed a pattern-matching algorithm : first , from the training corpus the patterns that license non- local dependencies are extracted , and then these patterns are detected in unseen trees , dependencies being added when matches are found . 
	</s>
	

	<s id="138">
		 Building on these ideas , 
		<ref citStr="Jijkoun ( 2003 )" id="21" label="CEPF" position="26561">
			Jijkoun ( 2003 )
		</ref>
		 used a machine learning classifier to detect matches . 
	</s>
	

	<s id="139">
		 We extended Jijkoun’s approach by providing the classifier with lexical information and using richer patterns with labels containing the Penn functional tags and empty nodes , detected at steps 1 and 2 . 
	</s>
	

	<s id="140">
		 First , we compared the output of the parsers on the strings of the training corpus after steps 0 , 1 and 2 to the dependency structures in the training corpus . 
	</s>
	

	<s id="141">
		 For every dependency that is missing in the parser’s output , we find the shortest undirected path in the dependency graph connecting the head and the dependent . 
	</s>
	

	<s id="142">
		 These paths , connected sequences of labelled dependencies , define the set of possible patterns . 
	</s>
	

	<s id="143">
		 For our experiments we only considered patterns occuring more than 100 times in the training corpus . 
	</s>
	

	<s id="144">
		 E.g. , for Collins’ parser , 67 different patterns were found . 
	</s>
	

	<s id="145">
		 Next , from the parsers’ output on the strings of the training corpus , we extracted all occurrences of the patterns , along with information about the nodes involved . 
	</s>
	

	<s id="146">
		 For every node in an occurrence of a pattern we extracted the following features : the word and its POS tag ; whether the word has subject and object dependents ; whether the word is the head of a finite verb cluster . 
	</s>
	

	<s id="147">
		 Type We then trained TiMBL to predict the label of the missing dependency ( or ‘none’ ) , given an occurrence of a pattern and the features of all the nodes involved . 
	</s>
	

	<s id="148">
		 We trained a separate classifier for each pattern . 
	</s>
	

	<s id="149">
		 For evaluation purposes we extracted all occurrences of the patterns and the features of their nodes from the parsers’ outputs for section 23 after steps 0 , 1 and 2 and used TiMBL to predict and insert new dependencies . 
	</s>
	

	<s id="150">
		 Then we compared the resulting dependency structures to the gold corpus . 
	</s>
	

	<s id="151">
		 The results are shown in Table 1 ( the row “step 3” ) . 
	</s>
	

	<s id="152">
		 As expected , adding missing dependencies substantially improves the recall ( by 4 % for both parsers ) and allows both parsers to achieve an 84 % f-score on dependencies with functional tags ( 90 % on unlabelled dependencies ) . 
	</s>
	

	<s id="153">
		 The unlabelled f-score 89.9 % for Collins’ parser is close to the 90.9 % reported in 
		<ref citStr="Collins , 1999" id="22" label="OEPF" position="28849">
			( Collins , 1999 )
		</ref>
		 for the evaluation on unlabelled local dependencies only ( without empty nodes and traces ) . 
	</s>
	

	<s id="154">
		 Since as many as 5 % of all dependencies in WSJ involve traces or empty nodes , the results in Table 1 are encouraging . 
	</s>
	

	<s id="155">
		 8.1 Comparison to Earlier Work Recently , several methods for the recovery of non- local dependencies have been described in the literature . 
	</s>
	

	<s id="156">
		 
		<ref citStr="Johnson ( 2002 )" id="23" label="CEPF" position="29250">
			Johnson ( 2002 )
		</ref>
		 and 
		<ref citStr="Jijkoun ( 2003 )" id="24" label="CEPF" position="29271">
			Jijkoun ( 2003 )
		</ref>
		 used pattern-matching on local phrase or dependency structures . 
	</s>
	

	<s id="157">
		 
		<ref citStr="Dienes and Dubey ( 2003 )" id="25" label="CERF" position="29371">
			Dienes and Dubey ( 2003 )
		</ref>
		 used shallow preprocessing to insert empty elements in raw sentences , making the parser itself capable of finding non-local dependencies . 
	</s>
	

	<s id="158">
		 Their method achieves a considerable improvement over the results reported in 
		<ref citStr="Johnson , 2002" id="26" label="CJPN" position="29617">
			( Johnson , 2002 )
		</ref>
		 and gives the best evaluation results published to date . 
	</s>
	

	<s id="159">
		 To compare our results to Dienes and Dubey’s , we carried out the transformation steps 0–3 described above , with a single modification : when adding missing dependencies ( step 3 ) , we only considered patterns that introduce non- local dependencies ( i.e. , traces : we kept the information whether a dependency is a trace when converting WSJ to a dependency corpus ) . 
	</s>
	

	<s id="160">
		 As before , a dependency is correctly found if its head , dependent , and label are correct . 
	</s>
	

	<s id="161">
		 For traces , this corresponds to the evaluation using the head-based antecedent representation described in 
		<ref citStr="Johnson , 2002" id="27" label="CEPF" position="30297">
			( Johnson , 2002 )
		</ref>
		 , and for empty nodes without antecedents ( e.g. , NP PRO ) this is the measure used in Section 7.1 . 
	</s>
	

	<s id="162">
		 To make the results comparable to other methods , we strip functional tags from the dependency labels before label comparison . 
	</s>
	

	<s id="163">
		 Below are the overall precision , recall , and f-score for our method and the scores reported in 
		<ref citStr="Dienes and Dubey , 2003" id="28" label="CEPF" position="30670">
			( Dienes and Dubey , 2003 )
		</ref>
		 for antecedent recovery using Collins’ parser . 
	</s>
	

	<s id="164">
		 Method P R f Dienes and Dubey This paper 81.5 68.7 74.6 82.8 67.8 74.6 Interestingly , the overall performance of our post- processing method is very similar to that of the pre- and in-processing methods of 
		<ref citStr="Dienes and Dubey ( 2003 )" id="29" label="CEPF" position="30961">
			Dienes and Dubey ( 2003 )
		</ref>
		 . 
	</s>
	

	<s id="165">
		 Hence , for most cases , traces and empty nodes can be reliably identified using only local information provided by a parser , using the parser itself as a black box . 
	</s>
	

	<s id="166">
		 This is important , since making parsers aware of non-local relations need not improve the overall performance : 
		<ref citStr="Dienes and Dubey ( 2003 )" id="30" label="OEPF" position="31288">
			Dienes and Dubey ( 2003 )
		</ref>
		 report a decrease in PARSEVAL f- score from 88.2 % to 86.4 % after modifying Collins’ parser to resolve traces internally , although this allowed them to achieve high accuracy for traces . 
	</s>
	

	<s id="167">
		 9 Discussion The experiments described in the previous sections indicate that although statistical parsers do not explicitly output some information available in the corpus they were trained on ( grammatical and semantic tags , empty nodes , non-local dependencies ) , this information can be recovered with reasonably high accuracy , using pattern matching and machine learning methods . 
	</s>
	

	<s id="168">
		 For our task , using dependency structures rather than phrase trees has several advantages . 
	</s>
	

	<s id="169">
		 First , after converting both the treebank trees and parsers’ outputs to graphs with head–modifier relations , our method needs very little information about the linguistic nature of the data , and thus is largely corpus- and parser-independent . 
	</s>
	

	<s id="170">
		 Indeed , after the conversion , the only linguistically informed operation is the straightforward extraction of features indicating the presence of subject and object dependents , and finiteness of verb groups . 
	</s>
	

	<s id="171">
		 Second , using a dependency formalism facilitates a very straightforward evaluation of the systems that produce structures more complex than trees . 
	</s>
	

	<s id="172">
		 It is not clear whether the PARSEVAL evaluation can be easily extended to take non-local relations into account ( see 
		<ref citStr="Johnson , 2002" id="31" label="CEPF" position="32761">
			( Johnson , 2002 )
		</ref>
		 for examples of such extension ) . 
	</s>
	

	<s id="173">
		 Finally , the independence from the details of the parser and the corpus suggests that our method can be applied to systems based on other formalisms , e.g. , 
		<ref citStr="Hockenmaier , 2003" id="32" label="CEPF" position="32987">
			( Hockenmaier , 2003 )
		</ref>
		 , to allow a meaningful dependency-based comparison of very different parsers . 
	</s>
	

	<s id="174">
		 Furthermore , with the fine-grained set of dependency labels that our system provides , it is possible to map the resulting structures to other dependency formalisms , either automatically in case annotated corpora exist , or with a manually developed set of rules . 
	</s>
	

	<s id="175">
		 Our preliminary experiments with Collins’ parser and the corpus annotated with grammatical relations 
		<ref citStr="Carroll et al. , 2003" id="33" label="OEPF" position="33480">
			( Carroll et al. , 2003 )
		</ref>
		 are promising : the system achieves 76 % precision/recall f- score , after the parser’s output is enriched with our method and transformed to grammatical relations using a set of 40 simple rules . 
	</s>
	

	<s id="176">
		 This is very close to the performance reported by 
		<ref citStr="Carroll et al . ( 2003 )" id="34" label="CEPF" position="33762">
			Carroll et al . ( 2003 )
		</ref>
		 for the parser specifically designed for the extraction of grammatical relations . 
	</s>
	

	<s id="177">
		 Despite the high-dimensional feature spaces , the large number of lexical features , and the lack of independence between features , we achieved high accuracy using a memory-based learner . 
	</s>
	

	<s id="178">
		 TiMBL performed well on tasks where structured , more complicated and task-specific statistical models have been used previously 
		<ref citStr="Blaheta and Charniak , 2000" id="35" label="CJPN" position="34214">
			( Blaheta and Charniak , 2000 )
		</ref>
		 . 
	</s>
	

	<s id="179">
		 For all subtasks we used the same settings for TiMBL : simple feature overlap measure , 5 nearest neighbours with majority voting . 
	</s>
	

	<s id="180">
		 During further experiments with our method on different corpora , we found that quite different settings led to a better performance . 
	</s>
	

	<s id="181">
		 It is clear that more careful and systematic parameter tuning and the analysis of the contribution of different features have to be addressed . 
	</s>
	

	<s id="182">
		 Finally , our method is not restricted to syntactic structures . 
	</s>
	

	<s id="183">
		 It has been successfully applied to the identification of semantic relations 
		<ref citStr="Ahn et al. , 2004" id="36" label="CEPF" position="34836">
			( Ahn et al. , 2004 )
		</ref>
		 , using FrameNet as the training corpus . 
	</s>
	

	<s id="184">
		 For this task , we viewed semantic relations ( e.g. , Speaker , Topic , Addressee ) as dependencies between a predicate and its arguments . 
	</s>
	

	<s id="185">
		 Adding such semantic relations to syntactic dependency graphs was simply an additional graph transformation step . 
	</s>
	

	<s id="186">
		 10 Conclusions We presented a method to automatically enrich the output of a parser with information that is not provided by the parser itself , but is available in a tree- bank . 
	</s>
	

	<s id="187">
		 Using the method with two state of the art statistical parsers and the Penn Treebank allowed us to recover functional tags ( grammatical and semantic ) , empty nodes and traces . 
	</s>
	

	<s id="188">
		 Thus , we are able to provide virtually all information available in the corpus , without modifying the parser , viewing it , indeed , as a black box . 
	</s>
	

	<s id="189">
		 Our method allows us to perform a meaningful dependency-based comparison of phrase structure parsers . 
	</s>
	

	<s id="190">
		 The evaluation on a dependency corpus derived from the Penn Treebank showed that , after our post-processing , two state of the art statistical parsers achieve 84 % accuracy on a fine-grained set of dependency labels . 
	</s>
	

	<s id="191">
		 Finally , our method for enriching the output of a parser is , to a large extent , independent of a specific parser and corpus , and can be used with other syntactic and semantic resources . 
	</s>
	

	<s id="192">
		 11 Acknowledgements We are grateful to David Ahn and Stefan Schlobach and to the anonymous referees for their useful suggestions . 
	</s>
	

	<s id="193">
		 This research was supported by grants from the Netherlands Organization for Scientific Research ( NWO ) under project numbers 220- 80-001 , 365-20-005 , 612.069.006 , 612.000.106 , 612.000.207 and 612.066.302 . 
	</s>
	

	<s id="194">
		 References David Ahn , Sisay Fissaha , Valentin Jijkoun , and Maarten de Rijke . 
	</s>
	

	<s id="195">
		 2004. The University of Amsterdam at Senseval-3 : semantic roles and logic forms . 
	</s>
	

	<s id="196">
		 In Proceedings of the ACL-2004 Workshop on Evaluation of Systems for the Semantic Analysis of Text . 
	</s>
	

	<s id="197">
		 Don Blaheta and Eugene Charniak . 
	</s>
	

	<s id="198">
		 2000 . 
	</s>
	

	<s id="199">
		 Assigning function tags to parsed text . 
	</s>
	

	<s id="200">
		 In Proceedings of the 1st Meeting ofNAACL , pages 234–240 . 
	</s>
	

	<s id="201">
		 John Carroll , Guido Minnen , and Ted Briscoe . 
	</s>
	

	<s id="202">
		 2003. Parser evaluation using a grammatical relation annotation scheme . 
	</s>
	

	<s id="203">
		 In Anne Abeill´e , editor , Building and Using Parsed Corpora , pages 299–316 . 
	</s>
	

	<s id="204">
		 Kluwer . 
	</s>
	

	<s id="205">
		 Eugene Charniak . 
	</s>
	

	<s id="206">
		 2000. A maximum-entropy-inspired parser . 
	</s>
	

	<s id="207">
		 In Proceedings of the 1 st Meeting of NAACL , pages 132–139 . 
	</s>
	

	<s id="208">
		 Michael Collins . 
	</s>
	

	<s id="209">
		 1999. Head-Driven Statistical Models for Natural Language Parsing . 
	</s>
	

	<s id="210">
		 Ph.D . 
	</s>
	

	<s id="211">
		 thesis , University of Pennsylvania . 
	</s>
	

	<s id="212">
		 Walter Daelemans , Jakub Zavrel , Ko van der Sloot , and Antal van den Bosch , 2003 . 
	</s>
	

	<s id="213">
		 TiMBL : Tilburg Memory Based Learner , version 5. 0 , Reference Guide . 
	</s>
	

	<s id="214">
		 ILK Technical Report 03-10 . 
	</s>
	

	<s id="215">
		 Available from http://ilk.kub.nl/downloads/pub/papers/ilk0310.ps.gz . 
	</s>
	

	<s id="216">
		 P´eter Dienes and Amit Dubey . 
	</s>
	

	<s id="217">
		 2003. Antecedent recovery : Experiments with a trace tagger . 
	</s>
	

	<s id="218">
		 In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing , pages 33–40 . 
	</s>
	

	<s id="219">
		 Julia Hockenmaier . 
	</s>
	

	<s id="220">
		 2003. Parsing with generative models of predicate-argument structure . 
	</s>
	

	<s id="221">
		 In Proceedings of the 41st Meeting ofACL , pages 359–366 . 
	</s>
	

	<s id="222">
		 Valentin Jijkoun . 
	</s>
	

	<s id="223">
		 2003. Finding non-local dependencies : Beyond pattern matching . 
	</s>
	

	<s id="224">
		 In Proceedings of the ACL-2003 Student Research Workshop , pages 37–43 . 
	</s>
	

	<s id="225">
		 Mark Johnson . 
	</s>
	

	<s id="226">
		 2002. A simple pattern-matching algorithm for recovering empty nodes and their antecedents . 
	</s>
	

	<s id="227">
		 In Proceedings of the 40th meeting ofACL , pages 136–143 . 
	</s>
	

	<s id="228">
		 Dan Klein and Christopher D. Manning . 
	</s>
	

	<s id="229">
		 2003 . 
	</s>
	

	<s id="230">
		 Accurate unlexicalized parsing . 
	</s>
	

	<s id="231">
		 In Proceedings of the 41st Meeting ofACL , pages 423–430 . 
	</s>
	


</acldoc>
