<?xml version="1.0" encoding="iso-8859-1"?>
<acldoc acl_id="P04-1007">
	

	<s id="1">
		 Discriminative Language Modeling with Conditional Random Fields and the Perceptron Algorithm Brian Roark Murat Saraclar Michael Collins Mark Johnson AT&amp;T Labs - Research MIT CSAIL Brown University {roark,murat}@research.att.com mcollins@csail.mit.edu Mark Johnson@Brown.edu Abstract This paper describes discriminative language modeling for a large vocabulary speech recognition task . 
	</s>
	

	<s id="2">
		 We contrast two parameter estimation methods : the perceptron algorithm , and a method based on conditional random fields ( CRFs ) . 
	</s>
	

	<s id="3">
		 The models are encoded as deterministic weighted finite state automata , and are applied by intersecting the automata with word-lattices that are the output from a baseline recognizer . 
	</s>
	

	<s id="4">
		 The perceptron algorithm has the benefit of automatically selecting a relatively small feature set in just a couple of passes over the training data . 
	</s>
	

	<s id="5">
		 However , using the feature set output from the perceptron algorithm ( initialized with their weights ) , CRF training provides an additional 0.5 % reduction in word error rate , for a total 1.8 % absolute reduction from the baseline of 39.2 % . 
	</s>
	

	<s id="6">
		 1 Introduction A crucial component of any speech recognizer is the language model ( LM ) , which assigns scores or probabilities to candidate output strings in a speech recognizer . 
	</s>
	

	<s id="7">
		 The language model is used in combination with an acoustic model , to give an overall score to candidate word sequences that ranks them in order of probability or plausibility . 
	</s>
	

	<s id="8">
		 A dominant approach in speech recognition has been to use a “source-channel” , or “noisy-channel” model . 
	</s>
	

	<s id="9">
		 In this approach , language modeling is effectively framed as density estimation : the language model’s task is to define a distribution over the source – i.e. , the possible strings in the language . 
	</s>
	

	<s id="10">
		 Markov ( n-gram ) models are often used for this task , whose parameters are optimized to maximize the likelihood of a large amount of training text . 
	</s>
	

	<s id="11">
		 Recognition performance is a direct measure of the effectiveness of a language model ; an indirect measure which is frequently proposed within these approaches is the perplexity of the LM ( i.e. , the log probability it assigns to some held-out data set ) . 
	</s>
	

	<s id="12">
		 This paper explores alternative methods for language modeling , which complement the source-channel approach through discriminatively trained models . 
	</s>
	

	<s id="13">
		 The language models we describe do not attempt to estimate a generative model P(w) over strings . 
	</s>
	

	<s id="14">
		 Instead , they are trained on acoustic sequences with their transcriptions , in an attempt to directly optimize error-rate . 
	</s>
	

	<s id="15">
		 Our work builds on previous work on language modeling using the perceptron algorithm , described in 
		<ref citStr="Roark et al . ( 2004 )" id="1" label="CERF" position="2815">
			Roark et al . ( 2004 )
		</ref>
		 . 
	</s>
	

	<s id="16">
		 In particular , we explore conditional random field methods , as an alternative training method to the perceptron . 
	</s>
	

	<s id="17">
		 We describe how these models can be trained over lat- tices that are the output from a baseline recognizer . 
	</s>
	

	<s id="18">
		 We also give a number of experiments comparing the two approaches . 
	</s>
	

	<s id="19">
		 The perceptron method gave a 1.3 % absolute improvement in recognition error on the Switchboard domain ; the CRF methods we describe give a further gain , the final absolute improvement being 1.8 % . 
	</s>
	

	<s id="20">
		 A central issue we focus on concerns feature selection . 
	</s>
	

	<s id="21">
		 The number of distinct n-grams in our training data is close to 45 million , and we show that CRF training converges very slowly even when trained with a subset ( of size 12 million ) of these features . 
	</s>
	

	<s id="22">
		 Because of this , we explore methods for picking a small subset of the available features . 
	</s>
	

	<s id="23">
		 ' The perceptron algorithm can be used as one method for feature selection , selecting around 1.5 million features in total . 
	</s>
	

	<s id="24">
		 The CRF trained with this feature set , and initialized with parameters from perceptron training , converges much more quickly than other approaches , and also gives the optimal performance on the held-out set . 
	</s>
	

	<s id="25">
		 We explore other approaches to feature selection , but find that the perceptron-based approach gives the best results in our experiments . 
	</s>
	

	<s id="26">
		 While we focus on n-gram models , we stress that our methods are applicable to more general language modeling features – for example , syntactic features , as explored in , e.g. , 
		<ref citStr="Khudanpur and Wu ( 2000 )" id="2" label="CEPF" position="4446">
			Khudanpur and Wu ( 2000 )
		</ref>
		 . 
	</s>
	

	<s id="27">
		 We intend to explore methods with new features in the future . 
	</s>
	

	<s id="28">
		 Experimental results with n-gram models on 1000-best lists show a very small drop in accuracy compared to the use of lattices . 
	</s>
	

	<s id="29">
		 This is encouraging , in that it suggests that models with more flexible features than n-gram models , which therefore cannot be efficiently used with lattices , may not be unduly harmed by their restriction to n-best lists . 
	</s>
	

	<s id="30">
		 1.1 Related Work Large vocabulary ASR has benefitted from discriminative estimation of Hidden Markov Model ( HMM ) parameters in the form of Maximum Mutual Information Estimation ( MMIE ) or Conditional Maximum Likelihood Estimation ( CMLE ) . 
	</s>
	

	<s id="31">
		 
		<ref citStr="Woodland and Povey ( 2000 )" id="3" label="CEPF" position="5182">
			Woodland and Povey ( 2000 )
		</ref>
		 have shown the effectiveness of lattice-based MMIE/CMLE in challenging large scale ASR tasks such as Switchboard . 
	</s>
	

	<s id="32">
		 In fact , state-of-the-art acoustic modeling , as seen , for example , at annual Switchboard evaluations , invariably includes some kind of discriminative training . 
	</s>
	

	<s id="33">
		 Discriminative estimation of language models has also been proposed in recent years . 
	</s>
	

	<s id="34">
		 
		<ref citStr="Jelinek ( 1995 )" id="4" label="CEPF" position="5593">
			Jelinek ( 1995 )
		</ref>
		 suggested an acoustic sensitive language model whose parameters ' Note also that in addition to concerns about training time , a lan- guage model with fewer features is likely to be considerably more efficient when decoding new utterances . 
	</s>
	

	<s id="35">
		 are estimated by minimizing H(W |A ) , the expected uncertainty of the spoken text W , given the acoustic sequence A. 
		<ref citStr="Stolcke and Weintraub ( 1998 )" id="5" label="CEPF" position="5992">
			Stolcke and Weintraub ( 1998 )
		</ref>
		 experimented with various discriminative approaches including MMIE with mixed results . 
	</s>
	

	<s id="36">
		 This work was followed up with some success by 
		<ref citStr="Stolcke et al . ( 2000 )" id="6" label="CEPF" position="6161">
			Stolcke et al . ( 2000 )
		</ref>
		 where an “anti- LM” , estimated from weighted N-best hypotheses of a baseline ASR system , was used with a negative weight in combination with the baseline LM . 
	</s>
	

	<s id="37">
		 
		<ref citStr="Chen et al . ( 2000 )" id="7" label="CEPF" position="6355">
			Chen et al . ( 2000 )
		</ref>
		 presented a method based on changing the trigram counts discriminatively , together with changing the lexicon to add new words . 
	</s>
	

	<s id="38">
		 
		<ref citStr="Kuo et al . ( 2002 )" id="8" label="CEPF" position="6514">
			Kuo et al . ( 2002 )
		</ref>
		 used the generalized probabilistic descent algorithm to train relatively small language models which attempt to minimize string error rate on the DARPA Communicator task . 
	</s>
	

	<s id="39">
		 
		<ref citStr="Banerjee et al . ( 2003 )" id="9" label="CEPF" position="6721">
			Banerjee et al . ( 2003 )
		</ref>
		 used a language model modification algorithm in the context of a reading tutor that listens . 
	</s>
	

	<s id="40">
		 Their algorithm first uses a classifier to predict what effect each parameter has on the error rate , and then modifies the parameters to reduce the error rate based on this prediction . 
	</s>
	

	<s id="41">
		 2 Linear Models , the Perceptron Algorithm , and Conditional Random Fields This section describes a general framework , global linear models , and two parameter estimation methods within the framework , the perceptron algorithm and a method based on conditional random fields . 
	</s>
	

	<s id="42">
		 The linear models we describe are general enough to be applicable to a diverse range of NLP and speech tasks – this section gives a general description of the approach . 
	</s>
	

	<s id="43">
		 In the next section of the paper we describe how global linear models can be applied to speech recognition . 
	</s>
	

	<s id="44">
		 In particular , we focus on how the decoding and parameter estimation problems can be implemented over lattices using finite-state techniques . 
	</s>
	

	<s id="45">
		 2.1 Global linear models We follow the framework outlined in Collins ( 2002 ; 2004 ) . 
	</s>
	

	<s id="46">
		 The task is to learn a mapping from inputs x E X to outputs y E Y. We assume the following compo- nents : ( 1 ) Training examples ( xi , yi ) for i = 1 ... 
	</s>
	

	<s id="47">
		 N. ( 2 ) A function GEN which enumerates a set of candidates GEN(x) for an input x. ( 3 ) A representation 4 ) mapping each ( x , y ) E X x Y to a feature vector 4)(x , y ) E Rd. ( 4 ) A parameter vector a¯ E Rd. . 
	</s>
	

	<s id="48">
		 The components GEN , 4 ) and a¯ define a mapping from an input x to an output F(x) through F(x) = argmax 4)(x , y ) • a¯ ( 1 ) yEGEN(x) where 4)(x , y ) • a¯ is the inner product Ps as4)s(x , y ) . 
	</s>
	

	<s id="49">
		 The learning task is to set the parameter values a¯ using the training examples as evidence . 
	</s>
	

	<s id="50">
		 The decoding algorithm is a method for searching for the y that maximizes Eq . 
	</s>
	

	<s id="51">
		 1. 2.2 The Perceptron algorithm We now turn to methods for training the parameters a¯ of the model , given a set of training examples Inputs : Training examples ( xi , yi ) Initialization : Set a¯ = 0 Algorithm : Fort =1 ... 
	</s>
	

	<s id="52">
		 T,i=1 ... 
	</s>
	

	<s id="53">
		 N Calculate zi = argmaxzEGEN(xj) 4)(xi , z ) • a¯ If(zi =~ yi ) then a¯ = a¯ + 4)(xi , yi ) — 4)(xi , zi ) Output : Parameters a¯ Figure 1 : A variant of the perceptron algorithm . 
	</s>
	

	<s id="54">
		 ( x1 , y1 ) ... ( xN , yN ) . 
	</s>
	

	<s id="55">
		 This section describes the per- ceptron algorithm , which was previously applied to language modeling in 
		<ref citStr="Roark et al . ( 2004 )" id="10" label="CEPF" position="9266">
			Roark et al . ( 2004 )
		</ref>
		 . 
	</s>
	

	<s id="56">
		 The next section describes an alternative method , based on conditional random fields . 
	</s>
	

	<s id="57">
		 The perceptron algorithm is shown in figure 1 . 
	</s>
	

	<s id="58">
		 At each training example ( xi , yi ) , the current best-scoring hypothesis zi is found , and if it differs from the reference yi , then the cost of each feature2 is increased by the count of that feature in zi and decreased by the count of that feature in yi . 
	</s>
	

	<s id="59">
		 The features in the model are updated , and the algorithm moves to the next utterance . 
	</s>
	

	<s id="60">
		 After each pass over the training data , performance on a held-out data set is evaluated , and the parameterization with the best performance on the held out set is what is ultimately produced by the algorithm . 
	</s>
	

	<s id="61">
		 Following 
		<ref citStr="Collins ( 2002 )" id="11" label="CEPF" position="10046">
			Collins ( 2002 )
		</ref>
		 , we used the averaged parameters from the training algorithm in decoding held- out and test examples in our experiments . 
	</s>
	

	<s id="62">
		 Say ¯ai is the parameter vector after the i’th example is processed on the t’th pass through the data in the algorithm in figure 1 . 
	</s>
	

	<s id="63">
		 Then the averaged parameters ¯aAVG are defined as ¯aAVG = Ei,t ¯ai/NT . 
	</s>
	

	<s id="64">
		 
		<ref citStr="Freund and Schapire ( 1999 )" id="12" label="CEPF" position="10436">
			Freund and Schapire ( 1999 )
		</ref>
		 originally proposed the averaged parameter method ; it was shown to give substantial improvements in accuracy for tagging tasks in 
		<ref citStr="Collins ( 2002 )" id="13" label="CEPF" position="10584">
			Collins ( 2002 )
		</ref>
		 . 
	</s>
	

	<s id="65">
		 2.3 Conditional Random Fields Conditional Random Fields have been applied to NLP tasks such as parsing 
		<ref citStr="Ratnaparkhi et al. , 1994" id="14" label="CEPF" position="10699">
			( Ratnaparkhi et al. , 1994 
		</ref>
		<ref citStr="Johnson et al. , 1999" id="15" label="CEPF" position="10727">
			; Johnson et al. , 1999 )
		</ref>
		 , and tagging or segmentation tasks 
		<ref citStr="Lafferty et al. , 2001" id="16" label="CEPF" position="10789">
			( Lafferty et al. , 2001 
		</ref>
		<ref citStr="Sha and Pereira , 2003" id="17" label="CEPF" position="10814">
			; Sha and Pereira , 2003 
		</ref>
		<ref citStr="McCallum and Li , 2003" id="18" label="CEPF" position="10839">
			; McCallum and Li , 2003 
		</ref>
		<ref citStr="Pinto et al. , 2003" id="19" label="CEPF" position="10864">
			; Pinto et al. , 2003 )
		</ref>
		 . 
	</s>
	

	<s id="66">
		 CRFs use the parameters a¯ to define a conditional distribution over the members of GEN ( x ) for a given input x : p¯^ ( y| x ) = Z(x ¯a ) exp (4)(x , y ) • ¯a ) where Z(x , ¯a ) = EyCGEN(x) exp (4)(x , y ) • ¯a ) is a normalization constant that depends on x and ¯a . 
	</s>
	

	<s id="67">
		 Given these definitions , the log-likelihood of the training data under parameters a¯ is log p¯^ ( yi l xi ) [ 4)(xi , yi ) • a¯ — log Z(xi , ¯a ) ] ( 2 ) 2Note that here lattice weights are interpreted as costs , which changes the sign in the algorithm presented in figure 1. LL(¯a) = ~N = i=1 ~N i=1 Following 
		<ref citStr="Johnson et al . ( 1999 )" id="20" label="CERF" position="11530">
			Johnson et al . ( 1999 )
		</ref>
		 and 
		<ref citStr="Lafferty et al . ( 2001 )" id="21" label="CERF" position="11560">
			Lafferty et al . ( 2001 )
		</ref>
		 , we use a zero-mean Gaussian prior on the parameters resulting in the regularized objective function : LLR ( ¯^ ) = ~N [ ^(xi , Yi ) ·^¯ — log Z(xi , ¯^ ) ] — ||al|2 ( 3 ) 2 i=1 The value ^ dictates the relative influence of the log- likelihood term vs. the prior , and is typically estimated using held-out data . 
	</s>
	

	<s id="68">
		 The optimal parameters under this criterion are ¯^* = argmax¯^ LLR ( ¯^ ) . 
	</s>
	

	<s id="69">
		 We use a limited memory variable metric method ( Benson and Mor´e , 2002 ) to optimize LLR . 
	</s>
	

	<s id="70">
		 There is a general implementation of this method in the Tao/PETSc software libraries 
		<ref citStr="Balay et al. , 2002" id="22" label="OEPF" position="12168">
			( Balay et al. , 2002 
		</ref>
		<ref citStr="Benson et al. , 2002" id="23" label="OEPF" position="12190">
			; Benson et al. , 2002 )
		</ref>
		 . 
	</s>
	

	<s id="71">
		 This technique has been shown to be very effective in a variety of NLP tasks 
		<ref citStr="Malouf , 2002" id="24" label="CEPF" position="12303">
			( Malouf , 2002 
		</ref>
		<ref citStr="Wallach , 2002" id="25" label="CEPF" position="12319">
			; Wallach , 2002 )
		</ref>
		 . 
	</s>
	

	<s id="72">
		 The main interface between the optimizer and the training data is a procedure which takes a parameter vector ^¯ as input , and in turn returns LLR(¯^) as well as the gradient of LLR at ¯^ . 
	</s>
	

	<s id="73">
		 The derivative of the objective function with respect to a parameter ^s at parameter values ^¯ is ^ ^ a3 , D3(Xi,yi) — X p¯^(y1Xi),D3(Xi,y)v — 2 ( 4 ) y^GEN(xi) Note that LLR ( ¯^ ) is a convex function , so that there is a globally optimal solution and the optimization method will find it . 
	</s>
	

	<s id="74">
		 The use of the Gaussian prior term 11¯^112 /2^2 in the objective function has been found to be useful in several NLP settings . 
	</s>
	

	<s id="75">
		 It effectively ensures that there is a large penalty for parameter values in the model becoming too large – as such , it tends to control over-training . 
	</s>
	

	<s id="76">
		 The choice of LLR as an objective function can be justified as maximum a-posteriori ( MAP ) training within a Bayesian approach . 
	</s>
	

	<s id="77">
		 An alternative justification comes through a connection to support vector machines and other large margin approaches . 
	</s>
	

	<s id="78">
		 SVM-based approaches use an optimization criterion that is closely related to LLR – see 
		<ref citStr="Collins ( 2004 )" id="26" label="CEPF" position="13532">
			Collins ( 2004 )
		</ref>
		 for more discussion . 
	</s>
	

	<s id="79">
		 3 Linear models for speech recognition We now describe how the formalism and algorithms in section 2 can be applied to language modeling for speech recognition . 
	</s>
	

	<s id="80">
		 3.1 The basic approach As described in the previous section , linear models require definitions of X , Y , xi , yi , GEN , 4 ) and a parameter estimation method . 
	</s>
	

	<s id="81">
		 In the language modeling setting we take X to be the set of all possible acoustic inputs ; Y is the set of all possible strings , E* , for some vocabulary E . 
	</s>
	

	<s id="82">
		 Each xi is an utterance ( a sequence of acoustic feature-vectors ) , and GEN(xi) is the set of possible transcriptions under a first pass recognizer . 
	</s>
	

	<s id="83">
		 ( GEN(xi) is a huge set , but will be represented compactly using a lattice – we will discuss this in detail shortly ) . 
	</s>
	

	<s id="84">
		 We take yi to be the member of GEN(xi) with lowest error rate with respect to the reference transcription of xi . 
	</s>
	

	<s id="85">
		 All that remains is to define the feature-vector representation , 4)(x , y ) . 
	</s>
	

	<s id="86">
		 In the general case , each component 4)i ( x , y ) could be essentially any function of the acoustic input x and the candidate transcription y . 
	</s>
	

	<s id="87">
		 The first feature we define is 4)0 ( x , y ) as the log -probability of y given x under the lattice produced by the baseline recognizer . 
	</s>
	

	<s id="88">
		 Thus this feature will include contributions from the acoustic model and the original language model . 
	</s>
	

	<s id="89">
		 The remaining features are restricted to be functions over the transcription y alone and they track all n-grams up to some length ( say n = 3 ) , for example : 4)1(x , y ) = Number of times “the the of” is seen in y At an abstract level , features of this form are introduced for all n-grams up to length 3 seen in some training data lattice , i.e. , n-grams seen in any word sequence within the lattices . 
	</s>
	

	<s id="90">
		 In practice , we consider methods that search for sparse parameter vectors ¯^ , thus assigning many n- grams 0 weight . 
	</s>
	

	<s id="91">
		 This will lead to more efficient algorithms that avoid dealing explicitly with the entire set of n-grams seen in training data . 
	</s>
	

	<s id="92">
		 3.2 Implementation using WFA We now give a brief sketch of how weighted finite-state automata ( WFA ) can be used to implement linear models for speech recognition . 
	</s>
	

	<s id="93">
		 There are several papers describing the use of weighted automata and transducers for speech in detail , e.g. , 
		<ref citStr="Mohri et al . ( 2002 )" id="27" label="CEPF" position="15984">
			Mohri et al . ( 2002 )
		</ref>
		 , but for clarity and completeness this section gives a brief description of the operations which we use . 
	</s>
	

	<s id="94">
		 For our purpose , a WFA A = ( E , Q , qs , F , E , ^ ) , where E is the vocabulary , Q is a ( finite ) set of states , qs E Q is a unique start state , F C Q is a set of final states , E is a ( finite ) set of transitions , and ^ : F ^ R is a function from final states to final weights . 
	</s>
	

	<s id="95">
		 Each tran- sition e E E is a tuple e = ( l [ e ] , p[e] , n[e] , w[e] ) , where l[e] E E is a label ( in our case , words ) , p[e] E Q is the origin state of e , n[e] E Q is the destination state of e , and w[e] E R is the weight of the transition . 
	</s>
	

	<s id="96">
		 A successful path 7r = e1 ... ej is a sequence of transitions , such that p[e1] = qs , n[ej] E F , and for 1 &lt; k &lt; j , n[ek_1] = p[ek] . 
	</s>
	

	<s id="97">
		 Let IIA be the set of successful paths 7r in a WFA A . 
	</s>
	

	<s id="98">
		 For any 7r = e1 ... ej , l[7r] = l[e1] ... l[ej] . 
	</s>
	

	<s id="99">
		 The weights of the WFA in our case are always in the log semiring , which means that the weight of a path 7r = e1 ... ej E IIA is defined as : wA[7r] = Xj w[ek] I + ^(ej) ( 5 ) k=1 By convention , we use negative log probabilities as weights , so lower weights are better . 
	</s>
	

	<s id="100">
		 All WFA that we will discuss in this paper are deterministic , i.e. there are no a transitions , and for any two transitions e , e ' E E , if p[e] = p[e'] , then l[e] =~ l[e'] . 
	</s>
	

	<s id="101">
		 Thus , for any string w = w1 ... wj , there is at most one successful path 7r E IIA , such that 7r = e1 ... ej and for 1 &lt; k &lt; j , l [ ek ] = wk , i.e. l[7r] = w . 
	</s>
	

	<s id="102">
		 The set of strings w such that there exists a 7r E IIA with l[7r] = w define a regular language LA C E. We can now define some operations that will be used in this paper . 
	</s>
	

	<s id="103">
		 8LLR = ~N i=1 8a3 • ^A . 
	</s>
	

	<s id="104">
		 For a set of transitions E and ^ E ]IR , define ^E = { ( l[e] , p[e] , n[e] , ^w[e] ) : e E E } . 
	</s>
	

	<s id="105">
		 Then , for any WFA A = ( E , Q , qs , F , E , ^ ) , define ^A for ^ E ]IR as follows : ^A = ( E , Q , qs , F , ^E , ^^ ) . 
	</s>
	

	<s id="106">
		 • A o A ' . 
	</s>
	

	<s id="107">
		 The intersection of two deterministic WFAs A o A ' in the log semiring is a deterministic WFA such that LAoA ' = LA n LA ' . 
	</s>
	

	<s id="108">
		 For any 7r E IIAoA ' , wAoA'[7r] = wA [ 7r1 ] + wA'[7r2] , where l[7r] = l[7r1] = l[7r2] . 
	</s>
	

	<s id="109">
		 • BestPath(A) . 
	</s>
	

	<s id="110">
		 This operation takes a WFA A , and returns the best scoring path 7rˆ = argmin7rErlA wA[7r] . 
	</s>
	

	<s id="111">
		 • MinErr(A , y ) . 
	</s>
	

	<s id="112">
		 Given a WFA A , a string y , and an error-function E(y , w ) , this operation returns 7rˆ = argmin7rErlA E(y , l[7r] ) . 
	</s>
	

	<s id="113">
		 This operation will generally be used with y as the reference transcription for a particular training example , and E(y , w ) as some measure of the number of errors in w when compared to y . 
	</s>
	

	<s id="114">
		 In this case , the MinErr operation returns the path 7r E IIA such l[7r] has the smallest number of errors when compared to y. • Norm(A) . 
	</s>
	

	<s id="115">
		 Given a WFA A , this operation yields a WFA A ' such that LA = LA ' and for every 7r E IIA there is a 7r ' E IIA ' such that l[7r] = l[7r'] and exp(—wA [7r]))(6) Note that E exp(—wNorm(A)[7r]) = 1 ( 7 ) 7rENorm(A) In other words the weights define a probability distribution over the paths . 
	</s>
	

	<s id="116">
		 • ExpCount(A , w ) . 
	</s>
	

	<s id="117">
		 Given a WFA A and an n-gram w , we define the expected count of w in A as ExpCount ( A , w ) = E wNorm(A) [ 7r ] C ( w , l[7r] ) 7rErlA where C ( w , l[7r] ) is defined to be the number of times the n-gram w appears in a string l[7r] . 
	</s>
	

	<s id="118">
		 Given an acoustic input x , let L. , be a deterministic word-lattice produced by the baseline recognizer . 
	</s>
	

	<s id="119">
		 The lattice L. , is an acyclic WFA , representing a weighted set of possible transcriptions of x under the baseline recognizer . 
	</s>
	

	<s id="120">
		 The weights represent the combination of acoustic and language model scores in the original recognizer . 
	</s>
	

	<s id="121">
		 The new , discriminative language model constructed during training consists of a deterministic WFA which we will denote D , together with a single parameter a0 . 
	</s>
	

	<s id="122">
		 The parameter a0 is the weight for the log probability feature 4)0 given by the baseline recognizer . 
	</s>
	

	<s id="123">
		 The WFA D is constructed so that LD = E^ and for all 7r E IID d wD [ 7r ] = E 4)j ( x , l [ 7r ] ) aj j=1 Recall that 4)j ( x , w ) for j &gt; 0 is the count of the j’th n- gram in w , and aj is the parameter associated with that Figure 2 : Representation of a trigram model with failure transitions . 
	</s>
	

	<s id="124">
		 n-gram . 
	</s>
	

	<s id="125">
		 Then , by definition , a0L o D accepts the same set of strings as L , but d waoLoD[7r] = E 4)j ( x , l [ 7r ] ) aj j=0 and argmin 4)(x , l[7r] ) · a¯ = BestPath(a0L o D ) . 
	</s>
	

	<s id="126">
		 7rEL Thus decoding under our new model involves first producing a lattice L from the baseline recognizer ; second , scaling L with a0 and intersecting it with the discriminative language model D ; third , finding the best scoring path in the new WFA . 
	</s>
	

	<s id="127">
		 We now turn to training a model , or more explicitly , deriving a discriminative language model ( D , a0 ) from a set of training examples . 
	</s>
	

	<s id="128">
		 Given a training set ( xi , ri ) for i = 1 ... 
	</s>
	

	<s id="129">
		 N , where xi is an acoustic sequence , and ri is a reference transcription , we can construct lattices Li for i = 1 ... 
	</s>
	

	<s id="130">
		 N using the baseline recognizer . 
	</s>
	

	<s id="131">
		 We can also derive target transcriptions yi = MinErr(Li , ri ) . 
	</s>
	

	<s id="132">
		 The training algorithm is then a mapping from ( Li , yi ) for i = 1 ... 
	</s>
	

	<s id="133">
		 N to a pair ( D , a0 ) . 
	</s>
	

	<s id="134">
		 Note that the construction of the language model requires two choices . 
	</s>
	

	<s id="135">
		 The first concerns the choice of the set of n-gram features 4)i for i = 1 ... d implemented by D . 
	</s>
	

	<s id="136">
		 The second concerns the choice ofparameters ai for i = 0 ... d which assign weights to the n-gram features as well as the baseline feature 4)0 . 
	</s>
	

	<s id="137">
		 Before describing methods for training a discriminative language model using perceptron and CRF algorithms , we give a little more detail about the structure of D , focusing on how n-gram language models can be implemented with finite-state techniques . 
	</s>
	

	<s id="138">
		 3.3 Representation of n-gram language models An n-gram model can be efficiently represented in a deterministic WFA , through the use of failure transitions 
		<ref citStr="Allauzen et al. , 2003" id="28" label="CEPF" position="22293">
			( Allauzen et al. , 2003 )
		</ref>
		 . 
	</s>
	

	<s id="139">
		 Every string accepted by such an automaton has a single path through the automaton , and the weight of the string is the sum of the weights of the transitions in that path . 
	</s>
	

	<s id="140">
		 In such a representation , every state in the automaton represents an n-gram history h , e.g. wi_2wi_1 , and there are transitions leaving the state for every word wi such that the feature hwi has a weight . 
	</s>
	

	<s id="141">
		 There is also a failure transition leaving the state , labeled with some reserved symbol ^ , which can only be traversed if the next symbol in the input does not match any transition leaving the state . 
	</s>
	

	<s id="142">
		 This failure transition points to the backoff state h ' , i.e. the n-gram history h minus its initial word . 
	</s>
	

	<s id="143">
		 Figure 2 shows how a trigram model can be represented in such an automaton . 
	</s>
	

	<s id="144">
		 See 
		<ref citStr="Allauzen et al . ( 2003 )" id="29" label="CERF" position="23150">
			Allauzen et al . ( 2003 )
		</ref>
		 for more details . 
	</s>
	

	<s id="145">
		 w ; -2w ;-1w ; wi-1 w ; 0 w ; w;-1 w ; 0 w ; wA'[7r'] = wA [ 7r ] + log I E ¯7rErlA Note that in such a deterministic representation , the entire weight of all features associated with the word wi following history h must be assigned to the transition labeled with wi leaving the state h in the automaton . 
	</s>
	

	<s id="146">
		 For example , if h = wi_2wi_1 , then the trigram wi_2wi_1wi is a feature , as is the bigram wi_1wi and the unigram wi . 
	</s>
	

	<s id="147">
		 In this case , the weight on the transition wi leaving state h must be the sum of the trigram , bigram and unigram feature weights . 
	</s>
	

	<s id="148">
		 If only the trigram feature weight were assigned to the transition , neither the unigram nor the bigram feature contribution would be included in the path weight . 
	</s>
	

	<s id="149">
		 In order to ensure that the correct weights are assigned to each string , every transition encoding an order k n-gram must carry the sum of the weights for all n-gram features of orders ^ k . 
	</s>
	

	<s id="150">
		 To ensure that every string in E* receives the correct weight , for any n-gram hw represented explicitly in the automaton , h'w must also be represented explicitly in the automaton , even if its weight is 0 . 
	</s>
	

	<s id="151">
		 3.4 The perceptron algorithm The perceptron algorithm is incremental , meaning that the language model D is built one training example at a time , during several passes over the training set . 
	</s>
	

	<s id="152">
		 Initially , we build D to accept all strings in E* with weight 0 . 
	</s>
	

	<s id="153">
		 For the perceptron experiments , we chose the parameter a0 to be a fixed constant , chosen by optimization on the held-out set . 
	</s>
	

	<s id="154">
		 The loop in the algorithm in figure 1 is implemented as : Fort= 1 ... 
	</s>
	

	<s id="155">
		 T,i=1 ... 
	</s>
	

	<s id="156">
		 N : • Calculate zi = argmaxyEGEN(x) 4'(x , y ) · a¯ = BestPath(a0Li o D ) • If zi =~ MinErr(Li , ri ) , then update the feature weights as in figure 1 ( modulo the sign , because of the use of costs ) , and modify D so as to assign the correct weight to all strings . 
	</s>
	

	<s id="157">
		 In addition , averaged parameters need to be stored ( see section 2.2 ) . 
	</s>
	

	<s id="158">
		 These parameters will replace the unaveraged parameters in D once training is completed . 
	</s>
	

	<s id="159">
		 Note that the only n-gram features to be included in D at the end of the training process are those that occur in either a best scoring path zi or a minimum error path yi at some point during training . 
	</s>
	

	<s id="160">
		 Thus the perceptron algorithm is in effect doing feature selection as a by-product of training . 
	</s>
	

	<s id="161">
		 Given N training examples , and T passes over the training set , O(NT) n-grams will have non-zero weight after training . 
	</s>
	

	<s id="162">
		 Experiments in 
		<ref citStr="Roark et al . ( 2004 )" id="30" label="CEPF" position="25822">
			Roark et al . ( 2004 )
		</ref>
		 suggest that the perceptron reaches optimal performance after a small number of training iterations , for example T = 1 or T = 2 . 
	</s>
	

	<s id="163">
		 Thus O(NT) can be very small compared to the full number of n-grams seen in all training lattices . 
	</s>
	

	<s id="164">
		 In our experiments , the perceptron method chose around 1.4 million n-grams with non-zero weight . 
	</s>
	

	<s id="165">
		 This compares to 43.65 million possible n-grams seen in the training data . 
	</s>
	

	<s id="166">
		 This is a key contrast with conditional random fields , which optimize the parameters of a fixed feature set . 
	</s>
	

	<s id="167">
		 Feature selection can be critical in our domain , as training and applying a discriminative language model over all n-grams seen in the training data ( in either correct or incorrect transcriptions ) may be computationally very demanding . 
	</s>
	

	<s id="168">
		 One training scenario that we will consider will be using the output of the perceptron algorithm ( the averaged parameters ) to provide the feature set and the initial feature weights for use in the CRF algorithm . 
	</s>
	

	<s id="169">
		 This leads to a model which is reasonably sparse , but has the benefit of CRF training , which as we will see gives gains in performance . 
	</s>
	

	<s id="170">
		 3.5 Conditional Random Fields The CRF methods that we use assume a fixed definition of the n-gram features 4'i for i = 1 ... d in the model . 
	</s>
	

	<s id="171">
		 In the experimental section we will describe a number of ways of defining the feature set . 
	</s>
	

	<s id="172">
		 The optimization methods we use begin at some initial setting for ¯a , and then search for the parameters ¯a* which maximize LLR(¯a) as defined in Eq . 
	</s>
	

	<s id="173">
		 3. The optimization method requires calculation of LLR(¯a) and the gradient of LLR(¯a) for a series of values for ¯a . 
	</s>
	

	<s id="174">
		 The first step in calculating these quantities is to take the parameter values ¯a , and to construct an acceptor D which accepts all strings in E* , such that d wD [ 7r ] = E 4'j ( x , l[7r])aj j=1 For each training lattice Li , we then construct a new lattice L'i = Norm(a0Li o D ) . 
	</s>
	

	<s id="175">
		 The lattice L'i represents ( in the log domain ) the distribution p¯^(ylxi) over strings y E GEN(xi) . 
	</s>
	

	<s id="176">
		 The value of logp¯^(yilxi) for any i can be computed by simply taking the path weight of 7r such that l[7r] = yi in the new lattice L'i . 
	</s>
	

	<s id="177">
		 Hence computation of LLR(¯a) in Eq . 
	</s>
	

	<s id="178">
		 3 is straightforward . 
	</s>
	

	<s id="179">
		 Calculating the n-gram feature gradients for the CRF optimization is also relatively simple , once L'i has been constructed . 
	</s>
	

	<s id="180">
		 From the derivative in Eq . 
	</s>
	

	<s id="181">
		 4 , for each i = 1 ... 
	</s>
	

	<s id="182">
		 N , j = 1 ... d the quantity 4'j ( xi , yi ) ^ X p¯^(yl xi ) 4'j(xi,y) ( 8 ) yEGEN(x;) must be computed . 
	</s>
	

	<s id="183">
		 The first term is simply the number of times the j’th n-gram feature is seen in yi . 
	</s>
	

	<s id="184">
		 The second term is the expected number of times that the j’th n-gram is seen in the acceptor L'i . 
	</s>
	

	<s id="185">
		 If the j’ th n-gram is w1 ... w , , , , then this can be computed as ExpCount(L'i , w1 ... w , , , ) . 
	</s>
	

	<s id="186">
		 The GRM library , which was presented in 
		<ref citStr="Allauzen et al . ( 2003 )" id="31" label="OEPF" position="28891">
			Allauzen et al . ( 2003 )
		</ref>
		 , has a direct implementation of the function ExpCount , which simultaneously calculates the expected value of all n-grams of order less than or equal to a given n in a lattice L . 
	</s>
	

	<s id="187">
		 The one non-ngram feature weight that is being estimated is the weight a0 given to the baseline ASR negative log probability . 
	</s>
	

	<s id="188">
		 Calculation of the gradient of LLR with respect to this parameter again requires calculation of the term in Eq . 
	</s>
	

	<s id="189">
		 8 for j = 0 and i = 1 ... 
	</s>
	

	<s id="190">
		 N. Com- putation of PyEGEN(x;) p¯^ ( yl xi)4'0 ( xi , y ) turns out to be not as straightforward as calculating n-gram expectations . 
	</s>
	

	<s id="191">
		 To do so , we rely upon the fact that 4'0 ( xi , y ) , the negative log probability of the path , decomposes to the sum of negative log probabilities of each transition in the path . 
	</s>
	

	<s id="192">
		 We index each transition in the lattice Li , and store its negative log probability under the baseline model . 
	</s>
	

	<s id="193">
		 We can then calculate the required gradient from L~i , by calculating the expected value in L~i of each indexed transition in Li . 
	</s>
	

	<s id="194">
		 We found that an approximation to the gradient of a0 , however , performed nearly identically to this exact gradient , while requiring substantially less computation . 
	</s>
	

	<s id="195">
		 Let wn1 be a string of n words , labeling a path in word- lattice L~i . 
	</s>
	

	<s id="196">
		 For brevity , let Pi(wn1 ) = p~^ ( wn1 I xi ) be the conditional probability under the current model , and let Qi ( wn1 ) be the probability of wn1 in the normalized baseline ASR lattice Norm(Li) . 
	</s>
	

	<s id="197">
		 Let Li be the set of strings in the language defined by Li . 
	</s>
	

	<s id="198">
		 Then we wish to compute Ei for i = 1 ... 
	</s>
	

	<s id="199">
		 N , where Ei = X Pi ( wn1 )logQi(wn1 ) wn1 ELi X= X Pi(wn1 ) logQi(wklwk^1 1 ) ( 9 ) wn1 ELi k=1 ... n The approximation is to make the following Markov assumption : Ei , .. X X Pi(wn1 ) log Qi(wklwk-2) wn1 ELi k=1 ... n = X ExpCount(G~i , xyz ) log Qi(zlxy)(10) xyzESi where Si is the set of all trigrams seen in Li . 
	</s>
	

	<s id="200">
		 The term log Qi(zIxy) can be calculated once before training for every lattice in the training set ; the ExpCount term is calculated as before using the GRM library . 
	</s>
	

	<s id="201">
		 We have found this approximation to be effective in practice , and it was used for the trials reported below . 
	</s>
	

	<s id="202">
		 When the gradients and conditional likelihoods are collected from all of the utterances in the training set , the contributions from the regularizer are combined to give an overall gradient and objective function value . 
	</s>
	

	<s id="203">
		 These values are provided to the parameter estimation routine , which then returns the parameters for use in the next iteration . 
	</s>
	

	<s id="204">
		 The accumulation of gradients for the feature set is the most time consuming part of the approach , but this is parallelizable , so that the computation can be divided among many processors . 
	</s>
	

	<s id="205">
		 4 Empirical Results We present empirical results on the Rich Transcription 2002 evaluation test set ( rt02 ) , which we used as our development set , as well as on the Rich Transcription 2003 Spring evaluation CTS test set ( rt03 ) . 
	</s>
	

	<s id="206">
		 The rt02 set consists of 6081 sentences ( 63804 words ) and has three subsets : Switchboard 1 , Switchboard 2 , Switchboard Cellular . 
	</s>
	

	<s id="207">
		 The rt03 set consists of 9050 sentences ( 76083 words ) and has two subsets : Switchboard and Fisher . 
	</s>
	

	<s id="208">
		 We used the same training set as that used in 
		<ref citStr="Roark et al . ( 2004 )" id="32" label="CEPF" position="32317">
			Roark et al . ( 2004 )
		</ref>
		 . 
	</s>
	

	<s id="209">
		 The training set consists of 276726 transcribed utterances ( 3047805 words ) , with an additional 20854 utterances ( 249774 words ) as held out data . 
	</s>
	

	<s id="210">
		 For 40 39.5 Baseline recognizer Perceptron , Feat=PL , Lattice Perceptron , Feat=PN , N=1000 CRF , a = oo , Feat=PL , Lattice CRF , a = 0.5 , Feat=PL , Lattice CRF , a = 0.5 , Feat=PN , N=1000 37.5 370 500 1000 Iterations over training Figure 3 : Word error rate on the rt02 eval set versus training iterations for CRF trials , contrasted with baseline recognizer performance and perceptron performance . 
	</s>
	

	<s id="211">
		 Points are at every 20 iterations . 
	</s>
	

	<s id="212">
		 Each point ( x,y ) is the WER at the iteration with the best objective function value in the interval ( x-20,x ] . 
	</s>
	

	<s id="213">
		 each utterance , a weighted word-lattice was produced , representing alternative transcriptions , from the ASR system . 
	</s>
	

	<s id="214">
		 From each word-lattice , the oracle best path was extracted , which gives the best word-error rate from among all of the hypotheses in the lattice . 
	</s>
	

	<s id="215">
		 The oracle word-error rate for the training set lattices was 12.2 % . 
	</s>
	

	<s id="216">
		 We also performed trials with 1000-best lists for the same training set , rather than lattices . 
	</s>
	

	<s id="217">
		 The oracle score for the 1000-best lists was 16.7 % . 
	</s>
	

	<s id="218">
		 To produce the word-lattices , each training utterance was processed by the baseline ASR system . 
	</s>
	

	<s id="219">
		 However , these same utterances are what the acoustic and language models are built from , which leads to better performance on the training utterances than can be expected when the ASR system processes unseen utterances . 
	</s>
	

	<s id="220">
		 To somewhat control for this , the training set was partitioned into 28 sets , and baseline Katz backoff trigram models were built for each set by including only transcripts from the other 27 sets . 
	</s>
	

	<s id="221">
		 Since language models are generally far more prone to overtrain than standard acoustic models , this goes a long way toward making the training conditions similar to testing conditions . 
	</s>
	

	<s id="222">
		 There are three baselines against which we are comparing . 
	</s>
	

	<s id="223">
		 The first is the ASR baseline , with no reweighting from a discriminatively trained n-gram model . 
	</s>
	

	<s id="224">
		 The other two baselines are with perceptron-trained n-gram model re-weighting , and were reported in 
		<ref citStr="Roark et al . ( 2004 )" id="33" label="CEPF" position="34649">
			Roark et al . ( 2004 )
		</ref>
		 . 
	</s>
	

	<s id="225">
		 The first of these is for a pruned-lattice trained trigram model , which showed a reduction in word error rate ( WER ) of 1.3 % , from 39.2 % to 37.9 % on rt02 . 
	</s>
	

	<s id="226">
		 The second is for a 1000-best list trained trigram model , which performed only marginally worse than the lattice- trained perceptron , at 38.0 % on rt02 . 
	</s>
	

	<s id="227">
		 4.1 Perceptron feature set We use the perceptron-trained models as the starting point for our CRF algorithm : the feature set given to the CRF algorithm is the feature set selected by the perceptron algorithm ; the feature weights are initialized to those of the averaged perceptron . 
	</s>
	

	<s id="228">
		 Figure 3 shows the performance of our three baselines versus three trials of 39 38.5 38 Iterations over training Figure 4 : Word error rate on the rt02 eval set versus training iterations for CRF trials , contrasted with baseline recognizer performance and perceptron performance . 
	</s>
	

	<s id="229">
		 Points are at every 20 iterations . 
	</s>
	

	<s id="230">
		 Each point ( x,y ) is the WER at the iteration with the best objective function value in the interval ( x-20,x ] . 
	</s>
	

	<s id="231">
		 the CRF algorithm . 
	</s>
	

	<s id="232">
		 In the first two trials , the training set consists of the pruned lattices , and the feature set is from the perceptron algorithm trained on pruned lattices . 
	</s>
	

	<s id="233">
		 There were 1.4 million features in this feature set . 
	</s>
	

	<s id="234">
		 The first trial set the regularizer constant u = oo , so that the algorithm was optimizing raw conditional likelihood . 
	</s>
	

	<s id="235">
		 The second trial is with the regularizer constant u = 0.5 , which we found empirically to be a good parameterization on the held-out set . 
	</s>
	

	<s id="236">
		 As can be seen from these results , regularization is critical . 
	</s>
	

	<s id="237">
		 The third trial in this set uses the feature set from the perceptron algorithm trained on 1000-best lists , and uses CRF optimization on these on these same 1000-best lists . 
	</s>
	

	<s id="238">
		 There were 0.9 million features in this feature set . 
	</s>
	

	<s id="239">
		 For this trial , we also used u = 0.5 . 
	</s>
	

	<s id="240">
		 As with the percep- tron baselines , the n-best trial performs nearly identically with the pruned lattices , here also resulting in 37.4 % WER . 
	</s>
	

	<s id="241">
		 This may be useful for techniques that would be more expensive to extend to lattices versus n-best lists ( e.g. models with unbounded dependencies ) . 
	</s>
	

	<s id="242">
		 These trials demonstrate that the CRF algorithm can do a better job of estimating feature weights than the perceptron algorithm for the same feature set . 
	</s>
	

	<s id="243">
		 As mentioned in the earlier section , feature selection is a by-product of the perceptron algorithm , but the CRF algorithm is given a set of features . 
	</s>
	

	<s id="244">
		 The next two trials looked at selecting feature sets other than those provided by the perceptron algorithm . 
	</s>
	

	<s id="245">
		 4.2 Other feature sets In order for the feature weights to be non-zero in this approach , they must be observed in the training set . 
	</s>
	

	<s id="246">
		 The number of unigram , bigram and trigram features with non-zero observations in the training set lattices is 43.65 million , or roughly 30 times the size of the perceptron feature set . 
	</s>
	

	<s id="247">
		 Many of these features occur only rarely with very low conditional probabilities , and hence cannot meaningfully impact system performance . 
	</s>
	

	<s id="248">
		 We pruned this feature set to include all unigrams and bigrams , but only those trigrams with an expected count of greater than 0.01 in the training set . 
	</s>
	

	<s id="249">
		 That is , to be included , a Trial Iter rt02 rt03 ASR Baseline - 39.2 38.2 Perceptron , Lattice - 37.9 36.9 Perceptron , N-best - 38.0 37.2 CRF , Lattice , Percep Feats ( 1.4M ) 769 37.4 36.5 CRF , N-best , Percep Feats ( 0.9M ) 946 37.4 36.6 CRF , Lattice , 0 = 0.01 ( 12M ) 2714 37.6 36.5 CRF , Lattice , 0 = 0.9 ( 1.5M ) 1679 37.5 36.6 Table 1 : Word-error rate results at convergence iteration for various trials , on both Switchboard 2002 test set ( rt02 ) , which was used as the dev set , and Switchboard 2003 test set ( rt03 ) . 
	</s>
	

	<s id="250">
		 trigram must occur in a set of paths , the sum of the conditional probabilities of which must be greater than our threshold 0 = 0.01 . 
	</s>
	

	<s id="251">
		 This threshold resulted in a feature set of roughly 12 million features , nearly 10 times the size of the perceptron feature set . 
	</s>
	

	<s id="252">
		 For better comparability with that feature set , we set our thresholds higher , so that trigrams were pruned if their expected count fell be- low 0 = 0.9 , and bigrams were pruned if their expected count fell below 0 = 0.1 . 
	</s>
	

	<s id="253">
		 We were concerned that this may leave out some of the features on the oracle paths , so we added back in all bigram and trigram features that occurred on oracle paths , giving a feature set of 1.5 million features , roughly the same size as the perceptron feature set . 
	</s>
	

	<s id="254">
		 Figure 4 shows the results for three CRF trials versus our ASR baseline and the perceptron algorithm baseline trained on lattices . 
	</s>
	

	<s id="255">
		 First , the result using the perceptron feature set provides us with a WER of 37.4 % , as previously shown . 
	</s>
	

	<s id="256">
		 The WER at convergence for the big feature set ( 12 million features ) is 37.6 % ; the WER at convergence for the smaller feature set ( 1.5 million features ) is 37.5 % . 
	</s>
	

	<s id="257">
		 While both of these other feature sets converge to performance close to that using the perceptron features , the number of iterations over the training data that are required to reach that level of performance are many more than for the perceptron-initialized feature set . 
	</s>
	

	<s id="258">
		 Table 1 shows the word-error rate at the convergence iteration for the various trials , on both rt02 and rt03 . 
	</s>
	

	<s id="259">
		 All of the CRF trials are significantly better than the perceptron performance , using the Matched Pair Sentence Segment test for WER included with SCTK ( NIST , 2000 ) . 
	</s>
	

	<s id="260">
		 On rt02 , the N-best and perceptron initialized CRF trials were were significantly better than the lattice perceptron at p &lt; 0.001 ; the other two CRF trials were significantly better than the lattice perceptron at p &lt; 0.01 . 
	</s>
	

	<s id="261">
		 On rt03 , the N-best CRF trial was significantly better than the lat- tice perceptron at p &lt; 0.002 ; the other three CRF tri- als were significantly better than the lattice perceptron at p &lt; 0.001 . 
	</s>
	

	<s id="262">
		 Finally , we measured the time of a single iteration over the training data on a single machine for the perceptron algorithm , the CRF algorithm using the approximation to the gradient of ao , and the CRF algorithm using an exact gradient of ao . 
	</s>
	

	<s id="263">
		 Table 2 shows these times in hours . 
	</s>
	

	<s id="264">
		 Because of the frequent update of the weights in the model , the perceptron algorithm is more expensive than the CRF algorithm for a single iteration . 
	</s>
	

	<s id="265">
		 Further , the CRF algorithm is parallelizable , so that most of the work of an 40 39.5 Baseline recognizer Perceptron , Feat=PL , Lattice CRF , a = 0.5 , Feat=PL , Lattice CRF , ^ = 0.5 , Feat=E , 0=0.01 CRF , ^ = 0.5 , Feat=E , 0=0.9 39 38.5 38 37.5 370 500 1000 1500 2000 2500 Features Percep CRF approx exact Lattice , Percep Feats ( 1.4M ) 7.10 1.69 3.61 N-best , Percep Feats ( 0.9M ) 3.40 0.96 1.40 Lattice , ^ = 0.01(12M) - 2.24 4.75 Table 2 : Time ( in hours ) for one iteration on a single Intel Xeon 2.4Ghz processor with 4GB RAM . 
	</s>
	

	<s id="266">
		 iteration can be shared among multiple processors . 
	</s>
	

	<s id="267">
		 Our most common training setup for the CRF algorithm was parallelized between 20 processors , using the approximation to the gradient . 
	</s>
	

	<s id="268">
		 In that setup , using the 1 .4M feature set , one iteration of the perceptron algorithm took the same amount of real time as approximately 80 iterations of CRF . 
	</s>
	

	<s id="269">
		 5 Conclusion We have contrasted two approaches to discriminative language model estimation on a difficult large vocabulary task , showing that they can indeed scale effectively to handle this size of a problem . 
	</s>
	

	<s id="270">
		 Both algorithms have their benefits . 
	</s>
	

	<s id="271">
		 The perceptron algorithm selects a relatively small subset of the total feature set , and requires just a couple of passes over the training data . 
	</s>
	

	<s id="272">
		 The CRF algorithm does a better job of parameter estimation for the same feature set , and is parallelizable , so that each pass over the training set can require just a fraction of the real time of the perceptron algorithm . 
	</s>
	

	<s id="273">
		 The best scenario from among those that we investigated was a combination of both approaches , with the output of the perceptron algorithm taken as the starting point for CRF estimation . 
	</s>
	

	<s id="274">
		 As a final point , note that the methods we describe do not replace an existing language model , but rather complement it . 
	</s>
	

	<s id="275">
		 The existing language model has the benefit that it can be trained on a large amount of text that does not have speech transcriptions . 
	</s>
	

	<s id="276">
		 It has the disadvantage of not being a discriminative model . 
	</s>
	

	<s id="277">
		 The new language model is trained on the speech transcriptions , meaning that it has less training data , but that it has the advantage of discriminative training – and in particular , the advantage of being able to learn negative evidence in the form of negative weights on n-grams which are rarely or never seen in natural language text ( e.g. , “the of” ) , but are produced too frequently by the recognizer . 
	</s>
	

	<s id="278">
		 The methods we describe combines the two language models , allowing them to complement each other . 
	</s>
	

	<s id="279">
		 References Cyril Allauzen , Mehryar Mohri , and Brian Roark . 
	</s>
	

	<s id="280">
		 2003. Generalized algorithms for constructing language models . 
	</s>
	

	<s id="281">
		 In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics , pages 40–47 . 
	</s>
	

	<s id="282">
		 Satish Balay , William D. Gropp , Lois Curfman McInnes , and Barry F. Smith . 
	</s>
	

	<s id="283">
		 2002. Petsc users manual . 
	</s>
	

	<s id="284">
		 Technical Report ANL-95/1 1 - Revision 2.1.2 , Argonne National Laboratory . 
	</s>
	

	<s id="285">
		 Satanjeev Banerjee , Jack Mostow , Joseph Beck , and Wilson Tam . 
	</s>
	

	<s id="286">
		 2003. Improving language models by learning from speech recognition errors in a reading tutor that listens . 
	</s>
	

	<s id="287">
		 In Proceedings of the Second International Conference on Applied Artificial Intelligence , Fort Panhala , Kolhapur , India . 
	</s>
	

	<s id="288">
		 Steven J. Benson and Jorge J. Mor´e . 
	</s>
	

	<s id="289">
		 2002. A limited memory variable metric method for bound constrained minimization . 
	</s>
	

	<s id="290">
		 Preprint ANL/ACSP909-0901 , Argonne National Laboratory . 
	</s>
	

	<s id="291">
		 Steven J. Benson , Lois Curfman McInnes , Jorge J. Mor´e , and Jason Sarich . 
	</s>
	

	<s id="292">
		 2002. Tao users manual . 
	</s>
	

	<s id="293">
		 Technical Report ANL/MCS-TM242-Revision 1.4 , Argonne National Laboratory . 
	</s>
	

	<s id="294">
		 Zheng Chen , Kai-Fu Lee , and Ming Jing Li . 
	</s>
	

	<s id="295">
		 2000. Discriminative training on language model . 
	</s>
	

	<s id="296">
		 In Proceedings of the Sixth International Conference on Spoken Language Processing ( ICSLP ) , Beijing , China . 
	</s>
	

	<s id="297">
		 Michael Collins . 
	</s>
	

	<s id="298">
		 2002. Discriminative training methods for hidden markov models : Theory and experiments with perceptron algorithms . 
	</s>
	

	<s id="299">
		 In Proceedings of the Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1–8 . 
	</s>
	

	<s id="300">
		 Michael Collins . 
	</s>
	

	<s id="301">
		 2004. Parameter estimation for statistical parsing models : Theory and practice of distribution-free methods . 
	</s>
	

	<s id="302">
		 In Harry Bunt , John Carroll , and Giorgio Satta , editors , New Developments in Parsing Technology . 
	</s>
	

	<s id="303">
		 Kluwer . 
	</s>
	

	<s id="304">
		 Yoav Freund and Robert Schapire . 
	</s>
	

	<s id="305">
		 1999. Large margin classification using the perceptron algorithm . 
	</s>
	

	<s id="306">
		 Machine Learning , 3(37):277–296 . 
	</s>
	

	<s id="307">
		 Frederick Jelinek . 
	</s>
	

	<s id="308">
		 1995. Acoustic sensitive language modeling . 
	</s>
	

	<s id="309">
		 Technical report , Center for Language and Speech Processing , Johns Hopkins University , Baltimore , MD . 
	</s>
	

	<s id="310">
		 Mark Johnson , Stuart Geman , Steven Canon , Zhiyi Chi , and Stefan Riezler . 
	</s>
	

	<s id="311">
		 1999. Estimators for stochastic “unification-based” grammars . 
	</s>
	

	<s id="312">
		 In Proceedings ofthe 37th Annual Meeting of the Association for Computational Linguistics , pages 535–541 . 
	</s>
	

	<s id="313">
		 Sanjeev Khudanpur and Jun Wu . 
	</s>
	

	<s id="314">
		 2000. Maximum entropy techniques for exploiting syntactic , semantic and collocational dependencies in language modeling . 
	</s>
	

	<s id="315">
		 Computer Speech and Language , 14(4):355– 372 . 
	</s>
	

	<s id="316">
		 Hong-Kwang Jeff Kuo , Eric Fosler-Lussier , Hui Jiang , and Chin- Hui Lee . 
	</s>
	

	<s id="317">
		 2002. Discriminative training of language models for speech recognition . 
	</s>
	

	<s id="318">
		 In Proceedings of the International Conference on Acoustics , Speech , and Signal Processing ( ICASSP ) , Orlando , Florida . 
	</s>
	

	<s id="319">
		 John Lafferty , Andrew McCallum , and Fernando Pereira . 
	</s>
	

	<s id="320">
		 2001. Conditional random fields : Probabilistic models for segmenting and labeling sequence data . 
	</s>
	

	<s id="321">
		 In Proc . 
	</s>
	

	<s id="322">
		 ICML , pages 282–289 , Williams College , Williamstown , MA , USA . 
	</s>
	

	<s id="323">
		 Robert Malouf . 
	</s>
	

	<s id="324">
		 2002. A comparison of algorithms for maximum entropy parameter estimation . 
	</s>
	

	<s id="325">
		 In Proc . 
	</s>
	

	<s id="326">
		 CoNLL , pages 49–55 . 
	</s>
	

	<s id="327">
		 Andrew McCallum and Wei Li . 
	</s>
	

	<s id="328">
		 2003. Early results for named entity recognition with conditional random fields , feature induction and web-enhanced lexicons . 
	</s>
	

	<s id="329">
		 In Proc . 
	</s>
	

	<s id="330">
		 CoNLL . 
	</s>
	

	<s id="331">
		 Mehryar Mohri , Fernando C. N. Pereira , and Michael Riley . 
	</s>
	

	<s id="332">
		 2002. Weighted finite-state transducers in speech recognition . 
	</s>
	

	<s id="333">
		 Computer Speech and Language , 16(1):69–88 . 
	</s>
	

	<s id="334">
		 NIST . 
	</s>
	

	<s id="335">
		 2000. Speech recognition scoring toolkit ( sctk ) version 1.2c . 
	</s>
	

	<s id="336">
		 Available athttp://www.nist.gov/speech/tools . 
	</s>
	

	<s id="337">
		 David Pinto , Andrew McCallum , Xing Wei , and W. Bruce Croft . 
	</s>
	

	<s id="338">
		 2003. Table extraction using conditional random fields . 
	</s>
	

	<s id="339">
		 In Proc . 
	</s>
	

	<s id="340">
		 ACM SIGIR . 
	</s>
	

	<s id="341">
		 Adwait Ratnaparkhi , Salim Roukos , and R. Todd Ward . 
	</s>
	

	<s id="342">
		 1994. A maximum entropy model for parsing . 
	</s>
	

	<s id="343">
		 In Proceedings of the International Conference on Spoken Language Processing ( ICSLP ) , pages 803–806 . 
	</s>
	

	<s id="344">
		 Brian Roark , Murat Saraclar , and Michael Collins . 
	</s>
	

	<s id="345">
		 2004. Corrective language modeling for large vocabulary ASR with the perceptron algorithm . 
	</s>
	

	<s id="346">
		 In Proceedings of the International Conference on Acoustics , Speech , and Signal Processing ( ICASSP ) , pages 749–752 . 
	</s>
	

	<s id="347">
		 Fei Sha and Fernando Pereira . 
	</s>
	

	<s id="348">
		 2003. Shallow parsing with conditional random fields . 
	</s>
	

	<s id="349">
		 In Proc . 
	</s>
	

	<s id="350">
		 HLT-NAACL , Edmonton , Canada . 
	</s>
	

	<s id="351">
		 A. Stolcke and M. Weintraub . 
	</s>
	

	<s id="352">
		 1998. Discriminitive language modeling . 
	</s>
	

	<s id="353">
		 In Proceedings ofthe 9th Hub-5 Conversational Speech Recognition Workshop . 
	</s>
	

	<s id="354">
		 A. Stolcke , H. Bratt , J. Butzberger , H. Franco , V. R. Rao Gadde , M. Plauche , C. Richey , E. Shriberg , K. Sonmez , F. Weng , and J. Zheng . 
	</s>
	

	<s id="355">
		 2000. The SRI March 2000 Hub-5 conversational speech transcription system . 
	</s>
	

	<s id="356">
		 In Proceedings of the NIST Speech Transcription Workshop . 
	</s>
	

	<s id="357">
		 Hanna Wallach . 
	</s>
	

	<s id="358">
		 2002. Efficient training of conditional random fields . 
	</s>
	

	<s id="359">
		 Master’s thesis , University of Edinburgh . 
	</s>
	

	<s id="360">
		 P.C. Woodland and D. Povey . 
	</s>
	

	<s id="361">
		 2000. Large scale discriminative training for speech recognition . 
	</s>
	

	<s id="362">
		 In Proc . 
	</s>
	

	<s id="363">
		 ISCAITRWASR2000 , pages 7–16 . 
	</s>
	


</acldoc>
