<?xml version="1.0" encoding="iso-8859-1"?>
<acldoc acl_id="P04-3031">
	

	<s id="1">
		 NLTK : The Natural Language Toolkit Steven Bird Department of Computer Science and Software Engineering University of Melbourne Victoria 3010 , Australia sb@csse.unimelb.edu.au Edward Loper Department of Computer and Information Science University of Pennsylvania Philadelphia PA 19104-6389 , USA edloper@gradient.cis.upenn.edu Abstract The Natural Language Toolkit is a suite of program modules , data sets , tutorials and exercises , covering symbolic and statistical natural language processing . 
	</s>
	

	<s id="2">
		 NLTK is written in Python and distributed under the GPL open source license . 
	</s>
	

	<s id="3">
		 Over the past three years , NLTK has become popular in teaching and research . 
	</s>
	

	<s id="4">
		 We describe the toolkit and report on its current state of development . 
	</s>
	

	<s id="5">
		 1 Introduction The Natural Language Toolkit ( NLTK ) was developed in conjunction with a computational linguistics course at the University of Pennsylvania in 2001 
		<ref citStr="Loper and Bird , 2002" id="1" label="CEPF" position="960">
			( Loper and Bird , 2002 )
		</ref>
		 . 
	</s>
	

	<s id="6">
		 It was designed with three pedagogical applications in mind : assignments , demonstrations , and projects . 
	</s>
	

	<s id="7">
		 Assignments . 
	</s>
	

	<s id="8">
		 NLTK supports assignments of varying difficulty and scope . 
	</s>
	

	<s id="9">
		 In the simplest assignments , students experiment with existing components to perform a wide variety of NLP tasks . 
	</s>
	

	<s id="10">
		 As students become more familiar with the toolkit , they can be asked to modify existing components , or to create complete systems out of existing components . 
	</s>
	

	<s id="11">
		 Demonstrations . 
	</s>
	

	<s id="12">
		 NLTK’s interactive graphical demonstrations have proven to be very useful for students learning NLP concepts . 
	</s>
	

	<s id="13">
		 The demonstrations give a step-by-step execution of important algorithms , displaying the current state of key data structures . 
	</s>
	

	<s id="14">
		 A screenshot of the chart parsing demonstration is shown in Figure 1 . 
	</s>
	

	<s id="15">
		 Projects . 
	</s>
	

	<s id="16">
		 NLTK provides students with a flexible framework for advanced projects . 
	</s>
	

	<s id="17">
		 Typical projects might involve implementing a new algorithm , developing a new component , or implementing a new task . 
	</s>
	

	<s id="18">
		 We chose Python because it has a shallow learning curve , its syntax and semantics are transparent , and it has good string-handling functionality . 
	</s>
	

	<s id="19">
		 As an interpreted language , Python facilitates interactive exploration . 
	</s>
	

	<s id="20">
		 As an object-oriented language , Python permits data and methods to be encapsulated and re-used easily . 
	</s>
	

	<s id="21">
		 Python comes with an extensive standard library , including tools for graphical programming and numerical processing . 
	</s>
	

	<s id="22">
		 The recently added generator syntax makes it easy to create interactive implementations of algorithms 
		<ref citStr="Loper , 2004" id="2" label="CEPF" position="2657">
			( Loper , 2004 
		</ref>
		<ref citStr="Rossum , 2003a" id="3" label="CEPF" position="2672">
			; Rossum , 2003a 
		</ref>
		<ref citStr="Rossum , 2003b" id="4" label="CEPF" position="2689">
			; Rossum , 2003b )
		</ref>
		 . 
	</s>
	

	<s id="23">
		 Figure 1 : Interactive Chart Parsing Demonstration 2 Design NLTK is implemented as a large collection of minimally interdependent modules , organized into a shallow hierarchy . 
	</s>
	

	<s id="24">
		 A set of core modules defines basic data types that are used throughout the toolkit . 
	</s>
	

	<s id="25">
		 The remaining modules are task modules , each devoted to an individual natural language processing task . 
	</s>
	

	<s id="26">
		 For example , the nltk.parser module encompasses to the task of parsing , or deriving the syntactic structure of a sentence ; and the nltk.tokenizer module is devoted to the task of tokenizing , or dividing a text into its constituent parts . 
	</s>
	

	<s id="27">
		 2.1 Tokens and other core data types To maximize interoperability between modules , we use a single class to encode information about natural language texts – the Token class . 
	</s>
	

	<s id="28">
		 Each Token instance represents a unit of text such as a word , sentence , or document , and is defined by a ( partial ) mapping from property names to values . 
	</s>
	

	<s id="29">
		 For example , the TEXT property is used to encode a token’s text content:1 &gt;&gt;&gt; from nltk.token import * &gt;&gt;&gt; Token(TEXT=&quot;Hello World ! 
	</s>
	

	<s id="30">
		 &quot; ) &lt;Hello World!&gt; The TAG property is used to encode a token’s partof-speech tag : &gt;&gt;&gt; Token(TEXT=&quot;python&quot; , TAG=&quot;NN&quot; ) &lt;python/NN&gt; The SUBTOKENS property is used to store a tokenized text : &gt;&gt;&gt; from nltk.tokenizer import * &gt;&gt;&gt; tok = Token(TEXT=&quot;Hello World ! 
	</s>
	

	<s id="31">
		 &quot; ) &gt;&gt;&gt; WhitespaceTokenizer().tokenize(tok) &gt;&gt;&gt; print tok[’SUBTOKENS’] ) [ &lt;Hello&gt; , &lt;World!&gt; ] In a similar fashion , other language processing tasks such as word-sense disambiguation , chunking and parsing all add properties to the Token data structure . 
	</s>
	

	<s id="32">
		 In general , language processing tasks are formulated as annotations and transformations involving Tokens . 
	</s>
	

	<s id="33">
		 In particular , each processing task takes a token and extends it to include new information . 
	</s>
	

	<s id="34">
		 These modifications are typically monotonic ; new information is added but existing information is not deleted or modified . 
	</s>
	

	<s id="35">
		 Thus , tokens serve as a blackboard , where information about a piece of text is collated . 
	</s>
	

	<s id="36">
		 This architecture contrasts with the more typical pipeline architecture where each processing task’s output discards its input information . 
	</s>
	

	<s id="37">
		 We chose the blackboard approach over the pipeline approach because it allows more flexibility when combining tasks into a single system . 
	</s>
	

	<s id="38">
		 In addition to the Token class and its derivatives , NLTK defines a variety of other data types . 
	</s>
	

	<s id="39">
		 For instance , the probability module defines classes for probability distributions and statistical smoothing techniques ; and the cfg module defines classes for encoding context free grammars and probabilistic context free grammars . 
	</s>
	

	<s id="40">
		 ' Some code samples are specific to NLTK version 1.4. 2.2 The corpus module Many language processing tasks must be developed and tested using annotated data sets or corpora . 
	</s>
	

	<s id="41">
		 Several such corpora are distributed with NLTK , as listed in Table 1 . 
	</s>
	

	<s id="42">
		 The corpus module defines classes for reading and processing many of these corpora . 
	</s>
	

	<s id="43">
		 The following code fragment illustrates how the Brown Corpus is accessed . 
	</s>
	

	<s id="44">
		 &gt;&gt;&gt; from nltk.corpus import brown &gt;&gt;&gt; brown.groups() [ ’skill and hobbies’ , ’popular lore’ , ’humor’ , ’fiction : mystery’ , ... ] &gt;&gt;&gt; brown.items(’humor’) ( ’cr01’ , ’cr02’ , ’cr03’ , ’cr04’ , ’cr05’ , ’cr06’ , ’cr07’ , ’cr08’ , ’cr09’ ) &gt;&gt;&gt; brown.tokenize(’cr01’) &lt;[&lt;It/pps&gt; , &lt;was/bedz&gt; , &lt;among/in&gt; , &lt;these/dts&gt; , &lt;that/cs&gt; , &lt;Hinkle/np&gt; , &lt;identified/vbd&gt; , &lt;a/at&gt; , ...]&gt; A selection of 5 % of the Penn Treebank corpus is included with NLTK , and it is accessed as follows : &gt;&gt;&gt; from nltk.corpus import treebank &gt;&gt;&gt; treebank.groups() ( ’raw’ , ’tagged’ , ’parsed’ , ’merged’ ) &gt;&gt;&gt; treebank.items(’parsed’) [ ’wsj_0001.prd’ , ’wsj_0002.prd’ , ... ] &gt;&gt;&gt; item = ’parsed/wsj_0001.prd’ &gt;&gt;&gt; sentences = treebank.tokenize(item) &gt;&gt;&gt; for sent in sentences[’SUBTOKENS’] : ... print sent.pp() # pretty-print ( S : ( NP-SBJ : ( NP : &lt;Pierre&gt; &lt;Vinken&gt; ) ( ADJP : ( NP : &lt;61&gt; &lt;years&gt; ) &lt;old&gt; ) ... 2.3 Processing modules Each language processing algorithm is implemented as a class . 
	</s>
	

	<s id="45">
		 For example , the ChartParser and Recurs iveDescentParser classes each define a single algorithm for parsing a text . 
	</s>
	

	<s id="46">
		 We implement language processing algorithms using classes instead of functions for three reasons . 
	</s>
	

	<s id="47">
		 First , all algorithm-specific options can be passed to the constructor , allowing a consistent interface for applying the algorithms . 
	</s>
	

	<s id="48">
		 Second , a number of algorithms need to have their state initialized before they can be used . 
	</s>
	

	<s id="49">
		 For example , the NthOrderTagger class Corpus Contents and Wordcount Example Application 20 Newsgroups ( selection ) 3 newsgroups , 4000 posts , 780kw text classification Brown Corpus 15 genres , 1.15Mw , tagged training &amp; testing taggers , text classification CoNLL 2000 Chunking Data 270kw , tagged and chunked training &amp; testing chunk parsers Project Gutenberg ( selection ) 14 texts , 1.7Mw text classification , language modelling NIST 1999 IEER ( selection ) 63kw , named-entity markup training &amp; testing named-entity recognizers Levin Verb Index 3k verbs with Levin classes parser development Names Corpus 8k male &amp; female names text classification PP Attachment Corpus 28k prepositional phrases , tagged parser development Roget’s Thesaurus 200kw , formatted text word-sense disambiguation SEMCOR 880kw , POS &amp; sense tagged word-sense disambiguation SENSEVAL 2 Corpus 600kw , POS &amp; sense tagged word-sense disambiguation Stopwords Corpus 2,400 stopwords for 11 lgs text retrieval Penn Treebank ( sample ) 40kw , tagged &amp; parsed parser development Wordnet 1.7 180kw in a semantic network WSD , NL understanding Wordlist Corpus 960kw and 20k affixes for 8lgs spell checking Table 1 : Corpora and Corpus Samples Distributed with NLTK must be initialized by training on a tagged corpus before it can be used . 
	</s>
	

	<s id="50">
		 Third , subclassing can be used to create specialized versions of a given algorithm . 
	</s>
	

	<s id="51">
		 Each processing module defines an interface for its task . 
	</s>
	

	<s id="52">
		 Interface classes are distinguished by naming them with a trailing capital “I,” such as Pa r s e r I . 
	</s>
	

	<s id="53">
		 Each interface defines a single action method which performs the task defined by the interface . 
	</s>
	

	<s id="54">
		 For example , the P a r s e r I interface defines the parse method and the Tokenizer interface defines the tokenize method . 
	</s>
	

	<s id="55">
		 When appropriate , an interface defines extended action methods , which provide variations on the basic action method . 
	</s>
	

	<s id="56">
		 For example , the Pa r s e r I interface defines the parse n method which finds at most n parses for a given sentence ; and the Tokenizer I interface defines the x t o ke n i z e method , which outputs an iterator over subtokens instead of a list of subtokens . 
	</s>
	

	<s id="57">
		 NLTK includes the following modules : cfg , corpus , draw ( cfg , chart , corpus , featurestruct , fsa , graph , plot , rdparser , srparser , tree ) , eval , featurestruct , parser ( chart , chunk , probabilistic ) , probability , sense , set , stemmer ( porter ) , tagger , test , token , tokenizer , tree , and util . 
	</s>
	

	<s id="58">
		 Please see the online documentation for details . 
	</s>
	

	<s id="59">
		 2.4 Documentation Three different types of documentation are available . 
	</s>
	

	<s id="60">
		 Tutorials explain how to use the toolkit , with detailed worked examples . 
	</s>
	

	<s id="61">
		 The API documentation describes every module , interface , class , method , function , and variable in the toolkit . 
	</s>
	

	<s id="62">
		 Technical reports explain and justify the toolkit’s design and implementation . 
	</s>
	

	<s id="63">
		 All are available from http:// nltk.sf.net/docs.html . 
	</s>
	

	<s id="64">
		 3 Installing NLTK NLTK is available from nltk.sf.net , and is packaged for easy installation under Unix , Mac OS X and Windows . 
	</s>
	

	<s id="65">
		 The full distribution consists of four packages : the Python source code ( nltk ) ; the corpora ( nltk-data ) ; the documentation ( nltk-docs ) ; and third-party contributions ( nltk-contrib ) . 
	</s>
	

	<s id="66">
		 Before installing NLTK , it is necessary to install Python version 2.3 or later , available from www. python . 
	</s>
	

	<s id="67">
		 org . 
	</s>
	

	<s id="68">
		 Full installation instructions and a quick start guide are available from the NLTK homepage . 
	</s>
	

	<s id="69">
		 As soon as NLTK is installed , users can run the demonstrations . 
	</s>
	

	<s id="70">
		 On Windows , the demonstrations can be run by double-clicking on their Python source files . 
	</s>
	

	<s id="71">
		 Alternatively , from the Python interpreter , this can be done as follows : &gt;&gt;&gt; import nltk.draw.rdparser &gt;&gt;&gt; nltk.draw.rdparser.demo() &gt;&gt;&gt; nltk.draw.srparser.demo() &gt;&gt;&gt; nltk.draw.chart.demo() 4 Using and contributing to NLTK NLTK has been used at the University of Pennsylvania since 2001 , and has subsequently been adopted by several NLP courses at other universities , including those listed in Table 2 . 
	</s>
	

	<s id="72">
		 Third party contributions to NLTK include : Brill tagger ( Chris Maloof ) , hidden Markov model tagger ( Trevor Cohn , Phil Blunsom ) , GPSG-style feature-based grammar and parser ( Rob Speer , Bob Berwick ) , finite-state morphological analyzer ( Carl de Marcken , Beracah Yankama , Bob Berwick ) , decision list and decision tree classifiers ( Trevor Cohn ) , and Discourse Representation Theory implementation ( Edward Ivanovic ) . 
	</s>
	

	<s id="73">
		 NLTK is an open source project , and we welcome any contributions . 
	</s>
	

	<s id="74">
		 There are several ways to contribute : users can report bugs , suggest features , or contribute patches on Sourceforge ; users can participate in discussions on the NLTK-Devel mailing list2 or in the NLTK public forums ; and users can submit their own NLTK-based projects for inclusion in the nltk contrib directory . 
	</s>
	

	<s id="75">
		 New code modules that are relevant , substantial , original and well-documented will be considered for inclusion in NLTK proper . 
	</s>
	

	<s id="76">
		 All source code is distributed under the GNU General Public License , and all documentation is distributed under a Creative Commons non-commercial license . 
	</s>
	

	<s id="77">
		 Thus , potential contributors can be confident that their work will remain freely available to all . 
	</s>
	

	<s id="78">
		 Further information about contributing to NLTK is available at http://nltk.sf.net/contrib.html . 
	</s>
	

	<s id="79">
		 5 Conclusion NLTK is a broad-coverage natural language toolkit that provides a simple , extensible , uniform framework for assignments , demonstrations and projects . 
	</s>
	

	<s id="80">
		 It is thoroughly documented , easy to learn , and simple to use . 
	</s>
	

	<s id="81">
		 NLTK is now widely used in research and teaching . 
	</s>
	

	<s id="82">
		 Readers who would like to receive occasional announcements about NLTK are encouraged to sign up for the low-volume , moderated mailing list NLTK-Announce.3 6 Acknowledgements We are indebted to our students and colleagues for feedback on the toolkit , and to many contributors listed on the NLTK website . 
	</s>
	

	<s id="83">
		 2http://lists.sourceforge.net/ lists/listinfo/nltk-devel 3http://lists.sourceforge.net/ lists/listinfo/nltk-announce Graz University of Technology , Austria Information Search and Retrieval Macquarie University , Australia Intelligent Text Processing Massachusetts Institute of Technology , USA Natural Language Processing National Autonomous University of Mexico , Mexico Introduction to Natural Language Processing in Python Ohio State University , USA Statistical Natural Language Processing University of Amsterdam , Netherlands Language Processing and Information Access University of Colorado , USA Natural Language Processing University of Edinburgh , UK Introduction to Computational Linguistics University of Magdeburg , Germany Natural Language Systems University of Malta , Malta Natural Language Algorithms University of Melbourne , Australia Human Language Technology University of Pennsylvania , USA Introduction to Computational Linguistics University of Pittsburgh , USA Artificial Intelligence Application Development Simon Fraser University , Canada Computational Linguistics Table 2 : University Courses using NLTK References Edward Loper and Steven Bird . 
	</s>
	

	<s id="84">
		 2002. NLTK : The Natural Language Toolkit . 
	</s>
	

	<s id="85">
		 In Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics , pages 62–69 . 
	</s>
	

	<s id="86">
		 Somerset , NJ : Association for Computational Linguistics . 
	</s>
	

	<s id="87">
		 http://arXiv.org/abs/ cs/0205028 . 
	</s>
	

	<s id="88">
		 Edward Loper . 
	</s>
	

	<s id="89">
		 2004. NLTK : Building a pedagogical toolkit in Python . 
	</s>
	

	<s id="90">
		 In PyCon DC 2004 . 
	</s>
	

	<s id="91">
		 Python Software Foundation . 
	</s>
	

	<s id="92">
		 http : //www. python . 
	</s>
	

	<s id="93">
		 org/pycon/dc2004/papers/ . 
	</s>
	

	<s id="94">
		 Guido Van Rossum . 
	</s>
	

	<s id="95">
		 2003a . 
	</s>
	

	<s id="96">
		 An Introduction to Python . 
	</s>
	

	<s id="97">
		 Network Theory Ltd. . 
	</s>
	

	<s id="98">
		 Guido Van Rossum . 
	</s>
	

	<s id="99">
		 2003b . 
	</s>
	

	<s id="100">
		 The Python Language Reference . 
	</s>
	

	<s id="101">
		 Network Theory Ltd. 
	</s>
	


</acldoc>
