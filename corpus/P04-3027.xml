<?xml version="1.0" encoding="iso-8859-1"?>
<acldoc acl_id="P04-3027">
	

	<s id="1">
		 Automatic clustering of collocation for detecting practical sense boundary Saim Shin Key-Sun Choi KAIST KAIST KorTerm KorTerm BOLA BOLA miror@world.kaist.ac.kr kschoi@world.kaist.ac.kr Abstract This paper talks about the deciding practical sense boundary of homonymous words . 
	</s>
	

	<s id="2">
		 The important problem in dictionaries or thesauri is the confusion of the sense boundary by each resource . 
	</s>
	

	<s id="3">
		 This also becomes a bottleneck in the practical language processing systems . 
	</s>
	

	<s id="4">
		 This paper proposes the method about discovering sense boundary using the collocation from the large corpora and the clustering methods . 
	</s>
	

	<s id="5">
		 In the experiments , the proposed methods show the similar results with the sense boundary from a corpus-based dictionary and sense-tagged corpus . 
	</s>
	

	<s id="6">
		 1 Introduction There are three types of sense boundary confusion for the homonyms in the existing dictionaries . 
	</s>
	

	<s id="7">
		 One is sense boundaries’ overlapping : two senses are overlapped from some semantic features . 
	</s>
	

	<s id="8">
		 Second , some senses in the dictionary are null ( or non-existing ) in the used corpora . 
	</s>
	

	<s id="9">
		 Conversely , we have to generate more senses depending on the corpora , and we define these senses with practical senses . 
	</s>
	

	<s id="10">
		 Our goal in this study is to revise sense boundary in the existing dictionaries with practical senses from the large- scaled corpus . 
	</s>
	

	<s id="11">
		 The collocation from the large-scaled corpus contains semantic information . 
	</s>
	

	<s id="12">
		 The collocation for ambiguous words also contains semantic information about multiple senses for this ambiguous word . 
	</s>
	

	<s id="13">
		 This paper uses the ambiguity of collocation for the homonyms . 
	</s>
	

	<s id="14">
		 With the clustering algorithms , we extract practical sense boundary from the collocations . 
	</s>
	

	<s id="15">
		 This paper explains the collocation ambiguity in chapter 2 , defines the extracted collocation and proposes the used clustering methods and the labeling algorithms in chapter 3 . 
	</s>
	

	<s id="16">
		 After explaining the experimental results in chapter 4 , this paper comes to the conclusion in chapter 5. 2 Collocation and Senses 2.1 Impractical senses in dictionary In 
		<ref citStr="Patrick and Lin , 2002" id="1" label="CEPF" position="2174">
			( Patrick and Lin , 2002 )
		</ref>
		 , senses in dictionary – especially in WordNet – sometimes don’t contain the senses appearing in the corpus . 
	</s>
	

	<s id="17">
		 Some senses in the manual dictionary don’t appear in the corpus . 
	</s>
	

	<s id="18">
		 This situation means that there exist differences between the senses in the manual dictionaries and practical senses from corpus . 
	</s>
	

	<s id="19">
		 These differences make problems in developing word sense disambiguation systems and applying semantic information to language processing applications . 
	</s>
	

	<s id="20">
		 The senses in the corpus are continuously changed . 
	</s>
	

	<s id="21">
		 In order to reflect these changes , we must analyze corpus continuously . 
	</s>
	

	<s id="22">
		 This paper discusses about the analyzing method in order to detect practical senses using the collocation . 
	</s>
	

	<s id="23">
		 2.2 Homonymous collocation The words in the collocation also have their collocation . 
	</s>
	

	<s id="24">
		 A target word for collocation is called the ‘central word’ , and a word in a collocation is referred to as the ‘contextual word’ . 
	</s>
	

	<s id="25">
		 ‘Surrounding words’ mean the collocation for all contextual words . 
	</s>
	

	<s id="26">
		 The assumption for extracting sense boundary is like this : the contextual words used in the same sense of the central word show the similar pattern of context . 
	</s>
	

	<s id="27">
		 If collocation patterns between contextual words are similar , it means that the contextual words are used in a similar context - where used and interrelated in same sense of the central word - in the sentence . 
	</s>
	

	<s id="28">
		 If contextual words are clustered according to the similarity in collocations , contextual words for homonymous central words can be classified according to the senses of the central words . 
	</s>
	

	<s id="29">
		 
		<ref citStr="Shin and Choi , 2004" id="2" label="CERF" position="3869">
			( Shin and Choi , 2004 )
		</ref>
		 The following is a mathematical representation used in this paper . 
	</s>
	

	<s id="30">
		 A collocation of the central word x , window size w and corpus c is expressed with function f : V N C 4 2PC/ V . 
	</s>
	

	<s id="31">
		 In this formula , V means a set of vocabulary , N is the size of the contextual window that is an integer , and C means a set of corpus . 
	</s>
	

	<s id="32">
		 In this paper , vocabulary refers to all content words in the corpus . 
	</s>
	

	<s id="33">
		 Function f shows all collocations . 
	</s>
	

	<s id="34">
		 C/V means that C is limited to V as well as that all vocabularies are selected from a given corpus and 2PC/VP is all sets of C/V . 
	</s>
	

	<s id="35">
		 In the equation ( 1 ) , the frequency of x is m in c . 
	</s>
	

	<s id="36">
		 We can also express m=|c/x| . 
	</s>
	

	<s id="37">
		 The window size of a collocation is 2w+1 . 
	</s>
	

	<s id="38">
		 g(x)={(x,;),;^ Ix } is a word sense assignment function that gives the word senses numbered i of the word x. Ix is the word sense indexing function of x that gives an index to each sense of the word x . 
	</s>
	

	<s id="39">
		 All contextual words x;±j of a central word x have their own contextual words in their collocation , and they also have multiple senses . 
	</s>
	

	<s id="40">
		 This problem is expressed by the combination of g and f as follows : h4 . 
	</s>
	

	<s id="41">
		 ( go In this paper , the problem is that the collocation of the central word is ordered according to word senses . 
	</s>
	

	<s id="42">
		 Figure 1 show the overall process for this purpose . 
	</s>
	

	<s id="43">
		 Figure 1 Processing for detecting sense boundary hd ; ( ( , ) ) m num x d = ; D d d d = { , ... , , ... , 0 ; n Sx = { sx0,sx1 , ... , xxm 3 Automatic clusteringofcollocation Forextracting practical senses , the contextual words foracentral word are clustered byanalyzing the pattern ofthe surroundingwords . 
	</s>
	

	<s id="44">
		 Withthis method , we cangetthe collocationwithoutsense ambiguity , andalso discoverthe practical sense boundary . 
	</s>
	

	<s id="45">
		 Inorderto extractthe correct sense boundary fromthe clustering phase , itneeds to remove the noise andtrivial collocation . 
	</s>
	

	<s id="46">
		 We call this process normalization , anditis specifically provided as [ 8 ] . 
	</s>
	

	<s id="47">
		 The statistically unrelatedwords can be said that the words with highfrequency appearregardless of theirsemantic features . 
	</s>
	

	<s id="48">
		 Afterdeciding the statistically unrelatedwords by calculating tf·idf values , we filteredthemfromthe ori ginal surrounding words . 
	</s>
	

	<s id="49">
		 The second normalization is usingLSI(Latent Semantic Indexing ) . 
	</s>
	

	<s id="50">
		 Throughout the LSI transformation , we can remove the dimension ofthe contextvectoran d express the hidden features into the surface of the context vector . 
	</s>
	

	<s id="51">
		 3.1 Discovering senseboundary We discoveredthe senses ofthe homonyms with clustering the normalizedcollocation . 
	</s>
	

	<s id="52">
		 The clustering classifies the contextual words having similarcontext – the contextual words having similarpatternofsurroundingwords - into same cluster . 
	</s>
	

	<s id="53">
		 Extracted clusters throughout the clustering symbolize the senses forthe central words and theircollocation . 
	</s>
	

	<s id="54">
		 In orderto extractclusters , we used several clustering algorithms . 
	</s>
	

	<s id="55">
		 Followings are the usedclustering methods : • K-means clustering ( K ) 
		<ref citStr="Ray andTuri , 1999" id="3" label="CERF" position="7018">
			( Ray andTuri , 1999 )
		</ref>
		 • Buckshot(B) 
		<ref citStr="Jensen , Beitzel , Pilotto , Goharian andFrieder , 2002" id="4" label="CERF" position="7093">
			( Jensen , Beitzel , Pilotto , Goharian andFrieder , 2002 )
		</ref>
		 • Committee basedclustering(CBC) 
		<ref citStr="Patrick andLin , 2002" id="5" label="CERF" position="7153">
			( Patrick andLin , 2002 )
		</ref>
		 • Markovclustering ( M1 , M2 ) 1 
		<ref citStr="Stijnvan Dongen , 2000" id="6" label="CERF" position="7214">
			( Stijnvan Dongen , 2000 )
		</ref>
		 • Fuzzyclustering ( F1 , F2)2 
		<ref citStr="Song , Cao and Bruza , 2003" id="7" label="CERF" position="7277">
			( Song , Cao and Bruza , 2003 )
		</ref>
		 Usedclusteringmethods coverboththe popularityand the variety ofthe algorithms – soft andhardclusteringandgraph clusteringetc . 
	</s>
	

	<s id="56">
		 In all clustering methods , usedsimilaritymeasure is the cosine similaritybetween two sense vectors for each contextual word . 
	</s>
	

	<s id="57">
		 We extracted clusters with these clustering methods , tried to compare theirdiscoveredsenses andthe manually distributed senses . 
	</s>
	

	<s id="58">
		 3.2 Decidingfinalsense boundary Afterclustering the normalized collocation , we combined all clusteri ng results and decided the optimal sense boundary for a central word . 
	</s>
	

	<s id="59">
		 ( g f(x,w,c))=Sxd={h l d1,...,h m d}(2) Inequation(2) , we define equation ( 1 ) as Sxd ; , this means extractedsense boundary foracentral word xwith d ; . 
	</s>
	

	<s id="60">
		 The elements ofDare the applied clustering methods , andSxis the final combination results ofall clusteri ng methods for x. 1 M1 and M2 have differenttranslatingmethods between contextan d graph . 
	</s>
	

	<s id="61">
		 2 F1and F2 are different methods deciding initial centers . 
	</s>
	

	<s id="62">
		 g0 x 1(x,1),gK),...g( , 1 h w ) , ... , ( ) , ( , ) , ( ) , ... , ( g x x I g x g x ^ + + 1 1 w h x h h m m m gx^ ( h m f(x , w,0 ) ^ ^ ^ ^ ^ ( 1 ) ^ ^ ^ ^ ^ } } This paper proposes the voting of applied clustering methods when decides final sense boundary like equation ( 3 ) . 
	</s>
	

	<s id="63">
		 Num(x) = max { num(w , d ; ) } = Sx ( 3 ) d;^D We determined the number of the final sense boundary for each central word with the number of clusters that the most clustering algorithms were extracted . 
	</s>
	

	<s id="64">
		 After deciding the final number of senses , we mapped clusters between clustering methods . 
	</s>
	

	<s id="65">
		 By comparing the agreement , the pairs of the maximum agreement are looked upon the same clusters expressing the same sense , and agreement is calculated like equation ( 4 ) , which is the agreement between k-th cluster with ;-th clustering method and l-th cluster with j-th clustering method for central word x . 
	</s>
	

	<s id="66">
		 The final step is the assigning elements into the final clusters . 
	</s>
	

	<s id="67">
		 In equation ( 5 ) , all contextual words w are classified into the maximum results of clustering methods . 
	</s>
	

	<s id="68">
		 New centers of each cluster are recalculated with the equation ( 6 ) based on the final clusters and their elements . 
	</s>
	

	<s id="69">
		 Figure 2 represents the clustering result for the central word ‘chair’ . 
	</s>
	

	<s id="70">
		 The pink box shows the central word ‘chair’ and the white boxes show the selected contextual words . 
	</s>
	

	<s id="71">
		 The white and blue area means the each clusters separated by the clustering methods . 
	</s>
	

	<s id="72">
		 The central word ‘chair’ finally makes two clusters . 
	</s>
	

	<s id="73">
		 The one located in blue area contains the collocation for the sense about ‘the position of professor’ . 
	</s>
	

	<s id="74">
		 Another cluster in the white area is the cluster for the sense about ‘furniture’ . 
	</s>
	

	<s id="75">
		 The words in each cluster are the representative contextual words which similarity is included in ranking 10 . 
	</s>
	

	<s id="76">
		 4 Experimental results We extracted sense clusters with the proposed methods from the large-scaled corpus , and compared the results with the sense distribution of the existing thesaurus . 
	</s>
	

	<s id="77">
		 Applied corpus for the experiments for English and Korean is Penn tree bank3 corpus and KAIST4 corpus . 
	</s>
	

	<s id="78">
		 3 http://www.cis.upenn.edu/—treebank/home.html 4 http://kibs.kaist.ac.kr Figure 2 The clustering example for ' chair ' For evaluation , we try to compare clustering results and sense distribution of dictionary . 
	</s>
	

	<s id="79">
		 In case of English , used dictionary is WordNet 1.75 - Fine- grained ( WF ) and coarse-grained distribution ( WC ) . 
	</s>
	

	<s id="80">
		 The coarse-grained senses in WordNet are adjusted sense based on corpus for SENSEVAL task . 
	</s>
	

	<s id="81">
		 In order to evaluate the practical word sense disambiguation systems , the senses in the WordNet 1.7 are adjusted by the analyzing the appearing senses from the Semcor . 
	</s>
	

	<s id="82">
		 For the evaluation of Korean we used Korean Unabridged Dictionary ( KD ) for fine-grained senses and Yonsei Dictionary ( YD ) for corpus-based senses . 
	</s>
	

	<s id="83">
		 Table 1 shows the clustering results by each clustering algorithms . 
	</s>
	

	<s id="84">
		 The used central words are 786 target homonyms for the English lexical samples in SENSEVAL26 . 
	</s>
	

	<s id="85">
		 The numbers in Table 1 shows the average number of clusters with each clustering method shown chapter 3 by the part of speech . 
	</s>
	

	<s id="86">
		 WC and WF are the average number of senses by the part of speech . 
	</s>
	

	<s id="87">
		 In Table 1 and 2 , the most clustering methods show the similar results . 
	</s>
	

	<s id="88">
		 But , CBC extracts more clusters comparing other clustering methods . 
	</s>
	

	<s id="89">
		 Except CBC other methods extract similar sense distribution with the Coarse-grained WordNet ( WC ) . 
	</s>
	

	<s id="90">
		 Nouns Adjectives Verbs All K 3 3.046 3.039 3.027 B 3.258 3.218 3.286 3.266 CBC 6.998 3.228 5.008 5.052 F 1 3.917 2.294 3.645 3.515 F2 4.038 5.046 3.656 4.013 Final 3.141 3.08 3.114 3.13 WC 3.261 2.887 3.366 3.252 WF 8.935 8.603 9.422 9.129 Table 1 The results of English { }U{hldj} Vot(Sx,w)max{hd;(go f ( x , w , c ^V d;^D 5 http://www.cogsci.princeton.edu/—wn/ 6 http://www.cs.unt.edu/—rada/senseval/ ) ) } xk { }I{hIx} h kd h kd agreement ( 4 ) ( 5 ) Nn 1zrSx=(^wa1,1 N ^wa2,...,1N^ Wan )(6) N n n Nouns Nouns K B C 5.5 F1 F2 M1 2.917 2.917 M2 2.833 2.583 4.083 KD YD 3.833 11.25 3.333 Table 2 The results of Korean Table 3 is the evaluating the correctness of the elements of cluster . 
	</s>
	

	<s id="91">
		 Using the sense-tagged collocation from English test suit in SENSEVAL27 , we calculated the average agreement for all central words by each clustering algorithms . 
	</s>
	

	<s id="92">
		 K B C F1 F2 98.666 98.578 90.91 97.316 88.333 Table 3 The average agreement by clustering methods As shown in Table 3 , overall clustering methods record high agreement . 
	</s>
	

	<s id="93">
		 Among the various clustering algorithms , the results of K-means and buckshot are higher than other algorithms . 
	</s>
	

	<s id="94">
		 In the K-means and fuzzy clustering , the deciding random initial shows higher agreements . 
	</s>
	

	<s id="95">
		 But , clustering time in hierarchical deciding is faster than random deciding 5 Conclusion This paper proposes the method for boundary discovery of homonymous senses . 
	</s>
	

	<s id="96">
		 In order to extract practical senses from corpus , we use the collocation from the large corpora and the clustering methods . 
	</s>
	

	<s id="97">
		 In these experiments , the results of the proposed methods are different from the fine-grained sense distribution - manually analyzed by the experts . 
	</s>
	

	<s id="98">
		 But the results are similar to the coarse-grained results – corpus-based sense distribution . 
	</s>
	

	<s id="99">
		 Therefore , these experimental results prove that we can extract practical sense distribution using the proposed methods . 
	</s>
	

	<s id="100">
		 For the conclusion , the proposed methods show the similar results with the corpus-based sense boundary . 
	</s>
	

	<s id="101">
		 For the future works , using this result , it’ll be possible to combine these results with the practical thesaurus automatically . 
	</s>
	

	<s id="102">
		 The proposed method can apply in the evaluation and tuning process for existing senses . 
	</s>
	

	<s id="103">
		 So , if overall research is successfully processed , we can get a automatic mechanism about adjusting and constructing knowledge base like thesaurus which is practical and containing enough knowledge from corpus . 
	</s>
	

	<s id="104">
		 There are some related works about this research . 
	</s>
	

	<s id="105">
		 Wortchartz is the collocation dictionary with the assumption that Collocation of a word expresses English lexical sample for the same central words he meaning of the word 
		<ref citStr="Heyer , Quasthoff and Wolff , 2001" id="8" label="CEPF" position="14838">
			( Heyer , Quasthoff and Wolff , 2001 )
		</ref>
		 . 
	</s>
	

	<s id="106">
		 
		<ref citStr="Patrick and Lin , 2002" id="9" label="CEPF" position="14876">
			( Patrick and Lin , 2002 )
		</ref>
		 tried to discover senses from the large-scaled corpus with CBC ( Committee Based Clustering ) algorithm .. In this paper , used context features are limited only 1,000 nouns by their frequency . 
	</s>
	

	<s id="107">
		 
		<ref citStr="Hyungsuk , Ploux and Wehrli , 2003" id="10" label="CEPF" position="15119">
			( Hyungsuk , Ploux and Wehrli , 2003 )
		</ref>
		 tried to extract sense differences using clustering in the multi-lingual collocation . 
	</s>
	

	<s id="108">
		 6 Acknowledgements This work has been supported by Ministry of Science and Technology in Korea . 
	</s>
	

	<s id="109">
		 The result of this work is enhanced and distributed through Bank of Language Resources supported by grant No . 
	</s>
	

	<s id="110">
		 R21-2003-000-10042-0 from Korea Science &amp; Technology Foundation . 
	</s>
	

	<s id="111">
		 References Ray S. and Turi R.H. 1999 . 
	</s>
	

	<s id="112">
		 Determination of Number of Clusters in K-means Clustering and Application in Colour Image Segmentation , In “The 4th International Conference on Advances in Pattern Recognition and Digital Techniques” , Calcuta . 
	</s>
	

	<s id="113">
		 Heyer G. , Quasthoff U. and Wolff C. 2001 . 
	</s>
	

	<s id="114">
		 Information Extraction from Text Corpora , In “IEEE Intelligent Systems and Their Applications” , Volume 16 , No. 2 . 
	</s>
	

	<s id="115">
		 Patrick Pantel and Dekang Lin . 
	</s>
	

	<s id="116">
		 2002 . 
	</s>
	

	<s id="117">
		 Discovering Word Senses from Text , In “ACM Conference on Knowledge Discovery and Data Mining” , pages 613–619 , Edmonton . 
	</s>
	

	<s id="118">
		 Hyungsuk Ji , Sabine Ploux and Eric Wehrli . 
	</s>
	

	<s id="119">
		 2003 , Lexical Knowledge Representation with Contexonyms , In “The 9th Machine Translation” , pages 194-201 , New Orleans Eric C.Jensen , Steven M.Beitzel , Angelo J.Pilotto , Nazli Goharian , Ophir Frieder . 
	</s>
	

	<s id="120">
		 2002 , Parallelizing the Buckshot Algorithm for Efficient Document Clustering , In “The 2002 ACM International Conference on Information and Knowledge Management , pages 04-09 , McLean , Virginia , USA . 
	</s>
	

	<s id="121">
		 Stijn van Dongen . 
	</s>
	

	<s id="122">
		 2000 , A cluster algorithm for graphs , In “Technical Report INS-R0010” , National Research Institute for Mathematics and Computer Science in the Netherlands . 
	</s>
	

	<s id="123">
		 Song D. , Cao G. , and Bruza P.D. 2003 , Fuzzy K- means Clustering in Information Retrieval , In “DSTC Technical Report” . 
	</s>
	

	<s id="124">
		 7 Saim Shin and Key-Sun Choi . 
	</s>
	

	<s id="125">
		 2004 , Automatic Word Sense Clustering using Collocation for Sense Adaptation , In “Global WordNet conference” , pages 320-325 , Brno , Czech . 
	</s>
	


</acldoc>
