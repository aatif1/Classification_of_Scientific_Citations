<?xml version="1.0" encoding="iso-8859-1"?>
<acldoc acl_id="P04-1030">
	

	<s id="1">
		 Head-Driven Parsing for Word Lattices Bob Carpenter Alias I , Inc. . 
	</s>
	

	<s id="2">
		 Brooklyn , NY , USA carp@alias-i.com Christopher Collins Department of Computer Science University of Toronto Toronto , ON , Canada ccollins@cs.utoronto.ca Gerald Penn Department of Computer Science University of Toronto Toronto , ON , Canada gpenn@cs.utoronto.ca Abstract We present the first application of the head-driven statistical parsing model of 
		<ref citStr="Collins ( 1999 )" id="1" label="CERF" position="453">
			Collins ( 1999 )
		</ref>
		 as a simultaneous language model and parser for large- vocabulary speech recognition . 
	</s>
	

	<s id="3">
		 The model is adapted to an online left to right chart-parser for word lattices , integrating acoustic , n-gram , and parser probabilities . 
	</s>
	

	<s id="4">
		 The parser uses structural and lexical dependencies not considered by n- gram models , conditioning recognition on more linguistically-grounded relationships . 
	</s>
	

	<s id="5">
		 Experiments on the Wall Street Journal treebank and lattice corpora show word error rates competitive with the standard n-gram language model while extracting additional structural information useful for speech understanding . 
	</s>
	

	<s id="6">
		 1 Introduction The question of how to integrate high-level knowledge representations of language with automatic speech recognition ( ASR ) is becoming more important as ( 1 ) speech recognition technology matures , ( 2 ) the rate of improvement of recognition accuracy decreases , and ( 3 ) the need for additional information ( beyond simple transcriptions ) becomes evident . 
	</s>
	

	<s id="7">
		 Most of the currently best ASR systems use an n-gram language model of the type pioneered by 
		<ref citStr="Bahl et al . ( 1983 )" id="2" label="CEPF" position="1605">
			Bahl et al . ( 1983 )
		</ref>
		 . 
	</s>
	

	<s id="8">
		 Recently , research has begun to show progress towards application of new and better models of spoken language 
		<ref citStr="Hall and Johnson , 2003" id="3" label="CEPF" position="1728">
			( Hall and Johnson , 2003 
		</ref>
		<ref citStr="Roark , 2001" id="4" label="CEPF" position="1754">
			; Roark , 2001 
		</ref>
		<ref citStr="Chelba and Jelinek , 2000" id="5" label="CEPF" position="1769">
			; Chelba and Jelinek , 2000 )
		</ref>
		 . 
	</s>
	

	<s id="9">
		 Our goal is integration of head-driven lexicalized parsing with acoustic and n-gram models for speech recognition , extracting high-level structure from speech , while simultaneously selecting the best path in a word lattice . 
	</s>
	

	<s id="10">
		 Parse trees generated by this process will be useful for automated speech understanding , such as in higher semantic parsing 
		<ref citStr="Ng and Zelle , 1997" id="6" label="CEPF" position="2194">
			( Ng and Zelle , 1997 )
		</ref>
		 . 
	</s>
	

	<s id="11">
		 
		<ref citStr="Collins ( 1999 )" id="7" label="CEPF" position="2222">
			Collins ( 1999 )
		</ref>
		 presents three lexicalized models which consider long-distance dependencies within a sentence . 
	</s>
	

	<s id="12">
		 Grammar productions are conditioned on headwords . 
	</s>
	

	<s id="13">
		 The conditioning context is thus more focused than that of a large n-gram covering the same span , so the sparse data problems arising from the sheer size of the parameter space are less pressing . 
	</s>
	

	<s id="14">
		 However , sparse data problems arising from the limited availability of annotated training data become a problem . 
	</s>
	

	<s id="15">
		 We test the head-driven statistical lattice parser with word lattices from the NIST HUB-1 corpus , which has been used by others in related work 
		<ref citStr="Hall and Johnson , 2003" id="8" label="CEPF" position="2864">
			( Hall and Johnson , 2003 
		</ref>
		<ref citStr="Roark , 2001" id="9" label="CEPF" position="2890">
			; Roark , 2001 
		</ref>
		<ref citStr="Chelba and Jelinek , 2000" id="10" label="CEPF" position="2905">
			; Chelba and Jelinek , 2000 )
		</ref>
		 . 
	</s>
	

	<s id="16">
		 Parse accuracy and word error rates are reported . 
	</s>
	

	<s id="17">
		 We present an analysis of the effects of pruning and heuristic search on efficiency and accuracy and note several simplifying assumptions common to other reported experiments in this area , which present challenges for scaling up to real- world applications . 
	</s>
	

	<s id="18">
		 This work shows the importance of careful algorithm and data structure design and choice of dynamic programming constraints to the efficiency and accuracy of a head-driven probabilistic parser for speech . 
	</s>
	

	<s id="19">
		 We find that the parsing model of 
		<ref citStr="Collins ( 1999 )" id="11" label="CERF" position="3540">
			Collins ( 1999 )
		</ref>
		 can be successfully adapted as a language model for speech recognition . 
	</s>
	

	<s id="20">
		 In the following section , we present a review of recent works in high-level language modelling for speech recognition . 
	</s>
	

	<s id="21">
		 We describe the word lattice parser developed in this work in Section 3 . 
	</s>
	

	<s id="22">
		 Section 4 is a description of current evaluation metrics , and suggestions for new metrics . 
	</s>
	

	<s id="23">
		 Experiments on strings and word lattices are reported in Section 5 , and conclusions and opportunities for future work are outlined in Section 6 . 
	</s>
	

	<s id="24">
		 2 Previous Work The largest improvements in word error rate ( WER ) have been seen with n-best list rescoring . 
	</s>
	

	<s id="25">
		 The best n hypotheses of a simple speech recognizer are processed by a more sophisticated language model and re-ranked . 
	</s>
	

	<s id="26">
		 This method is algorithmically simpler than parsing lattices , as one can use a model developed for strings , which need not operate strictly . 
	</s>
	

	<s id="27">
		 left to right . 
	</s>
	

	<s id="28">
		 However , we confirm the observation of 
		<ref citStr="Ravishankar , 1997" id="12" label="CEPF" position="4563">
			( Ravishankar , 1997 
		</ref>
		<ref citStr="Hall and Johnson , 2003" id="13" label="CEPF" position="4584">
			; Hall and Johnson , 2003 )
		</ref>
		 that parsing word lattices saves computation time by only parsing common substrings once . 
	</s>
	

	<s id="29">
		 
		<ref citStr="Chelba ( 2000 )" id="14" label="CEPN" position="4727">
			Chelba ( 2000 )
		</ref>
		 reports WER reduction by rescoring word lattices with scores of a structured language model 
		<ref citStr="Chelba and Jelinek , 2000" id="15" label="CEPF" position="4849">
			( Chelba and Jelinek , 2000 )
		</ref>
		 , interpolated with trigram scores . 
	</s>
	

	<s id="30">
		 Word predictions of the structured language model are conditioned on the two previous phrasal heads not yet contained in a bigger constituent . 
	</s>
	

	<s id="31">
		 This is a computationally intensive process , as the dependencies considered can be of arbitrarily long distances . 
	</s>
	

	<s id="32">
		 All possible sentence prefixes are considered at each extension step . 
	</s>
	

	<s id="33">
		 
		<ref citStr="Roark ( 2001 )" id="16" label="CJPF" position="5268">
			Roark ( 2001 )
		</ref>
		 reports on the use of a lexicalized probabilistic top-down parser for word lattices , evaluated both on parse accuracy and WER . 
	</s>
	

	<s id="34">
		 Our work is different from 
		<ref citStr="Roark ( 2001 )" id="17" label="CJPF" position="5448">
			Roark ( 2001 )
		</ref>
		 in that we use a bottom-up parsing algorithm with dynamic programming based on the parsing model II of 
		<ref citStr="Collins ( 1999 )" id="18" label="CERF" position="5568">
			Collins ( 1999 )
		</ref>
		 . 
	</s>
	

	<s id="35">
		 Bottom-up chart parsing , through various forms of extensions to the CKY algorithm , has been applied to word lattices for speech recognition 
		<ref citStr="Hall and Johnson , 2003" id="19" label="CEPF" position="5722">
			( Hall and Johnson , 2003 
		</ref>
		<ref citStr="Chappelier and Rajman , 1998" id="20" label="CEPF" position="5748">
			; Chappelier and Rajman , 1998 
		</ref>
		<ref citStr="Chelba and Jelinek , 2000" id="21" label="CEPF" position="5779">
			; Chelba and Jelinek , 2000 )
		</ref>
		 . 
	</s>
	

	<s id="36">
		 Full acoustic and n-best lattices filtered by trigram scores have been parsed . 
	</s>
	

	<s id="37">
		 
		<ref citStr="Hall and Johnson ( 2003 )" id="22" label="CEPF" position="5934">
			Hall and Johnson ( 2003 )
		</ref>
		 use a best-first probabilistic context free grammar ( PCFG ) to parse the input lattice , pruning to a set of local trees ( candidate partial parse trees ) , which are then passed to a version of the parser of 
		<ref citStr="Charniak ( 2001 )" id="23" label="OEPF" position="6162">
			Charniak ( 2001 )
		</ref>
		 for more refined parsing . 
	</s>
	

	<s id="38">
		 Unlike 
		<ref citStr="Roark , 2001" id="24" label="CJPN" position="6206">
			( Roark , 2001 
		</ref>
		<ref citStr="Chelba , 2000" id="25" label="CJPN" position="6221">
			; Chelba , 2000 )
		</ref>
		 , 
		<ref citStr="Hall and Johnson ( 2003 )" id="26" label="CEPF" position="6266">
			Hall and Johnson ( 2003 )
		</ref>
		 achieve improvement in WER over the trigram model without interpolating its lattice parser probabilities directly with trigram probabilities . 
	</s>
	

	<s id="39">
		 3 Word Lattice Parser Parsing models based on headword dependency relationships have been reported , such as the structured language model of 
		<ref citStr="Chelba and Jelinek ( 2000 )" id="27" label="CJPN" position="6588">
			Chelba and Jelinek ( 2000 )
		</ref>
		 . 
	</s>
	

	<s id="40">
		 These models use much less conditioning information than the parsing models of 
		<ref citStr="Collins ( 1999 )" id="28" label="CEPF" position="6695">
			Collins ( 1999 )
		</ref>
		 , and do not provide Penn Treebank format parse trees as output . 
	</s>
	

	<s id="41">
		 In this section we outline the adaptation of the 
		<ref citStr="Collins ( 1999 )" id="29" label="CERF" position="6836">
			Collins ( 1999 )
		</ref>
		 parsing model to word lattices . 
	</s>
	

	<s id="42">
		 The intended action of the parser is illustrated in Figure 1 , which shows parse trees built directly upon a word lattice . 
	</s>
	

	<s id="43">
		 3.1 Parameterization The parameterization of model II of 
		<ref citStr="Collins ( 1999 )" id="30" label="CERF" position="7085">
			Collins ( 1999 )
		</ref>
		 is used in our word lattice parser . 
	</s>
	

	<s id="44">
		 Parameters are S S. NP NP VP NN IN CC NNP AUX IN DT NN MD VB rise arise Figure 1 : Example of a partially-parsed word lattice . 
	</s>
	

	<s id="45">
		 Different paths through the lattice are simultaneously parsed . 
	</s>
	

	<s id="46">
		 The example shows two final parses , one of low probability ( S ) and one of high probability ( S ) . 
	</s>
	

	<s id="47">
		 maximum likelihood estimates of conditional probabilities — the probability of some event of interest ( e.g. , a left-modifier attachment ) given a context ( e.g. , parent non-terminal , distance , headword ) . 
	</s>
	

	<s id="48">
		 One notable difference between the word lattice parser and the original implementation of 
		<ref citStr="Collins ( 1999 )" id="31" label="CEPF" position="7780">
			Collins ( 1999 )
		</ref>
		 is the handling of part-of-speech ( POS ) tagging of unknown words ( words seen fewer than 5 times in training ) . 
	</s>
	

	<s id="49">
		 The conditioning context of the parsing model parameters includes POS tagging . 
	</s>
	

	<s id="50">
		 
		<ref citStr="Collins ( 1999 )" id="32" label="CEPF" position="8010">
			Collins ( 1999 )
		</ref>
		 falls back to the POS tagging of 
		<ref citStr="Ratnaparkhi ( 1996 )" id="33" label="CEPF" position="8064">
			Ratnaparkhi ( 1996 )
		</ref>
		 for words seen fewer than 5 times in the training corpus . 
	</s>
	

	<s id="51">
		 As the tagger of 
		<ref citStr="Ratnaparkhi ( 1996 )" id="34" label="OJPF" position="8170">
			Ratnaparkhi ( 1996 )
		</ref>
		 cannot tag a word lattice , we cannot back off to this tagging . 
	</s>
	

	<s id="52">
		 We rely on the tag assigned by the parsing model in all cases . 
	</s>
	

	<s id="53">
		 Edges created by the bottom-up parsing are assigned a score which is the product of the inside and outside probabilities of the 
		<ref citStr="Collins ( 1999 )" id="35" label="CEPF" position="8462">
			Collins ( 1999 )
		</ref>
		 model . 
	</s>
	

	<s id="54">
		 3.2 Parsing Algorithm The algorithm is a variation of probabilistic online , bottom-up , left-to-right Cocke-KasamiYounger parsing similar to 
		<ref citStr="Chappelier and Rajman ( 1998 )" id="36" label="CERF" position="8652">
			Chappelier and Rajman ( 1998 )
		</ref>
		 . 
	</s>
	

	<s id="55">
		 Our parser produces trees ( bottom-up ) in a right- branching manner , using unary extension and binary adjunction . 
	</s>
	

	<s id="56">
		 Starting with a proposed headword , left modifiers are added first using right-branching , then right modifiers using left-branching . 
	</s>
	

	<s id="57">
		 Word lattice edges are iteratively added to the agenda . 
	</s>
	

	<s id="58">
		 Complete closure is carried out , and the next word edge is added to the agenda . 
	</s>
	

	<s id="59">
		 This process is repeated until all word edges are read from the . 
	</s>
	

	<s id="60">
		 speculation in tokyo was that the yen could and the unit lattice , and at least one complete parse is found . 
	</s>
	

	<s id="61">
		 Edges are each assigned a score , used to rank parse candidates . 
	</s>
	

	<s id="62">
		 For parsing of strings , the score for a chart edge is the product of the scores of any child edges and the score for the creation of the new edge , as given by the model parameters . 
	</s>
	

	<s id="63">
		 This score , defined solely by the parsing model , will be referred to as the parser score . 
	</s>
	

	<s id="64">
		 The total score for chart edges for the lattice parsing task is a combination of the parser score , an acoustic model score , and a trigram model score . 
	</s>
	

	<s id="65">
		 Scaling factors follow those of 
		<ref citStr="Chelba and Jelinek , 2000" id="37" label="CEPF" position="9850">
			( Chelba and Jelinek , 2000 
		</ref>
		<ref citStr="Roark , 2001" id="38" label="CEPF" position="9878">
			; Roark , 2001 )
		</ref>
		 . 
	</s>
	

	<s id="66">
		 3.3 Smoothing and Pruning The parameter estimation techniques ( smoothing and back-off ) of 
		<ref citStr="Collins ( 1999 )" id="39" label="CERF" position="10014">
			Collins ( 1999 )
		</ref>
		 are reimplemented . 
	</s>
	

	<s id="67">
		 Additional techniques are required to prune the search space of possible parses , due to the complexity of the parsing algorithm and the size of the word lattices . 
	</s>
	

	<s id="68">
		 The main technique we employ is a variation of the beam search of 
		<ref citStr="Collins ( 1999 )" id="40" label="CERF" position="10300">
			Collins ( 1999 )
		</ref>
		 to restrict the chart size by excluding low probability edges . 
	</s>
	

	<s id="69">
		 The total score ( combined acoustic and language model scores ) of candidate edges are compared against edge with the same span and category . 
	</s>
	

	<s id="70">
		 Proposed edges with score outside the beam are not added to the chart . 
	</s>
	

	<s id="71">
		 The drawback to this process is that we can no longer guarantee that a model-optimal solution will be found . 
	</s>
	

	<s id="72">
		 In practice , these heuristics have a negative effect on parse accuracy , but the amount of pruning can be tuned to balance relative time and space savings against precision and recall degradation 
		<ref citStr="Collins , 1999" id="41" label="CEPF" position="10941">
			( Collins , 1999 )
		</ref>
		 . 
	</s>
	

	<s id="73">
		 
		<ref citStr="Collins ( 1999 )" id="42" label="CJPF" position="10969">
			Collins ( 1999 )
		</ref>
		 uses a fixed size beam ( 10 000 ) . 
	</s>
	

	<s id="74">
		 We experiment with several variable beam ( ˆb ) sizes , where the beam is some function of a base beam ( b ) and the edge width ( the number of terminals dominated by an edge ) . 
	</s>
	

	<s id="75">
		 The base beam starts at a low beam size and increases iteratively by a specified increment if no parse is found . 
	</s>
	

	<s id="76">
		 This allows parsing to operate quickly ( with a minimal number of edges added to the chart ) . 
	</s>
	

	<s id="77">
		 However , if many iterations are required to obtain a parse , the utility of starting with a low beam and iterating becomes questionable 
		<ref citStr="Goodman , 1997" id="43" label="CEPF" position="11586">
			( Goodman , 1997 )
		</ref>
		 . 
	</s>
	

	<s id="78">
		 The base beam is limited to control the increase in the chart size . 
	</s>
	

	<s id="79">
		 The selection of the base beam , beam increment , and variable beam function is governed by the familiar speed/accuracy trade-off.1 The variable beam function found to allow fast convergence with minimal loss of accuracy is : b bˆ ( 1 ) log w 2 2 1Details of the optimization can be found in 
		<ref citStr="Collins ( 2004 )" id="44" label="CEPF" position="11985">
			Collins ( 2004 )
		</ref>
		 . 
	</s>
	

	<s id="80">
		 
		<ref citStr="Charniak et al . ( 1998 )" id="45" label="CEPF" position="12022">
			Charniak et al . ( 1998 )
		</ref>
		 introduce overparsing as a technique to improve parse accuracy by continuing parsing after the first complete parse tree is found . 
	</s>
	

	<s id="81">
		 The technique is employed by 
		<ref citStr="Hall and Johnson ( 2003 )" id="46" label="CEPF" position="12218">
			Hall and Johnson ( 2003 )
		</ref>
		 to ensure that early stages of parsing do not strongly bias later stages . 
	</s>
	

	<s id="82">
		 We adapt this idea to a single stage process . 
	</s>
	

	<s id="83">
		 Due to the restrictions of beam search and thresholds , the first parse found by the model may not be the model optimal parse ( i.e. , we cannot guarantee best-first search ) . 
	</s>
	

	<s id="84">
		 We therefore employ a form of overparsing — once a complete parse tree is found , we further extend the base beam by the beam increment and parse again . 
	</s>
	

	<s id="85">
		 We continue this process as long as extending the beam results in an improved best parse score . 
	</s>
	

	<s id="86">
		 4 Expanding the Measures of Success Given the task of simply generating a transcription of speech , WER is a useful and direct way to measure language model quality for ASR . 
	</s>
	

	<s id="87">
		 WER is the count of incorrect words in hypothesis Wˆ per word in the true string W . 
	</s>
	

	<s id="88">
		 For measurement , we must assume prior knowledge of W and the best alignment of the reference and hypothesis strings.2 Errors are categorized as insertions , deletions , or substitutions . 
	</s>
	

	<s id="89">
		 Word Error Rate 100Insertions Substitutions Deletions ( 2 ) Total Words in Correct Transcript It is important to note that most models — 
		<ref citStr="Mangu et al . ( 2000 )" id="47" label="CEPF" position="13452">
			Mangu et al . ( 2000 )
		</ref>
		 is an innovative exception — minimize sentence error . 
	</s>
	

	<s id="90">
		 Sentence error rate is the percentage of sentences for which the proposed utterance has at least one error . 
	</s>
	

	<s id="91">
		 Models ( such as ours ) which optimize prediction of test sentences Wt , generated by the source , minimize the sentence error . 
	</s>
	

	<s id="92">
		 Thus even though WER is useful practically , it is formally not the appropriate measure for the commonly used language models . 
	</s>
	

	<s id="93">
		 Unfortunately , as a practical measure , sentence error rate is not as useful — it is not as fine-grained as WER . 
	</s>
	

	<s id="94">
		 Perplexity is another measure of language model quality , measurable independent of ASR performance 
		<ref citStr="Jelinek , 1997" id="48" label="CEPF" position="14154">
			( Jelinek , 1997 )
		</ref>
		 . 
	</s>
	

	<s id="95">
		 Perplexity is related to the entropy of the source model which the language model attempts to estimate . 
	</s>
	

	<s id="96">
		 These measures , while informative , do not capture success of extraction of high-level information from speech . 
	</s>
	

	<s id="97">
		 Task-specific measures should be used in tandem with extensional measures such as perplexity and WER . 
	</s>
	

	<s id="98">
		 
		<ref citStr="Roark ( 2002 )" id="49" label="CEPF" position="14529">
			Roark ( 2002 )
		</ref>
		 , when reviewing 2SCLITE ( http://www.nist.gov/speech/ tools / ) by KIST is the most commonly used alignment tool . 
	</s>
	

	<s id="99">
		 parsing for speech recognition , discusses a modelling trade-off between producing parse trees and producing strings . 
	</s>
	

	<s id="100">
		 Most models are evaluated either with measures of success for parsing or for word recognition , but rarely both . 
	</s>
	

	<s id="101">
		 Parsing models are difficult to implement as word-predictive language models due to their complexity . 
	</s>
	

	<s id="102">
		 Generative random sampling is equally challenging , so the parsing correlate of perplexity is not easy to measure . 
	</s>
	

	<s id="103">
		 Traditional ( i.e. , n-gram ) language models do not produce parse trees , so parsing metrics are not useful . 
	</s>
	

	<s id="104">
		 However , 
		<ref citStr="Roark ( 2001 )" id="50" label="CERF" position="15287">
			Roark ( 2001 )
		</ref>
		 argues for using parsing metrics , such as labelled precision and recall,3 along with WER , for parsing applications in ASR . 
	</s>
	

	<s id="105">
		 Weighted WER 
		<ref citStr="Weber et al. , 1997" id="51" label="CEPF" position="15459">
			( Weber et al. , 1997 )
		</ref>
		 is also a useful measurement , as the most often ill-recognized words are short , closed-class words , which are not as important to speech understanding as phrasal head words . 
	</s>
	

	<s id="106">
		 We will adopt the testing strategy of 
		<ref citStr="Roark ( 2001 )" id="52" label="CERF" position="15699">
			Roark ( 2001 )
		</ref>
		 , but find that measurement of parse accuracy and WER on the same data set is not possible given currently available corpora . 
	</s>
	

	<s id="107">
		 Use of weighted WER and development of methods to simultaneously measure WER and parse accuracy remain a topic for future research . 
	</s>
	

	<s id="108">
		 5 Experiments The word lattice parser was evaluated with several metrics — WER , labelled precision and recall , crossing brackets , and time and space resource usage . 
	</s>
	

	<s id="109">
		 Following 
		<ref citStr="Roark ( 2001 )" id="53" label="CERF" position="16181">
			Roark ( 2001 )
		</ref>
		 , we conducted evaluations using two experimental sets — strings and word lattices . 
	</s>
	

	<s id="110">
		 We optimized settings ( thresholds , variable beam function , base beam value ) for parsing using development test data consisting of strings for which we have annotated parse trees . 
	</s>
	

	<s id="111">
		 The parsing accuracy for parsing word lattices was not directly evaluated as we did not have annotated parse trees for comparison . 
	</s>
	

	<s id="112">
		 Furthermore , standard parsing measures such as labelled precision and recall are not directly applicable in cases where the number of words differs between the proposed parse tree and the gold standard . 
	</s>
	

	<s id="113">
		 Results show scores for parsing strings which are lower than the original implementation of 
		<ref citStr="Collins ( 1999 )" id="54" label="CEPF" position="16933">
			Collins ( 1999 )
		</ref>
		 . 
	</s>
	

	<s id="114">
		 The WER scores for this , the first application of the 
		<ref citStr="Collins ( 1999 )" id="55" label="CEPF" position="17016">
			Collins ( 1999 )
		</ref>
		 model to parsing word lattices , are comparable to other recent work in syntactic language modelling , and better than a simple trigram model trained on the same data . 
	</s>
	

	<s id="115">
		 3Parse trees are commonly scored with the PARSEVAL set of metrics 
		<ref citStr="Black et al. , 1991" id="56" label="CEPF" position="17284">
			( Black et al. , 1991 )
		</ref>
		 . 
	</s>
	

	<s id="116">
		 5.1 Parsing Strings The lattice parser can parse strings by creating a single-path lattice from the input ( all word transitions are assigned an input score of 1.0 ) . 
	</s>
	

	<s id="117">
		 The lattice parser was trained on sections 02-21 of the Wall Street Journal portion of the Penn Treebank 
		<ref citStr="Taylor et al. , 2003" id="57" label="OEPF" position="17602">
			( Taylor et al. , 2003 )
		</ref>
		 Development testing was carried out on section 23 in order to select model thresholds and variable beam functions . 
	</s>
	

	<s id="118">
		 Final testing was carried out on section 00 , and the PARSEVAL measures 
		<ref citStr="Black et al. , 1991" id="58" label="CEPF" position="17823">
			( Black et al. , 1991 )
		</ref>
		 were used to evaluate the performance . 
	</s>
	

	<s id="119">
		 The scores for our experiments are lower than the scores of the original implementation of model II 
		<ref citStr="Collins , 1999" id="59" label="CEPF" position="17991">
			( Collins , 1999 )
		</ref>
		 . 
	</s>
	

	<s id="120">
		 This difference is likely due in part to differences in POS tagging . 
	</s>
	

	<s id="121">
		 Tag accuracy for our model was 93.2 % , whereas for the original implementation of 
		<ref citStr="Collins ( 1999 )" id="60" label="OEPF" position="18181">
			Collins ( 1999 )
		</ref>
		 , model II achieved tag accuracy of 96.75 % . 
	</s>
	

	<s id="122">
		 In addition to different tagging strategies for unknown words , mentioned above , we restrict the tag-set considered by the parser for each word to those suggested by a simple first-stage tagger.4 By reducing the tag-set considered by the parsing model , we reduce the search space and increase the speed . 
	</s>
	

	<s id="123">
		 However , the simple tagger used to narrow the search also introduces tagging error . 
	</s>
	

	<s id="124">
		 The utility of the overparsing extension can be seen in Table 1 . 
	</s>
	

	<s id="125">
		 Each of the PARSEVAL measures improves when overparsing is used . 
	</s>
	

	<s id="126">
		 5.2 Parsing Lattices The success of the parsing model as a language model for speech recognition was measured both by parsing accuracy ( parsing strings with annotated reference parses ) , and by WER . 
	</s>
	

	<s id="127">
		 WER is measured by parsing word lattices and comparing the sentence yield of the highest scoring parse tree to the reference transcription ( using NIST SCLITE for alignment and error calculation).5 We assume the parsing performance achieved by parsing strings carries over approximately to parsing word lattices . 
	</s>
	

	<s id="128">
		 Two different corpora were used in training the parsing model on word lattices : sections 02-21 of the WSJ Penn Treebank ( the same sections as used to train the model for parsing strings ) [ 1 million words ] 4The original implementation 
		<ref citStr="Collins , 1999" id="61" label="CEPF" position="19589">
			( Collins , 1999 )
		</ref>
		 of this model considered all tags for all words . 
	</s>
	

	<s id="129">
		 5To properly model language using a parser , one should sum parse tree scores for each sentence hypothesis , and choose the sentence with the best sum of parse tree scores . 
	</s>
	

	<s id="130">
		 We choose the yield of the parse tree with the highest score . 
	</s>
	

	<s id="131">
		 Summation is too computationally expensive given the model —we do not even generate all possible parse trees , but instead restrict generation using dynamic programming . 
	</s>
	

	<s id="132">
		 Exp . 
	</s>
	

	<s id="133">
		 OP LP ( % ) LR ( % ) CB 0 CB ( % ) 2 CB ( % ) Ref N 88.7 89.0 0.95 65.7 85.6 1 N 79.4 80.6 1.89 46.2 74.5 2 Y 80.8 81.4 1.70 44.3 80.4 Table 1 : Results for parsing section 0 ( 40 words ) of the WSJ Penn Treebank : OP = overparsing , LP/LR = labelled precision/recall . 
	</s>
	

	<s id="134">
		 CB is the average number of crossing brackets per sentence . 
	</s>
	

	<s id="135">
		 0 CB , 2 CB are the percentage of sentences with 0 or 2 crossing brackets respectively . 
	</s>
	

	<s id="136">
		 Ref is model II of 
		<ref citStr="Collins , 1999" id="62" label="CEPF" position="20584">
			( Collins , 1999 )
		</ref>
		 . 
	</s>
	

	<s id="137">
		 section “1987” of the BLLIP corpus 
		<ref citStr="Charniak et al. , 1999" id="63" label="OEPF" position="20659">
			( Charniak et al. , 1999 )
		</ref>
		 [ 20 million words ] The BLLIP corpus is a collection of Penn Treebank-style parses of the three-year ( 1987-1989 ) Wall Street Journal collection from the ACL/DCI corpus ( approximately 30 million words).6 The parses were automatically produced by the parser of 
		<ref citStr="Charniak ( 2001 )" id="64" label="OEPF" position="20940">
			Charniak ( 2001 )
		</ref>
		 . 
	</s>
	

	<s id="138">
		 As the memory usage of our model corresponds directly to the amount of training data used , we were restricted by available memory to use only one section ( 1987 ) of the total corpus . 
	</s>
	

	<s id="139">
		 Using the BLLIP corpus , we expected to get lower quality parse results due to the higher parse error of the corpus , when compared to the manually annotated Penn Treebank . 
	</s>
	

	<s id="140">
		 The WER was expected to improve , as the BLLIP corpus has much greater lexical coverage . 
	</s>
	

	<s id="141">
		 The training corpora were modified using a utility by Brian Roark to convert newspaper text to speech- like text , before being used as training input to the model . 
	</s>
	

	<s id="142">
		 Specifically , all numbers were converted to words ( 6 0 sixty ) and all punctuation was removed . 
	</s>
	

	<s id="143">
		 We tested the performance of our parser on the word lattices from the NIST HUB-1 evaluation task of 1993 . 
	</s>
	

	<s id="144">
		 The lattices are derived from a set of utterances produced from Wall Street Journal text — the same domain as the Penn Treebank and the BLLIP training data . 
	</s>
	

	<s id="145">
		 The word lattices were previously pruned to the 50-best paths by Brian Roark , using the A* decoding of 
		<ref citStr="Chelba ( 2000 )" id="65" label="OEPF" position="22115">
			Chelba ( 2000 )
		</ref>
		 . 
	</s>
	

	<s id="146">
		 The word lattices of the HUB-1 corpus are directed acyclic graphs in the HTK Standard Lattice Format ( SLF ) , consisting of a set of vertices and a set of edges . 
	</s>
	

	<s id="147">
		 Vertices , or nodes , are defined by a time-stamp and labelled with a word . 
	</s>
	

	<s id="148">
		 The set of labelled , weighted edges , represents the word utterances . 
	</s>
	

	<s id="149">
		 A word w is hypothesized over edge e if e ends at a vertex v labelled w. Edges are associated with transition probabilities and are labelled with an acoustic score and a language model score . 
	</s>
	

	<s id="150">
		 The lattices of the HUB- 6The sentences of the HUB-1 corpus are a subset of those in BLLIP . 
	</s>
	

	<s id="151">
		 We removed all HUB-1 sentences from the BLLIP corpus used in training . 
	</s>
	

	<s id="152">
		 1 corpus are annotated with trigram scores trained using a 20 thousand word vocabulary and 40 million word training sample . 
	</s>
	

	<s id="153">
		 The word lattices have a unique start and end point , and each complete path through a lattice represents an utterance hypothesis . 
	</s>
	

	<s id="154">
		 As the parser operates in a left-to-right manner , and closure is performed at each node , the input lattice edges must be processed in topological order . 
	</s>
	

	<s id="155">
		 Input lattices were sorted before parsing . 
	</s>
	

	<s id="156">
		 This corpus has been used in other work on syntactic language modelling 
		<ref citStr="Chelba , 2000" id="66" label="CEPF" position="23417">
			( Chelba , 2000 
		</ref>
		<ref citStr="Roark , 2001" id="67" label="CEPF" position="23433">
			; Roark , 2001 
		</ref>
		<ref citStr="Hall and Johnson , 2003" id="68" label="CEPF" position="23448">
			; Hall and Johnson , 2003 )
		</ref>
		 . 
	</s>
	

	<s id="157">
		 The word lattices of the HUB-1 corpus are annotated with an acoustic score , a , and a trigram probability , lm , for each edge . 
	</s>
	

	<s id="158">
		 The input edge score stored in the word lattice is : log PZnpyd ^log a ^log lm ( 3 ) where a is the acoustic score and lm is the trigram score stored in the lattice . 
	</s>
	

	<s id="159">
		 The total edge weight in the parser is a scaled combination of these scores with the parser score derived with the model parameters : log w ^ log a ^ log lm s ( 4 ) where w is the edge weight , and s is the score assigned by the parameters of the parsing model . 
	</s>
	

	<s id="160">
		 We optimized performance on a development subset of test data , yielding ^ 1 16 and ^ 1 . 
	</s>
	

	<s id="161">
		 There is an important difference in the tokenization of the HUB-1 corpus and the Penn Treebank format . 
	</s>
	

	<s id="162">
		 Clitics ( i.e. , he’s , wasn’t ) are split from their hosts in the Penn Treebank ( i.e. , he ’s , was n’ t ) , but not in the word lattices . 
	</s>
	

	<s id="163">
		 The Tree- bank format cannot easily be converted into the lattice format , as often the two parts fall into different parse constituents . 
	</s>
	

	<s id="164">
		 We used the lattices modified by 
		<ref citStr="Chelba ( 2000 )" id="69" label="CEPF" position="24637">
			Chelba ( 2000 )
		</ref>
		 in dealing with this problem — contracted words are split into two parts and the edge scores redistributed . 
	</s>
	

	<s id="165">
		 We followed 
		<ref citStr="Hall and Johnson ( 2003 )" id="70" label="CEPF" position="24794">
			Hall and Johnson ( 2003 )
		</ref>
		 and used the Treebank tokenization for measuring the WER . 
	</s>
	

	<s id="166">
		 The model was tested with and without overparsing . 
	</s>
	

	<s id="167">
		 We see from Table 2 that overparsing has little effect on the WER . 
	</s>
	

	<s id="168">
		 The word sequence most easily parsed by the model ( i.e. , generating the first complete parse tree ) is likely also the word sequence found by overparsing . 
	</s>
	

	<s id="169">
		 Although overparsing may have little effect on WER , we know from the experiments on strings that overparsing increases parse accuracy . 
	</s>
	

	<s id="170">
		 This introduces a speed-accuracy tradeoff : depending on what type of output is required from the model ( parse trees or strings ) , the additional time and resource requirements of overparsing may or may not be warranted . 
	</s>
	

	<s id="171">
		 5.3 Parsing N-Best Lattices vs. N-Best Lists The application of the model to 50-best word lattices was compared to rescoring the 50-best paths individually ( 50-best list parsing ) . 
	</s>
	

	<s id="172">
		 The results are presented in Table 2 . 
	</s>
	

	<s id="173">
		 The cumulative number of edges added to the chart per word for n-best lists is an order of magnitude larger than for corresponding n-best lattices , in all cases . 
	</s>
	

	<s id="174">
		 As the WERs are similar , we conclude that parsing n-best lists requires more work than parsing n-best lattices , for the same result . 
	</s>
	

	<s id="175">
		 Therefore , parsing lattices is more efficient . 
	</s>
	

	<s id="176">
		 This is because common substrings are only considered once per lattice . 
	</s>
	

	<s id="177">
		 The amount of computational savings is dependent on the density of the lattices — for very dense lattices , the equivalent n-best list parsing will parse common substrings up to n times . 
	</s>
	

	<s id="178">
		 In the limit of lowest density , a lattice may have paths without overlap , and the number of edges per word would be the same for the lattice and lists . 
	</s>
	

	<s id="179">
		 5.4 Time and Space Requirements The algorithms and data structures were designed to minimize parameter lookup times and memory usage by the chart and parameter set 
		<ref citStr="Collins , 2004" id="71" label="CEPF" position="26789">
			( Collins , 2004 )
		</ref>
		 . 
	</s>
	

	<s id="180">
		 To increase parameter lookup speed , all parameter values are calculated for all levels of back-off at training time . 
	</s>
	

	<s id="181">
		 By contrast , 
		<ref citStr="Collins , 1999" id="72" label="CJPF" position="26961">
			( Collins , 1999 )
		</ref>
		 calculates parameter values by looking up event counts at run-time . 
	</s>
	

	<s id="182">
		 The implementation was then optimized using a memory and processor profiler and debugger . 
	</s>
	

	<s id="183">
		 Parsing the complete set of HUB-1 lattices ( 213 sentences , a total of 3,446 words ) on average takes approximately 8 hours , on an Intel Pentium 4 ( 1.6GHz ) Linux system , using 1GB memory . 
	</s>
	

	<s id="184">
		 Memory requirements for parsing lattices is vastly greater than equivalent parsing of a single sentence , as chart size increases with the number of divergent paths in a lattice . 
	</s>
	

	<s id="185">
		 Additional analysis of resource issues can be found in 
		<ref citStr="Collins ( 2004 )" id="73" label="CEPF" position="27603">
			Collins ( 2004 )
		</ref>
		 . 
	</s>
	

	<s id="186">
		 5.5 Comparison to Previous Work The results of our best experiments for lattice- and list-parsing are compared with previous results in Table 3 . 
	</s>
	

	<s id="187">
		 The oracle WER7 for the HUB-1 corpus is 3.4 % . 
	</s>
	

	<s id="188">
		 For the pruned 50-best lattices , the oracle WER is 7.8 % . 
	</s>
	

	<s id="189">
		 We see that by pruning the lattices using the trigram model , we already introduce additional error . 
	</s>
	

	<s id="190">
		 Because of the memory usage and time required for parsing word lattices , we were unable to test our model on the original “acoustic” HUB-1 lattices , and are thus limited by the oracle WER of the 50-best lattices , and the bias introduced by pruning using a trigram model . 
	</s>
	

	<s id="191">
		 Where available , we also present comparative scores of the sentence error rate ( SER ) — the percentage of sentences in the test set for which there was at least one recognition error . 
	</s>
	

	<s id="192">
		 Note that due to the small ( 213 samples ) size of the HUB-1 corpus , the differences seen in SER may not be significant . 
	</s>
	

	<s id="193">
		 We see an improvement in WER for our pars- ing model alone ( ^ ^ 0 ) trained on 1 million words of the Penn Treebank compared to a trigram model trained on the same data — the “Treebank Trigram” noted in Table 3 . 
	</s>
	

	<s id="194">
		 This indicates that the larger context considered by our model allows for performance improvements over the trigram model alone . 
	</s>
	

	<s id="195">
		 Further improvement is seen with the com- bination of acoustic , parsing , and trigram scores ( ^ 1 16^ 1 ) . 
	</s>
	

	<s id="196">
		 However , the combination of the parsing model ( trained on 1M words ) with the lattice trigram ( trained on 40M words ) resulted in a higher WER than the lattice trigram alone . 
	</s>
	

	<s id="197">
		 This indicates that our 1M word training set is not sufficient to permit effective combination with the lattice trigram . 
	</s>
	

	<s id="198">
		 When the training of the head-driven parsing model was extended to the BLLIP 1987 corpus ( 20M words ) , the combination of models ( ^ 1 16^ 1 ) achieved additional improvement in WER over the lattice trigram alone . 
	</s>
	

	<s id="199">
		 The current best-performing models , in terms of WER , for the HUB-1 corpus , are the models of 
		<ref citStr="Roark ( 2001 )" id="74" label="CEPF" position="29761">
			Roark ( 2001 )
		</ref>
		 , 
		<ref citStr="Charniak ( 2001 )" id="75" label="CEPF" position="29781">
			Charniak ( 2001 )
		</ref>
		 ( applied to n-best lists by 
		<ref citStr="Hall and Johnson ( 2003 )" id="76" label="CEPN" position="29836">
			Hall and Johnson ( 2003 )
		</ref>
		 ) , and the SLM of 
		<ref citStr="Chelba and Jelinek ( 2000 )" id="77" label="CEPF" position="29883">
			Chelba and Jelinek ( 2000 )
		</ref>
		 ( applied to n-best lists by 
		<ref citStr="Xu et al . ( 2002 )" id="78" label="CEPN" position="29932">
			Xu et al . ( 2002 )
		</ref>
		 ) . 
	</s>
	

	<s id="200">
		 However , n-best list parsing , as seen in our evaluation , requires repeated analysis of common subsequences , a less efficient process than directly parsing the word lattice . 
	</s>
	

	<s id="201">
		 The reported results of 
		<ref citStr="Roark , 2001" id="79" label="CEPF" position="30173">
			( Roark , 2001 )
		</ref>
		 and 
		<ref citStr="Chelba , 2000" id="80" label="CEPF" position="30195">
			( Chelba , 2000 )
		</ref>
		 are for parsing models interpolated with the lattice trigram probabilities . 
	</s>
	

	<s id="202">
		 Hall and John- 7The WER of the hypothesis which best matches the true utterance , i.e. , the lowest WER possible given the hypotheses set . 
	</s>
	

	<s id="203">
		 Training Size Lattice/List OP WER Number of Edges ( per word ) S D I T 1M Lattice N 10.4 3.3 1.5 15.2 1788 1M List N 10.4 3.2 1.4 15.0 10211 1M Lattice Y 10.3 3.2 1.4 14.9 2855 1M List Y 10.2 3.2 1.4 14.8 16821 20M Lattice N 9.0 3.1 1.0 13.1 1735 20M List N 9.0 3.1 1.0 13.1 9999 20M Lattice Y 9.0 3.1 1.0 13.1 2801 20M List Y 9.0 3.3 0.9 13.3 16030 Table 2 : Results for parsing HUB-1 n-best word lattices and lists : OP = overparsing , S = substutitions ( % ) , D = deletions ( % ) , I = insertions ( % ) , T = total WER ( % ) . 
	</s>
	

	<s id="204">
		 Variable beam function : bˆ b log w 2 2 . 
	</s>
	

	<s id="205">
		 Training corpora : 1M = Penn Treebank sections 02-21 ; 20M = BLLIP section 1987 . 
	</s>
	

	<s id="206">
		 Model n-best List/Lattice Training Size WER ( % ) SER ( % ) Oracle ( 50-best lattice ) Lattice 7.8 
		<ref citStr="Charniak ( 2001 )" id="81" label="CEPF" position="31230">
			Charniak ( 2001 )
		</ref>
		 List 40M 11.9 
		<ref citStr="Xu ( 2002 )" id="82" label="CEPF" position="31256">
			Xu ( 2002 )
		</ref>
		 List 20M 12.3 
		<ref citStr="Roark ( 2001 )" id="83" label="CEPF" position="31285">
			Roark ( 2001 )
		</ref>
		 ( with EM ) List 2M 12.7 
		<ref citStr="Hall ( 2003 )" id="84" label="CEPF" position="31324">
			Hall ( 2003 )
		</ref>
		 Lattice 30M 13.0 
		<ref citStr="Chelba ( 2000 )" id="85" label="CEPF" position="31357">
			Chelba ( 2000 )
		</ref>
		 Lattice 20M 13.0 Current ( ^ 1 16 ^ 1 ) List 20M 13.1 71.0 Current ( ^ 1 16 ^ 1 ) Lattice 20M 13.1 70.4 
		<ref citStr="Roark ( 2001 )" id="86" label="CEPF" position="31476">
			Roark ( 2001 )
		</ref>
		 ( no EM ) List 1M 13.4 Lattice Trigram Lattice 40M 13.7 69.0 Current ( ^ 1 16 ^ 1 ) List 1M 14.8 74.3 Current ( ^ 1 16 ^ 1 ) Lattice 1M 14.9 74.0 Current ( ^ ^ 0 ) Lattice 1M 16.0 75.5 Treebank Trigram Lattice 1M 16.5 79.8 No language model Lattice 16.8 84.0 Table 3 : Comparison of WER for parsing HUB-1 words lattices with best results of other works . 
	</s>
	

	<s id="207">
		 SER = sentence error rate . 
	</s>
	

	<s id="208">
		 WER = word error rate . 
	</s>
	

	<s id="209">
		 “Speech-like” transformations were applied to all training corpora . 
	</s>
	

	<s id="210">
		 
		<ref citStr="Xu ( 2002 )" id="87" label="OEPF" position="32002">
			Xu ( 2002 )
		</ref>
		 is an implementation of the model of 
		<ref citStr="Chelba ( 2000 )" id="88" label="CEPF" position="32055">
			Chelba ( 2000 )
		</ref>
		 for n-best list parsing . 
	</s>
	

	<s id="211">
		 
		<ref citStr="Hall ( 2003 )" id="89" label="OEPF" position="32104">
			Hall ( 2003 )
		</ref>
		 is a lattice-parser related to 
		<ref citStr="Charniak ( 2001 )" id="90" label="CEPF" position="32153">
			Charniak ( 2001 )
		</ref>
		 . 
	</s>
	

	<s id="212">
		 son ( 2003 ) does not use the lattice trigram scores directly . 
	</s>
	

	<s id="213">
		 However , as in other works , the lattice trigram is used to prune the acoustic lattice to the 50 best paths . 
	</s>
	

	<s id="214">
		 The difference in WER between our parser and those of 
		<ref citStr="Charniak ( 2001 )" id="91" label="OJPF" position="32429">
			Charniak ( 2001 )
		</ref>
		 and 
		<ref citStr="Roark ( 2001 )" id="92" label="OJPF" position="32448">
			Roark ( 2001 )
		</ref>
		 applied to word lists may be due in part to the lower PARSEVAL scores of our system . 
	</s>
	

	<s id="215">
		 
		<ref citStr="Xu et al . ( 2002 )" id="93" label="CEPF" position="32563">
			Xu et al . ( 2002 )
		</ref>
		 report inverse correlation between labelled precision/recall and WER . 
	</s>
	

	<s id="216">
		 We achieve 73.2/76.5 % LP/LR on section 23 of the Penn Treebank , compared to 82.9/82.4 % LP/LR of 
		<ref citStr="Roark ( 2001 )" id="94" label="CEPF" position="32757">
			Roark ( 2001 )
		</ref>
		 and 90.1/90.1 % LP/LR of 
		<ref citStr="Charniak ( 2000 )" id="95" label="CEPF" position="32800">
			Charniak ( 2000 )
		</ref>
		 . 
	</s>
	

	<s id="217">
		 Another contributing factor to the accuracy of 
		<ref citStr="Charniak ( 2001 )" id="96" label="CEPF" position="32876">
			Charniak ( 2001 )
		</ref>
		 is the size of the training set — 20M words larger than that used in this work . 
	</s>
	

	<s id="218">
		 The low WER of 
		<ref citStr="Roark ( 2001 )" id="97" label="CEPF" position="32997">
			Roark ( 2001 )
		</ref>
		 , a top-down probabilistic parsing model , was achieved by training the model on 1 million words of the Penn Treebank , then performing a single pass of Expectation Maximization ( EM ) on a further 1.2 million words . 
	</s>
	

	<s id="219">
		 6 Conclusions In this work we present an adaptation of the parsing model of 
		<ref citStr="Collins ( 1999 )" id="98" label="CERF" position="33317">
			Collins ( 1999 )
		</ref>
		 for application to ASR . 
	</s>
	

	<s id="220">
		 The system was evaluated over two sets of data : strings and word lattices . 
	</s>
	

	<s id="221">
		 As PARSEVAL measures are not applicable to word lattices , we measured the parsing accuracy using string input . 
	</s>
	

	<s id="222">
		 The resulting scores were lower than that original implementation of the model . 
	</s>
	

	<s id="223">
		 Despite this , the model was successful as a language model for speech recognition , as measured by WER and ability to extract high-level information . 
	</s>
	

	<s id="224">
		 Here , the system performs better than a simple n-gram model trained on the same data , while simultaneously providing syntactic information in the form of parse trees . 
	</s>
	

	<s id="225">
		 WER scores are comparable to related works in this area . 
	</s>
	

	<s id="226">
		 The large size of the parameter set of this parsing model necessarily restricts the size of training data that may be used . 
	</s>
	

	<s id="227">
		 In addition , the resource requirements currently present a challenge for scaling up from the relatively sparse word lattices of the NIST HUB-1 corpus ( created in a lab setting by professional readers ) to lattices created with spontaneous speech in non-ideal conditions . 
	</s>
	

	<s id="228">
		 An investigation into the relevant importance of each parameter for the speech recognition task may allow a reduction in the size of the parameter space , with minimal loss of recognition accuracy . 
	</s>
	

	<s id="229">
		 A speedup may be achieved , and additional training data could be used . 
	</s>
	

	<s id="230">
		 Tuning of parameters using EM has lead to improved WER for other models . 
	</s>
	

	<s id="231">
		 We encourage investigation of this technique for lexicalized head-driven lattice parsing . 
	</s>
	

	<s id="232">
		 Acknowledgements This research was funded in part by the Natural Sciences and Engineering Research Council ( NSERC ) of Canada . 
	</s>
	

	<s id="233">
		 Advice on training and test data was provided by Keith Hall of Brown University . 
	</s>
	

	<s id="234">
		 References L. R. Bahl , F. Jelinek , and R. L. Mercer . 
	</s>
	

	<s id="235">
		 1983. A maximum likelihood approach to continuous speech recognition . 
	</s>
	

	<s id="236">
		 IEEE Transactions on Pattern Analysis and Machine Intelligence , 5:179–190 . 
	</s>
	

	<s id="237">
		 E. Black , S. Abney , D. Flickenger , C. Gdaniec , R. Grishman , P. Harrison , D. Hindle , R. Ingria , F. Jelinek , J. Klavans , M. Liberman , M. Marcus , S. Roukos , B. Santorini , and T. Strzalkowski . 
	</s>
	

	<s id="238">
		 1991. A procedure for quantitatively comparing the syntactic coverage of English grammars . 
	</s>
	

	<s id="239">
		 In Proceedings of Fourth DARPA Speech and Natural Language Workshop , pages 306– 311 . 
	</s>
	

	<s id="240">
		 J.-C. Chappelier and M. Rajman . 
	</s>
	

	<s id="241">
		 1998. A practical bottom-up algorithm for on-line parsing with stochas- tic context-free grammars . 
	</s>
	

	<s id="242">
		 Technical Report 98-284 , Swiss Federal Institute of Technology , July . 
	</s>
	

	<s id="243">
		 Eugene Charniak , Sharon Goldwater , and Mark Johnson . 
	</s>
	

	<s id="244">
		 1998. Edge-Based Best-First Chart Parsing . 
	</s>
	

	<s id="245">
		 In 6th Annual Workshop for Very Large Corpora , pages 127–133 . 
	</s>
	

	<s id="246">
		 Eugene Charniak , Don Blaheta , Niyu Ge , Keith Hall , John Hale , and Mark Johnson . 
	</s>
	

	<s id="247">
		 1999. BLLIP 1987-89 WSJ Corpus Release 1 . 
	</s>
	

	<s id="248">
		 Linguistic Data Consortium . 
	</s>
	

	<s id="249">
		 Eugene Charniak . 
	</s>
	

	<s id="250">
		 2000. A maximum-entropy-inspired parser . 
	</s>
	

	<s id="251">
		 In Proceedings of the 2000 Conference of the North American Chapter of the Association for Computational Linguistics , pages 132–129 , New Brunswick , U.S.A. . 
	</s>
	

	<s id="252">
		 Eugene Charniak . 
	</s>
	

	<s id="253">
		 2001. Immediate-head parsing for language models . 
	</s>
	

	<s id="254">
		 In Proceedings of the 39th Annual Meeting of the ACL . 
	</s>
	

	<s id="255">
		 Ciprian Chelba and Frederick Jelinek . 
	</s>
	

	<s id="256">
		 2000. Structured language modeling . 
	</s>
	

	<s id="257">
		 Computer Speech and Language , 14:283–332 . 
	</s>
	

	<s id="258">
		 Ciprian Chelba . 
	</s>
	

	<s id="259">
		 2000. Exploiting Syntactic Structure for Natural Language Modeling . 
	</s>
	

	<s id="260">
		 Ph.D . 
	</s>
	

	<s id="261">
		 thesis , Johns Hopkins University . 
	</s>
	

	<s id="262">
		 Christopher Collins . 
	</s>
	

	<s id="263">
		 2004. Head-Driven Probabilistic Parsingfor Word Lattices . 
	</s>
	

	<s id="264">
		 M.Sc . 
	</s>
	

	<s id="265">
		 thesis , University of Toronto . 
	</s>
	

	<s id="266">
		 Michael Collins . 
	</s>
	

	<s id="267">
		 1999. Head-Driven Statistical Models for Natural Language Parsing . 
	</s>
	

	<s id="268">
		 Ph.D . 
	</s>
	

	<s id="269">
		 thesis , University of Pennsylvania . 
	</s>
	

	<s id="270">
		 Joshua Goodman . 
	</s>
	

	<s id="271">
		 1997. Global thresholding and multiple-pass parsing . 
	</s>
	

	<s id="272">
		 In Proceedings ofthe 2nd Conference on Empirical Methods in Natural Language Processing . 
	</s>
	

	<s id="273">
		 Keith Hall and Mark Johnson . 
	</s>
	

	<s id="274">
		 2003. Language modeling using efficient best-first bottom-up parsing . 
	</s>
	

	<s id="275">
		 In Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop . 
	</s>
	

	<s id="276">
		 Frederick Jelinek . 
	</s>
	

	<s id="277">
		 1997. Information Extraction From Speech And Text . 
	</s>
	

	<s id="278">
		 MIT Press . 
	</s>
	

	<s id="279">
		 Lidia Mangu , Eric Brill , and Andreas Stolcke . 
	</s>
	

	<s id="280">
		 2000. Finding consensus in speech recognition : Word error minimization and other applications of confusion networks . 
	</s>
	

	<s id="281">
		 Computer Speech and Language , 14(4):373– 400 . 
	</s>
	

	<s id="282">
		 Hwee Tou Ng and John Zelle . 
	</s>
	

	<s id="283">
		 1997. Corpus-based approaches to semantic interpretation in natural language processing . 
	</s>
	

	<s id="284">
		 AI Magazine , 18:45–54 . 
	</s>
	

	<s id="285">
		 A. Ratnaparkhi . 
	</s>
	

	<s id="286">
		 1996. A maximum entropy model for part-of-speech tagging . 
	</s>
	

	<s id="287">
		 In Conference on Empirical Methods in Natural Language Processing , May . 
	</s>
	

	<s id="288">
		 Mosur K. Ravishankar . 
	</s>
	

	<s id="289">
		 1997. Some results on search complexity vs accuracy . 
	</s>
	

	<s id="290">
		 In DARPA Speech Recognition Workshop , pages 104–107 , February . 
	</s>
	

	<s id="291">
		 Brian Roark . 
	</s>
	

	<s id="292">
		 2001 . 
	</s>
	

	<s id="293">
		 Robust Probabilistic Predictive Syntactic Processing : Motivations , Models , and Applications . 
	</s>
	

	<s id="294">
		 Ph.D . 
	</s>
	

	<s id="295">
		 thesis , Brown University . 
	</s>
	

	<s id="296">
		 Brian Roark . 
	</s>
	

	<s id="297">
		 2002. Markov parsing : Lattice rescoring with a statistical parser . 
	</s>
	

	<s id="298">
		 In Proceedings of the 40th Annual Meeting of the ACL , pages 287–294 . 
	</s>
	

	<s id="299">
		 Ann Taylor , Mitchell Marcus , and Beatrice Santorini , 2003 . 
	</s>
	

	<s id="300">
		 The Penn TreeBank : An Overview , chapter 1. Kluwer , Dordrecht , The Netherlands . 
	</s>
	

	<s id="301">
		 Hans Weber , J¨org Spilker , and G¨unther G¨orz . 
	</s>
	

	<s id="302">
		 1997. Parsing n best trees from a word lattice . 
	</s>
	

	<s id="303">
		 Kunstliche Intelligenz , pages 279–288 . 
	</s>
	

	<s id="304">
		 Peng Xu , Ciprian Chelba , and Frederick Jelinek . 
	</s>
	

	<s id="305">
		 2002. A study on richer syntactic dependencies in structured language modeling . 
	</s>
	

	<s id="306">
		 In Proceedings of the 40th Annual Meeting of the ACL , pages 191–198 . 
	</s>
	


</acldoc>
