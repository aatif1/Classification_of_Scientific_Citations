<?xml version="1.0" encoding="iso-8859-1"?>
<acldoc acl_id="P04-1002">
	

	<s id="1">
		 Constructivist Development of Grounded Construction Grammars Luc Steels University of Brussels ( VUB AI Lab ) SONY Computer Science Lab - Paris 6 Rue Amyot , 75005 Paris steels@arti.vub.ac.be Abstract The paper reports on progress in building computational models of a constructivist approach to language development . 
	</s>
	

	<s id="2">
		 It introduces a formalism for construction grammars and learning strategies based on invention , abduction , and induction . 
	</s>
	

	<s id="3">
		 Examples are drawn from experiments exercising the model in situated language games played by embodied artificial agents . 
	</s>
	

	<s id="4">
		 1 Introduction The constructivist approach to language learning proposes that ”children acquire linguistic competence ( ... ) only gradually , beginning with more concrete linguistic structures based on particular words and morphemes , and then building up to more abstract and productive structures based on various types of linguistic categories , schemas , and constructions.” 
		<ref citStr="TomaselloBrooks , 1999" id="1" label="CEPF" position="1007">
			( TomaselloBrooks , 1999 )
		</ref>
		 , p. 161 . 
	</s>
	

	<s id="5">
		 The approach furthermore assumes that language development is ( i ) grounded in cognition because prior to ( or in a co-development with language ) there is an understanding and conceptualisation of scenes in terms of events , objects , roles that objects play in events , and perspectives on the event , and ( ii ) grounded in communication because language learning is intimately embedded in interactions with specific communicative goals . 
	</s>
	

	<s id="6">
		 In contrast to the nativist position , defended , for example , by Pinker 
		<ref citStr="Pinker , 1998" id="2" label="CJPF" position="1571">
			( Pinker , 1998 )
		</ref>
		 , the constructivist approach does not assume that the semantic and syntactic categories as well as the linking rules ( specifying for example that the agent of an action is linked to the subject of a sentence ) are universal and innate . 
	</s>
	

	<s id="7">
		 Rather , semantic and syntactic categories as well as the way they are linked is built up in a gradual developmental process , starting from quite specific ‘verb-island constructions’ . 
	</s>
	

	<s id="8">
		 Although the constructivist approach appears to explain a lot of the known empirical data about child language acquisition , there is so far no worked out model that details how constructivist language development works concretely , i.e. what kind of computational mechanisms are implied and how they work together to achieve adult ( or even child ) level competence . 
	</s>
	

	<s id="9">
		 Moreover only little work has been done so far to build computational models for handling the sort of ’construction grammars’ assumed by this approach . 
	</s>
	

	<s id="10">
		 Both challenges inform the research discussed in this paper . 
	</s>
	

	<s id="11">
		 2 Abductive Learning In the constructivist literature , there is often the implicit assumption that grammatical development is the result of observational learning , and several research efforts are going on to operationalise this approach for acquiring grounded lexicons and grammars ( see e.g. 
		<ref citStr="Roy , 2001" id="3" label="CEPF" position="2940">
			( Roy , 2001 )
		</ref>
		 ) . 
	</s>
	

	<s id="12">
		 The agents are given pairs with a real world situation , as perceived by the sensori-motor apparatus , and a language utterance . 
	</s>
	

	<s id="13">
		 For example , an image of a ball is shown and at the same time a stretch of speech containing the word “ball” . 
	</s>
	

	<s id="14">
		 Based on a generalisation process that uses statistical pattern recognition algorithms or neural networks , the learner then gradually extracts what is common between the various situations in which the same word or construction is used , thus progressively building a grounded lexicon and grammar of a language . 
	</s>
	

	<s id="15">
		 The observational learning approach has had some success in learning words for objects and acquiring simple grammatical constructions , but there seem to be two inherent limitations . 
	</s>
	

	<s id="16">
		 First , there is the well known poverty of the stimulus argument , widely accepted in linguistics , which says that there is not enough data in the sentences normally available to the language learner to arrive at realistic lexicons and grammars , let alone learn at the same time the categorisations and conceptualisations of the world implied by the language . 
	</s>
	

	<s id="17">
		 This has lead many linguists to adopt the nativist position mentioned earlier . 
	</s>
	

	<s id="18">
		 The nativist position could in principle be integrated in an observational learning framework by introducing strong biases on the generalisation process , incorporating the constraints of universal grammar , but it has been difficult to identify and operationalise enough of these constraints to do concrete experiments in realistic settings . 
	</s>
	

	<s id="19">
		 Second , observational learning assumes that the language system ( lexicon and grammar ) exists as a fixed static system . 
	</s>
	

	<s id="20">
		 However , observations of language in use shows that language users constantly align their language conventions to suit the purposes of specific conversations 
		<ref citStr="ClarkBrennan , 1991" id="4" label="CEPF" position="4860">
			( ClarkBrennan , 1991 )
		</ref>
		 . 
	</s>
	

	<s id="21">
		 Natural languages therefore appear more to be like complex adaptive systems , similar to living systems that constantly adapt and evolve . 
	</s>
	

	<s id="22">
		 This makes it difficult to rely exclusively on statistical generalisation . 
	</s>
	

	<s id="23">
		 It does not capture the inherently creative nature of language use . 
	</s>
	

	<s id="24">
		 This paper explores an alternative approach , which assumes a much more active stance from language users based on the Peircian notion of abduction 
		<ref citStr="Fann , 1970" id="5" label="CEPF" position="5346">
			( Fann , 1970 )
		</ref>
		 . 
	</s>
	

	<s id="25">
		 The speaker first attempts to use constructions from his existing inventory to express whatever he wants to express . 
	</s>
	

	<s id="26">
		 However when that fails or is judged unsatisfactory , the speaker may extend his existing repertoire by inventing new constructions . 
	</s>
	

	<s id="27">
		 These new constructions should be such that there is a high chance that the hearer may be able to guess their meaning . 
	</s>
	

	<s id="28">
		 The hearer also uses as much as possible constructions stored in his own inventory to make sense of what is being said . 
	</s>
	

	<s id="29">
		 But when there are unknown constructions , or the meanings do not fit with the situation being talked about , the hearer makes an educated guess about what the meaning of the unknown language constructions could be , and adds them as new hypotheses to his own inventory . 
	</s>
	

	<s id="30">
		 Abductive constructivist learning hence relies crucially on the fact that both agents have sufficient common ground , share the same situation , have established joint attention , and share communicative goals . 
	</s>
	

	<s id="31">
		 Both speaker and hearer use themselves as models of the other in order to guess how the other one will interpret a sentence or why the speaker says things in a particular way . 
	</s>
	

	<s id="32">
		 Because both speaker and hearer are taking risks making abductive leaps , a third activity is needed , namely induction , not in the sense of statistical generalisation as in observational learning but in the sense of Peirce 
		<ref citStr="Fann , 1970" id="6" label="CEPF" position="6815">
			( Fann , 1970 )
		</ref>
		 : A hypothesis arrived at by making educated guesses is tested against further data coming from subsequent interactions . 
	</s>
	

	<s id="33">
		 When a construction leads to a successful interaction , there is some evidence that this construction is ( or could become ) part of the set of conventions adopted by the group , and language users should therefore prefer it in the future . 
	</s>
	

	<s id="34">
		 When the construction fails , the language user should avoid it if alternatives are available . 
	</s>
	

	<s id="35">
		 Implementing these visions of language learning and use is obviously an enormous challenge for computational linguistics . 
	</s>
	

	<s id="36">
		 It requires not only cognitive and communicative grounding , but also grammar formalisms and associated parsing and production algorithms which are extremely flexible , both from the viewpoint of getting as far as possible in the interpretation or production process despite missing rules or incompatibilities in the inventories of speaker and hearer , and from the viewpoint of supporting continuous change . 
	</s>
	

	<s id="37">
		 3 Language Games The research reported here uses a methodological approach which is quite common in Artificial Life research but still relatively novel in ( computational ) linguistics : Rather than attempting to develop simulations that generate natural phenomena directly , as one does when using Newton’s equations to simulate the trajectory of a ball falling from a tower , we engage in computational simulations and robotic experiments that create ( new ) artificial phenomena that have some of the characteristics of natural phenomena and hence are seen as explaining them . 
	</s>
	

	<s id="38">
		 Specifically , we implement artificial agents with components modeling certain cognitive operations ( such as introducing a new syntactic category , computing an analogy between two events , etc. ) , and then see what language phenomena result if these agents exercise these components in embodied situated language games . 
	</s>
	

	<s id="39">
		 This way we can investigate very precisely what causal factors may underly certain phenomena and can focus on certain aspects of ( grounded ) language use without having to face the vast full complexity of real human languages . 
	</s>
	

	<s id="40">
		 A survey of work which follows a similar methodology is found in 
		<ref citStr="CangelosiParisi , 2003" id="7" label="CERF" position="9106">
			( CangelosiParisi , 2003 )
		</ref>
		 . 
	</s>
	

	<s id="41">
		 The artificial agents used in the experiments driving our research observe real-world scenes through their cameras . 
	</s>
	

	<s id="42">
		 The scenes consist of interactions between puppets , as shown in figure 1 . 
	</s>
	

	<s id="43">
		 These scenes enact common events like movement of people and objects , actions such as push or pull , give or take , etc. . 
	</s>
	

	<s id="44">
		 In order to achieve the cognitive grounding assumed in constructivist language learning , the scenes are processed by a battery of relatively standard machine vision algorithms that segment objects based on color and movement , track objects in real-time , and compute a stream of low- level features indicating which objects are touching , in which direction objects are moving , etc. . 
	</s>
	

	<s id="45">
		 These low-level features are input to an event- recognition system that uses an inventory of hierarchical event structures and matches them against the data streaming in from low-level vision , similar to the systems described in 
		<ref citStr="SteelsBaillie , 2003" id="8" label="CERF" position="10113">
			( SteelsBaillie , 2003 )
		</ref>
		 . 
	</s>
	

	<s id="46">
		 Figure 1 : Scene enacted with puppets so that typical interactions between humans involving agency can be perceived and described . 
	</s>
	

	<s id="47">
		 In order to achieve the communicative grounding required for constructivist learning , agents go through scripts in which they play various language games , similar to the setups described in 
		<ref citStr="Steels , 2003" id="9" label="CERF" position="10475">
			( Steels , 2003 )
		</ref>
		 . 
	</s>
	

	<s id="48">
		 These language games are deliberately quite similar to the kind of scenes and interactions used in a lot of child language research . 
	</s>
	

	<s id="49">
		 A language game is a routinised interaction between two agents about a shared situation in the world that involves the exchange of symbols . 
	</s>
	

	<s id="50">
		 Agents take turns playing the role of speaker and hearer and give each other feedback about the outcome of the game . 
	</s>
	

	<s id="51">
		 In the game further used in this paper , one agent describes to another agent an event that happened in the most recently experienced scene . 
	</s>
	

	<s id="52">
		 The game succeeds if the hearer agrees that the event being described occurred in the recent scene . 
	</s>
	

	<s id="53">
		 4 The Lexicon Visual processing and event recognition results in a world model in the form of a series of facts describing the scene . 
	</s>
	

	<s id="54">
		 To play the description game , the speaker selects one event as the topic and then seeks a series of facts which discriminate this event and its objects against the other events and objects in the context . 
	</s>
	

	<s id="55">
		 We use a standard predicate calculus-style representation for meanings . 
	</s>
	

	<s id="56">
		 A semantic structure consists of a set of units where each unit has a referent , which is the object or event to which the unit draws attention , and a meaning , which is a set of clauses constraining the referent . 
	</s>
	

	<s id="57">
		 A semantic structure with one unit is for example written down as follows : [1]unit1 ev1 fall(ev1,true) , fall- 1(ev1,obj 1 ) , ball(obj 1 ) where unit1 is the unit , ev1 the referent , and fall(ev1 , true ) , fall- 1(ev1,obj1) , ball(obj1) the meaning . 
	</s>
	

	<s id="58">
		 The different arguments of an event are decomposed into different predicates . 
	</s>
	

	<s id="59">
		 For example , for “John gives a book to Mary” , there would be four clauses : give(ev1,true) for the event itself , give-1(ev1 , John ) , for the one who gives , give-2(ev1,book1) , for the object given , and give-3(ev1,Mary) , for the recipient . 
	</s>
	

	<s id="60">
		 This representation is more flexible and makes it possible to add new components ( like the manner of an event ) at any time . 
	</s>
	

	<s id="61">
		 Syntactic structures mirror semantic structures . 
	</s>
	

	<s id="62">
		 They also consist of units and the name of units are shared with semantic structures so that cross- reference between them is straightforward . 
	</s>
	

	<s id="63">
		 The form aspects of the sentence are represented in a declarative predicate calculus style , using the units as arguments . 
	</s>
	

	<s id="64">
		 For example , the following unit is constrained as introducing the string “fall” : [ 2 ] unit1 string(unit1 , “fall” ) The rule formalism we have developed uses ideas from several existing formalisms , particularly unification grammars and is most similar to the Embodied Construction Grammars proposed in 
		<ref citStr="BergenChang , 2003" id="10" label="CERF" position="13259">
			( BergenChang , 2003 )
		</ref>
		 . 
	</s>
	

	<s id="65">
		 Lexical rules link parts of semantic structure with parts of syntactic structure . 
	</s>
	

	<s id="66">
		 All rules are reversable . 
	</s>
	

	<s id="67">
		 When producing , the left side of a rule is matched against the semantic structure and , if there is a match , the right side is unified with the syntactic structure . 
	</s>
	

	<s id="68">
		 Conversely when parsing , the right side is matched against the syntactic structure and the left side unified with the semantic structure . 
	</s>
	

	<s id="69">
		 Here is a lexical entry for the word ”fall” . 
	</s>
	

	<s id="70">
		 [ 3 ] ?unit ?ev fall(?ev,?state) , fall- 1(?ev,?obj) ?unit string(?unit,“fall”) It specifies that a unit whose meaning is fall(?ev,?state) , fall- 1(?ev,?obj) is expressed with the string “fall” . 
	</s>
	

	<s id="71">
		 Variables are written down with a question mark in front . 
	</s>
	

	<s id="72">
		 Their scope is restricted to the structure or rule in which they appear and rule application often implies the renaming of certain variables to take care of the scope constraints . 
	</s>
	

	<s id="73">
		 Here is a lexical entry for “ball” : [ 4 ] ?unit ?obj ball(?obj) ?unit string(?unit,“ball”) Lexicon lookup attempts to find the minimal set of rules that covers the total semantic structure . 
	</s>
	

	<s id="74">
		 New units may get introduced ( both in the syntactic and semantic structure ) if the meaning of a unit is broken down in the lexicon into more than one word . 
	</s>
	

	<s id="75">
		 Thus , the original semantic structure in [ 1 ] results after the application of the two rules [ 3 ] and [ 4 ] in the following syntactic and semantic structures : [5]unit1 ev1 fall(ev1,true) , fall- 1(ev1,obj1) unit2 obj 1 ball(obj 1 ) —– unit1 string(unit1 , “fall” ) unit2 string(unit2 , “ball” ) If this syntactic structure is rendered , it produces the utterance “fall ball” . 
	</s>
	

	<s id="76">
		 No syntax is implied yet . 
	</s>
	

	<s id="77">
		 In the reverse direction , the parser starts with the two units forming the syntactic structure in [ 5 ] and application of the rules produces the following semantic structure : [ 6 ] unit1 ?ev fall(?ev,?state) , fall- 1(?ev,?obj) unit2 ?obj 1 ball(?obj 1 ) The semantic structure in [ 6 ] now contains variables for the referent of each unit and for the various predicate-arguments in their meanings . 
	</s>
	

	<s id="78">
		 The interpretation process matches these variables against the facts in the world model . 
	</s>
	

	<s id="79">
		 If a single consistent series of bindings can be found , then interpretation is successful . 
	</s>
	

	<s id="80">
		 For example , assume that the facts in the meaning part of [ 1 ] are in the world model then matching [ 6 ] against them results in the bindings : [ 7 ] ?ev/ev1 , ?state/true , ?obj/obj 1 , ?obj 1/obj 1 When the same word or the same meaning is covered by more than one rule , a choice needs to be made . 
	</s>
	

	<s id="81">
		 Competing rules may develop if an agent invented a new word for a particular meaning but is later confronted with another word used by somebody else for the same meaning . 
	</s>
	

	<s id="82">
		 Every rule has a score and in production and parsing , rules with the highest score are preferred . 
	</s>
	

	<s id="83">
		 When the speaker performs lexicon lookup and rules were found to cover the complete semantic structure , no new rules are needed . 
	</s>
	

	<s id="84">
		 But when some part is uncovered , the speaker should create a new rule . 
	</s>
	

	<s id="85">
		 We have experimented so far with a simple strategy where agents lump together the uncovered facts in a unit and create a brand new word , consisting of a randomly chosen configuration of syllables . 
	</s>
	

	<s id="86">
		 For example , if no word for ball(obj1) exists yet to cover the semantic structure in [ 1 ] , a new rule such as [ 4 ] can be constructed by the speaker and subsequently used . 
	</s>
	

	<s id="87">
		 If there is no word at all for the whole semantic structure in [ 1 ] , a single word covering the whole meaning will be created , giving the effect of holophrases . 
	</s>
	

	<s id="88">
		 The hearer first attempts to parse as far as possible the given sentence , and then interprets the resulting semantic structure , possibly using joint attention or other means that may help to find the intended interpretation . 
	</s>
	

	<s id="89">
		 If this results in a unique set of bindings , the language game is deemed successful . 
	</s>
	

	<s id="90">
		 But if there were parts of the sentence which were not covered by any rule , then the hearer can use abductive learning . 
	</s>
	

	<s id="91">
		 The first critical step is to guess as well as possible the meaning of the unknown word(s) . 
	</s>
	

	<s id="92">
		 Thus suppose the sentence is “fall ball” , resulting in the semantic structure : [ 8 ] unit1 ?ev fall(?ev,?state) , fall- 1(?ev,?obj) If this structure is matched , bindings for ?ev and ?obj are found . 
	</s>
	

	<s id="93">
		 The agent can now try to find the possible meaning of the unknown word “ball” . 
	</s>
	

	<s id="94">
		 He can assume that this meaning must somehow help in the interpretation process . 
	</s>
	

	<s id="95">
		 He therefore conceptualises the same way as if he would be the speaker and constructs a distinctive description that draws attention to the event in question , for example by constraining the referent of ?obj with an additional predicate . 
	</s>
	

	<s id="96">
		 Although there are usually several ways in which obj 1 differs from other objects in the context . 
	</s>
	

	<s id="97">
		 There is a considerable chance that the predicate ball is chosen and hence ball(?obj) is abductively inferred as the meaning of “ball” resulting in a rule like [ 4 ] . 
	</s>
	

	<s id="98">
		 Agents use induction to test whether the rules they created by invention and abduction have been adopted by the group . 
	</s>
	

	<s id="99">
		 Every rule has a score , which is local to each agent . 
	</s>
	

	<s id="100">
		 When the speaker or hearer has success with a particular rule , its score is increased and the score of competing rules is decreased , thus implementing lateral inhibition . 
	</s>
	

	<s id="101">
		 When there is a failure , the score of the rule that was used is decreased . 
	</s>
	

	<s id="102">
		 Because the agents prefer rules with the highest score , there is a positive feedback in the system . 
	</s>
	

	<s id="103">
		 The more a word is used for a particular meaning , the more success that word will have . 
	</s>
	

	<s id="104">
		 Figure 2 : Winner-take-all effect in words competing for same meaning . 
	</s>
	

	<s id="105">
		 The x-axis plots language games and the y-axis the use frequency . 
	</s>
	

	<s id="106">
		 Scores rise in all the agents for these words and so progressively we see a winner-take-all effect with one word dominating for the expression of a particular meaning ( see figure 2 ) . 
	</s>
	

	<s id="107">
		 Many experiments have by now been performed showing that this kind of lateral inhibition dynamics allows a population of agents to negotiate a shared inventory of form- meaning pairs for content words 
		<ref citStr="Steels , 2003" id="11" label="CEPF" position="19806">
			( Steels , 2003 )
		</ref>
		 . 
	</s>
	

	<s id="108">
		 5 Syntactisation The reader may have noticed that the semantic structure in [ 6 ] resulting from parsing the sentence “fall ball” , includes two variables which will both get bound to the same object , namely ?obj , introduced by the predicate fall- 1(?ev,?obj) , and ?obj 1 , introduced by the predicate ball(?obj1) . 
	</s>
	

	<s id="109">
		 We say that in this case ?obj and ?obj 1 form an equality . 
	</s>
	

	<s id="110">
		 Just from parsing the two words , the hearer cannot know that the object involved in the fall event is the same as the object introduced by ball . 
	</s>
	

	<s id="111">
		 He can only figure this out when looking at the scene ( i.e. the world model ) . 
	</s>
	

	<s id="112">
		 In fact , if there are several balls in the scene and only one of them is falling , there is no way to know which object is intended . 
	</s>
	

	<s id="113">
		 And even if the hearer can figure it out , it is still desirable that the speaker should provide extra-information about equalities to optimise the hearer’s interpretation efforts . 
	</s>
	

	<s id="114">
		 A major thesis of the present paper is that resolving equivalences between variables is the main motor for the introduction of syntax . 
	</s>
	

	<s id="115">
		 To achieve it , the agents could , as a first approximation , use rules like the following one , to be applied after all lexical rules have been applied : [ 9 ] ?unit1 ?ev1 fall- 1(?ev1,?obj2) ?unit2 ?obj2 ball(?obj2) ?unit2 string(?unit2 , ”ball” ) This rule is formally equivalent to the lexical rules discussed earlier in the sense that it links parts of a semantic structure with parts of a syntactic structure . 
	</s>
	

	<s id="116">
		 But now more than one unit is involved . 
	</s>
	

	<s id="117">
		 Rule [ 9 ] will do the job , because when unifying its right side with the semantic structure ( in parsing ) ?obj2 unifies with the variables ?obj ( supplied by ”fall” ) and ?obj 1 ( supplied by ”ball” ) and this forces them to be equivalent . 
	</s>
	

	<s id="118">
		 Note that ?unit1 in [ 9 ] only contains those parts of the original meaning that involve the variables which need to be made equal . 
	</s>
	

	<s id="119">
		 The above rule works but is completely specific to this case . 
	</s>
	

	<s id="120">
		 It is an example of the ad hoc ‘verb-island’ constructions reported in an early stage of child language development . 
	</s>
	

	<s id="121">
		 Obviously it is much more desirable to have a more general rule , which can be achieved by introducing syntactic and semantic categories . 
	</s>
	

	<s id="122">
		 A semantic category ( such as agent , perfective , countable , male ) is a categorisation of a conceptual relation , which is used to constrain the semantic side of grammatical rules . 
	</s>
	

	<s id="123">
		 A syntactic category ( such as noun , verb , nominative ) is a categorisation of a word or a group of words , which can be used to constrain the syntactic side of grammatical rules . 
	</s>
	

	<s id="124">
		 A rule using categories can be formed by taking rule [ 9 ] above and turning all predicates or content words into semantic or syntactic categories . 
	</s>
	

	<s id="125">
		 [ 10 ] ?unit1 ?ev1 semcat1(?ev1,?obj2) ?unit2 ?obj2 semcat2(?obj2) ?unit1 syncat1 ( ?unit1 ) ?unit2 syncat2(?unit2) The agent then needs to create sem-rules to categorise a predicate as belonging to a semantic category , as in : [ 11 ] ?unit1 ?ev1 fall- 1(?ev1,?obj2) ?unit1 ?ev1 semcat1(?ev1,?obj1) and syn-rules to categorise a word as belonging to a syntactic category , as in : [ 12 ] ?unit1 string(?unit1,”fall”) ?unit1 ?ev1 syncat1(?unit1) These rules have arrows going only in one direction because they are only applied in one way . 
	</s>
	

	<s id="126">
		 ' During production , the sem-rules are applied first , then the lexical rules , next the syn-rules and then the gram- ' Actually if word morphology is integrated , syn-rules need to be bi-directional , but this topic is not discussed further here due to space limitations . 
	</s>
	

	<s id="127">
		 ?unit1 string(?unit1 , ”fall” ) matical rules . 
	</s>
	

	<s id="128">
		 In parsing , the lexical rules are applied first ( in reverse direction ) , then the syn-rules and the sem-rules , and only then the grammatical rules ( in reverse direction ) . 
	</s>
	

	<s id="129">
		 The complete syntactic and semantic structures for example [ 9 ] look as follows : [ 13 ] unit1 ?ev1 fall(?ev1,?state) , fall- 1(?ev1,?obj) , semcat1 ( ?ev 1,?obj ) unit2 ?obj 1 ball(?obj 1 ) , semcat2(?obj 1 ) —– unit1 string(unit1 , “fall” ) , syncat-1 ( unit 1 ) unit2 string(unit2 , “ball” ) , syncat-2(unit2) The right side of rule [ 10 ] matches with this syntactic structure , and if the left side of rule [ 10 ] is unified with the semantic structure in [ 13 ] the variable ?obj2 unifies with ?obj and ?obj 1 , thus resolving the equality before semantic interpretation ( matching against the world model ) starts . 
	</s>
	

	<s id="130">
		 How can language users develop such rules ? 
	</s>
	

	<s id="131">
		 The speaker can detect equalities that need to be resolved by re-entrance : Before rendering a sentence and communicating it to the hearer , the speaker re- parses his own sentence and interprets it against the facts in his own world model . 
	</s>
	

	<s id="132">
		 If the resulting set of bindings contains variables that are bound to the same object after interpretation , then these equalities are candidates for the construction of a rule and new syntactic and semantic categories are made as a side effect . 
	</s>
	

	<s id="133">
		 Note how the speaker uses himself as a model of the hearer and fixes problems that the hearer might otherwise encounter . 
	</s>
	

	<s id="134">
		 The hearer can detect equalities by first interpreting the sentence based on the constructions that are already part of his own inventory and the shared situation and prior joint attention . 
	</s>
	

	<s id="135">
		 These equalities are candidates for new rules to be constructed by the hearer , and they again involve the introduction of syntactic and semantic categories . 
	</s>
	

	<s id="136">
		 Note that syntactic and semantic categories are always local to an agent . 
	</s>
	

	<s id="137">
		 The same lateral inhibition dynamics is used for grammatical rules as for lexical rules , and so is also a positive feedback loop leading to a winner-take-all effect for grammatical rules . 
	</s>
	

	<s id="138">
		 6 Hierarchy Natural languages heavily use categories to tighten rule application , but they also introduce additional syntactic markings , such as word order , function words , affixes , morphological variation of word forms , and stress or intonation patterns . 
	</s>
	

	<s id="139">
		 These markings are often used to signal to which category certain words belong . 
	</s>
	

	<s id="140">
		 They can be easily incorporated in the formalism developed so far by adding additional descriptors of the units in the syntactic structure . 
	</s>
	

	<s id="141">
		 For example , rule [ 10 ] can be expanded with word order constraints and the introduction of a particle “ba” : [ 14 ] ?unit1 ?ev1 semcat1(?ev1,?obj2) ?unit2 ?obj2 semcat2(?obj2) ?unit1 syncat1 ( ?unit1 ) ?unit2 syncat2(?unit2) ?unit3 string ( ?unit3 , “ba” ) ?unit4 syn-subunits ( ?unit1 , ?unit2 , ?unit3 ) , preceeds(?unit2 , ?unit3 ) Note that it was necessary to introduce a superunit ?unit4 in order to express the word order constraints between the ba-particle and the unit that introduces the object . 
	</s>
	

	<s id="142">
		 Applying this rule as well as the synrules and sem-rules discussed earlier to the semantic structure in [ 5 ] yields : [ 13 ] unit1 ev1 fall(ev1,true) , fall- 1(ev1,obj) , semcat1(ev1,obj) unit2 obj 1 ball(obj 1 ) , semcat2(obj 1 ) —– unit1 string(unit1,“fall”) , syncat-1(unit1) unit2 string(unit2 , “ball” ) , syncat-2(unit2) unit3 string(unit3 , “ba” ) unit4 syn-subunits( unit1,unit2,unit3 ) , preceeds ( unit2,unit3 ) When this syntactic structure is rendered , it produces ”fall ball ba” , or equivalently ”ball ba fall” , because only the order between “ball” and “ba” is constrained . 
	</s>
	

	<s id="143">
		 Obviously the introduction of additional syntactic features makes the learning of grammatical rules more difficult . 
	</s>
	

	<s id="144">
		 Natural languages appear to have meta-level strategies for invention and abduction . 
	</s>
	

	<s id="145">
		 For example , a language ( like Japanese ) tends to use particles for expressing the roles of objects in events and this usage is a strategy both for inventing the expression of a new relation and for guessing what the use of an unknown word in the sentence might be . 
	</s>
	

	<s id="146">
		 Another language ( like Swahili ) uses morphological variations similar to Latin for the same purpose and thus has ended up with a rich set of affixes . 
	</s>
	

	<s id="147">
		 In our experiments so far , we have implemented such strategies directly , so that invention and abduction is strongly constrained . 
	</s>
	

	<s id="148">
		 We still need to work out a formalism for describing these strategies as meta- rules and research the associated learning mechanisms . 
	</s>
	

	<s id="149">
		 Figure 3 : The graph shows the dependency structure as well as the phrase-structure emerging through the application of multiple rules When the same word participates in several rules , we automatically get the emergence of hierarchical structures . 
	</s>
	

	<s id="150">
		 For example , suppose that two predicates are used to draw attention to obj 1 in [ 5 ] : ball and red . 
	</s>
	

	<s id="151">
		 If the lexicon has two separate words for each predicate , then the initial semantic structure would introduce different variables so that the meaning after parsing ”fall ball ba red” would be : [ 15 ] fall(?ev,?state) , fall-1(?ev,?obj) , ball ( ?obj ) , red(?obj2) To resolve the equality between ?obj and ?obj2 , the speaker could create the following rule : [ 14 ] ?unit1 ?obj semcat3(?obj) ?unit2 ?obj semcat4(?obj) ?unit1 syncat3(?unit1) ?unit2 syncat4(?unit2) ?unit3 syn-subunits ( unit1,unit2 ) , pre- ceeds(unit1,unit2) The predicate ball is declared to belong to semcat4 and the word “ball” to syncat4 . 
	</s>
	

	<s id="152">
		 The predicate red belongs to semcat3 and the word “red” to syncat3 . 
	</s>
	

	<s id="153">
		 Rendering the syntactic structure after application of this rule gives the sentence ”fall red ball ba” . 
	</s>
	

	<s id="154">
		 A hierarchical structure ( figure 3 ) emerges because “ball” participates in two rules . 
	</s>
	

	<s id="155">
		 7 Re-use Agents obviously should not invent new conventions from scratch every time they need one , but rather use as much as possible existing categorisations and hence existing rules . 
	</s>
	

	<s id="156">
		 This simple economy principle quickly leads to the kind of syntagmatic and paradigmatic regularities that one finds in natural grammars . 
	</s>
	

	<s id="157">
		 For example , if the speaker wants to express that a block is falling , no new semantic or syntactic categories or linking rules are needed but block can simply be declared to belong to semcat4 and “block” to syncat3 and rule [ 14 ] applies . 
	</s>
	

	<s id="158">
		 Re-use should be driven by analogy . 
	</s>
	

	<s id="159">
		 In one of the largest experiments we have carried out so far , agents had a way to compute the similarity between two event-structures by pairing the primitive operations making up an event . 
	</s>
	

	<s id="160">
		 For example , a pick-up action is decomposed into : an object moving into the direction of another stationary object , the first object then touching the second object , and next the two objects moving together in ( roughly ) the opposite direction . 
	</s>
	

	<s id="161">
		 A put-down action has similar sub- events , except that their ordering is different . 
	</s>
	

	<s id="162">
		 The roles of the objects involved ( the hand , the object being picked up ) are identical and so their grammatical marking could be re-used with very low risk of being misunderstood . 
	</s>
	

	<s id="163">
		 When a speaker reuses a grammatical marking for a particular semantic category , this gives a strong hint to the hearer what kind of analogy is expected . 
	</s>
	

	<s id="164">
		 By using these invention and abduction strategies , semantic categories like agent or patient gradually emerged in the artificial grammars . 
	</s>
	

	<s id="165">
		 Figure 4 visualises the result of this experiment ( after 700 games between 2 agents taking turns ) . 
	</s>
	

	<s id="166">
		 The x-axis ( randomly ) ranks the different predicate-argument relations , the y-axis their markers . 
	</s>
	

	<s id="167">
		 Without re-use , every argument would have its own marker . 
	</s>
	

	<s id="168">
		 Now several markers ( such as “va” or “zu” ) cover more than one relation . 
	</s>
	

	<s id="169">
		 Figure 4 : More compact grammars result from reuse based on semantic analogies . 
	</s>
	

	<s id="170">
		 8 Conclusions The paper reports significant steps towards the computational modeling of a constructivist approach to language development . 
	</s>
	

	<s id="171">
		 It has introduced aspects of a construction grammar formalism that is designed to handle the flexibility required for emergent developing grammars . 
	</s>
	

	<s id="172">
		 It also proposed that invention , abduction , and induction are necessary and sufficient for language learning . 
	</s>
	

	<s id="173">
		 Much more technical work remains to be done but already significant experimental results have been obtained with embod- ied agents playing situated language games . 
	</s>
	

	<s id="174">
		 Most of the open questions concern under what circumstances syntactic and semantic categories should be re-used . 
	</s>
	

	<s id="175">
		 Research funded by Sony CSL with additional funding from ESF-OMLL program , EU FET-ECAgents and CNRS OHLL . 
	</s>
	

	<s id="176">
		 References Bergen , B.K. and N.C. Chang . 
	</s>
	

	<s id="177">
		 2003. Embodied Construction Grammar in Simulation-Based Language Understanding . 
	</s>
	

	<s id="178">
		 TR 02-004 , ICSI , Berkeley . 
	</s>
	

	<s id="179">
		 Cangelosi , and D. Parisi 2003 . 
	</s>
	

	<s id="180">
		 Simulating the Evo- lution of Language . 
	</s>
	

	<s id="181">
		 Springer-Verlag , Berlin . 
	</s>
	

	<s id="182">
		 Clark , H. and S. Brennan 1991 . 
	</s>
	

	<s id="183">
		 Grounding in communication . 
	</s>
	

	<s id="184">
		 In : Resnick , L. J. Levine and S. Teasley ( eds . 
	</s>
	

	<s id="185">
		 ) Perspectives on Socially Shared Cognition . 
	</s>
	

	<s id="186">
		 APA Books , Washington . 
	</s>
	

	<s id="187">
		 p. 127-149 . 
	</s>
	

	<s id="188">
		 Fann , K.T. 1970 . 
	</s>
	

	<s id="189">
		 Peirce’s Theory of Abduction Martinus Nijhoff , The Hague . 
	</s>
	

	<s id="190">
		 Roy , D. 2001 . 
	</s>
	

	<s id="191">
		 Learning Visually Grounded Words and Syntax of Natural Spoken Language . 
	</s>
	

	<s id="192">
		 Evolution of communication 4(1). Pinker , S. 1998 . 
	</s>
	

	<s id="193">
		 Learnability and Cognition : The acquisition of Argument Structure . 
	</s>
	

	<s id="194">
		 The MIT Press , Cambridge Ma . 
	</s>
	

	<s id="195">
		 Steels , L. 2003 Evolving grounded communication for robots . 
	</s>
	

	<s id="196">
		 Trends in Cognitive Science . 
	</s>
	

	<s id="197">
		 Volume 7 , Issue 7 , July 2003 , pp. 308-312 . 
	</s>
	

	<s id="198">
		 Steels , L. and J-C. Baillie 2003 . 
	</s>
	

	<s id="199">
		 Shared Grounding ofEvent Descriptions by Autonomous Robots . 
	</s>
	

	<s id="200">
		 Journal of Robotics and Autonomous Systems 43 , 2003 , pp. 163-173 . 
	</s>
	

	<s id="201">
		 Tomasello , M. and P.J. Brooks 1999 . 
	</s>
	

	<s id="202">
		 Early syntactic development : A Construction Grammar approach In : Barrett , M. ( ed . 
	</s>
	

	<s id="203">
		 ) ( 1999 ) The Development of Language Psychology Press , London . 
	</s>
	

	<s id="204">
		 pp. 161-190 . 
	</s>
	


</acldoc>
