<?xml version="1.0" encoding="iso-8859-1"?>
<acldoc acl_id="P04-1047">
	

	<s id="1">
		 Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II Treebank Ruth O’Donovan , Michael Burke , Aoife Cahill , Josef van Genabith , Andy Way National Centre for Language Technology and School of Computing Dublin City University Glasnevin Dublin 9 Ireland {rodonovan,mburke,acahill,josef,away}@computing.dcu.ie Abstract In this paper we present a methodology for extracting subcategorisation frames based on an automatic LFG f-structure annotation algorithm for the Penn-II Treebank . 
	</s>
	

	<s id="2">
		 We extract abstract syntactic function-based subcategorisation frames ( LFG semantic forms ) , traditional CFG category- based subcategorisation frames as well as mixed function/category-based frames , with or without preposition information for obliques and particle information for particle verbs . 
	</s>
	

	<s id="3">
		 Our approach does not predefine frames , associates probabilities with frames conditional on the lemma , distinguishes between active and passive frames , and fully reflects the effects of long-distance dependencies in the source data structures . 
	</s>
	

	<s id="4">
		 We extract 3586 verb lemmas , 14348 semantic form types ( an average of 4 per lemma ) with 577 frame types . 
	</s>
	

	<s id="5">
		 We present a large-scale evaluation of the complete set of forms extracted against the full COMLEX resource . 
	</s>
	

	<s id="6">
		 1 Introduction Lexical resources are crucial in the construction of wide-coverage computational systems based on modern syntactic theories ( e.g. LFG , HPSG , CCG , LTAG etc. ) . 
	</s>
	

	<s id="7">
		 However , as manual construction of such lexical resources is time-consuming , error- prone , expensive and rarely ever complete , it is often the case that limitations of NLP systems based on lexicalised approaches are due to bottlenecks in the lexicon component . 
	</s>
	

	<s id="8">
		 Given this , research on automating lexical acquisition for lexically-based NLP systems is a particularly important issue . 
	</s>
	

	<s id="9">
		 In this paper we present an approach to automating subcategorisation frame acquisition for LFG 
		<ref citStr="Kaplan and Bresnan , 1982" id="1" label="CEPF" position="2044">
			( Kaplan and Bresnan , 1982 )
		</ref>
		 i.e. grammatical function-based systems . 
	</s>
	

	<s id="10">
		 LFG has two levels of structural representation : c(onstituent)- structure , and f(unctional)-structure . 
	</s>
	

	<s id="11">
		 LFG differentiates between governable ( argument ) and non- governable ( adjunct ) grammatical functions . 
	</s>
	

	<s id="12">
		 Sub- categorisation requirements are enforced through semantic forms specifying the governable grammatical functions required by a particular predicate ( e.g. FOCUS((T SUBJ)(T OBL , , , , , ) ) ) . 
	</s>
	

	<s id="13">
		 Our approach is based on earlier work on LFG semantic form extraction ( van Genabith et al. , 1999 ) and recent progress in automatically annotating the Penn-II treebank with LFG f-structures 
		<ref citStr="Cahill et al. , 2004b" id="2" label="CEPF" position="2751">
			( Cahill et al. , 2004b )
		</ref>
		 . 
	</s>
	

	<s id="14">
		 Depending on the quality of the f-structures , reliable LFG semantic forms can then be generated quite simply by recursively reading off the subcategorisable grammatical functions for each local pre d value at each level of embedding in the f-structures . 
	</s>
	

	<s id="15">
		 The work reported in ( van Genabith et al. , 1999 ) was small scale ( 100 trees ) , proof of concept and required considerable manual annotation work . 
	</s>
	

	<s id="16">
		 In this paper we show how the extraction process can be scaled to the complete Wall Street Journal ( WSJ ) section of the Penn-II treebank , with about 1 million words in 50,000 sentences , based on the automatic LFG f-structure annotation algorithm described in 
		<ref citStr="Cahill et al. , 2004b" id="3" label="CEPF" position="3477">
			( Cahill et al. , 2004b )
		</ref>
		 . 
	</s>
	

	<s id="17">
		 In addition to extracting grammatical function-based subcategorisation frames , we also include the syntactic categories of the predicate and its subcategorised arguments , as well as additional details such as the prepositions required by obliques , and particles accompanying particle verbs . 
	</s>
	

	<s id="18">
		 Our method does not predefine the frames to be extracted . 
	</s>
	

	<s id="19">
		 In contrast to many other approaches , it discriminates between active and passive frames , properly reflects long distance dependencies and assigns conditional probabilities to the semantic forms associated with each predicate . 
	</s>
	

	<s id="20">
		 Section 2 reviews related work in the area of automatic subcategorisation frame extraction . 
	</s>
	

	<s id="21">
		 Our methodology and its implementation are presented in Section 3 . 
	</s>
	

	<s id="22">
		 Section 4 presents the results of our lexical extraction . 
	</s>
	

	<s id="23">
		 In Section 5 we evaluate the complete extracted lexicon against the COMLEX resource 
		<ref citStr="MacLeod et al. , 1994" id="4" label="OEPF" position="4456">
			( MacLeod et al. , 1994 )
		</ref>
		 . 
	</s>
	

	<s id="24">
		 To our knowledge , this is the largest evaluation of subcategorisation frames for English . 
	</s>
	

	<s id="25">
		 In Section 6 , we conclude and give suggestions for future work . 
	</s>
	

	<s id="26">
		 2 Related Work Creating a ( subcategorisation ) lexicon by hand is time-consuming , error-prone , requires considerable linguistic expertise and is rarely , if ever , complete . 
	</s>
	

	<s id="27">
		 In addition , a system incorporating a manually constructed lexicon cannot easily be adapted to specific domains . 
	</s>
	

	<s id="28">
		 Accordingly , many researchers have attempted to construct lexicons automatically , especially for English . 
	</s>
	

	<s id="29">
		 
		<ref citStr="Brent , 1993" id="5" label="CEPF" position="5089">
			( Brent , 1993 )
		</ref>
		 relies on local morphosyntactic cues ( such as the -ing suffix , except where such a word follows a determiner or a preposition other than to ) in the untagged Brown Corpus as probabilistic indicators of six different predefined subcategorisation frames . 
	</s>
	

	<s id="30">
		 The frames do not include details of specific prepositions . 
	</s>
	

	<s id="31">
		 
		<ref citStr="Manning , 1993" id="6" label="CEPF" position="5443">
			( Manning , 1993 )
		</ref>
		 observes that Brent’s recognition technique is a “rather simplistic and inadequate approach to verb detection , with a very high error rate” . 
	</s>
	

	<s id="32">
		 Manning feeds the output from a stochastic tagger into a finite state parser , and applies statistical filtering to the parsing results . 
	</s>
	

	<s id="33">
		 He predefines 19 different subcategorisation frames , including details of prepositions . 
	</s>
	

	<s id="34">
		 Applying this technique to approx . 
	</s>
	

	<s id="35">
		 4 million words of New York Times newswire , Manning acquires 4900 sub- categorisation frames for 3104 verbs , an average of 1.6 per verb . 
	</s>
	

	<s id="36">
		 
		<ref citStr="Ushioda et al. , 1993" id="7" label="CEPF" position="6064">
			( Ushioda et al. , 1993 )
		</ref>
		 run a finite state NP parser on a POS-tagged corpus to calculate the relative frequency of just six subcategorisation verb classes . 
	</s>
	

	<s id="37">
		 In addition , all prepositional phrases are treated as adjuncts . 
	</s>
	

	<s id="38">
		 For 1565 tokens of 33 selected verbs , they report an accuracy rate of 83 % . 
	</s>
	

	<s id="39">
		 
		<ref citStr="Briscoe and Carroll , 1997" id="8" label="CEPF" position="6399">
			( Briscoe and Carroll , 1997 )
		</ref>
		 observe that in the work of 
		<ref citStr="Brent , 1993" id="9" label="CEPF" position="6444">
			( Brent , 1993 )
		</ref>
		 , 
		<ref citStr="Manning , 1993" id="10" label="CEPF" position="6465">
			( Manning , 1993 )
		</ref>
		 and 
		<ref citStr="Ushioda et al. , 1993" id="11" label="CEPF" position="6495">
			( Ushioda et al. , 1993 )
		</ref>
		 , “the maximum number of distinct subcategorization classes recognized is sixteen , and only Ushioda et al . 
	</s>
	

	<s id="40">
		 attempt to derive relative subcategorization frequency for individual predicates” . 
	</s>
	

	<s id="41">
		 In contrast , the system of 
		<ref citStr="Briscoe and Carroll , 1997" id="12" label="CEPF" position="6767">
			( Briscoe and Carroll , 1997 )
		</ref>
		 distinguishes 163 verbal subcategorisation classes by means of a statistical shallow parser , a classifier of subcategorisation classes , and a priori estimates of the probability that any verb will be a member of those classes . 
	</s>
	

	<s id="42">
		 More recent work by 
		<ref citStr="Korhonen ( 2002 )" id="13" label="CEPF" position="7044">
			Korhonen ( 2002 )
		</ref>
		 on the filtering phase of this approach has improved results . 
	</s>
	

	<s id="43">
		 Korhonen experiments with the use of linguistic verb classes for obtaining more accurate back-off estimates for use in hypothesis selection . 
	</s>
	

	<s id="44">
		 Using this extended approach , the average results for 45 semantically classified test verbs evaluated against hand judgements are precision 87.1 % and recall 71.2 % . 
	</s>
	

	<s id="45">
		 By comparison , the average results for 30 verbs not classified semantically are precision 78.2 % and recall 58.7 % . 
	</s>
	

	<s id="46">
		 
		<ref citStr="Carroll and Rooth ( 1998 )" id="14" label="CEPF" position="7598">
			Carroll and Rooth ( 1998 )
		</ref>
		 use a hand-written head-lexicalised context-free grammar and a text corpus to compute the probability of particular sub- categorisation scenarios . 
	</s>
	

	<s id="47">
		 The extracted frames do not contain details of prepositions . 
	</s>
	

	<s id="48">
		 More recently , a number of researchers have applied similar techniques to derive resources for other languages , especially German . 
	</s>
	

	<s id="49">
		 One of these , 
		<ref citStr="Schulte im Walde , 2002" id="15" label="CEPF" position="8012">
			( Schulte im Walde , 2002 )
		</ref>
		 , induces a computational subcategorisation lexicon for over 14,000 German verbs . 
	</s>
	

	<s id="50">
		 Using sentences of limited length , she extracts 38 distinct frame types , which contain maximally three arguments each . 
	</s>
	

	<s id="51">
		 The frames may optionally contain details of particular prepositional use . 
	</s>
	

	<s id="52">
		 Her evaluation on over 3000 frequently occurring verbs against the German dictionary Duden - Das Stilw¨orterbuch is similar in scale to ours and is discussed further in Section 5 . 
	</s>
	

	<s id="53">
		 There has also been some work on extracting subcategorisation details from the Penn Treebank . 
	</s>
	

	<s id="54">
		 
		<ref citStr="Kinyon and Prolo , 2002" id="16" label="OEPF" position="8643">
			( Kinyon and Prolo , 2002 )
		</ref>
		 introduce a tool which uses fine-grained rules to identify the arguments , including optional arguments , of each verb occurrence in the Penn Treebank , along with their syntactic functions . 
	</s>
	

	<s id="55">
		 They manually examined the 150+ possible sequences of tags , both functional and categorial , in Penn-II and determined whether the sequence in question denoted a modifier , argument or optional argument . 
	</s>
	

	<s id="56">
		 Arguments were then mapped to traditional syntactic functions . 
	</s>
	

	<s id="57">
		 As they do not include an evaluation , currently it is impossible to say how effective this technique is . 
	</s>
	

	<s id="58">
		 
		<ref citStr="Xia et al. , 2000" id="17" label="CEPF" position="9270">
			( Xia et al. , 2000 )
		</ref>
		 and 
		<ref citStr="Chen and Vijay-Shanker , 2000" id="18" label="CEPF" position="9308">
			( Chen and Vijay-Shanker , 2000 )
		</ref>
		 extract lexicalised TAGs from the Penn Tree- bank . 
	</s>
	

	<s id="59">
		 Both techniques implement variations on the approaches of 
		<ref citStr="Magerman , 1994" id="19" label="CEPF" position="9447">
			( Magerman , 1994 )
		</ref>
		 and 
		<ref citStr="Collins , 1997" id="20" label="CEPF" position="9470">
			( Collins , 1997 )
		</ref>
		 for the purpose of differentiating between complement and adjunct . 
	</s>
	

	<s id="60">
		 In the case of 
		<ref citStr="Xia et al. , 2000" id="21" label="CEPF" position="9584">
			( Xia et al. , 2000 )
		</ref>
		 , invalid elementary trees produced as a result of annotation errors in the treebank are filtered out using linguistic heuristics . 
	</s>
	

	<s id="61">
		 
		<ref citStr="Hockenmaier et al. , 2002" id="22" label="CEPF" position="9755">
			( Hockenmaier et al. , 2002 )
		</ref>
		 outline a method for the automatic extraction of a large syntactic CCG lexicon from Penn-II . 
	</s>
	

	<s id="62">
		 For each tree , the algorithm annotates the nodes with CCG categories in a top- down recursive manner . 
	</s>
	

	<s id="63">
		 In order to examine the coverage of the extracted lexicon in a manner similar to 
		<ref citStr="Xia et al. , 2000" id="23" label="CEPF" position="10074">
			( Xia et al. , 2000 )
		</ref>
		 , 
		<ref citStr="Hockenmaier et al. , 2002" id="24" label="OEPF" position="10106">
			( Hockenmaier et al. , 2002 )
		</ref>
		 compared the reference lexicon acquired from Sections 02-21 with a test lexicon extracted from Section 23 of the WSJ . 
	</s>
	

	<s id="64">
		 It was found that the reference CCG lexicon contained 95.09 % of the entries in the test lexicon , while 94.03 % of the entries in the test TAG lexicon also occurred in the reference lexicon . 
	</s>
	

	<s id="65">
		 Both approaches involve extensive correction and clean-up of the treebank prior to lexical extraction . 
	</s>
	

	<s id="66">
		 3 Our Methodology The first step in the application of our methodology is the production of a treebank annotated with LFG f-structure information . 
	</s>
	

	<s id="67">
		 F-structures are feature structures which represent abstract syntactic information , approximating to basic predicate-argumentmodifier structures . 
	</s>
	

	<s id="68">
		 We utilise the automatic annotation algorithm of 
		<ref citStr="Cahill et al. , 2004b" id="25" label="CERF" position="10938">
			( Cahill et al. , 2004b )
		</ref>
		 to derive a version of Penn-II where each node in each tree is annotated with an LFG functional annotation ( i.e. an attribute value structure equation ) . 
	</s>
	

	<s id="69">
		 Trees are traversed top-down , and annotation is driven by categorial , basic configurational , trace and Penn-II functional tag information in local subtrees of mostly depth one ( i.e. CFG rules ) . 
	</s>
	

	<s id="70">
		 The annotation procedure is dependent on locating the head daughter , for which the scheme of 
		<ref citStr="Magerman , 1994" id="26" label="CERF" position="11426">
			( Magerman , 1994 )
		</ref>
		 with some changes and amendments is used . 
	</s>
	

	<s id="71">
		 The head is annotated with the LFG equation T= J. Linguistic generalisations are provided over the left ( the prefix ) and the right ( suffix ) context of the head for each syntactic category occurring as the mother node of such heads . 
	</s>
	

	<s id="72">
		 To give a simple example , the rightmost NP to the left of a VP head under an S is likely to be its subject ( T SUBJ =J ) , while the leftmost NP to the right of the V head of a VP is most probably its object ( T OBJ =J ) . 
	</s>
	

	<s id="73">
		 
		<ref citStr="Cahill et al. , 2004b" id="27" label="CEPF" position="11983">
			( Cahill et al. , 2004b )
		</ref>
		 provide four sets of annotation principles , one for non-coordinate configurations , one for coordinate configurations , one for traces ( long distance dependencies ) and a final ‘catch all and clean up’ phase . 
	</s>
	

	<s id="74">
		 Distinguishing between argument and adjunct is an inherent step in the automatic assignment of functional annotations . 
	</s>
	

	<s id="75">
		 The satisfactory treatment of long distance dependencies by the annotation algorithm is imperative for the extraction of accurate semantic forms . 
	</s>
	

	<s id="76">
		 The Penn Treebank employs a rich arsenal of traces and empty productions ( nodes which do not realise any lexical material ) to co-index displaced material with the position where it should be interpreted semantically . 
	</s>
	

	<s id="77">
		 The algorithm of 
		<ref citStr="Cahill et al. , 2004b" id="28" label="CEPF" position="12763">
			( Cahill et al. , 2004b )
		</ref>
		 translates the traces into corresponding re-entrancies in the f-structure representation ( Figure 1 ) . 
	</s>
	

	<s id="78">
		 Passive movement is also captured and expressed at f-structure level using a pas s ive : + annotation . 
	</s>
	

	<s id="79">
		 Once a treebank tree is annotated with feature structure equations by the annotation algorithm , the equations are collected and passed to a constraint solver which produces the f-structures . 
	</s>
	

	<s id="80">
		 In order to ensure the quality of the seman- S signs treaty ^SUBJ [ PRED U.N. ] TOPIC PRED sign J _OBJ LPRED treaty~ ^ ^SUBJ ( SPEC the 1 LPRED headline J ^ PRED say COMP 1 Figure 1 : Penn-II style tree with long distance dependency trace and corresponding reentrancy in f-structure tic forms extracted by our method , we must first ensure the quality of the f-structure annotations . 
	</s>
	

	<s id="81">
		 
		<ref citStr="Cahill et al. , 2004b" id="29" label="CEPF" position="13611">
			( Cahill et al. , 2004b )
		</ref>
		 measure annotation quality in terms of precision and recall against manually constructed , gold-standard f-structures for 105 randomly selected trees from section 23 of the WSJ section of Penn-II . 
	</s>
	

	<s id="82">
		 The algorithm currently achieves an F-score of 96.3 % for complete f-structures and 93.6 % for preds-only f-structures.1 Our semantic form extraction methodology is based on the procedure of ( van Genabith et al. , 1999 ) : For each f-structure generated , for each level of embedding we determine the local PRED value and collect the subcategorisable grammatical functions present at that level of embedding . 
	</s>
	

	<s id="83">
		 Consider the f-structure in Figure 1 . 
	</s>
	

	<s id="84">
		 From this we recursively extract the following non- empty semantic forms : s a y ( [ s ub j , c omp ] ) , sign ( [ subj , obj ] ) . 
	</s>
	

	<s id="85">
		 In effect , in both ( van Genabith et al. , 1999 ) and our approach semantic forms are reverse engineered from automatically generated f-structures for treebank trees . 
	</s>
	

	<s id="86">
		 We extract the following subcategorisable syntactic functions : SUBJ , OBJ , OBJ2 , OBLpTep , OBL2pTep , COMP , XCOMP and PART . 
	</s>
	

	<s id="87">
		 Adjuncts ( e.g. ADJ , APP etc ) are not included in the semantic forms . 
	</s>
	

	<s id="88">
		 PART is not a syntactic function in the strict sense but we capture the relevant co-occurrence patterns of verbs and particles in the semantic forms . 
	</s>
	

	<s id="89">
		 Just as OBL includes the prepositional head of the PP , PART includes the actual particle which occurs e.g. add([subj,obj,part:up]) . 
	</s>
	

	<s id="90">
		 In the work presented here we substantially extend the approach of ( van Genabith et al. , 1999 ) as 1Preds-only measures only paths ending in PRED:VALUE so features such as number , person etc are not included . 
	</s>
	

	<s id="91">
		 S-TPC- 1 NP VP Det S N V VP the NP U.N. headline said T- 1 V NP ^ ^^^^^^ 1 regards coverage , granularity and evaluation : First , we scale the approach of ( van Genabith et al. , 1999 ) which was proof of concept on 100 trees to the full WSJ section of the Penn-II Treebank . 
	</s>
	

	<s id="92">
		 Second , our approach fully reflects long distance dependencies , indicated in terms of traces in the Penn-II Tree- bank and corresponding re-entrancies at f-structure . 
	</s>
	

	<s id="93">
		 Third , in addition to abstract syntactic function- based subcategorisation frames we compute frames for syntactic function-CFG category pairs , both for the verbal heads and their arguments and also generate pure CFG-based subcat frames . 
	</s>
	

	<s id="94">
		 Fourth , our method differentiates between frames captured for active or passive constructions . 
	</s>
	

	<s id="95">
		 Fifth , our method associates conditional probabilities with frames . 
	</s>
	

	<s id="96">
		 In contrast to much of the work reviewed in the previous section , our system is able to produce surface syntactic as well as abstract functional subcategorisation details . 
	</s>
	

	<s id="97">
		 To incorporate CFG details into the extracted semantic forms , we add an extra feature to the generated f-structures , the value of which is the syntactic category of the pred at each level of embedding . 
	</s>
	

	<s id="98">
		 Exploiting this information , the extracted semantic form for the verb sign looks as follows : sign ( v , [ subj ( np ) , obj ( np ) ] ) . 
	</s>
	

	<s id="99">
		 We have also extended the algorithm to deal with passive voice and its effect on subcategorisation behaviour . 
	</s>
	

	<s id="100">
		 Consider Figure 2 : not taking voice into account , the algorithm extracts an intransitive frame outlaw ( [ subj ] ) for the transitive outlaw . 
	</s>
	

	<s id="101">
		 To correct this , the extraction algorithm uses the feature value pair p a s s ive : + , which appears in the f-structure at the level of embedding of the verb in question , to mark that predicate as occurring in the passive : outlaw ( [ subj ] , p ) . 
	</s>
	

	<s id="102">
		 In order to estimate the likelihood of the cooccurrence of a predicate with a particular argument list , we compute conditional probabilities for sub- categorisation frames based on the number of token occurrences in the corpus . 
	</s>
	

	<s id="103">
		 Given a lemma l and an argument list s , the probability of s given l is estimated as : count ( l , s ) P(s|l) := .i=1 count(l , si ) We use thresholding to filter possible error judgements by our system . 
	</s>
	

	<s id="104">
		 Table 1 shows the attested semantic forms for the verb accept with their associated conditional probabilities . 
	</s>
	

	<s id="105">
		 Note that were the distinction between active and passive not taken into account , the intransitive occurrence of accept would have been assigned an unmerited probability . 
	</s>
	

	<s id="106">
		 subj : spec : quant : pred : all adjunct : 2 : pred : almost adjunct : 3 : pred:remain participle : pres 4 : obj : adjunct : 5 : pred : cancer-causing pers : 3 pred : asbestos num : sg pform : of pers : 3 pred : use num : pl passive : + adjunct : 1 : obj : pred : 1997 pform : by xcomp : subj : spec : quant : pred : all adjunct : 2 : pred : almost ... ... passive : + xcomp : subj : spec : quant : pred : all adjunct : 2 : pred : almost ... ... passive : + pred : outlaw tense : past pred : be pred : will modal : + Figure 2 : Automatically generated f-structure for the string wsj 0003 23“B y 1997 , almost all remaining uses of cancer-causing asbestos will be outlawed.” Semantic Form accept([subj,obj]) - accept ( [ subj ] , p ) accept([subj,comp]) -accept([subj,obl:as],p) accept([subj,obj,obl:as]) accept([subj,obj,obl:from]) - accept ( [ subj ] ) accept([subj,obj,obl:at]) accept([subj,obj,obl:for]) accept([subj,obj,xcomp]) Table 1 : Semantic Forms for the verb accept marked with p for passive use . 
	</s>
	

	<s id="107">
		 4 Results We extract non-empty semantic forms2 for 3586 verb lemmas and 10969 unique verbal semantic form types ( lemma followed by non-empty argument list ) . 
	</s>
	

	<s id="108">
		 Including prepositions associated with the OBLs and particles , this number rises to 14348 , an average of 4.0 per lemma ( Table 2 ) . 
	</s>
	

	<s id="109">
		 The number of unique frame types ( without lemma ) is 38 without specific prepositions and particles , 577 with ( Table 3 ) . 
	</s>
	

	<s id="110">
		 F-structure annotations allow us to distinguish passive and active frames . 
	</s>
	

	<s id="111">
		 5 COMLEX Evaluation We evaluated our induced ( verbal ) semantic forms against COMLEX 
		<ref citStr="MacLeod et al. , 1994" id="30" label="OEPF" position="19752">
			( MacLeod et al. , 1994 )
		</ref>
		 . 
	</s>
	

	<s id="112">
		 COM- 2Frames with at least one subcategorised grammatical function . 
	</s>
	

	<s id="113">
		 Frequency Probability 122 0.813 9 0.060 5 0.033 3 0.020 3 0.020 3 0.020 2 0.013 1 0.007 1 0.007 1 0.007 Without Prep/Part With Prep/Part Sem . 
	</s>
	

	<s id="114">
		 Form Types 10969 14348 Active 8516 11367 Passive 2453 2981 Table 2 : Number of Semantic Form Types Without Prep/Part With Prep/Part # Frame Types 38 577 # Singletons 1 243 # Twice Occurring 1 84 # Occurring maz . 
	</s>
	

	<s id="115">
		 5 7 415 # Occurring &gt; 5 31 162 Table 3 : Number of Distinct Frames for Verbs ( not including syntactic category for grammatical function ) LEX defines 138 distinct verb frame types without the inclusion of specific prepositions or particles . 
	</s>
	

	<s id="116">
		 The following is a sample entry for the verb reimburse : ( VERB :ORTH “reimburse” :SUBC ( ( NP-NP ) ( NP-PP :PVAL ( “for” ) ) ( NP ) ) ) Each verb has a :SUBC feature , specifying its subcategorisation behaviour . 
	</s>
	

	<s id="117">
		 For example , reimburse can occur with two noun phrases ( NP-NP ) , a noun phrase and a prepositional phrase headed by “for” ( NP-PP :PVAL ( “for” ) ) or a single noun phrase ( NP ) . 
	</s>
	

	<s id="118">
		 Note that the details of the subject noun phrase are not included in COMLEX frames . 
	</s>
	

	<s id="119">
		 Each of the complement types which make up the value of the :SUBC feature is associated with a formal frame definition which looks as follows : ( vp-frame np-np :cs ( ( np 2)(np 3 ) ) :gs ( :subject 1 :obj 2 :obj2 3 ) :ex “she asked him his name” ) The value of the :cs feature is the constituent structure of the subcategorisation frame , which lists the syntactic CF-PSG constituents in sequence . 
	</s>
	

	<s id="120">
		 The value of the :gs feature is the grammatical structure which indicates the functional role played by each of the CF-PSG constituents . 
	</s>
	

	<s id="121">
		 The elements of the constituent structure are indexed , and referenced in the :gs field . 
	</s>
	

	<s id="122">
		 This mapping between constituent structure and functional structure makes the information contained in COMLEX suitable as an evaluation standard for the LFG semantic forms which we induce . 
	</s>
	

	<s id="123">
		 5.1 COMLEX-LFG Mapping We devised a common format for our induced semantic forms and those contained in COMLEX . 
	</s>
	

	<s id="124">
		 This is summarised in Table 4 . 
	</s>
	

	<s id="125">
		 COMLEX does not distinguish between obliques and objects so we converted Obji to OBLi as required . 
	</s>
	

	<s id="126">
		 In addition , COMLEX does not explicitly differentiate between COMPs and XCOMPs , but does encode control information for any Comps which occur , thus allowing us to deduce the distinction automatically . 
	</s>
	

	<s id="127">
		 The manually constructed COMLEX entries provided us with a gold standard against which we evaluated the automatically induced frames for the 2992 ( active ) verbs that both resources have in common . 
	</s>
	

	<s id="128">
		 LFG COMLEX Merged SUBJ Subject SUBJ OBJ Object OBJ OBJ2 Obj2 OBJ2 OBL Obj3 OBL OBL2 Obj4 OBL2 COMP Comp COMP XCOMP Comp XCOMP PART Part PART Table 4 : COMLEX and LFG Syntactic Functions We use the computed conditional probabilities to set a threshold to filter the selection of semantic forms . 
	</s>
	

	<s id="129">
		 As some verbs occur less frequently than others we felt it was important to use a relative rather than absolute threshold . 
	</s>
	

	<s id="130">
		 For a threshold of 1 % , we disregard any frames with a conditional probability of less than or equal to 0.01 . 
	</s>
	

	<s id="131">
		 We carried out the evaluation in a similar way to 
		<ref citStr="Schulte im Walde , 2002" id="31" label="CEPF" position="23175">
			( Schulte im Walde , 2002 )
		</ref>
		 . 
	</s>
	

	<s id="132">
		 The scale of our evaluation is comparable to hers . 
	</s>
	

	<s id="133">
		 This allows us to make tentative comparisons between our respective results . 
	</s>
	

	<s id="134">
		 The figures shown in Table 5 are the results of three different kinds of evaluation with the threshold set to 1 % and 5 % . 
	</s>
	

	<s id="135">
		 The effect of the threshold increase is obvious in that Precision goes up for each of the experiments while Recall goes down . 
	</s>
	

	<s id="136">
		 For Ezp 1 , we excluded prepositional phrases entirely from the comparison , i.e. assumed that PPs were adjunct material ( e.g. [ subj,obl:for ] becomes [ subj ] ) . 
	</s>
	

	<s id="137">
		 Our results are better for Precision than for Recall compared to Schulte im Walde ( op cit . 
	</s>
	

	<s id="138">
		 ) , who reports Precision of 74.53 % , Recall of 69.74 % and an F-score of 72.05 % . 
	</s>
	

	<s id="139">
		 Ezp 2 includes prepositional phrases but not parameterised for particular prepositions ( e.g. [ subj,obl:for ] becomes [ subj,obl ] ) . 
	</s>
	

	<s id="140">
		 While our figures for Recall are again lower , our results for Precision are considerably higher than those of Schulte im Walde ( op cit . 
	</s>
	

	<s id="141">
		 ) who recorded Precision of 60.76 % , Recall of 63.91 % and an F-score of 62.30 % . 
	</s>
	

	<s id="142">
		 For Ezp . 
	</s>
	

	<s id="143">
		 3 , we used semantic forms which contained details of specific prepositions for any subcategorised prepositional phrase . 
	</s>
	

	<s id="144">
		 Our Precision figures are again high ( in comparison to 65.52 % as recorded by 
		<ref citStr="Schulte im Walde , 2002" id="32" label="CJPN" position="24617">
			( Schulte im Walde , 2002 )
		</ref>
		 ) . 
	</s>
	

	<s id="145">
		 However , Threshold 1 % Threshold 5 % P R F-Score P R F-Score Exp . 
	</s>
	

	<s id="146">
		 1 79.0 % 59.6 % 68.0 % 83.5 % 54.7 % 66.1 % Exp . 
	</s>
	

	<s id="147">
		 2 77.1 % 50.4 % 61.0 % 81.4 % 44.8 % 57.8 % Exp . 
	</s>
	

	<s id="148">
		 2a 76.4 % 44.5 % 56.3 % 80.9 % 39.0 % 52.6 % Exp . 
	</s>
	

	<s id="149">
		 3 73.7 % 22.1 % 34.0 % 78.0 % 18.3 % 29.6 % Exp . 
	</s>
	

	<s id="150">
		 3a 73.3 % 19.9 % 31.3 % 77.6 % 16.2 % 26.8 % Precision Recall F-Score Experiment 3 81.7 % 40.8 % 54.4 % Experiment 3a 83.1 % 35.4 % 49.7 % Table 6 : COMLEX Comparison using p-dir(Threshold of 1 % ) Table 5 : COMLEX Comparison our Recall is very low ( compared to the 50.83 % that Schulte im Walde ( op cit . 
	</s>
	

	<s id="151">
		 ) reports ) . 
	</s>
	

	<s id="152">
		 Consequently our F-score is also low ( Schulte im Walde ( op cit . 
	</s>
	

	<s id="153">
		 ) records an F-score of 57.24 % ) . 
	</s>
	

	<s id="154">
		 Experiments 2a and 3a are similar to Experiments 2 and 3 respectively except they include the specific particle associated with each PART . 
	</s>
	

	<s id="155">
		 5.1.1 Directional Prepositions There are a number of possible reasons for our low recall scores for Experiment 3 in Table 5 . 
	</s>
	

	<s id="156">
		 It is a well-documented fact 
		<ref citStr="Briscoe and Carroll , 1997" id="33" label="CEPF" position="25749">
			( Briscoe and Carroll , 1997 )
		</ref>
		 that subcategorisation frames ( and their frequencies ) vary across domains . 
	</s>
	

	<s id="157">
		 We have extracted frames from one domain ( the WSJ ) whereas COMLEX was built using examples from the San Jose Mercury News , the Brown Corpus , several literary works from the Library of America , scientific abstracts from the U.S. Department of Energy , and the WSJ . 
	</s>
	

	<s id="158">
		 For this reason , it is likely to contain a greater variety of subcategorisation frames than our induced lexicon . 
	</s>
	

	<s id="159">
		 It is also possible that due to human error COMLEX contains subcategorisation frames , the validity of which may be in doubt . 
	</s>
	

	<s id="160">
		 This is due to the fact that the aim of the COMLEX project was to construct as complete a set of subcategorisation frames as possible , even for infrequent verbs . 
	</s>
	

	<s id="161">
		 Lexicographers were allowed to extrapolate from the citations found , a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples . 
	</s>
	

	<s id="162">
		 Our recall figure was particularly low in the case of evaluation using details of prepositions ( Experiment 3 ) . 
	</s>
	

	<s id="163">
		 This can be accounted for by the fact that COMLEX errs on the side of overgeneration when it comes to preposition assignment . 
	</s>
	

	<s id="164">
		 This is particularly true of directional prepositions , a list of 31 of which has been prepared and is assigned in its entirety by default to any verb which can potentially appear with any directional preposition . 
	</s>
	

	<s id="165">
		 In a subsequent experiment , we incorporate this list of directional prepositions by default into our semantic form induction process in the same way as the creators of COMLEX have done . 
	</s>
	

	<s id="166">
		 Table 6 shows the results of this experiment . 
	</s>
	

	<s id="167">
		 As expected there is a significant im- Passive Precision Recall F-Score Experiment 2 80.2 % 54.7 % 65.1 % Experiment 2a 79.7 % 46.2 % 58.5 % Experiment 3 72.6 % 33.4 % 45.8 % Experiment 3a 72.3 % 29.3 % 41.7 % Table 7 : Passive evaluation ( Threshold of 1 % ) provement in the recall figure , being almost double the figures reported in Table 5 for Experiments 3 and 3a . 
	</s>
	

	<s id="168">
		 5.1.2 Passive Evaluation Table 7 presents the results of our evaluation of the passive semantic forms we extract . 
	</s>
	

	<s id="169">
		 It was carried out for 1422 verbs which occur with passive frames and are shared by the induced lexicon and COMLEX . 
	</s>
	

	<s id="170">
		 As COMLEX does not provide explicit passive entries , we applied Lexical Redundancy Rules 
		<ref citStr="Kaplan and Bresnan , 1982" id="34" label="CEPF" position="28228">
			( Kaplan and Bresnan , 1982 )
		</ref>
		 to automatically convert the active COMLEX frames to their passive counterparts . 
	</s>
	

	<s id="171">
		 For example , the COMLEX entry see ( [ subj , obj ] ) is converted to s e e ( [ s ub j ] ) . 
	</s>
	

	<s id="172">
		 The resulting precision is very high , a slight increase on that for the active frames . 
	</s>
	

	<s id="173">
		 The recall score drops for passive frames ( from 54.7 % to 29.3 % ) in a similar way to that for active frames when prepositional details are included . 
	</s>
	

	<s id="174">
		 5.2 Lexical Accession Rates As well as evaluating the quality of our extracted semantic forms , we also examine the rate at which they are induced . 
	</s>
	

	<s id="175">
		 
		<ref citStr="Charniak , 1996" id="35" label="CEPF" position="28859">
			( Charniak , 1996 )
		</ref>
		 and 
		<ref citStr="Krotov et al. , 1998" id="36" label="CEPF" position="28888">
			( Krotov et al. , 1998 )
		</ref>
		 observed that treebank grammars ( CFGs extracted from treebanks ) are very large and grow with the size of the treebank . 
	</s>
	

	<s id="176">
		 We were interested in discovering whether the acquisition of lexical material on the same data displays a similar propensity . 
	</s>
	

	<s id="177">
		 Figure 3 displays the accession rates for the semantic forms induced by our method for sections 0–24 of the WSJ section of the Penn-II treebank . 
	</s>
	

	<s id="178">
		 When we do not distinguish semantic forms by category , all semantic forms together with those for verbs display smaller accession rates than for the PCFG . 
	</s>
	

	<s id="179">
		 We also examined the coverage of our system in a similar way to 
		<ref citStr="Hockenmaier et al. , 2002" id="37" label="CEPF" position="29571">
			( Hockenmaier et al. , 2002 )
		</ref>
		 . 
	</s>
	

	<s id="180">
		 We extracted a verb-only reference lexicon from Sections 02-21 of the WSJ and subsequently compared this to a test lexicon constructed in the same way from 0 5 10 15 20 25 WSJ Section Figure 3 : Accession Rates for Semantic Forms and CFG Rules Entries also in reference lexicon : 89.89 % Entries not in reference lexicon : 10.11 % Known words : 7.85 % -Known words , known frames : 7.85 % -Known words , unknown frames : - Unknown words : 2.32 % -Unknown words , known frames : 2.32 % -Unknown words , unknown frames : - Table 8 : Coverage of induced lexicon on unseen data ( Verbs Only ) Section 23 . 
	</s>
	

	<s id="181">
		 Table 8 shows the results of this experiment . 
	</s>
	

	<s id="182">
		 89.89 % of the entries in the test lexicon appeared in the reference lexicon . 
	</s>
	

	<s id="183">
		 6 Conclusions We have presented an algorithm and its implementation for the extraction of semantic forms or subcategorisation frames from the Penn-II Treebank , automatically annotated with LFG f-structures . 
	</s>
	

	<s id="184">
		 We have substantially extended an earlier approach by ( van Genabith et al. , 1999 ) . 
	</s>
	

	<s id="185">
		 The original approach was small-scale and ‘proof of concept’ . 
	</s>
	

	<s id="186">
		 We have scaled our approach to the entire WSJ Sections of Penn- II ( 50,000 trees ) . 
	</s>
	

	<s id="187">
		 Our approach does not predefine the subcategorisation frames we extract as many other approaches do . 
	</s>
	

	<s id="188">
		 We extract abstract syntactic function-based subcategorisation frames ( LFG semantic forms ) , traditional CFG category-based frames as well as mixed function-category based frames . 
	</s>
	

	<s id="189">
		 Unlike many other approaches to subcategorisation frame extraction , our system properly reflects the effects of long distance dependencies and distinguishes between active and passive frames . 
	</s>
	

	<s id="190">
		 Finally our system associates conditional probabilities with the frames we extract . 
	</s>
	

	<s id="191">
		 We carried out an extensive evaluation of the complete induced lexicon ( not just a sample ) against the full COMLEX resource . 
	</s>
	

	<s id="192">
		 To our knowledge , this is the most extensive qualitative evaluation of subcategorisation extraction in English . 
	</s>
	

	<s id="193">
		 The only evaluation of a similar scale is that carried out by 
		<ref citStr="Schulte im Walde , 2002" id="38" label="CEPF" position="31770">
			( Schulte im Walde , 2002 )
		</ref>
		 for German . 
	</s>
	

	<s id="194">
		 Our results compare well with hers . 
	</s>
	

	<s id="195">
		 We believe our semantic forms are fine-grained and by choosing to evaluate against COMLEX we set our sights high : COMLEX is considerably more detailed than the OALD or LDOCE used for other evaluations . 
	</s>
	

	<s id="196">
		 Currently work is under way to extend the coverage of our acquired lexicons by applying our methodology to the Penn-III treebank , a more balanced corpus resource with a number of text genres ( in addition to the WSJ sections ) . 
	</s>
	

	<s id="197">
		 It is important to realise that the induction of lexical resources is part of a larger project on the acquisition of wide-coverage , robust , probabilistic , deep unification grammar resources from treebanks . 
	</s>
	

	<s id="198">
		 We are already using the extracted semantic forms in parsing new text with robust , wide-coverage PCFG-based LFG grammar approximations automatically acquired from the f-structure annotated Penn-II tree- bank 
		<ref citStr="Cahill et al. , 2004a" id="39" label="OEPF" position="32744">
			( Cahill et al. , 2004a )
		</ref>
		 . 
	</s>
	

	<s id="199">
		 We hope to be able to apply our lexical acquisition methodology beyond existing parse-annotated corpora ( Penn-II and Penn- III ) : new text is parsed by our PCFG-based LFG approximations into f-structures from which we can then extract further semantic forms . 
	</s>
	

	<s id="200">
		 The work reported here is part of the core component for bootstrapping this approach . 
	</s>
	

	<s id="201">
		 As the extraction algorithm we presented derives semantic forms at f-structure level , it is easily applied to other , even typologically different , languages . 
	</s>
	

	<s id="202">
		 We have successfully ported our automatic annotation algorithm to the TIGER Treebank , despite German being a less configurational language than English , and extracted wide-coverage , probabilistic LFG grammar approximations and lexical resources for German 
		<ref citStr="Cahill et al. , 2003" id="40" label="CEPF" position="33577">
			( Cahill et al. , 2003 )
		</ref>
		 . 
	</s>
	

	<s id="203">
		 Currently , we are migrating the technique to Spanish , which has freer word order than English and less morphological marking than German . 
	</s>
	

	<s id="204">
		 Preliminary results have been very encouraging . 
	</s>
	

	<s id="205">
		 7 Acknowledgements The research reported here is supported by Enterprise Ireland Basic Research Grant SC/2001/186 and an IRCSET PhD fellowship award . 
	</s>
	

	<s id="206">
		 All SF Frames All Verbs All SF Frames , no category All Verbs , no category 25000 20000 15000 10000 5000 0 References M. Brent . 
	</s>
	

	<s id="207">
		 1993. From Grammar to Lexicon : Unsupervised Learning of Lexical Syntax . 
	</s>
	

	<s id="208">
		 Computational Linguistics , 19(2):203–222 . 
	</s>
	

	<s id="209">
		 E. Briscoe and J. Carroll . 
	</s>
	

	<s id="210">
		 1997. Automatic Extraction of Subcategorization from Corpora . 
	</s>
	

	<s id="211">
		 In Proceedings of the 5th ACL Conference on Applied Natural Language Processing , pages 356–363 , Washington , DC . 
	</s>
	

	<s id="212">
		 A. Cahill , M. Forst , M. McCarthy , R. O’Donovan , C. Rohrer , J. van Genabith , and A. Way . 
	</s>
	

	<s id="213">
		 2003. Treebank-Based Multilingual Unification- Grammar Development . 
	</s>
	

	<s id="214">
		 In Proceedings of the Workshop on Ideas and Strategies for Multilingual Grammar Development at the 15th ESSLLI , pages 17–24 , Vienna , Austria . 
	</s>
	

	<s id="215">
		 A. Cahill , M. Burke , R. O’Donovan , J. van Genabith , and A. Way . 
	</s>
	

	<s id="216">
		 2004a . 
	</s>
	

	<s id="217">
		 Long-Distance Dependency Resolution in Automatically Acquired Wide-Coverage PCFG-Based LFG Approximations . 
	</s>
	

	<s id="218">
		 In Proceedings of the 42nd Annual Conference of the Association for Computational Linguistics ( ACL-04 ) , Barcelona , Spain . 
	</s>
	

	<s id="219">
		 A. Cahill , M. McCarthy , M. Burke , R. O’Donovan , J. van Genabith , and A. Way . 
	</s>
	

	<s id="220">
		 2004b . 
	</s>
	

	<s id="221">
		 Evaluating Automatic F-Structure Annotation for the Penn- II Treebank . 
	</s>
	

	<s id="222">
		 Journal of Research on Language and Computation . 
	</s>
	

	<s id="223">
		 G. Carroll and M. Rooth . 
	</s>
	

	<s id="224">
		 1998. Valence Induction with a Head-Lexicalised PCFG . 
	</s>
	

	<s id="225">
		 In Proceedings of the 3rd Conference on Empirical Methods in Natural Language Processing , pages 36– 45 , Granada , Spain . 
	</s>
	

	<s id="226">
		 E. Charniak . 
	</s>
	

	<s id="227">
		 1996. Tree-bank Grammars . 
	</s>
	

	<s id="228">
		 In AAAI96 : Proceedings of the Thirteenth National Conference on Artificial Intelligence , MIT Press , pages 1031–1036 , Cambridge , MA . 
	</s>
	

	<s id="229">
		 J. Chen and K. Vijay-Shanker . 
	</s>
	

	<s id="230">
		 2000. Automated Extraction of TAGs from the Penn Treebank . 
	</s>
	

	<s id="231">
		 In Proceedings of the 38th Annual Meeting of the Association of Computational Linguistics , pages 65–76 , Hong Kong . 
	</s>
	

	<s id="232">
		 M. Collins . 
	</s>
	

	<s id="233">
		 1997. Three generative lexicalised models for statistical parsing . 
	</s>
	

	<s id="234">
		 In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics , pages 16–23 . 
	</s>
	

	<s id="235">
		 J. Hockenmaier , G. Bierner , and J. Baldridge . 
	</s>
	

	<s id="236">
		 2002. Extending the Coverage of a CCG System . 
	</s>
	

	<s id="237">
		 Journal ofLanguage and Computation , ( 2 ) . 
	</s>
	

	<s id="238">
		 R. Kaplan and J. Bresnan . 
	</s>
	

	<s id="239">
		 1982. Lexical Functional Grammar : A Formal System for Grammatical Representation . 
	</s>
	

	<s id="240">
		 In Joan Bresnan , editor , The Mental Representation of Grammatical Re- lations , pages 206–250 . 
	</s>
	

	<s id="241">
		 MIT Press , Cambridge , MA , Mannheim , 8th Edition . 
	</s>
	

	<s id="242">
		 A. Kinyon and C. Prolo . 
	</s>
	

	<s id="243">
		 2002. Identifying Verb Arguments and their Syntactic Function in the Penn Treebank . 
	</s>
	

	<s id="244">
		 In Proceedings of the 3rd LREC Conference , pages 1982–1987 , Las Palmas , Spain . 
	</s>
	

	<s id="245">
		 A. Korhonen . 
	</s>
	

	<s id="246">
		 2002. Subcategorization Acquisition . 
	</s>
	

	<s id="247">
		 PhD thesis published as Techical Report UCAMCL-TR-530 , Computer Laboratory , University of Cambridge , UK . 
	</s>
	

	<s id="248">
		 A. Krotov , M. Hepple , R. Gaizauskas , and Y. Wilks . 
	</s>
	

	<s id="249">
		 1998. Compacting the Penn Treebank Grammar . 
	</s>
	

	<s id="250">
		 In Proceedings of COLING -ACL’98 , pages 669– 703 , Montreal , Canada . 
	</s>
	

	<s id="251">
		 C. MacLeod , R. Grishman , and A. Meyers . 
	</s>
	

	<s id="252">
		 1994. The Comlex Syntax Project : The First Year . 
	</s>
	

	<s id="253">
		 In Proceedings of the ARPA Workshop on Human Language Technology , pages 669–703 , Princeton , NJ . 
	</s>
	

	<s id="254">
		 D. Magerman . 
	</s>
	

	<s id="255">
		 1994. Natural Language Parsing as Statistical Pattern Recognition . 
	</s>
	

	<s id="256">
		 PhD Thesis , Stanford University , CA . 
	</s>
	

	<s id="257">
		 C. Manning . 
	</s>
	

	<s id="258">
		 1993. Automatic Acquisition of a Large Subcategorisation Dictionary from Corpora . 
	</s>
	

	<s id="259">
		 In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics , pages 235–242 , Columbus , OH . 
	</s>
	

	<s id="260">
		 S. Schulte im Walde . 
	</s>
	

	<s id="261">
		 2002. Evaluating Verb Sub- categorisation Frames learned by a German Statistical Grammar against Manual Definitions in the Duden Dictionary . 
	</s>
	

	<s id="262">
		 In Proceedings of the 10th EURALEX International Congress , pages 187– 197 , Copenhagen , Denmark . 
	</s>
	

	<s id="263">
		 A. Ushioda , D. Evans , T. Gibson , and A. Waibel . 
	</s>
	

	<s id="264">
		 1993. The Automatic Acquisition of Frequencies of Verb Subcategorization Frames from Tagged Corpora . 
	</s>
	

	<s id="265">
		 In SIGLEX ACL Workshop on the Acquisition of Lexical Knowledge from Text , pages 95–106 , Columbus , OH . 
	</s>
	

	<s id="266">
		 J. van Genabith , A. Way , and L. Sadler . 
	</s>
	

	<s id="267">
		 1999. Data- driven Compilation of LFG Semantic Forms . 
	</s>
	

	<s id="268">
		 In EACL-99 Workshop on Linguistically Interpreted Corpora , pages 69–76 , Bergen , Norway . 
	</s>
	

	<s id="269">
		 F. Xia , M. Palmer , and A. Joshi . 
	</s>
	

	<s id="270">
		 2000. A Uniform Method of Grammar Extraction and its Applications . 
	</s>
	

	<s id="271">
		 In Proceedings of the Conference on Empirical Methods in Natural Language Processing ( EMNLP-2000 ) , pages 53–62 , Hong Kong . 
	</s>
	


</acldoc>
