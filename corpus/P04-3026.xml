<?xml version="1.0" encoding="iso-8859-1"?>
<acldoc acl_id="P04-3026">
	

	<s id="1">
		 A Practical Solution to the Problem of Automatic Word Sense Induction Reinhard Rapp University of Mainz , FASK D-76711 Germersheim , Germany rapp@mail.fask.uni-mainz.de Abstract Recent studies in word sense induction are based on clustering global co-occurrence vectors , i.e. vectors that reflect the overall behavior of a word in a corpus . 
	</s>
	

	<s id="2">
		 If a word is semantically ambiguous , this means that these vectors are mixtures of all its senses . 
	</s>
	

	<s id="3">
		 Inducing a word’s senses therefore involves the difficult problem of recovering the sense vectors from the mixtures . 
	</s>
	

	<s id="4">
		 In this paper we argue that the demixing problem can be avoided since the contextual behavior of the senses is directly observable in the form of the local contexts of a word . 
	</s>
	

	<s id="5">
		 From human disambiguation performance we know that the context of a word is usually sufficient to determine its sense . 
	</s>
	

	<s id="6">
		 Based on this observation we describe an algorithm that discovers the different senses of an ambiguous word by clustering its contexts . 
	</s>
	

	<s id="7">
		 The main difficulty with this approach , namely the problem of data sparseness , could be minimized by looking at only the three main dimensions of the context matrices . 
	</s>
	

	<s id="8">
		 1 Introduction The topic of this paper is word sense induction , that is the automatic discovery of the possible senses of a word . 
	</s>
	

	<s id="9">
		 A related problem is word sense disambiguation : Here the senses are assumed to be known and the task is to choose the correct one when given an ambiguous word in context . 
	</s>
	

	<s id="10">
		 Whereas until recently the focus of research had been on sense disambiguation , papers like Pantel &amp; 
		<ref citStr="Lin ( 2002 )" id="1" label="CEPF" position="1676">
			Lin ( 2002 )
		</ref>
		 , 
		<ref citStr="Neill ( 2002 )" id="2" label="CEPF" position="1693">
			Neill ( 2002 )
		</ref>
		 , and 
		<ref citStr="Rapp ( 2003 )" id="3" label="CEPF" position="1713">
			Rapp ( 2003 )
		</ref>
		 give evidence that sense induction now also attracts attention . 
	</s>
	

	<s id="11">
		 In the approach by Pantel &amp; 
		<ref citStr="Lin ( 2002 )" id="4" label="CEPF" position="1832">
			Lin ( 2002 )
		</ref>
		 , all words occurring in a parsed corpus are clustered on the basis of the distances of their co-occurrence vectors . 
	</s>
	

	<s id="12">
		 This is called global clustering . 
	</s>
	

	<s id="13">
		 Since ( by looking at differential vectors ) their algorithm allows a word to belong to more than one cluster , each cluster a word is assigned to can be considered as one of its senses . 
	</s>
	

	<s id="14">
		 A problem that we see with this approach is that it allows only as many senses as clusters , thereby limiting the granularity of the meaning space . 
	</s>
	

	<s id="15">
		 This problem is avoided by 
		<ref citStr="Neill ( 2002 )" id="5" label="CEPF" position="2400">
			Neill ( 2002 )
		</ref>
		 who uses local instead of global clustering . 
	</s>
	

	<s id="16">
		 This means , to find the senses of a given word only its close associations are clustered , that is for each word new clusters will be found . 
	</s>
	

	<s id="17">
		 Despite many differences , to our knowledge almost all approaches to sense induction that have been published so far have a common limitation : They rely on global co-occurrence vectors , i.e. on vectors that have been derived from an entire corpus . 
	</s>
	

	<s id="18">
		 Since most words are semantically ambiguous , this means that these vectors reflect the sum of the contextual behavior of a word’s underlying senses , i.e. they are mixtures of all senses occurring in the corpus . 
	</s>
	

	<s id="19">
		 However , since reconstructing the sense vectors from the mixtures is difficult , the question is if we really need to base our work on mixtures or if there is some way to directly observe the contextual behavior of the senses thereby avoiding the mixing beforehand . 
	</s>
	

	<s id="20">
		 In this paper we suggest to look at local instead of global co-occurrence vectors . 
	</s>
	

	<s id="21">
		 As can be seen from human performance , in almost all cases the local context of an ambiguous word is sufficient to disambiguate its sense . 
	</s>
	

	<s id="22">
		 This means that the local context of a word usually carries no ambiguities . 
	</s>
	

	<s id="23">
		 The aim of this paper is to show how this observation whose application tends to severely suffer from the sparse-data problem can be successfully exploited for word sense induction . 
	</s>
	

	<s id="24">
		 2 Approach The basic idea is that we do not cluster the global co-occurrence vectors of the words ( based on an entire corpus ) but local ones which are derived from the contexts of a single word . 
	</s>
	

	<s id="25">
		 That is , our computations are based on the concordance of a word . 
	</s>
	

	<s id="26">
		 Also , we do not consider a term/term but a term/context matrix . 
	</s>
	

	<s id="27">
		 This means , for each word that we want to analyze we get an entire matrix . 
	</s>
	

	<s id="28">
		 Let us exemplify this using the ambiguous word palm with its tree and hand senses . 
	</s>
	

	<s id="29">
		 If we assume that our corpus has six occurrences of palm , i.e. there are six local contexts , then we can derive six local co-occurrence vectors for palm . 
	</s>
	

	<s id="30">
		 Considering only strong associations to palm , these vectors could , for example , look as shown in table 1 . 
	</s>
	

	<s id="31">
		 The dots in the matrix indicate if the respective word occurs in a context or not . 
	</s>
	

	<s id="32">
		 We use binary vectors since we assume short contexts where words usually occur only once . 
	</s>
	

	<s id="33">
		 By looking at the matrix it is easy to see that contexts c1 , c3 , and c6 seem to relate to the hand sense of palm , whereas contexts c2 , c4 , and c5 relate to its tree sense . 
	</s>
	

	<s id="34">
		 Our intuitions can be resembled by using a method for computing vector similarities , for example the cosine coefficient or the ( binary ) Jaccard-measure . 
	</s>
	

	<s id="35">
		 If we then apply an appropriate clustering algorithm to the context vectors , we should obtain the two expected clusters . 
	</s>
	

	<s id="36">
		 Each of the two clusters corresponds to one of the senses of palm , and the words closest to the geometric centers of the clusters should be good descriptors of each sense . 
	</s>
	

	<s id="37">
		 However , as matrices of the above type can be extremely sparse , clustering is a difficult task , and common algorithms often deliver sub-optimal results . 
	</s>
	

	<s id="38">
		 Fortunately , the problem of matrix sparseness can be minimized by reducing the dimensionality of the matrix . 
	</s>
	

	<s id="39">
		 An appropriate algebraic method that has the capability to reduce the dimensionality of a rectangular or square matrix in an optimal way is singular value decomposition ( SVD ) . 
	</s>
	

	<s id="40">
		 As shown by Schütze ( 1997 ) by reducing the dimensionality a generalization effect can be achieved that often improves the results . 
	</s>
	

	<s id="41">
		 The approach that we suggest in this paper involves reducing the number of columns ( contexts ) and then applying a clustering algorithm to the row vectors ( words ) of the resulting matrix . 
	</s>
	

	<s id="42">
		 This works well since it is a strength of SVD to reduce the effects of sampling errors and to close gaps in the data . 
	</s>
	

	<s id="43">
		 c1 c2 c3 c4 c5 c6 arm • • beach • • coconut • • • finger • • hand • • • shoulder • • tree • • Table 1 : Term/context matrix for the word palm . 
	</s>
	

	<s id="44">
		 3 Algorithm As in previous work 
		<ref citStr="Rapp , 2002" id="6" label="CEPF" position="6737">
			( Rapp , 2002 )
		</ref>
		 , our computations are based on a partially lemmatized version of the British National Corpus ( BNC ) which has the function words removed . 
	</s>
	

	<s id="45">
		 Starting from the list of 12 ambiguous words provided by 
		<ref citStr="Yarowsky ( 1995 )" id="7" label="CEPF" position="6962">
			Yarowsky ( 1995 )
		</ref>
		 which is shown in table 2 , we created a concordance for each word , with the lines in the concordances each relating to a context window of ±20 words . 
	</s>
	

	<s id="46">
		 From the concordances we computed 12 term/context-matrices ( analogous to table 1 ) whose binary entries indicate if a word occurs in a particular context or not . 
	</s>
	

	<s id="47">
		 Assuming that the amount of information that a context word pro vides depends on its association strength to the ambiguous word , in each matrix we removed all words that are not among the top 30 first order associations to the ambiguous word . 
	</s>
	

	<s id="48">
		 These top 30 associations were computed fully automatically based on the log-likelihood ratio . 
	</s>
	

	<s id="49">
		 We used the procedure described in 
		<ref citStr="Rapp ( 2002 )" id="8" label="CERF" position="7706">
			Rapp ( 2002 )
		</ref>
		 , with the only modification being the multiplication of the log- likelihood values with a triangular function that depends on the logarithm of a word’s frequency . 
	</s>
	

	<s id="50">
		 This way preference is given to words that are in the middle of the frequency range . 
	</s>
	

	<s id="51">
		 Figures 1 to 3 are based on the association lists for the words palm and poach . 
	</s>
	

	<s id="52">
		 Given that our term/context matrices are very sparse with each of their individual entries seeming somewhat arbitrary , it is necessary to detect the regularities in the patterns . 
	</s>
	

	<s id="53">
		 For this purpose we applied the SVD to each of the matrices , thereby reducing their number of columns to the three main dimensions . 
	</s>
	

	<s id="54">
		 This number of dimensions may seem low . 
	</s>
	

	<s id="55">
		 However , it turned out that with our relatively small matrices ( matrix size is the occurrence frequency of a word times the number of associations considered ) it was sometimes not possible to compute more than three singular values , as there are dependencies in the data . 
	</s>
	

	<s id="56">
		 Therefore , we decided to use three dimensions for all matrices . 
	</s>
	

	<s id="57">
		 The last step in our procedure involves applying a clustering algorithm to the 30 words in each matrix . 
	</s>
	

	<s id="58">
		 For our condensed matrices of 3 rows and 30 columns this is a rather simple task . 
	</s>
	

	<s id="59">
		 We decided to use the hierarchical clustering algorithm readily available in the MATLAB ( MATrix LABoratory ) programming language . 
	</s>
	

	<s id="60">
		 After some testing with various similarity functions and linkage types , we finally opted for the cosine coefficient and single linkage which is the combination that apparently gave the best results . 
	</s>
	

	<s id="61">
		 axes : grid/tools bass : fish/music crane : bird/machine drug : medicine/narcotic duty : tax/obligation motion : legal/physical palm : tree/hand plant : living/factory poach : steal/boil sake : benefit/drink space : volume/outer tank : vehicle/container Table 2 : Ambiguous words and their senses . 
	</s>
	

	<s id="62">
		 4 Results Before we proceed to a quantitative evaluation , by looking at a few examples let us first give a qualitative impression of some results and consider the contribution of SVD to the performance of our algorithm . 
	</s>
	

	<s id="63">
		 Figure 1 shows a dendrogram for the word palm ( corpus frequency in the lemmatized BNC : 2054 ) as obtained after applying the algo- rithm described in the previous section , with the only modification that the SVD step was omitted , i.e. no dimensionality reduction was performed . 
	</s>
	

	<s id="64">
		 The horizontal axes in the dendrogram is dissimilarity ( 1 – cosine ) , i.e. 0 means identical items and 1 means no similarity . 
	</s>
	

	<s id="65">
		 The vertical axes has no special meaning . 
	</s>
	

	<s id="66">
		 Only the order of the words is chosen in such a way that line crossings are avoided when connecting clusters . 
	</s>
	

	<s id="67">
		 As we can see , the dissimilarities among the top 30 associations to palm are all in the upper half of the scale and not very distinct . 
	</s>
	

	<s id="68">
		 The two expected clusters for palm , one relating to its hand and the other to its tree sense , have essentially been found . 
	</s>
	

	<s id="69">
		 According to our judgment , all words in the upper branch of the hierarchical tree are related to the hand sense of palm , and all other words are related to its tree sense . 
	</s>
	

	<s id="70">
		 However , it is somewhat unsatisfactory that the word frond seems equally similar to both senses , whereas intuitively we would clearly put it in the tree section . 
	</s>
	

	<s id="71">
		 Let us now compare figure 1 to figure 2 which has been generated using exactly the same procedure with the only difference that the SVD step ( reduction to 3 dimensions ) has been conducted in this case . 
	</s>
	

	<s id="72">
		 In figure 2 the similarities are generally at a higher level ( dissimilarities lower ) , the relative differences are bigger , and the two expected clusters are much more salient . 
	</s>
	

	<s id="73">
		 Also , the word frond is now well within the tree cluster . 
	</s>
	

	<s id="74">
		 Obviously , figure 2 reflects human intuitions better than figure 1 , and we can conclude that SVD was able to find the right generalizations . 
	</s>
	

	<s id="75">
		 Although space constraints prevent us from showing similar comparative diagrams for other words , we hope that this novel way of comparing dendrograms makes it clearer what the virtues of SVD are , and that it is more than just another method for smoothing . 
	</s>
	

	<s id="76">
		 Our next example ( figure 3 ) is the dendrogram for poach ( corpus frequency : 458 ) . 
	</s>
	

	<s id="77">
		 It is also based on a matrix that had been reduced to 3 dimensions . 
	</s>
	

	<s id="78">
		 The two main clusters nicely distinguish between the two senses of poach , namely boil and steal . 
	</s>
	

	<s id="79">
		 The upper branch of the hierarchical tree consists of words related to cooking , the lower one mainly contains words related to the unauthorized killing of wildlife in Africa which apparently is an important topic in the BNC. . 
	</s>
	

	<s id="80">
		 Figure 3 nicely demonstrates what distinguishes the clustering of local contexts from the clustering of global co-occurrence vectors . 
	</s>
	

	<s id="81">
		 To see this , let us bring our attention to the various species of animals that are among the top 30 associations to poach . 
	</s>
	

	<s id="82">
		 Some of them seem more often affected by cooking ( pheasant , chicken , salmon ) , others by poaching ( elephant , tiger , rhino ) . 
	</s>
	

	<s id="83">
		 According to the diagram only the rabbit is equally suitable for both activities , although fortunately its affinity to cooking is lower than it is for the chicken , and to poaching it is lower than it is for the rhino . 
	</s>
	

	<s id="84">
		 That is , by clustering local contexts our algorithm was able to separate the different kinds of animals according to their relationship to poach . 
	</s>
	

	<s id="85">
		 If we instead clustered global vectors , it would most likely be impossible to obtain this separation , as from a global perspective all animals have most properties ( context words ) in common , so they are likely to end up in a single cluster . 
	</s>
	

	<s id="86">
		 Note that what we exemplified here for animals applies to all linkage decisions made by the algorithm , i.e. all decisions must be seen from the perspective of the ambiguous word . 
	</s>
	

	<s id="87">
		 This implies that often the clustering may be counterintuitive from the global perspective that as humans we tend to have when looking at isolated words . 
	</s>
	

	<s id="88">
		 That is , the clusters shown in figures 2 and 3 can only be understood if the ambiguous words they are derived from are known . 
	</s>
	

	<s id="89">
		 However , this is exactly what we want in sense induction . 
	</s>
	

	<s id="90">
		 In an attempt to provide a quantitative evaluation of our results , for each of the 12 ambiguous words shown in table 1 we manually assigned the top 30 first-order associations to one of the two senses provided by 
		<ref citStr="Yarowsky ( 1995 )" id="9" label="CEPF" position="14417">
			Yarowsky ( 1995 )
		</ref>
		 . 
	</s>
	

	<s id="91">
		 We then looked at the first split in our hierarchical trees and assigned each of the two clusters to one of the given senses . 
	</s>
	

	<s id="92">
		 In no case was there any doubt on which way round to assign the two clusters to the two given senses . 
	</s>
	

	<s id="93">
		 Finally , we checked if there were any misclassified items in the clusters . 
	</s>
	

	<s id="94">
		 According to this judgment , on average 25.7 of the 30 items were correctly classified , and 4.3 items were misclassified . 
	</s>
	

	<s id="95">
		 This gives an overall accuracy of 85.6 % . 
	</s>
	

	<s id="96">
		 Reasons for misclassifications include the following : Some of the top 30 associations are more or less neutral towards the senses , so even for us it was not always possible to clearly assign them to one of the two senses . 
	</s>
	

	<s id="97">
		 In other cases , outliers led to a poor first split , like if in figure 1 the first split would be located between frond and the rest of the vocabulary . 
	</s>
	

	<s id="98">
		 In the case of sake the beverage sense is extremely rare in the BNC and therefore was not represented among the top 30 associations . 
	</s>
	

	<s id="99">
		 For this reason the clustering algorithm had no chance to find the expected clusters . 
	</s>
	

	<s id="100">
		 5 Conclusions and prospects From the observations described above we conclude that avoiding the mixture of senses , i.e. clustering local context vectors instead of global co-occurrence vectors , is a good way to deal with the problem of word sense induction . 
	</s>
	

	<s id="101">
		 However , there is a pitfall , as the matrices of local vectors are extremely sparse . 
	</s>
	

	<s id="102">
		 Fortunately , our simulations suggest that computing the main dimensions of a matrix through SVD solves the problem of sparseness and greatly improves clustering results . 
	</s>
	

	<s id="103">
		 Although the results that we presented in this paper seem useful even for practical purposes , we can not claim that our algorithm is capable of finding all the fine grained distinctions that are listed in manually created dictionaries such as the Longman Dictionary of Contemporary English ( LDOCE ) , or in lexical databases such as WordNet . 
	</s>
	

	<s id="104">
		 For future improvement of the algorithm we see two main possibilities : 1 ) Considering all context words instead of only the top 30 associations would further reduce the sparse data problem . 
	</s>
	

	<s id="105">
		 However , this requires finding an appropriate association function . 
	</s>
	

	<s id="106">
		 This is difficult , as for example the log-likelihood ratio , although delivering almost perfect rankings , has an inappropriate value characteristic : The increase in computed strengths is over-proportional for stronger associations . 
	</s>
	

	<s id="107">
		 This prevents the SVD from finding optimal dimensions . 
	</s>
	

	<s id="108">
		 2 ) The principle of avoiding mixtures can be applied more consequently if not only local instead of global vectors are used , but if also the parts of speech of the context words are considered . 
	</s>
	

	<s id="109">
		 By operating on a part-of-speech tagged corpus those sense distinctions that have an effect on part of speech can be taken into account . 
	</s>
	

	<s id="110">
		 Acknowledgements I would like to thank Manfred Wettler , Robert Dale , Hinrich Schütze , and Raz Tamir for help and discussions , and the DFG for financial support . 
	</s>
	

	<s id="111">
		 References Neill , D. B. ( 2002 ) . 
	</s>
	

	<s id="112">
		 Fully Automatic Word Sense Induction by Semantic Clustering . 
	</s>
	

	<s id="113">
		 Cambridge University , Master’s Thesis , M.Phil . 
	</s>
	

	<s id="114">
		 in Computer Speech . 
	</s>
	

	<s id="115">
		 Pantel , P. ; Lin , D. ( 2002 ) . 
	</s>
	

	<s id="116">
		 Discovering word senses from text . 
	</s>
	

	<s id="117">
		 In : Proceedings of ACM SIGKDD , Edmonton , 613–619 . 
	</s>
	

	<s id="118">
		 Rapp , R. ( 2002 ) . 
	</s>
	

	<s id="119">
		 The computation of word associations : comparing syntagmatic and paradigmatic approaches . 
	</s>
	

	<s id="120">
		 Proc . 
	</s>
	

	<s id="121">
		 of 19th COLING , Taipei , ROC , Vol. 2 , 821–827 . 
	</s>
	

	<s id="122">
		 Rapp , R. ( 2003 ) . 
	</s>
	

	<s id="123">
		 Word sense discovery based on sense descriptor dissimilarity . 
	</s>
	

	<s id="124">
		 In : Ninth Machine Translation Summit , New Orleans , 315–322 . 
	</s>
	

	<s id="125">
		 Schütze , H. ( 1997 ) . 
	</s>
	

	<s id="126">
		 Ambiguity Resolution in Language Learning : Computational and Cognitive Models . 
	</s>
	

	<s id="127">
		 Stanford : CSLI Publications . 
	</s>
	

	<s id="128">
		 Yarowsky , D. ( 1995 ) . 
	</s>
	

	<s id="129">
		 Unsupervised word sense disambiguation rivaling supervised methods . 
	</s>
	

	<s id="130">
		 In : Proc . 
	</s>
	

	<s id="131">
		 of 33rd ACL , Cambridge , MA , 189–196 . 
	</s>
	

	<s id="132">
		 Figure 1 : Clustering results for palm without SVD . 
	</s>
	

	<s id="133">
		 Figure 2 : Clustering results for palm with SVD . 
	</s>
	

	<s id="134">
		 Figure 3 : Clustering results for poach with SVD . 
	</s>
	


</acldoc>
