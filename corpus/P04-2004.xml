<?xml version="1.0" encoding="iso-8859-1"?>
<acldoc acl_id="P04-2004">
	

	<s id="1">
		 Temporal Context : Applications and Implications for Computational Linguistics Robert A. Liebscher Department of Cognitive Science University of California , San Diego La Jolla , CA 92037 rliebsch@cogsci.ucsd.edu Abstract This paper describes several ongoing projects that are united by the theme of changes in lexical use over time . 
	</s>
	

	<s id="2">
		 We show that paying attention to a document’s temporal context can lead to improvements in information retrieval and text categorization . 
	</s>
	

	<s id="3">
		 We also explore a potential application in document clustering that is based upon different types of lexical changes . 
	</s>
	

	<s id="4">
		 1 Introduction Tasks in computational linguistics ( CL ) normally focus on the content of a document while paying little attention to the context in which it was produced . 
	</s>
	

	<s id="5">
		 The work described in this paper considers the importance of temporal context . 
	</s>
	

	<s id="6">
		 We show that knowing one small piece of information–a document’s publication date–can be beneficial for a variety of CL tasks , some familiar and some novel . 
	</s>
	

	<s id="7">
		 The field of historical linguistics attempts to categorize changes at all levels of language use , typically relying on data that span centuries 
		<ref citStr="Hock , 1991" id="1" label="CEPF" position="1228">
			( Hock , 1991 )
		</ref>
		 . 
	</s>
	

	<s id="8">
		 The recent availability of very large textual corpora allows for the examination of changes that take place across shorter time periods . 
	</s>
	

	<s id="9">
		 In particular , we focus on lexical change across decades in corpora of academic publications and show that the changes can be fairly dramatic during a relatively short period of time . 
	</s>
	

	<s id="10">
		 As a preview , consider Table 1 , which lists the top five unigrams that best distinguished the field of computational linguistics at different points in time , as derived from the ACL proceedings 1 using the odds ratio measure ( see Section 3 ) . 
	</s>
	

	<s id="11">
		 One can quickly glean that the field has become increasingly empirical through time . 
	</s>
	

	<s id="12">
		 1979-84 1985-90 1991-96 1997-02 system natural language knowledge database phrase plan structure logical interpret discourse tree word corpus training model data algorithm unification plan Table 1 : ACL’s most characteristic terms for four time periods , as measured by the odds ratio With respect to academic publications , the very nature of the enterprise forces the language used within a discipline to change . 
	</s>
	

	<s id="13">
		 An author’s word choice is shaped by the preceding literature , as she must say something novel while placing her contribution in the context of what has already been said . 
	</s>
	

	<s id="14">
		 This begets neologisms , new word senses , and other types of changes . 
	</s>
	

	<s id="15">
		 This paper is organized as follows : In Section 2 , we introduce temporal term weighting , a technique that implicitly encodes time into keyword weights to enhance information retrieval . 
	</s>
	

	<s id="16">
		 Section 3 describes the technique of temporalfeature modification , which exploits temporal information to improve the text categorization task . 
	</s>
	

	<s id="17">
		 Section 4 introduces several types of lexical changes and a potential application in document clustering . 
	</s>
	

	<s id="18">
		 1 The details of each corpus used in this paper can be found in the appendix . 
	</s>
	

	<s id="19">
		 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 Year Figure 1 : Changing frequencies in AI abstracts 2 Time in information retrieval In the task of retrieving relevant documents based upon keyword queries , it is customary to treat each document as a vector of terms with associated “weights” . 
	</s>
	

	<s id="20">
		 One notion of term weight simply counts the occurrences of each term . 
	</s>
	

	<s id="21">
		 Of more utility is the scheme known as term frequency-inverse document frequency ( TF.IDF ) : where is the weight of term k in document d , is the frequency of k in d , N is the total num- ber of documents in the corpus , and is the total number of documents containing k . 
	</s>
	

	<s id="22">
		 Very frequent terms ( such as function words ) that occur in many documents are downweighted , while those that are fairly unique have their weights boosted . 
	</s>
	

	<s id="23">
		 Many variations of TF.IDF have been suggested 
		<ref citStr="Singhal , 1997" id="2" label="CEPF" position="4094">
			( Singhal , 1997 )
		</ref>
		 . 
	</s>
	

	<s id="24">
		 Our variation , temporal term weighting ( TTW ) , incorporates a term’s IDF at different points in time : Under this scheme , the document collection is divided into T time slices , and N and are computed for each slice t . 
	</s>
	

	<s id="25">
		 Figure 1 illustrates why such a modification is useful . 
	</s>
	

	<s id="26">
		 It depicts the frequency of the terms neural networks and expert system for each year in a collection of Artificial Intelligence-related dissertation abstracts . 
	</s>
	

	<s id="27">
		 Both terms follow a fairly linear trend , moving in opposite directions . 
	</s>
	

	<s id="28">
		 As was demonstrated for CL in Section 1 , the terms which best characterize AI have also changed through time . 
	</s>
	

	<s id="29">
		 Table 2 lists the top five “rising” and “falling” bigrams in this corpus , along with their least-squares fit to a linear trend . 
	</s>
	

	<s id="30">
		 Lexical variants ( such as plurals ) are omitted . 
	</s>
	

	<s id="31">
		 Using an atemporal TF.IDF , both rising and falling terms would be assigned weights proportional only to . 
	</s>
	

	<s id="32">
		 A novice user issuing a query would be given a temporally random scattering of documents , some of which might be state-of-the- art , others very outdated . 
	</s>
	

	<s id="33">
		 But with TTW , the weights are proportional to the collective “community interest” in the term at a given point in time . 
	</s>
	

	<s id="34">
		 In academic research documents , this yields two benefits . 
	</s>
	

	<s id="35">
		 If a term rises from obscurity to popularity over the duration of a corpus , it is not unreasonable to assume that this term originated in one or a few seminal articles . 
	</s>
	

	<s id="36">
		 The term is not very frequent across documents when these articles are published , so its weight in the seminal articles will be amplified . 
	</s>
	

	<s id="37">
		 Similarly , the term will be downweighted in articles when it has become ubiquitous throughout the literature . 
	</s>
	

	<s id="38">
		 For a falling term , its weight in early documents will be dampened , while its later use will be emphasized . 
	</s>
	

	<s id="39">
		 If a term is very frequent in a document after it has been relegated to obscurity , this is likely to be an historical review article . 
	</s>
	

	<s id="40">
		 Such an article would be a good place to start an investigation for someone who is unfamiliar with the term . 
	</s>
	

	<s id="41">
		 Term r neural network 0.9283 fuzzy logic 0.9035 genetic algorithm 0.9624 real world 0.8509 reinforcement learning 0.8447 artificial intelligence -0.9309 expert system -0.9241 knowledge base -0.9144 problem solving -0.9490 knowledge representation -0.9603 Table 2 : Rising and falling AI terms , 1986-1997 neural networks expert system 4 3.5 3 2.5 2 1.5 1 0.5 2.1 Future work We have discovered clear frequency trends over time in several corpora . 
	</s>
	

	<s id="42">
		 Given this , TTW seems beneficial for use in information retrieval , but is in an embryonic stage . 
	</s>
	

	<s id="43">
		 The next step will be the development and implementation of empirical tests . 
	</s>
	

	<s id="44">
		 IR systems typically are evaluated by measures such as precision and recall , but a different test is necessary to compare TTW to an atemporal TF.IDF . 
	</s>
	

	<s id="45">
		 One idea we are exploring is to have a system explicitly tag seminal and historical review articles that are centered around a query term , and then compare the results with those generated by bibliometric methods . 
	</s>
	

	<s id="46">
		 Few bibliometric analyses have gone beyond examinations of citation networks and the keywords associated with each article . 
	</s>
	

	<s id="47">
		 We would consider the entire text . 
	</s>
	

	<s id="48">
		 3 Time in text categorization Text categorization ( TC ) is the problem of assigning documents to one or more pre-defined categories . 
	</s>
	

	<s id="49">
		 As Section 2 demonstrated , the terms which best characterize a category can change through time , so intelligent use of temporal context may prove useful in TC . 
	</s>
	

	<s id="50">
		 Consider the example of sorting newswire documents into the categories ENTERTAINMENT , BUSINESS , SPORTS , POLITICS , and WEATHER . 
	</s>
	

	<s id="51">
		 Suppose we come across the term athens in a training document . 
	</s>
	

	<s id="52">
		 We might expect a fairly uniform distribution of this term throughout the five categories ; that is , C athens = 0.20 for each C . 
	</s>
	

	<s id="53">
		 However , in the summer of 2004 , we would expect SPORTS athens to be greatly increased relative to the other categories due to the city’s hosting of the Olympic games . 
	</s>
	

	<s id="54">
		 Documents with “temporally perturbed” terms like athens contain potentially valuable information , but this is lost in a statistical analysis based purely on the content of each document , irrespective of its temporal context . 
	</s>
	

	<s id="55">
		 This information can be recovered with a technique we call temporal feature modification ( TFM ) . 
	</s>
	

	<s id="56">
		 We first outline a formal model of its use . 
	</s>
	

	<s id="57">
		 Each term k is assumed to have a generator Gk that produces a “true” distribution C k across all categories . 
	</s>
	

	<s id="58">
		 External events at time y can per turb k’s generator , causing C k to be differ- ent relative to the background C k computed over the entire corpus . 
	</s>
	

	<s id="59">
		 If the perturbation is significant , we want to separate the instances of k at time y from all other instances . 
	</s>
	

	<s id="60">
		 We thus treat athens and “athens+summer2004” as though they were actually different terms , because they came from two different generators . 
	</s>
	

	<s id="61">
		 TFM is a two step process that is captured by this pseudocode : VOCABULARY ADDITIONS : for each class C : for each year y : PreModList(C,y,L) = OddsRatio(C,y,L) ModifyList(y) = DecisionRule(PreModList(C,y,L)) for each term k in ModifyList(y) : Add pseudo-term &quot; k+y &quot; to Vocab DOCUMENT MODIFICATIONS : for each document : y = year of doc for each term k : if &quot; k+y &quot; in Vocab : replace k with &quot; k+y &quot; classify modified document PreModList(C,y,L) is a list of the top L lexemes that , by the odds ratio measure 2 , are highly associated with category C in year y . 
	</s>
	

	<s id="62">
		 We test the hypothesis that these come from a perturbed generator in year y , as opposed to the atemporal generator Gk , by comparing the odds ratios of term- category pairs in a PreModList in year y with the same pairs across the entire corpus . 
	</s>
	

	<s id="63">
		 Terms which pass this test are added to the final ModifyList(y) for year y . 
	</s>
	

	<s id="64">
		 For the results that we report , Decision- Rule is a simple ratio test with threshold factor f . 
	</s>
	

	<s id="65">
		 Suppose f is 2.0 : if the odds ratio between C and k is twice as great in year y as it is atemporally , the decision rule is “passed” . 
	</s>
	

	<s id="66">
		 The generator Gkis considered perturbed in year y and k is added to ModifyList(y) . 
	</s>
	

	<s id="67">
		 In the training and testing phases , the documents are modified so that a term k is replaced with the pseudo-term “k+y” if it passed the ratio test . 
	</s>
	

	<s id="68">
		 3.1 ACM Classifications We tested TFM on corpora representing genres from academic publications to Usenet postings , 2Odds ratio is defined as , wherep is Pr(k|C) , the probability that term k is present given category C , and q is Pr(k|!C) . 
	</s>
	

	<s id="69">
		 Corpus Vocab size No . 
	</s>
	

	<s id="70">
		 docs No . 
	</s>
	

	<s id="71">
		 cats SIGCHI 4542 1910 20 SIGPLAN 6744 3123 22 DAC 6311 2707 20 Table 3 : Corpora characteristics . 
	</s>
	

	<s id="72">
		 Terms occurring at least twice are included in the vocabulary . 
	</s>
	

	<s id="73">
		 and it improved classification accuracy in every case . 
	</s>
	

	<s id="74">
		 The results reported here are for abstracts from the proceedings of several of the Association for Computing Machinery’s conferences : SIGCHI , SIGPLAN , and DAC . 
	</s>
	

	<s id="75">
		 TFM can benefit the ACM community through retrospective categorization in two ways : ( 1 ) 7.73 % of abstracts ( nearly 6000 ) across the entire ACM corpus that are expected to have category labels do not have them ; ( 2 ) When a group of terms becomes popular enough to induce the formation of a new category , a frequent occurrence in the computing literature , TFM would separate the “old” uses from the “new” ones . 
	</s>
	

	<s id="76">
		 The ACM classifies its documents in a hierarchy of four levels ; we used an aggregating procedure to “flatten” these . 
	</s>
	

	<s id="77">
		 The characteristics of each corpus are described in Table 3 . 
	</s>
	

	<s id="78">
		 The “TC minutiae” used in these experiments are : Stoplist , Porter stemming , 90/10 % train/test split , Laplacian smoothing . 
	</s>
	

	<s id="79">
		 Parameters such as type of classifier ( Naïve Bayes , KNN , TF.IDF , Probabilistic indexing ) and threshold factor f were varied . 
	</s>
	

	<s id="80">
		 3.2 Results Figure 2 shows the improvement in classification accuracy for different percentages of terms modified , using the best parameter combinations for each corpus , which are noted in Table 4 . 
	</s>
	

	<s id="81">
		 A baseline of 0.0 indicates accuracy without any temporal modifications . 
	</s>
	

	<s id="82">
		 Despite the relative paucity of data in terms of document length , TFM still performs well on the abstracts . 
	</s>
	

	<s id="83">
		 The actual accuracies when no terms are modified are less than stellar , ranging from 30.7 % ( DAC ) to 33.7 % ( SIGPLAN ) when averaged across all conditions , due to the difficulty of the task ( 20-22 categories ; each document can only belong to one ) . 
	</s>
	

	<s id="84">
		 Our aim is simply to show improvement . 
	</s>
	

	<s id="85">
		 In most cases , the technique performs best when ^0.05 0 5 10 15 20 25 Percent terms modified Figure 2 : Improvement in categorization performance with TFM , using the best parameter combinations for each corpus making relatively few modifications : the left side of Figure 2 shows a rapid performance increase , particularly for SIGCHI , followed by a period of diminishing returns as more terms are modified . 
	</s>
	

	<s id="86">
		 After requiring the one-time computation of odds ratios in the training set for each category/year , TFM is very fast and requires negligible extra storage space . 
	</s>
	

	<s id="87">
		 3.3 Future work The “bare bones” version of TFM presented here is intended as a proof-of-concept . 
	</s>
	

	<s id="88">
		 Many of the parameters and procedures can be set arbitrarily . 
	</s>
	

	<s id="89">
		 For initial feature selection , we used odds ratio because it exhibits good performance in TC 
		<ref citStr="Mladenic , 1998" id="3" label="CEPF" position="14003">
			( Mladenic , 1998 )
		</ref>
		 , but it could be replaced by another method such as information gain . 
	</s>
	

	<s id="90">
		 The ratio test is not a very sophisticated way to choose which terms should be modified , and presently only detects the surges in the use of a term , while ignoring the ( admittedly rare ) declines . 
	</s>
	

	<s id="91">
		 Using TFM on a Usenet corpus that was more balanced in terms of documents per category and per year , we found that allowing different terms to “compete” for modification was more effective than the egalitarian practice of choosing L terms from each category/year . 
	</s>
	

	<s id="92">
		 There is no reason to believe that each category/year is equally likely to contribute temporally perturbed terms . 
	</s>
	

	<s id="93">
		 Finally , we would like to exploit temporal con- 0.45 SIGCHI 0.4 0.35 0.3 DAC 0.25 0.2 0.15 SIGPLAN 0.1 0.05 Atemporal baseline 0 Corpus Improvement Classifier n-gram size Vocab frequency min . 
	</s>
	

	<s id="94">
		 Ratio threshold f SIGCHI 41.0 % TF.IDF Bigram 10 1.0 SIGPLAN 19.4 % KNN Unigram 10 1.0 DAC 23.3 % KNN Unigram 2 1.0 Table 4 : Top parameter combinations for TFM by improvement in classification accuracy . 
	</s>
	

	<s id="95">
		 Vocab frequency min . 
	</s>
	

	<s id="96">
		 is the minimum number of times a term must appear in the corpus in order to be included . 
	</s>
	

	<s id="97">
		 tiguity . 
	</s>
	

	<s id="98">
		 The present implementation treats time slices as independent entities , which precludes the possibility of discovering temporal trends in the data . 
	</s>
	

	<s id="99">
		 One way to incorporate trends implicitly is to run a smoothing filter across the temporally aligned frequencies . 
	</s>
	

	<s id="100">
		 Also , we treat each slice at annual resolution . 
	</s>
	

	<s id="101">
		 Initial tests show that aggregating two or more years into one slice improves performance for some corpora , particularly those with temporally sparse data such as DAC . 
	</s>
	

	<s id="102">
		 4 Future work A third part of this research program , presently in the exploratory stage , concerns lexical ( semantic ) change , the broad class of phenomena in which words and phrases are coined or take on new meanings 
		<ref citStr="Bauer , 1994" id="4" label="CEPF" position="16002">
			( Bauer , 1994 
		</ref>
		<ref citStr="Jeffers and Lehiste , 1979" id="5" label="CEPF" position="16017">
			; Jeffers and Lehiste , 1979 )
		</ref>
		 . 
	</s>
	

	<s id="103">
		 Below we describe an application in document clustering and point toward a theoretical framework for lexical change based upon recent advances in network analysis . 
	</s>
	

	<s id="104">
		 Consider a scenario in which a user queries a document database for the term artificial intelligence . 
	</s>
	

	<s id="105">
		 We would like to create a system that will cluster the returned documents into three categories , corresponding to the types of change the query has undergone . 
	</s>
	

	<s id="106">
		 These responses illustrate the three categories , which are not necessarily mutually exclusive : 1 . 
	</s>
	

	<s id="107">
		 “This term is now more commonly referred to as AI in this collection” , 2 . 
	</s>
	

	<s id="108">
		 “These documents are about artificial intelligence , though it is now more commonly called machine learning” , 3 . 
	</s>
	

	<s id="109">
		 “The following documents are about artificial intelligence , though in this collection its use has become tacit” . 
	</s>
	

	<s id="110">
		 Figure 3 : Frequencies in the first ( left bar ) and second ( right bar ) halves of an AI discussion forum 4.1 Acronym formation In Section 2 , we introduced the notions of “rising” and “falling” terms . 
	</s>
	

	<s id="111">
		 Figure 3 shows relative frequencies of two common terms and their acronyms in the first and second halves of a corpus of AI discussion board postings collected from 1983-1988 . 
	</s>
	

	<s id="112">
		 While the acronyms increased in frequency , the expanded forms decreased or remained the same . 
	</s>
	

	<s id="113">
		 A reasonable conjecture is that in this informal register , the acronyms AI and CS largely replaced the expansions . 
	</s>
	

	<s id="114">
		 During the same time period , the more formal register of dissertation abstracts did not show this pattern for any acronym/expansion pairs . 
	</s>
	

	<s id="115">
		 4.2 Lexical replacement Terms can be replaced by their acronyms , or by other terms . 
	</s>
	

	<s id="116">
		 In Table 1 , database was listed among the top five terms that were most characteristic of the ACL proceedings in 1979- 1984 . 
	</s>
	

	<s id="117">
		 Bisecting this time slice and including bi- 3.5 2.5 0.5 1.5 4 3 2 0 1 1 2 3 4 AI artificial intelligence CS computer science grams in the analysis , data base ranks higher than database in 1979-1981 , but drops much lower in 1982-1984 . 
	</s>
	

	<s id="118">
		 Within this brief period of time , we see a lexical replacement event taking hold . 
	</s>
	

	<s id="119">
		 In the AI dissertation abstracts , artificial intelligence shows the greatest decline , while the conceptually similar terms machine learning and pattern recognition rank sixth and twelfth among the top rising terms . 
	</s>
	

	<s id="120">
		 There are social , geographic , and linguistic forces that influence lexical change . 
	</s>
	

	<s id="121">
		 One example stood out as having an easily identified cause : political correctness . 
	</s>
	

	<s id="122">
		 In a corpus of dissertation abstracts on communication disorders from 1982- 2002 , the term subject showed the greatest relative decrease in frequency , while participant showed the greatest increase . 
	</s>
	

	<s id="123">
		 Among the top ten bigrams showing the sharpest declines were three terms that included the word impaired and two that included disabled . 
	</s>
	

	<s id="124">
		 4.3 “Tacit” vocabulary Another , more subtle lexical change involves the gradual disappearance of terms due to their increasingly “tacit” nature within a particular community of discourse . 
	</s>
	

	<s id="125">
		 Their existence becomes so obvious that they need not be mentioned within the community , but would be necessary for an outsider to fully understand the discourse . 
	</s>
	

	<s id="126">
		 Take , for example , the terms backpropagation and hidden layer . 
	</s>
	

	<s id="127">
		 If a researcher of neural networks uses these terms in an abstract , then neural network does not even warrant printing , because they have come to imply the presence of neural network within this research community . 
	</s>
	

	<s id="128">
		 Applied to IR , one might call this “retrieval by implication” . 
	</s>
	

	<s id="129">
		 Discovering tacit terms is no simple matter , as many of them will not follow simple is-a relationships ( e.g. terrier is a dog ) . 
	</s>
	

	<s id="130">
		 The example of the previous paragraph seems to contain a hierarchical relation , but it is difficult to define . 
	</s>
	

	<s id="131">
		 We believe that examining the temporal trajectories of closely related networks of terms may be of use here , and is also part of a more general project that we hope to undertake . 
	</s>
	

	<s id="132">
		 Our intention is to improve existing models of lexical change using recent advances in network analysis 
		<ref citStr="Barabasi et al. , 2002" id="6" label="CEPF" position="20404">
			( Barabasi et al. , 2002 
		</ref>
		<ref citStr="Dorogovtsev and Mendes , 2001" id="7" label="CEPF" position="20429">
			; Dorogovtsev and Mendes , 2001 )
		</ref>
		 . 
	</s>
	

	<s id="133">
		 References A. Barabasi , H. Jeong , Z. Neda , A. Schubert , and T. Vi csek . 
	</s>
	

	<s id="134">
		 2002. Evolution of the social network of scientific collaborations . 
	</s>
	

	<s id="135">
		 Physica A , 311:590–614 . 
	</s>
	

	<s id="136">
		 L. Bauer . 
	</s>
	

	<s id="137">
		 1994. Watching English Change . 
	</s>
	

	<s id="138">
		 Longman Press , London . 
	</s>
	

	<s id="139">
		 S. N. Dorogovtsev and J. F. F. Mendes . 
	</s>
	

	<s id="140">
		 2001. Language as an evolving word web. . 
	</s>
	

	<s id="141">
		 Proceedings of The Royal Society ofLondon , Series B , 268(1485):2603– 2606 . 
	</s>
	

	<s id="142">
		 H. H. Hock . 
	</s>
	

	<s id="143">
		 1991. Principles ofHistoricalLingusitics . 
	</s>
	

	<s id="144">
		 Mouton de Gruyter , Berlin . 
	</s>
	

	<s id="145">
		 R. J. Jeffers and I. Lehiste . 
	</s>
	

	<s id="146">
		 1979. Principles and MethodsforHistorical Lingusitics . 
	</s>
	

	<s id="147">
		 The MIT Press , Cambridge , MA . 
	</s>
	

	<s id="148">
		 D. Mladenic . 
	</s>
	

	<s id="149">
		 1998. Machine Learning on non- homogeneous , distributed text data . 
	</s>
	

	<s id="150">
		 Ph.D . 
	</s>
	

	<s id="151">
		 thesis , University of Ljubljana , Slovenia . 
	</s>
	

	<s id="152">
		 A. Singhal . 
	</s>
	

	<s id="153">
		 1997. Term weighting revisited . 
	</s>
	

	<s id="154">
		 Ph.D . 
	</s>
	

	<s id="155">
		 thesis , Cornell University . 
	</s>
	

	<s id="156">
		 Appendix : Corpora The corpora used in this paper , preceded by the section in which they were introduced : 1 : The annual proceedings of the Association for Computational Linguistics conference ( 1978- 2002 ) . 
	</s>
	

	<s id="157">
		 Accessible at http://acl.ldc.upenn.edu/ . 
	</s>
	

	<s id="158">
		 2 : Over 5000 PhD and Masters dissertation abstracts related to Artificial Intelligence , 1986- 1997 . 
	</s>
	

	<s id="159">
		 Supplied by University Microfilms Inc. 3.1 : Abstracts from the ACM-IEEE Design Automation Conference ( DAC ; 1964-2002 ) , Special Interest Groups in Human Factors in Computing Systems ( SIGCHI ; 1982-2003 ) and Programming Languages ( SIGPLAN ; 1973-2003 ) . 
	</s>
	

	<s id="160">
		 Supplied by the ACM . 
	</s>
	

	<s id="161">
		 See also Table 3 . 
	</s>
	

	<s id="162">
		 3.3 : Hand-collected corpus of six dis- cussion groups : misc.consumers , alt.atheism , rec.arts.books , comp . 
	</s>
	

	<s id="163">
		 { arch , graphics.algorithms , lang.c } . 
	</s>
	

	<s id="164">
		 Each group contains 1000 documents per year from 1993-2002 . 
	</s>
	

	<s id="165">
		 Viewable at http://groups.google.com/ . 
	</s>
	

	<s id="166">
		 4.2 : Over 4000 PhD and Masters dissertation abstracts related to communication disorders , 1982-2002 . 
	</s>
	

	<s id="167">
		 Supplied by University Microfilms Inc. 
	</s>
	


</acldoc>
