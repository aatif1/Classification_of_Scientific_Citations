<?xml version="1.0" encoding="iso-8859-1"?>
<acldoc acl_id="P04-1016">
	

	<s id="1">
		 Convolution Kernels with Feature Selection for Natural Language Processing Tasks Jun Suzuki , Hideki Isozaki and Eisaku Maeda NTT Communication Science Laboratories , NTT Corp. 2-4 Hikaridai , Seika-cho , Soraku-gun , Kyoto,619-0237 Japan { jun , isozaki , maeda}@cslab.kecl.ntt.co.jp Abstract Convolution kernels , such as sequence and tree kernels , are advantageous for both the concept and accuracy of many natural language processing ( NLP ) tasks . 
	</s>
	

	<s id="2">
		 Experiments have , however , shown that the over-fitting problem often arises when these kernels are used in NLP tasks . 
	</s>
	

	<s id="3">
		 This paper discusses this issue of convolution kernels , and then proposes a new approach based on statistical feature selection that avoids this issue . 
	</s>
	

	<s id="4">
		 To enable the proposed method to be executed efficiently , it is embedded into an original kernel calculation process by using sub-structure mining algorithms . 
	</s>
	

	<s id="5">
		 Experiments are undertaken on real NLP tasks to confirm the problem with a conventional method and to compare its performance with that of the proposed method . 
	</s>
	

	<s id="6">
		 1 Introduction Over the past few years , many machine learning methods have been successfully applied to tasks in natural language processing ( NLP ) . 
	</s>
	

	<s id="7">
		 Especially , state-of-the-art performance can be achieved with kernel methods , such as Support Vector Machine 
		<ref citStr="Cortes and Vapnik , 1995" id="1" label="CEPF" position="1402">
			( Cortes and Vapnik , 1995 )
		</ref>
		 . 
	</s>
	

	<s id="8">
		 Examples include text categorization 
		<ref citStr="Joachims , 1998" id="2" label="CEPF" position="1470">
			( Joachims , 1998 )
		</ref>
		 , chunking 
		<ref citStr="Kudo and Matsumoto , 2002" id="3" label="CEPF" position="1511">
			( Kudo and Matsumoto , 2002 )
		</ref>
		 and parsing 
		<ref citStr="Collins and Duffy , 2001" id="4" label="CEPF" position="1552">
			( Collins and Duffy , 2001 )
		</ref>
		 . 
	</s>
	

	<s id="9">
		 Another feature of this kernel methodology is that it not only provides high accuracy but also allows us to design a kernel function suited to modeling the task at hand . 
	</s>
	

	<s id="10">
		 Since natural language data take the form of sequences of words , and are generally analyzed using discrete structures , such as trees ( parsed trees ) and graphs ( relational graphs ) , discrete kernels , such as sequence kernels 
		<ref citStr="Lodhi et al. , 2002" id="5" label="CEPF" position="1998">
			( Lodhi et al. , 2002 )
		</ref>
		 , tree kernels 
		<ref citStr="Collins and Duffy , 2001" id="6" label="CEPF" position="2042">
			( Collins and Duffy , 2001 )
		</ref>
		 , and graph kernels 
		<ref citStr="Suzuki et al. , 2003a" id="7" label="CEPF" position="2088">
			( Suzuki et al. , 2003a )
		</ref>
		 , have been shown to offer excellent results . 
	</s>
	

	<s id="11">
		 These discrete kernels are related to convolution kernels 
		<ref citStr="Haussler , 1999" id="8" label="CEPF" position="2222">
			( Haussler , 1999 )
		</ref>
		 , which provides the concept of kernels over discrete structures . 
	</s>
	

	<s id="12">
		 Convolution kernels allow us to treat structural features without explicitly representing the feature vectors from the input object . 
	</s>
	

	<s id="13">
		 That is , convolution kernels are well suited to NLP tasks in terms of both accuracy and concept . 
	</s>
	

	<s id="14">
		 Unfortunately , experiments have shown that in some cases there is a critical issue with convolution kernels , especially in NLP tasks 
		<ref citStr="Collins and Duffy , 2001" id="9" label="CEPF" position="2685">
			( Collins and Duffy , 2001 
		</ref>
		<ref citStr="Cancedda et al. , 2003" id="10" label="CEPF" position="2712">
			; Cancedda et al. , 2003 
		</ref>
		<ref citStr="Suzuki et al. , 2003b" id="11" label="CEPF" position="2737">
			; Suzuki et al. , 2003b )
		</ref>
		 . 
	</s>
	

	<s id="15">
		 That is , the over-fitting problem arises if large “substructures” are used in the kernel calculations . 
	</s>
	

	<s id="16">
		 As a result , the machine learning approach can never be trained efficiently . 
	</s>
	

	<s id="17">
		 To solve this issue , we generally eliminate large sub-structures from the set of features used . 
	</s>
	

	<s id="18">
		 However , the main reason for using convolution kernels is that we aim to use structural features easily and efficiently . 
	</s>
	

	<s id="19">
		 If use is limited to only very small structures , it negates the advantages of using convolution kernels . 
	</s>
	

	<s id="20">
		 This paper discusses this issue of convolution kernels , and proposes a new method based on statistical feature selection . 
	</s>
	

	<s id="21">
		 The proposed method deals only with those features that are statistically significant for kernel calculation , large significant substructures can be used without over-fitting . 
	</s>
	

	<s id="22">
		 Moreover , the proposed method can be executed efficiently by embedding it in an original kernel calculation process by using sub-structure mining algorithms . 
	</s>
	

	<s id="23">
		 In the next section , we provide a brief overview of convolution kernels . 
	</s>
	

	<s id="24">
		 Section 3 discusses one issue of convolution kernels , the main topic of this paper , and introduces some conventional methods for solving this issue . 
	</s>
	

	<s id="25">
		 In Section 4 , we propose a new approach based on statistical feature selection to offset the issue of convolution kernels using an example consisting of sequence kernels . 
	</s>
	

	<s id="26">
		 In Section 5 , we briefly discuss the application of the proposed method to other convolution kernels . 
	</s>
	

	<s id="27">
		 In Section 6 , we compare the performance of conventional methods with that of the proposed method by using real NLP tasks : question classification and sentence modality identification . 
	</s>
	

	<s id="28">
		 The experimental results described in Section 7 clarify the advantages of the proposed method . 
	</s>
	

	<s id="29">
		 2 Convolution Kernels Convolution kernels have been proposed as a concept of kernels for discrete structures , such as sequences , trees and graphs . 
	</s>
	

	<s id="30">
		 This framework defines the kernel function between input objects as the convolution of “sub-kernels” , i.e. the kernels for the decompositions ( parts ) of the objects . 
	</s>
	

	<s id="31">
		 Let X and Y be discrete objects . 
	</s>
	

	<s id="32">
		 Conceptually , convolution kernels K(X , Y ) enumerate all substructures occurring in X and Y and then calculate their inner product , which is simply written as : K(X,Y) _ ( O(X) , O(Y)) _ E Oi(X) - Oi(Y)• ( 1 ) i O represents the feature mapping from the discrete object to the feature space ; that is , O(X) = ( O1(X) , ... , Oi(X) , ... ) . 
	</s>
	

	<s id="33">
		 With sequence kernels 
		<ref citStr="Lodhi et al. , 2002" id="12" label="CEPF" position="5447">
			( Lodhi et al. , 2002 )
		</ref>
		 , input objects X and Y are sequences , and Oi ( X ) is a sub-sequence . 
	</s>
	

	<s id="34">
		 With tree kernels 
		<ref citStr="Collins and Duffy , 2001" id="13" label="CEPF" position="5576">
			( Collins and Duffy , 2001 )
		</ref>
		 , X and Y are trees , and Oi ( X ) is a sub-tree . 
	</s>
	

	<s id="35">
		 When implemented , these kernels can be efficiently calculated in quadratic time by using dynamic programming ( DP ) . 
	</s>
	

	<s id="36">
		 Finally , since the size of the input objects is not constant , the kernel value is normalized using the following equation . 
	</s>
	

	<s id="37">
		 ( X , K(X,Y) ˆK 1 ' ) _ ~K(X , X ) K(Y , Y ) ( 2 ) The value of ˆK(X , Y ) is from 0 to 1 , ˆK(X , Y ) = 1 if and only if X = Y. 2.1 Sequence Kernels To simplify the discussion , we restrict ourselves hereafter to sequence kernels . 
	</s>
	

	<s id="38">
		 Other convolution kernels are briefly addressed in Section 5 . 
	</s>
	

	<s id="39">
		 Many kinds of sequence kernels have been proposed for a variety of different tasks . 
	</s>
	

	<s id="40">
		 This paper basically follows the framework of word sequence kernels 
		<ref citStr="Cancedda et al. , 2003" id="14" label="CERF" position="6405">
			( Cancedda et al. , 2003 )
		</ref>
		 , and so processes gapped word sequences to yield the kernel value . 
	</s>
	

	<s id="41">
		 Let E be a set of finite symbols , and En be a set of possible ( symbol ) sequences whose sizes are n or less that are constructed by symbols in E . 
	</s>
	

	<s id="42">
		 The meaning of “size” in this paper is the number of symbols in the sub-structure . 
	</s>
	

	<s id="43">
		 Namely , in the case of sequence , size n means length n. S and T can represent any sequence . 
	</s>
	

	<s id="44">
		 si and tj represent the ith and jth symbols in S and T , respectively . 
	</s>
	

	<s id="45">
		 Therefore , a sequences sub-sequences S=abc ( a , b , c , ab , ac , bc , abc ) T = abac ( a , b , c , aa , ab , ac , ba , bc , aba , aac , abc , bac , abac ) Figure 1 : Example of sequence kernel output sequence S can be written as S = s1 ... si ... slSl , where ISI represents the length of S . 
	</s>
	

	<s id="46">
		 If sequence u is contained in sub-sequence S[i : j ] def= si . 
	</s>
	

	<s id="47">
		 . 
	</s>
	

	<s id="48">
		 . 
	</s>
	

	<s id="49">
		 sj of S ( allowing the existence of gaps ) , the position of u in S is written as i = ( i1 : ilul ) . 
	</s>
	

	<s id="50">
		 The length of S[i] is l(i) = ilul — i1 + 1 . 
	</s>
	

	<s id="51">
		 For example , if u = ab and S = cacbd , then i = ( 2 : 4 ) and l(i)=4—2+1=3 . 
	</s>
	

	<s id="52">
		 By using the above notations , sequence kernels can be defined as : KS- ( S,T ) _ E E A-Y(i) E ~~(j) , ( 3 ) uEEn ilu=S[i] jlu=T[j] where A is the decay factor that handles the gap present in a common sub-sequence u , and -y(i) = l ( i ) — I u I . 
	</s>
	

	<s id="53">
		 In this paper , I means “such that” . 
	</s>
	

	<s id="54">
		 Figure 1 shows a simple example of the output of this kernel . 
	</s>
	

	<s id="55">
		 However , in general , the number of features I En I , which is the dimension of the feature space , becomes very high , and it is computationally infeasible to calculate Equation ( 3 ) explicitly . 
	</s>
	

	<s id="56">
		 The efficient recursive calculation has been introduced in 
		<ref citStr="Cancedda et al. , 2003" id="15" label="CEPF" position="8247">
			( Cancedda et al. , 2003 )
		</ref>
		 . 
	</s>
	

	<s id="57">
		 To clarify the discussion , we redefine the sequence kernels with our notation . 
	</s>
	

	<s id="58">
		 The sequence kernel can be written as follows : n KS- ( S,T ) _ E E E J.(Si,Tj)• ( 4 ) .=1 1&lt;i&lt;S 1&lt;j&lt;lTl where Si and Tj represent the sub-sequences Si = s1 , s2 , ... , si and Tj = t1 , t2 , ... , tj , respectively . 
	</s>
	

	<s id="59">
		 Let Jm ( Si , Tj ) be a function that returns the value of common sub-sequences if si = tj. J.(Si,Tj) _ Jll.-1 ( Si,Tj ) -I(si,tj) ( 5 ) I(si , tj ) is a function that returns a matching value between si and tj . 
	</s>
	

	<s id="60">
		 This paper defines I ( si , tj ) as an indicator function that returns 1 if si = tj , otherwise 0. 0 prod . 
	</s>
	

	<s id="61">
		 2 1 1 0 1 A+A3 0 A 0 0 A 0 kernel value 5+3A+A 3 u a , b , c , aa , ab , ac , ba , bc , aba , aac , abc , bac , abac 0 1 1 1+A2 s T 1 2 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 Then , J~m ( Si , Tj ) and J~~m(Si , Tj ) are introduced to calculate the common gapped sub-sequences between Si and Tj . 
	</s>
	

	<s id="62">
		 Table 1 : Contingency table and notation for the chi- squared value I ( 6 ) J ; . 
	</s>
	

	<s id="63">
		 ( Si , Tj ) = 1 if m = 0 , 0 if j = 0 and m &gt; 0 , AJ;.(Si , Tj-1 ) + J ; ; . 
	</s>
	

	<s id="64">
		 ( Si , Tj-1 ) otherwise c c¯ E row u Ouc = y Ou¯c Ou = x u¯ O¯uc O¯u¯c O¯u E column Oc = M O¯c N 0 if i=0 , AJ ; ; . 
	</s>
	

	<s id="65">
		 ( Si -1 , Tj ) + J. ( Si-1 , Tj ) ( 7 ) otherwise If we calculate Equations ( 5 ) to ( 7 ) recursively , Equation ( 4 ) provides exactly the same value as Equation ( 3 ) . 
	</s>
	

	<s id="66">
		 3 Problem of Applying Convolution Kernels to NLP tasks This section discusses an issue that arises when applying convolution kernels to NLP tasks . 
	</s>
	

	<s id="67">
		 According to the original definition of convolution kernels , all the sub-structures are enumerated and calculated for the kernels . 
	</s>
	

	<s id="68">
		 The number of substructures in the input object usually becomes exponential against input object size . 
	</s>
	

	<s id="69">
		 As a result , all kernel values ˆK(X , Y ) are nearly 0 except the kernel value of the object itself , ˆK(X , X ) , which is 1 . 
	</s>
	

	<s id="70">
		 In this situation , the machine learning process becomes almost the same as memory-based learning . 
	</s>
	

	<s id="71">
		 This means that we obtain a result that is very precise but with very low recall . 
	</s>
	

	<s id="72">
		 To avoid this , most conventional methods use an approach that involves smoothing the kernel values or eliminating features based on the sub-structure size . 
	</s>
	

	<s id="73">
		 For sequence kernels , 
		<ref citStr="Cancedda et al. , 2003" id="16" label="CEPF" position="10689">
			( Cancedda et al. , 2003 )
		</ref>
		 use a feature elimination method based on the size of sub-sequence n . 
	</s>
	

	<s id="74">
		 This means that the kernel calculation deals only with those sub-sequences whose size is n or less . 
	</s>
	

	<s id="75">
		 For tree kernels , 
		<ref citStr="Collins and Duffy , 2001" id="17" label="CEPN" position="10927">
			( Collins and Duffy , 2001 )
		</ref>
		 proposed a method that restricts the features based on sub-trees depth . 
	</s>
	

	<s id="76">
		 These methods seem to work well on the surface , however , good results are achieved only when n is very small , i.e. n = 2 . 
	</s>
	

	<s id="77">
		 The main reason for using convolution kernels is that they allow us to employ structural features simply and efficiently . 
	</s>
	

	<s id="78">
		 When only small sized substructures are used ( i.e. n = 2 ) , the full benefits of convolution kernels are missed . 
	</s>
	

	<s id="79">
		 Moreover , these results do not mean that larger sized sub-structures are not useful . 
	</s>
	

	<s id="80">
		 In some cases we already know that larger sub-structures are significant features as regards solving the target problem . 
	</s>
	

	<s id="81">
		 That is , these significant larger sub-structures , which the conventional methods cannot deal with efficiently , should have a possibility of improving the performance furthermore . 
	</s>
	

	<s id="82">
		 The aim of the work described in this paper is to be able to use any significant sub-structure efficiently , regardless of its size , to solve NLP tasks . 
	</s>
	

	<s id="83">
		 4 Proposed Feature Selection Method Our approach is based on statistical feature selection in contrast to the conventional methods , which use sub-structure size . 
	</s>
	

	<s id="84">
		 For a better understanding , consider the two- class ( positive and negative ) supervised classification problem . 
	</s>
	

	<s id="85">
		 In our approach we test the statistical deviation of all the sub-structures in the training samples between the appearance of positive samples and negative samples . 
	</s>
	

	<s id="86">
		 This allows us to select only the statistically significant sub-structures when calculating the kernel value . 
	</s>
	

	<s id="87">
		 Our approach , which uses a statistical metric to select features , is quite natural . 
	</s>
	

	<s id="88">
		 We note , however , that kernels are calculated using the DP algorithm . 
	</s>
	

	<s id="89">
		 Therefore , it is not clear how to calculate kernels efficiently with a statistical feature selection method . 
	</s>
	

	<s id="90">
		 First , we briefly explain a statistical metric , the chi- squared ( x2 ) value , and provide an idea of how to select significant features . 
	</s>
	

	<s id="91">
		 We then describe a method for embedding statistical feature selection into kernel calculation . 
	</s>
	

	<s id="92">
		 4.1 Statistical Metric : Chi-squared Value There are many kinds of statistical metrics , such as chi-squared value , correlation coefficient and mutual information . 
	</s>
	

	<s id="93">
		 
		<ref citStr="Rogati and Yang , 2002" id="18" label="CEPF" position="13332">
			( Rogati and Yang , 2002 )
		</ref>
		 reported that chi-squared feature selection is the most effective method for text classification . 
	</s>
	

	<s id="94">
		 Following this information , we use x2 values as statistical feature selection criteria . 
	</s>
	

	<s id="95">
		 Although we selected x2 values , any other statistical metric can be used as long as it is based on the contingency table shown in Table 1 . 
	</s>
	

	<s id="96">
		 We briefly explain how to calculate the x2 value by referring to Table 1 . 
	</s>
	

	<s id="97">
		 In the table , c and c¯ represent the names of classes , c for the positive class J ; ; . 
	</s>
	

	<s id="98">
		 ( Si,Tj ) = I 2 1 1 0 1 A+A3 0 A 0 0 A 0 0 t ' ( u ) 0.1 0.5 1.2 1.5 0.9 0.8 2.5 0 0 1 1 0 0 A kernel value under the feature selection 2+A Figure 2 : Example of statistical feature selection and c¯ for the negative class . 
	</s>
	

	<s id="99">
		 Ou , , Ou¯ , , O¯u , and O¯u¯ , represent the number of u that appeared in the positive sample c , the number of u that appeared in the negative sample ¯c , the number of u that did not appear in c , and the number of u that did not appear in ¯c , respectively . 
	</s>
	

	<s id="100">
		 Let y be the number of samples of positive class c that contain sub-sequence u , and x be the number of samples that contain u . 
	</s>
	

	<s id="101">
		 Let N be the total number of ( training ) samples , and M be the number of positive samples . 
	</s>
	

	<s id="102">
		 Since N and M are constant for ( fixed ) data , X2 can be written as a function of x and y , X2(x , y ) = N(Ouc O¯u¯c _ O¯uc Ou¯c)2 .(8) Ou O¯u Oc O¯c X2 expresses the normalized deviation of the observation from the expectation . 
	</s>
	

	<s id="103">
		 We simply represent X2 ( x , y ) as X2 ( u ) . 
	</s>
	

	<s id="104">
		 4.2 Feature Selection Criterion The basic idea of feature selection is quite natural . 
	</s>
	

	<s id="105">
		 First , we decide the threshold T of the X2 value . 
	</s>
	

	<s id="106">
		 If X2 ( u ) &lt; T holds , that is , u is not statistically significant , then u is eliminated from the features and the value of u is presumed to be 0 for the kernel value . 
	</s>
	

	<s id="107">
		 The sequence kernel with feature selection ( FSSK ) can be defined as follows : K FSSK ( S T ) = E E A-Y(i) E A7(j) . 
	</s>
	

	<s id="108">
		 ( 9 ) ~&lt;~2 ( u ) juE~n iju=S[i] ju=T[j] The difference between Equations ( 3 ) and ( 9 ) is simply the condition of the first summation . 
	</s>
	

	<s id="109">
		 FSSK selects significant sub-sequence u by using the condition of the statistical metric T &lt; X2 ( u ) . 
	</s>
	

	<s id="110">
		 Figure 2 shows a simple example of what FSSK calculates for the kernel value . 
	</s>
	

	<s id="111">
		 4.3 Efficient X2(u) Calculation Method It is computationally infeasible to calculate X2 ( u ) for all possible u with a naive exhaustive method . 
	</s>
	

	<s id="112">
		 In our approach , we use a sub-structure mining algorithm to calculate X2 ( u ) . 
	</s>
	

	<s id="113">
		 The basic idea comes from a sequential pattern mining technique , PrefixSpan 
		<ref citStr="Pei et al. , 2001" id="19" label="CERF" position="16095">
			( Pei et al. , 2001 )
		</ref>
		 , and a statistical metric pruning ( SMP ) method , Apriori SMP 
		<ref citStr="Morishita and Sese , 2000" id="20" label="CERF" position="16189">
			( Morishita and Sese , 2000 )
		</ref>
		 . 
	</s>
	

	<s id="114">
		 By using these techniques , all the significant sub-sequences u that satisfy T &lt; X2(u) can be found efficiently by depth-first search and pruning . 
	</s>
	

	<s id="115">
		 Below , we briefly explain the concept involved in finding the significant features . 
	</s>
	

	<s id="116">
		 First , we denote uv , which is the concatenation of sequences u and v. . 
	</s>
	

	<s id="117">
		 Then , u is a specific sequence and uv is any sequence that is constructed by u with any suffix v. . 
	</s>
	

	<s id="118">
		 The upper bound of the X2 value of uv can be defined by the value of u 
		<ref citStr="Morishita and Sese , 2000" id="21" label="CEPF" position="16749">
			( Morishita and Sese , 2000 )
		</ref>
		 . 
	</s>
	

	<s id="119">
		 X2(uv)&lt;max ~X2 ( yu , yu ) , X2 ( xu _ yu , 0 ) ) ~X2(u) where xu and yu represent the value of x and y of u . 
	</s>
	

	<s id="120">
		 This inequation indicates that if ~X2(u) is less than a certain threshold T , all sub-sequences uv can be eliminated from the features , because no subsequence uv can be a feature . 
	</s>
	

	<s id="121">
		 The PrefixSpan algorithm enumerates all the significant sub-sequences by using a depth-first search and constructing a TRIE structure to store the significant sequences of internal results efficiently . 
	</s>
	

	<s id="122">
		 Specifically , PrefixSpan algorithm evaluates uw , where uw represents a concatenation of a sequence u and a symbol w , using the following three conditions . 
	</s>
	

	<s id="123">
		 1. T &lt; X2 ( uw ) 2 . 
	</s>
	

	<s id="124">
		 T &gt; X2(uw) , T &gt; ~X2(uw) 3 . 
	</s>
	

	<s id="125">
		 T &gt; X2(uw) , T ~X2(uw) With 1 , sub-sequence uw is selected as a significant feature . 
	</s>
	

	<s id="126">
		 With 2 , sub-sequence uw and arbitrary subsequences uwv , are less than the threshold T . 
	</s>
	

	<s id="127">
		 Then w is pruned from the TRIE , that is , all uwv where v represents any suffix pruned from the search space . 
	</s>
	

	<s id="128">
		 With 3 , uw is not selected as a significant feature because the X2 value of uw is less than T , however , uwv can be a significant feature because the upper- bound X2 value of uwv is greater than T , thus the search is continued to uwv . 
	</s>
	

	<s id="129">
		 Figure 3 shows a simple example of PrefixSpan with SMP that searches for the significant features u a , b , c , aa , ab , ac , ba , bc , aba , aac , abc , bac , abac 0 1 1 1+A2 prod . 
	</s>
	

	<s id="130">
		 2 1 1 0 1 A+A3 0 A 0 0 A 0 0 s T 1 2 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 5+3A+A ' kernel value feature selection r =1.0 threshold S=abc T = abac ( a , b , c , ab , ac , bc , abc ) ( a , b , c , aa , ab , ac , ba , bc , aba , aac , abc , bac , abac ) sequences sub-sequences = n E m=1 E 1&lt;j&lt;T E 1&lt;i&lt;ISI KFSSK(S , T ) = Km(Si,Tj) ( 10 ) /u(Si,Tj) , ( 11 ) Let Km ( Si , Tj ) be a function that returns the sum value of all statistically significant common subsequences u if si = tj. Km(Si,Tj) = E uE~m(Si,Tj) ( 12 ) /1u(Si , Tj ) ·Z(w) if uw E ~rIuw I ( Si , Tj ) , 0 otherwise ~ ~~ ~~ /1u(Si,Tj) = { /.&quot;(Si,Tj) = ( 13 ) 1 if u = A , 0 if j=0 and u=~A , A/1u ( Si , Tj-1 ) + /u1 ( Si , Tj-1 ) otherwise ( 14 ) 0 if i = 0 , A/.&quot;(Si-1,Tj) + /u(Si-1,Tj) otherwise { ( 16 ) ~rm(Si,Tj) = IF(~r1m-1(Si,Tj),si) if si = tj 0 otherwise { /uw(Si,Tj) = 2 2 0 5 4 4 2 a : b : c : d : w = c : d : 1 1 1 0 x Y u=A w= search order 2 50 50 5.0 22 a0:0 b0:8 c0.8 d2.2 pruned suffix a b c c d b c b a c a c d a b d +1 -1 +1 -1 -1 a b c c d b c a b a c a c d a b d +1 -1 +1 -1 -1 2 X Y M= 1 1 2 suffix u=a a b c c d b c a b a c a dabd +1 -1 +1 -1 -1 b : c : d : 2 3 1 1 2 1 w= class training data N=5 +1 -1 +1 -1 +1 suffix a b c c d b c b a c a c d a b d 3 c11 . 
	</s>
	

	<s id="131">
		 40.8 .8 ... z =1.0 X Y b1.9 5.0 0.1 c2.2 u=ab 4 4.8 .8 pruned 19 c1.9 w x ' ( ,, ) TRIE representation 5 Figure 3 : Efficient search for statistically significant sub-sequences using the PrefixSpan algorithm with SMP by using a depth-first search with a TRIE representation of the significant sequences . 
	</s>
	

	<s id="132">
		 The values of each symbol represent X2 ( u ) and ~X2 ( u ) that can be calculated from the number of xu and yu . 
	</s>
	

	<s id="133">
		 The TRIE structure in the figure represents the statistically significant sub-sequences that can be shown in a path from L to the symbol . 
	</s>
	

	<s id="134">
		 We exploit this TRIE structure and PrefixSpan pruning method in our kernel calculation . 
	</s>
	

	<s id="135">
		 4.4 Embedding Feature Selection in Kernel Calculation This section shows how to integrate statistical feature selection in the kernel calculation . 
	</s>
	

	<s id="136">
		 Our proposed method is defined in the following equations . 
	</s>
	

	<s id="137">
		 where Fm ( Si , Tj ) represents a set of sub-sequences whose size I u I is m and that satisfy the above condition 1 . 
	</s>
	

	<s id="138">
		 The Fm ( Si,Tj ) is defined in detail in Equation ( 15 ) . 
	</s>
	

	<s id="139">
		 Then , let Ju(Si,Tj) , J~u(Si,Tj) and Ju ( Si , Tj ) be functions that calculate the value of the common sub-sequences between Si and Tj recursively , as well as equations ( 5 ) to ( 7 ) for sequence kernels . 
	</s>
	

	<s id="140">
		 We introduce a special symbol A to represent an “empty sequence” , and define Aw = w and IAwI =1. where Z(w) is a function that returns a matching value of w . 
	</s>
	

	<s id="141">
		 In this paper , we define Z(w) is 1 . 
	</s>
	

	<s id="142">
		 ~Fm ( Si , Tj ) has realized conditions 2 and 3 ; the details are defined in Equation ( 16 ) . 
	</s>
	

	<s id="143">
		 The following five equations are introduced to select a set of significant sub-sequences . 
	</s>
	

	<s id="144">
		 Fm ( Si , Tj ) and ~Fm(Si,Tj) are sets of sub-sequences ( features ) that satisfy condition 1 and 3 , respectively , when calculating the value between Si and Tj in Equations ( 11 ) and ( 12 ) . 
	</s>
	

	<s id="145">
		 rm(Si,Tj) = { u I u E ~rm(Si,Tj),~ &lt; ~2(u) } ( 15 ) IF(F , w ) = { uw I u E F,T &lt; ~~2 ( uw ) } , ( 17 ) where F represents a set of sub-sequences . 
	</s>
	

	<s id="146">
		 Notice that Fm ( Si , Tj ) and ~F m ( Si , Tj ) have only subsequences u that satisfy T &lt; X2 ( uw ) or T &lt; ~X2 ( uw ) , respectively , if si = tj ( = w ) ; otherwise they become empty sets . 
	</s>
	

	<s id="147">
		 The following two equations are introduced for recursive set operations to calculate Fm(Si,Tj) and ~Fm(Si,Tj) . 
	</s>
	

	<s id="148">
		 ( 18 ) ( 19 ) ~r1m(Si,Tj) = ~r11m(Si , Tj ) = otherwise { { { A } if m = 0 , 0 if j = 0 and m &gt; 0 , ~r1m(Si,Tj-1) U otherwise 0 if i=0 , ~r11m(Si-1,Tj) U ~r11m(Si,Tj-1) ~rm(Si-1 , Tj ) In the implementation , Equations ( 11 ) to ( 14 ) can be performed in the same way as those used to calculate the original sequence kernels , if the feature selection condition of Equations ( 15 ) to ( 19 ) has been removed . 
	</s>
	

	<s id="149">
		 Then , Equations ( 15 ) to ( 19 ) , which select significant features , are performed by the PrefixSpan algorithm described above and the TRIE representation of statistically significant features . 
	</s>
	

	<s id="150">
		 The recursive calculation of Equations ( 12 ) to ( 14 ) and Equations ( 16 ) to ( 19 ) can be executed in the same way and at the same time in parallel . 
	</s>
	

	<s id="151">
		 As a result , statistical feature selection can be embedded in oroginal sequence kernel calculation based on a dynamic programming technique . 
	</s>
	

	<s id="152">
		 4.5 Properties The proposed method has several important advantages over the conventional methods . 
	</s>
	

	<s id="153">
		 First , the feature selection criterion is based on a statistical measure , so statistically significant features are automatically selected . 
	</s>
	

	<s id="154">
		 Second , according to Equations ( 10 ) to ( 18 ) , the proposed method can be embedded in an original kernel calculation process , which allows us to use the same calculation procedure as the conventional methods . 
	</s>
	

	<s id="155">
		 The only difference between the original sequence kernels and the proposed method is that the latter calculates a statistical metric x2 ( u ) by using a sub-structure mining algorithm in the kernel calculation . 
	</s>
	

	<s id="156">
		 Third , although the kernel calculation , which unifies our proposed method , requires a longer training time because of the feature selection , the selected sub-sequences have a TRIE data structure . 
	</s>
	

	<s id="157">
		 This means a fast calculation technique proposed in 
		<ref citStr="Kudo and Matsumoto , 2003" id="22" label="CERF" position="23931">
			( Kudo and Matsumoto , 2003 )
		</ref>
		 can be simply applied to our method , which yields classification very quickly . 
	</s>
	

	<s id="158">
		 In the classification part , the features ( subsequences ) selected in the learning part must be known . 
	</s>
	

	<s id="159">
		 Therefore , we store the TRIE of selected sub-sequences and use them during classification . 
	</s>
	

	<s id="160">
		 5 Proposed Method Applied to Other Convolution Kernels We have insufficient space to discuss this subject in detail in relation to other convolution kernels . 
	</s>
	

	<s id="161">
		 However , our proposals can be easily applied to tree kernels 
		<ref citStr="Collins and Duffy , 2001" id="23" label="CEPF" position="24496">
			( Collins and Duffy , 2001 )
		</ref>
		 by using string encoding for trees . 
	</s>
	

	<s id="162">
		 We enumerate nodes ( labels ) of tree in postorder traversal . 
	</s>
	

	<s id="163">
		 After that , we can employ a sequential pattern mining technique to select statistically significant sub-trees . 
	</s>
	

	<s id="164">
		 This is because we can convert to the original sub-tree form from the string encoding representation . 
	</s>
	

	<s id="165">
		 Table 2 : Parameter values of proposed kernels and Support Vector Machines parameter value soft margin for SVM ( C ) 1000 decay factor of gap ( A ) 0.5 2.7055 threshold of x2 ( T ) 3.8415 As a result , we can calculate tree kernels with statistical feature selection by using the original tree kernel calculation with the sequential pattern mining technique introduced in this paper . 
	</s>
	

	<s id="166">
		 Moreover , we can expand our proposals to hierarchically structured graph kernels 
		<ref citStr="Suzuki et al. , 2003a" id="24" label="CEPF" position="25350">
			( Suzuki et al. , 2003a )
		</ref>
		 by using a simple extension to cover hierarchical structures . 
	</s>
	

	<s id="167">
		 6 Experiments We evaluated the performance of the proposed method in actual NLP tasks , namely English question classification ( EQC ) , Japanese question classification ( JQC ) and sentence modality identification ( MI ) tasks . 
	</s>
	

	<s id="168">
		 We compared the proposed method ( FSSK ) with a conventional method ( SK ) , as discussed in Section 3 , and with bag-of-words ( BOW ) Kernel (BOW-K)
		<ref citStr="Joachims , 1998" id="25" label="CEPF" position="25829">
			(Joachims , 1998 )
		</ref>
		 as baseline methods . 
	</s>
	

	<s id="169">
		 Support Vector Machine ( SVM ) was selected as the kernel-based classifier for training and classification . 
	</s>
	

	<s id="170">
		 Table 2 shows some of the parameter values that we used in the comparison . 
	</s>
	

	<s id="171">
		 We set thresholds of T = 2.7055 ( FSSK1 ) and T = 3.8415 ( FSSK2 ) for the proposed methods ; these values represent the 10 % and 5 % level of significance in the x2 distribution with one degree of freedom , which used the x2 significant test . 
	</s>
	

	<s id="172">
		 6.1 Question Classification Question classification is defined as a task similar to text categorization ; it maps a given question into a question type . 
	</s>
	

	<s id="173">
		 We evaluated the performance by using data provided by 
		<ref citStr="Li and Roth , 2002" id="26" label="OEPF" position="26558">
			( Li and Roth , 2002 )
		</ref>
		 for English and 
		<ref citStr="Suzuki et al. , 2003b" id="27" label="OEPF" position="26600">
			( Suzuki et al. , 2003b )
		</ref>
		 for Japanese question classification and followed the experimental setting used in these papers ; namely we use four typical question types , LOCATION , NUMEX , ORGANIZATION , and TIME TOP for JQA , and “coarse” and “fine” classes for EQC . 
	</s>
	

	<s id="174">
		 We used the one-vs-rest classifier of SVM as the multi-class classification method for EQC . 
	</s>
	

	<s id="175">
		 Figure 4 shows examples of the question classification data used here . 
	</s>
	

	<s id="176">
		 question types input object : word sequences ( [ ] : information of chunk and ( ) : named entity ) ABBREVIATION what,[B-NP] be,[B-VP] the,[B-NP] abbreviation,[I-NP] for,[B-PP] Texas,[B-NP],(B-GPE) ?,[O] DESCRIPTION what,[B-NP] be,[B-VP] Aborigines,[B-NP] ?,[O] HUMAN who,[B-NP] discover,[B-VP] America,[B-NP],(B-GPE) ?,[O] Figure 4 : Examples of English question classification data Table 3 : Results of the Japanese question classification ( F-measure ) ( a ) TIME TOP ( b ) LOCATION ( c ) ORGANIZATION ( d ) NUMEX n 1 2 3 4 oo 1 2 3 4 oo 1 2 3 4 oo 1 2 3 4 oo FSSK1 - .961 .958 .957 .956 - .795 .793 .798 .792 - .709 .720 .720 .723 - .912 .915 .908 .908 FSSK2 -.961 .956 .957 .956 -.788 .799 .804 .800 -.703 .710 .716 .720 -.913 .916 .911 .913 SK -.946 .910 .866 .223 -.791 .775 .732 .169 -.705 .668 .594 .035 -.912 .885 .817 .036 BOW-K .902 .909 .886 .855 - .744 .768 .756 .747 - .641 690 .636 .572 - .842 .852 .807 .726 - 6.2 Sentence Modality Identification For example , sentence modality identification techniques are used in automatic text analysis systems that identify the modality of a sentence , such as “opinion” or “description” . 
	</s>
	

	<s id="177">
		 The data set was created from Mainichi news articles and one of three modality tags , “opinion” , “decision” and “description” was applied to each sentence . 
	</s>
	

	<s id="178">
		 The data size was 1135 sentences consisting of 123 sentences of “opinion” , 326 of “decision” and 686 of “description” . 
	</s>
	

	<s id="179">
		 We evaluated the results by using 5-fold cross validation . 
	</s>
	

	<s id="180">
		 7 Results and Discussion Tables 3 and 4 show the results of Japanese and English question classification , respectively . 
	</s>
	

	<s id="181">
		 Table 5 shows the results of sentence modality identification . 
	</s>
	

	<s id="182">
		 n in each table indicates the threshold of the sub-sequence size . 
	</s>
	

	<s id="183">
		 n = oc means all possible subsequences are used . 
	</s>
	

	<s id="184">
		 First , SK was consistently superior to BOW-K . 
	</s>
	

	<s id="185">
		 This indicates that the structural features were quite efficient in performing these tasks . 
	</s>
	

	<s id="186">
		 In general we can say that the use of structural features can improve the performance of NLP tasks that require the details of the contents to perform the task . 
	</s>
	

	<s id="187">
		 Most of the results showed that SK achieves its maximum performance when n = 2 . 
	</s>
	

	<s id="188">
		 The performance deteriorates considerably once n exceeds 4 . 
	</s>
	

	<s id="189">
		 This implies that SK with larger sub-structures degrade classification performance . 
	</s>
	

	<s id="190">
		 These results show the same tendency as the previous studies discussed in Section 3 . 
	</s>
	

	<s id="191">
		 Table 6 shows the precision and recall of SK when n = oc . 
	</s>
	

	<s id="192">
		 As shown in Table 6 , the classifier offered high precision but low recall . 
	</s>
	

	<s id="193">
		 This is evidence of over-fitting in learning . 
	</s>
	

	<s id="194">
		 As shown by the above experiments , FSSK pro- Table 6 : Precision and recall of SK : n = oc Precision Recall F MI:Opinion .917 .209 .339 JQA:LOCATION .896 .093 .168 vided consistently better performance than the conventional methods . 
	</s>
	

	<s id="195">
		 Moreover , the experiments confirmed one important fact . 
	</s>
	

	<s id="196">
		 That is , in some cases maximum performance was achieved with n = oc . 
	</s>
	

	<s id="197">
		 This indicates that sub-sequences created using very large structures can be extremely effective . 
	</s>
	

	<s id="198">
		 Of course , a larger feature space also includes the smaller feature spaces , En C En+1 . 
	</s>
	

	<s id="199">
		 If the performance is improved by using a larger n , this means that significant features do exist . 
	</s>
	

	<s id="200">
		 Thus , we can improve the performance of some classification problems by dealing with larger substructures . 
	</s>
	

	<s id="201">
		 Even if optimum performance was not achieved with n = oc , difference between the performance of smaller n are quite small compared to that of SK . 
	</s>
	

	<s id="202">
		 This indicates that our method is very robust as regards substructure size ; It therefore becomes unnecessary for us to decide sub-structure size carefully . 
	</s>
	

	<s id="203">
		 This indicates our approach , using large sub-structures , is better than the conventional approach of eliminating sub-sequences based on size . 
	</s>
	

	<s id="204">
		 8 Conclusion This paper proposed a statistical feature selection method for convolution kernels . 
	</s>
	

	<s id="205">
		 Our approach can select significant features automatically based on a statistical significance test . 
	</s>
	

	<s id="206">
		 Our proposed method can be embedded in the DP based kernel calculation process for convolution kernels by using substructure mining algorithms . 
	</s>
	

	<s id="207">
		 Table 4 : Results of English question classification ( Accuracy ) ( a ) coarse ( b ) fine n 1 2 3 4 oo 1 2 3 4 oo FSSK1 - .908 .914 .916 .912 - .852 .854 .852 .850 FSSK2 - .902 .896 .902 .906 - .858 .856 .854 .854 SK - .912 .914 .912 .892 - .850 .840 .830 .796 BOW-K .728 .836 .864 .858 - .754 .792 .790 .778 - Table 5 : Results of sentence modality identification ( F-measure ) ( a ) opinion ( b ) decision ( c ) description n 1 2 3 4 oo 1 2 3 4 oo 1 2 3 4 oo FSSK1 - .734 .743 .746 .751 - .828 .858 .854 .857 - .896 .906 .910 .910 FSSK2 - .740 .748 .750 .750 - .824 .855 .859 .860 - .894 .903 .909 .909 SK - .706 .672 .577 .058 - .816 .834 .830 .339 - .902 .913 .910 .808 BOW-K .507 .531 .438 .368 - .652 .708 .686 .665 - .819 .839 .826 .793 - Experiments show that our method is superior to conventional methods . 
	</s>
	

	<s id="208">
		 Moreover , the results indicate that complex features exist and can be effective . 
	</s>
	

	<s id="209">
		 Our method can employ them without over-fitting problems , which yields benefits in terms of concept and performance . 
	</s>
	

	<s id="210">
		 References N. Cancedda , E. Gaussier , C. Goutte , and J.-M. Renders . 
	</s>
	

	<s id="211">
		 2003. Word-Sequence Kernels . 
	</s>
	

	<s id="212">
		 Journal ofMachine Learning Research , 3:1059–1082 . 
	</s>
	

	<s id="213">
		 M. Collins and N. Duffy . 
	</s>
	

	<s id="214">
		 2001. Convolution Kernels for Natural Language . 
	</s>
	

	<s id="215">
		 In Proc . 
	</s>
	

	<s id="216">
		 ofNeural Information Processing Systems ( NIPS’2001 ) . 
	</s>
	

	<s id="217">
		 C. Cortes and V. N. Vapnik . 
	</s>
	

	<s id="218">
		 1995. Support Vector Networks . 
	</s>
	

	<s id="219">
		 Machine Learning , 20:273–297 . 
	</s>
	

	<s id="220">
		 D. Haussler . 
	</s>
	

	<s id="221">
		 1999. Convolution Kernels on Discrete Structures . 
	</s>
	

	<s id="222">
		 In Technical Report UCS-CRL99-10 . 
	</s>
	

	<s id="223">
		 UC Santa Cruz . 
	</s>
	

	<s id="224">
		 T. Joachims . 
	</s>
	

	<s id="225">
		 1998. Text Categorization with Support Vector Machines : Learning with Many Relevant Features . 
	</s>
	

	<s id="226">
		 In Proc . 
	</s>
	

	<s id="227">
		 ofEuropean Conference on Machine Learning ( ECML ’98 ) , pages 137– 142 . 
	</s>
	

	<s id="228">
		 T. Kudo and Y. Matsumoto . 
	</s>
	

	<s id="229">
		 2002. Japanese Dependency Analysis Using Cascaded Chunking . 
	</s>
	

	<s id="230">
		 In Proc . 
	</s>
	

	<s id="231">
		 of the 6th Conference on Natural Language Learning 
		<ref citStr="CoNLL 2002" id="28" label="CEPF" position="33578">
			( CoNLL 2002 )
		</ref>
		 , pages 63–69 . 
	</s>
	

	<s id="232">
		 T. Kudo and Y. Matsumoto . 
	</s>
	

	<s id="233">
		 2003. Fast Methods for Kernel-based Text Analysis . 
	</s>
	

	<s id="234">
		 In Proc . 
	</s>
	

	<s id="235">
		 of the 41st Annual Meeting of the Association for Computa- tional Linguistics ( ACL-2003 ) , pages 24–31 . 
	</s>
	

	<s id="236">
		 X. Li and D. Roth . 
	</s>
	

	<s id="237">
		 2002. Learning Question Clas- sifiers . 
	</s>
	

	<s id="238">
		 In Proc . 
	</s>
	

	<s id="239">
		 of the 19th International Con- ference on Computational Linguistics ( COLING 2002 ) , pages 556–562 . 
	</s>
	

	<s id="240">
		 H. Lodhi , C. Saunders , J. Shawe-Taylor , N. Cristianini , and C. Watkins . 
	</s>
	

	<s id="241">
		 2002. Text Classification Using String Kernel . 
	</s>
	

	<s id="242">
		 Journal ofMachine Learning Research , 2:419–444 . 
	</s>
	

	<s id="243">
		 S. Morishita and J. Sese . 
	</s>
	

	<s id="244">
		 2000. Traversing Item- set Lattices with Statistical Metric Pruning . 
	</s>
	

	<s id="245">
		 In Proc . 
	</s>
	

	<s id="246">
		 ofACM SIGACT-SIGMOD-SIGART Symp . 
	</s>
	

	<s id="247">
		 on Database Systems ( PODS’00 ) , pages 226– 236. J. Pei , J. Han , B. Mortazavi-Asl , and H. Pinto . 
	</s>
	

	<s id="248">
		 2001. PrefixSpan : Mining Sequential Patterns Efficiently by Prefix-Projected Pattern Growth . 
	</s>
	

	<s id="249">
		 In Proc . 
	</s>
	

	<s id="250">
		 of the 17th International Conference on Data Engineering ( ICDE 2001 ) , pages 215–224 . 
	</s>
	

	<s id="251">
		 M. Rogati and Y. Yang . 
	</s>
	

	<s id="252">
		 2002. High-performing Feature Selection for Text Classification . 
	</s>
	

	<s id="253">
		 In Proc . 
	</s>
	

	<s id="254">
		 of the 2002 ACM CIKMInternational Conference on Information and Knowledge Management , pages 659–661 . 
	</s>
	

	<s id="255">
		 J. Suzuki , T. Hirao , Y. Sasaki , and E. Maeda . 
	</s>
	

	<s id="256">
		 2003a . 
	</s>
	

	<s id="257">
		 Hierarchical Directed Acyclic Graph Kernel : Methods for Natural Language Data . 
	</s>
	

	<s id="258">
		 In Proc . 
	</s>
	

	<s id="259">
		 of the 41st Annual Meeting of the Association for Computational Linguistics ( ACL-2003 ) , pages 32–39 . 
	</s>
	

	<s id="260">
		 J. Suzuki , Y. Sasaki , and E. Maeda . 
	</s>
	

	<s id="261">
		 2003b . 
	</s>
	

	<s id="262">
		 Kernels for Structured Natural Language Data . 
	</s>
	

	<s id="263">
		 In Proc . 
	</s>
	

	<s id="264">
		 of the 17th Annual Conference on Neural Information Processing Systems ( NIPS2003 ) . 
	</s>
	


</acldoc>
