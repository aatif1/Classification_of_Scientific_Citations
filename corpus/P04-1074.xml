<?xml version="1.0" encoding="iso-8859-1"?>
<acldoc acl_id="P04-1074">
	

	<s id="1">
		 Applying Machine Learning to Chinese Temporal Relation Resolution Wenjie Li Department of Computing The Hong Kong Polytechnic University , Hong Kong cswjli@comp.polyu.edu.hk Guihong Cao Department of Computing The Hong Kong Polytechnic University , Hong Kong csghcao@comp.polyu.edu.hk Kam-Fai Wong Department of Systems Engineering and Engineering Management The Chinese University of Hong Kong , Hong Kong kfwong@se.cuhk.edu.hk Chunfa Yuan Department of Computer Science and Technology Tsinghua University , Beijing , China . 
	</s>
	

	<s id="2">
		 cfyuan@tsinghua.edu.cn Abstract Temporal relation resolution involves extraction of temporal information explicitly or implicitly embedded in a language . 
	</s>
	

	<s id="3">
		 This information is often inferred from a variety of interactive grammatical and lexical cues , especially in Chinese . 
	</s>
	

	<s id="4">
		 For this purpose , inter-clause relations ( temporal or otherwise ) in a multiple-clause sentence play an important role . 
	</s>
	

	<s id="5">
		 In this paper , a computational model based on machine learning and heterogeneous collaborative bootstrapping is proposed for analyzing temporal relations in a Chinese multiple-clause sentence . 
	</s>
	

	<s id="6">
		 The model makes use of the fact that events are represented in different temporal structures . 
	</s>
	

	<s id="7">
		 It takes into account the effects of linguistic features such as tense/aspect , temporal connectives , and discourse structures . 
	</s>
	

	<s id="8">
		 A set of experiments has been conducted to investigate how linguistic features could affect temporal relation resolution . 
	</s>
	

	<s id="9">
		 1 Introduction In language studies , temporal information describes changes and time of changes expressed in a language . 
	</s>
	

	<s id="10">
		 Such information is critical in many typical natural language processing ( NLP ) applications , e.g. language generation and machine translation , etc. . 
	</s>
	

	<s id="11">
		 Modeling temporal aspects of an event in a written text is more complex than capturing time in a physical time-stamped system . 
	</s>
	

	<s id="12">
		 Event time may be specified explicitly in a sentence , e.g. “JAIM , , 1997 ~M &amp;-T=Arin ' ZA Ip7E 
		<ref citStr="They solved the traffic problem of the city in 1997" id="1" label="CEPF" position="2132">
			( They solved the traffic problem of the city in 1997)
		</ref>
		” ; or it may be left implicit , to be recovered by readers from context . 
	</s>
	

	<s id="13">
		 For example , one may know that “~~~'Z~~~~JAIM&amp;-T=A ri n'ZAIp7E ( after the street bridge had been built , they solved the traffic problem of the city)” , yet without knowing the exact time when the street bridge was built . 
	</s>
	

	<s id="14">
		 As reported by Partee 
		<ref citStr="Partee , 1984" id="2" label="CEPF" position="2496">
			( Partee , 1984 )
		</ref>
		 , the expression of relative temporal relations in which precise times are not stated is common in natural language . 
	</s>
	

	<s id="15">
		 The objective of relative temporal relation resolution is to determine the type of relative relation embedded in a sentence . 
	</s>
	

	<s id="16">
		 In English , temporal expressions have been widely studied . 
	</s>
	

	<s id="17">
		 Lascarides and Asher 
		<ref citStr="Lascarides , Asher and Oberlander , 1992" id="3" label="CEPF" position="2894">
			( Lascarides , Asher and Oberlander , 1992 )
		</ref>
		 suggested that temporal relations between two events followed from discourse structures . 
	</s>
	

	<s id="18">
		 They investigated various contextual effects on five discourse relations ( namely narration , elaboration , explanation , background and result ) and then corresponded each of them to a kind of temporal relations . 
	</s>
	

	<s id="19">
		 Hitzeman et al . 
	</s>
	

	<s id="20">
		 
		<ref citStr="Hitzeman , Moens and Grover , 1995" id="4" label="CEPF" position="3282">
			( Hitzeman , Moens and Grover , 1995 )
		</ref>
		 described a method for analyzing temporal structure of a discourse by taking into account the effects of tense , aspect , temporal adverbials and rhetorical relations ( e.g. causation and elaboration ) on temporal ordering . 
	</s>
	

	<s id="21">
		 They argued that rhetorical relations could be further constrained by event temporal classification . 
	</s>
	

	<s id="22">
		 Later , Dorr and Gaasterland 
		<ref citStr="Dorr and Gaasterland , 2002" id="5" label="CEPF" position="3688">
			( Dorr and Gaasterland , 2002 )
		</ref>
		 developed a constraint-based approach to generate sentences , which reflect temporal relations , by making appropriate selections of tense , aspect and connecting words ( e.g. before , after and when ) . 
	</s>
	

	<s id="23">
		 Their works , however , are theoretical in nature and have not investigated computational aspects . 
	</s>
	

	<s id="24">
		 The pioneer work on Chinese temporal relation extraction was first reported by Li and Wong 
		<ref citStr="Li and Wong , 2002" id="6" label="CEPN" position="4124">
			( Li and Wong , 2002 )
		</ref>
		 . 
	</s>
	

	<s id="25">
		 To discover temporal relations embedded in a sentence , they devised a set of simple rules to map the combined effects of temporal indicators , which are gathered from different grammatical categories , to their corresponding relations . 
	</s>
	

	<s id="26">
		 However , their work did not focus on relative temporal relations . 
	</s>
	

	<s id="27">
		 Given a sentence describing two temporally related events , Li and Wong only took the temporal position words ( including before , after and when , which serve as temporal connectives ) and the tense/aspect markers of the second event into consideration . 
	</s>
	

	<s id="28">
		 The proposed rule-based approach was simple ; but it suffered from low coverage and was particularly ineffective when the interaction between the linguistic elements was unclear . 
	</s>
	

	<s id="29">
		 This paper studies how linguistic features in Chinese interact to influence relative relation resolution . 
	</s>
	

	<s id="30">
		 For this purpose , statistics-based machine learning approaches are applied . 
	</s>
	

	<s id="31">
		 The remainder of the paper is structured as follows : Section 2 summarizes the linguistic features , which must be taken into account in temporal relation resolution , and introduces how these features are expressed in Chinese . 
	</s>
	

	<s id="32">
		 In Section 3 , the proposed machine learning algorithms to identify temporal relations are outlined ; furthermore , a heterogeneous collaborative bootstrapping technique for smoothing is presented . 
	</s>
	

	<s id="33">
		 Experiments designed for studying the impact of different approaches and linguistic features are described in Section 4 . 
	</s>
	

	<s id="34">
		 Finally , Section 5 concludes the paper . 
	</s>
	

	<s id="35">
		 2 Modeling Temporal Relations 2.1 Temporal Relation Representations As the importance of temporal information processing has become apparent , a variety of temporal systems have been introduced , attempting to accommodate the characteristics of relative temporal information . 
	</s>
	

	<s id="36">
		 Among those who worked on temporal relation representations , many took the work of Reichenbach 
		<ref citStr="Reichenbach , 1947" id="7" label="CEPF" position="6149">
			( Reichenbach , 1947 )
		</ref>
		 as a starting point , while some others based their works on Allen’s 
		<ref citStr="Allen , 1981" id="8" label="CEPF" position="6236">
			( Allen , 1981 )
		</ref>
		 . 
	</s>
	

	<s id="37">
		 Reichenbach proposed a point-based temporal theory . 
	</s>
	

	<s id="38">
		 This was later enhanced by Bruce who defined seven relative temporal relations ( Bruce . 
	</s>
	

	<s id="39">
		 1972 ) . 
	</s>
	

	<s id="40">
		 Given two durative events , the interval relations between them were modeled by the order between the greatest lower bounding points and least upper bounding points of the two events . 
	</s>
	

	<s id="41">
		 In the other camp , instead of adopting time points , Allen took intervals as temporal primitives and introduced thirteen basic binary relations . 
	</s>
	

	<s id="42">
		 In this interval-based theory , points are relegated to a subsidiary status as ‘meeting places’ of intervals . 
	</s>
	

	<s id="43">
		 An extension to Allen’s theory , which treated both points and intervals as primitives on an equal footing , was later investigated by Ma and Knight 
		<ref citStr="Ma and Knight , 1994" id="9" label="CEPF" position="7072">
			( Ma and Knight , 1994 )
		</ref>
		 . 
	</s>
	

	<s id="44">
		 In natural language , events can either be punctual ( e.g. 1 ~ ( explore ) ) or durative ( e.g. 1~ ( built a house ) ) in nature . 
	</s>
	

	<s id="45">
		 Thus Ma and Knight’s model is adopted in our work ( see Figure 1 ) . 
	</s>
	

	<s id="46">
		 Taking the sen- tence “1MhAcA-Z~fir , f 417~~~i rinZA1p79 ( after the street bridge had been built , they solved the traffic problem of the city)” as an example , the relation held between building the bridge ( i.e. an interval ) and solving the problem ( i.e. a point ) is BEFORE . 
	</s>
	

	<s id="47">
		 BEFORE/AFTER MEETS/MET-BY OVERLAPS/OVERLAPPED-BY STARTS/STARTED-BY DURING/CONTAINS FINISHES/FINISHED-BY SAME-AS A punctual event ( i.e. represented in time point ) A durative event ( i.e. represented in time interval ) Figure 1. Thirteen temporal relations between points and intervals 2.2 Linguistic Features for Determining Relative Relations Relative relations are generally determined by tense/aspect , connecting words ( temporal or otherwise ) and event classes . 
	</s>
	

	<s id="48">
		 Tense/Aspect in English is manifested by verb inflections . 
	</s>
	

	<s id="49">
		 But such morphological variations are inapplicable to Chinese verbs ; instead , they are conveyed lexically 
		<ref citStr="Li and Wong , 2002" id="10" label="CEPF" position="8275">
			( Li and Wong , 2002 )
		</ref>
		 . 
	</s>
	

	<s id="50">
		 In other words , tense and aspect in Chinese are expressed using a combination of time words , auxiliaries , temporal position words , adverbs and prepositions , and particular verbs . 
	</s>
	

	<s id="51">
		 Temporal Connectives in English primarily involve conjunctions , e.g. after , before and when 
		<ref citStr="Dorr and Gaasterland , 2002" id="11" label="CEPF" position="8606">
			( Dorr and Gaasterland , 2002 )
		</ref>
		 . 
	</s>
	

	<s id="52">
		 They are key components in discourse structures . 
	</s>
	

	<s id="53">
		 In Chinese , however , conjunctions , conjunctive adverbs , prepositions and position words are required to represent connectives . 
	</s>
	

	<s id="54">
		 A few verbs which express cause and effect also imply a forward movement of event time . 
	</s>
	

	<s id="55">
		 The words , which contribute to the tense/aspect and temporal connective expressions , are explicit in a sentence and generally known as Temporal Indicators . 
	</s>
	

	<s id="56">
		 Event Class is implicit in a sentence . 
	</s>
	

	<s id="57">
		 Events can be classified according to their inherent temporal characteristics , such as the degree of telicity and/or atomicity 
		<ref citStr="Li and Wong , 2002" id="12" label="CEPF" position="9283">
			( Li and Wong , 2002 )
		</ref>
		 . 
	</s>
	

	<s id="58">
		 The four widespread accepted temporal classes1 are state , process , punctual event and developing event . 
	</s>
	

	<s id="59">
		 Based on their classes , events interact with the tense/aspect of verbs to define the temporal relations between two events . 
	</s>
	

	<s id="60">
		 Temporal indicators and event classes are together referred to as Linguistic Features ( see Table 1 ) . 
	</s>
	

	<s id="61">
		 For example , linguistic features are underlined in the sentence “(~~)1MhAcA-Z~(fir),f 417~~~i ri nZA1p7 9 after/because the street bridge had been built ( i.e. a developing event ) , they solved the traffic problem of the city ( i.e. a punctual event)” . 
	</s>
	

	<s id="62">
		 1 Temporal classification refers to aspectual classification . 
	</s>
	

	<s id="63">
		 Linguistic Feature Symbol POS Tag Effect Example With/Without punctuations PT Not Applica- Not Applicable Not Applicable ble Speech verbs VS TI _vs Tense ^^ , ^^ , ^ Trend verbs TR TI_tr Aspect ^^ , ^^ Preposition words P TI_p Discourse Structure/Aspect ^ , ^ , ^ Position words PS TI _f Discourse Structure ^ , ^ , ^^ Verbs with verb objects VV TI_vv Tense/Aspect ^^ , ^^ , ^ Verbs expressing wish/hope VA TI_va Tense ^^ , ^ , ^ Verbs related to causality VC TI_vc Discourse Structure ^^ , ^^ , ^^ Conjunctive words C TI_c Discourse Structure ^ , ^^ , ^^ Auxiliary words U TI_u Aspect ^ , ^ , ^ Time words T TI _t Tense ^^ , ^^ , ^^ Adverbs D TI_d Tense/Aspect/Discourse Structure ^ , ^ , ^^ , ^ Event class EC E0/E1/E2/E3 Event Classification State , Punctual Event , Developing Event , Process Table 1 . 
	</s>
	

	<s id="64">
		 Linguistic features : eleven temporal indicators and one event class Table 1 shows the mapping between a temporal indicator and its effects . 
	</s>
	

	<s id="65">
		 Notice that the mapping is not one-to-one . 
	</s>
	

	<s id="66">
		 For example , adverbs affect tense/aspect as well as discourse structure . 
	</s>
	

	<s id="67">
		 For another example , tense/aspect can be affected by auxiliary words , trend verbs , etc. . 
	</s>
	

	<s id="68">
		 This shows that classification of temporal indicators based on partof-speech ( POS ) information alone cannot determine relative temporal relations . 
	</s>
	

	<s id="69">
		 3 Machine Learning Approaches for Relative Relation Resolution Previous efforts in corpus-based natural language processing have incorporated machine learning methods to coordinate multiple linguistic features for example in accent restoration 
		<ref citStr="Yarowsky , 1994" id="13" label="CEPF" position="11626">
			( Yarowsky , 1994 )
		</ref>
		 and event classification 
		<ref citStr="Siegel and McKeown , 1998" id="14" label="CEPF" position="11681">
			( Siegel and McKeown , 1998 )
		</ref>
		 , etc. . 
	</s>
	

	<s id="70">
		 Relative relation resolution can be modeled as a relation classification task . 
	</s>
	

	<s id="71">
		 We model the thirteen relative temporal relations ( see Figure 1 ) as the classes to be decided by a classifier . 
	</s>
	

	<s id="72">
		 The resolution process is to assign an event pair ( i.e. the two events under concern)2 to one class according to their linguistic features . 
	</s>
	

	<s id="73">
		 For this purpose , we train two classifiers , a Probabilistic Decision Tree Classifier ( PDT ) and a Naïve Bayesian Classifier ( NBC ) . 
	</s>
	

	<s id="74">
		 We then combine the results by the Collaborative Bootstrappi,,g ( CB ) technique which is used to mediate the sparse data problem arose due to the limited number of training cases . 
	</s>
	

	<s id="75">
		 3.1 Probabilistic Decision Tree ( PDT ) Due to two domain-specific characteristics , we encounter some difficulties in classification . 
	</s>
	

	<s id="76">
		 ( a ) Unknown values are common , for many events are modified by less than three linguistic features . 
	</s>
	

	<s id="77">
		 ( b ) Both training and testing data are noisy . 
	</s>
	

	<s id="78">
		 For this reason , it is impossible to obtain a tree which can completely classify all training examples . 
	</s>
	

	<s id="79">
		 To overcome this predicament , we aim to obtain more adjusted probability distributions of event pairs over their possible classes . 
	</s>
	

	<s id="80">
		 Therefore , a probabilistic decision tree approach is preferred over conventional decision tree approaches ( e.g. C4.5 , ID3 ) . 
	</s>
	

	<s id="81">
		 We adopt a non-incremental supervised learning algorithm in TDIDT ( Top Down Induction of Decision Trees ) family . 
	</s>
	

	<s id="82">
		 It constructs a tree top-down and the process is guided by distributional information learned from examples 
		<ref citStr="Quinlan , 1993" id="15" label="CEPF" position="13363">
			( Quinlan , 1993 )
		</ref>
		 . 
	</s>
	

	<s id="83">
		 3.1.1 Parameter Estimation Based on probabilities , each object in the PDT approach can belong to a number of classes . 
	</s>
	

	<s id="84">
		 These probabilities could be estimated from training cases with Maximum Likelihood Estimation ( MLE ) . 
	</s>
	

	<s id="85">
		 Let l be the decision sequence , z the object and c the class . 
	</s>
	

	<s id="86">
		 The probability of z belonging to c is : Ac|z)=^ Al , c| z ) ^ ^Ac|OpY| z ) l l let l= B1 B2 ... 
	</s>
	

	<s id="87">
		 B , , , by MLE we have : p(c |l ) ^ p(c | B ) , , = f ( c , B,,)(2) f(B,,) f ( c , B , , ) is the count of the items whose leaf nodes are B , , and belonging to class c . 
	</s>
	

	<s id="88">
		 And ( 1 ) 2 It is an object in machine learning algorithms . 
	</s>
	

	<s id="89">
		 where p ( l | z ) = p(B1 | z)p(B2 ) | B1 , z)p(B3 | B1 , B2 , z ) ( 3 ) ...p ( Bn | Bn^1 ...B1,z p(BmBm^1Bm_2 ... 
	</s>
	

	<s id="90">
		 B1 | z ) p(Bm_1Bm2 ... 
	</s>
	

	<s id="91">
		 B1 | z ) p(Bm Bm— 2 ...B1 , z ) tance-based measurement is unbiased towards the attributes with a large number of values and is capable of generating smaller trees with no loss of accuracy 
		<ref citStr="Marquez , Padro and Rodriguez , 2000" id="16" label="CEPF" position="14431">
			( Marquez , Padro and Rodriguez , 2000 )
		</ref>
		 . 
	</s>
	

	<s id="92">
		 This characteristic makes it an ideal choice for our work , where most attributes have more than 200 values . 
	</s>
	

	<s id="93">
		 f ( BmBm^1Bm_2 ... 
	</s>
	

	<s id="94">
		 B1 | z ) f ( Bm_1Bm_2 ...B1 | z ) An object might traverse more than one decision path if it has unknown attribute values . 
	</s>
	

	<s id="95">
		 f(BmBm^1Bm^2 ...B1 | z ) is the count of the item z , which owns the decision paths from B1 to Bm. 3.1.2 Classification Attributes Objects are classified into classes based on their attributes . 
	</s>
	

	<s id="96">
		 In the context of temporal relation resolution , how to categorize linguistic features into classification attributes is a major design issue . 
	</s>
	

	<s id="97">
		 We extract all temporal indicators surrounding an event . 
	</s>
	

	<s id="98">
		 Assume m and n are the anterior and posterior window size . 
	</s>
	

	<s id="99">
		 They represent the numbers of the indicators BEFORE and AFTER respectively . 
	</s>
	

	<s id="100">
		 Consider the most extreme case where an event consists of at most 4 temporal indicators before and 2 after . 
	</s>
	

	<s id="101">
		 We set m and n to 4 and 2 initially . 
	</s>
	

	<s id="102">
		 Experiments show that learning performance drops when m&gt;4 and n&gt;2 and there is only very little difference otherwise ( i.e. when m^4 and n^2 ) . 
	</s>
	

	<s id="103">
		 In addition to temporal indicators alone , the position of the punctuation mark separating the two clauses describing the events and the classes of the events are also useful classification attributes . 
	</s>
	

	<s id="104">
		 We will outline why this is so in Section 4.1 . 
	</s>
	

	<s id="105">
		 Altogether , the following 15 attributes are used to train the PDT and NBC classifiers : TI ~ , TI ~~ , TI ~ , TI ~~ , class ( e1 ) , TI ~~ , TI ~~ , wi / wo punC , TI l4 , TI l3 , TI l2 , TI l1 , class ( e2 ) , TI2 , , TI r2 e2 e2 e2 e2 e , , e2 li ( i=1,2,3,4 ) and rj ( j=1,2 ) are the ith indictor before and the jth indicator after the event ek ( k=1,2 ) . 
	</s>
	

	<s id="106">
		 Given a sentence , for example , 5&quot;c/TI_d ~/E0 _T/TI_u ~T/n , /w A-/TI_d T,0/E2 _T/TI_u IRiI/n o /w , the at- tribute vector could be represented as : [ 0 , 0 , 0 , 5&quot;c , E0 , _T , 0 , 1 , 0 , 0 , 0 , A- , E2 , _T , 0 ] . 
	</s>
	

	<s id="107">
		 3.1.3 Attribute Selection Function Many similar attribute selection functions were used to construct a decision tree 
		<ref citStr="Marquez , 2000" id="17" label="CEPF" position="16633">
			( Marquez , 2000 )
		</ref>
		 . 
	</s>
	

	<s id="108">
		 These included information gain and information gain ratio 
		<ref citStr="Quinlan , 1993" id="18" label="CEPF" position="16722">
			( Quinlan , 1993 )
		</ref>
		 , X2 Test and Symmetrical Tau 
		<ref citStr="Zhou and Dillon , 1991" id="19" label="CEPF" position="16779">
			( Zhou and Dillon , 1991 )
		</ref>
		 . 
	</s>
	

	<s id="109">
		 We adopt the one proposed by Lopez de Mantaraz 
		<ref citStr="Mantaras , 1991" id="20" label="CEPF" position="16857">
			( Mantaras , 1991 )
		</ref>
		 for it shows more stable performance than Quinlan’s information gain ratio in our experiments . 
	</s>
	

	<s id="110">
		 Compared with Quinlan’s information gain ratio , Lopez’s dis- 3.2 Naïve Bayesian Classifier ( NBC ) NBC assumes independence among features . 
	</s>
	

	<s id="111">
		 Given the class label c , NBC learns from training data the conditional probability of each attribute Ai ( see Section 3.1.2 ) . 
	</s>
	

	<s id="112">
		 Classification is then performed by applying Bayes rule to compute the probability of c given the particular instance of A1,...,An , and then predicting the class with the highest posterior probability ratio . 
	</s>
	

	<s id="113">
		 c = arg max score(c | A1 , A2 , A3 , ... , An ) ( 4 ) c score(c | A1 , A2 , 3,”' A A ) = Ac | A1 , A2 , A3 , ... , An ) ( 5 ) , n Ac | A1 , A2 , A3 , ... , An ) Apply Bayesian rule to ( 5 ) , we have : An ) A1 , A2 , A3 , ... , An p(c | A1 , A2 , A3 , . 
	</s>
	

	<s id="114">
		 ..,An ) ) ( c A 1 , A 2 , A 3 , ... , n |c ( c ) ( 6 ) )p p(A1 , A2 , A3 , ... , An | c)p(c) p(Ai | c ) and p(Ai | c ) are estimated by MLE from training data with Dirichlet Smoothing method : p(Ai | c ) = n ( 7 ) j=1 ^ c c(Ai,c)+u ( A j , c )+ u × n l 1 p(A i | c ) = n ( 8 ) j=1 ^ c c(Ai,c)+u ( A j , c )+ u × n l 1 3.3 Collaborative Bootstrapping ( CB ) PDT and NB are both supervised learning approach . 
	</s>
	

	<s id="115">
		 Thus , the training processes require many labeled cases . 
	</s>
	

	<s id="116">
		 Recent results 
		<ref citStr="Blum and Mitchell , 1998" id="21" label="CEPF" position="18241">
			( Blum and Mitchell , 1998 
		</ref>
		<ref citStr="Collins , 1999" id="22" label="CEPF" position="18268">
			; Collins , 1999 )
		</ref>
		 have suggested that unlabeled data could also be used effectively to reduce the amount of labeled data by taking advantage of collaborative bootstrapping ( CB ) techniques . 
	</s>
	

	<s id="117">
		 In previous works , CB trained two homogeneous classifiers based on different independent feature spaces . 
	</s>
	

	<s id="118">
		 However , this approach is not applicable to our work since only a few temporal indicators occur in each case . 
	</s>
	

	<s id="119">
		 Therefore , we develop an alternative CB algorithm , i.e. to train two different classifiers based on the same feature spaces . 
	</s>
	

	<s id="120">
		 PDT ( a non-linear classifier ) and NBC ( a linear classifier ) are under consideration . 
	</s>
	

	<s id="121">
		 This is inspired by Blum and Mitchell’s theory that two collaborative classifiers should be conditionally , ( m = 2,3 , ... , n ) . 
	</s>
	

	<s id="122">
		 score(c | A , ... , An |c )p( c p(A1 , A2 , A3 ^ p(Ai fl 11 p(Ai i=1 | c)p(c) independent so that each classifier can make its own contribution 
		<ref citStr="Blum and Mitchell , 1998" id="23" label="CEPF" position="19257">
			( Blum and Mitchell , 1998 )
		</ref>
		 . 
	</s>
	

	<s id="123">
		 The learning steps are outlined in Figure 2. Inputs : A collection of the labeled cases and unlabeled cases is prepared . 
	</s>
	

	<s id="124">
		 The labeled cases are separated into three parts , training cases , test cases and held-out cases . 
	</s>
	

	<s id="125">
		 Loop : While the breaking criteria is not satisfied 1 Build the PDT and NBC classifiers us- ing training cases 2 Use PDT and NBC to classify the unla- beled cases , and exchange with the selected cases which have higher Classification Confidence ( i.e. the uncertainty is less than a threshold ) . 
	</s>
	

	<s id="126">
		 3 Evaluate the PDT and NBC classifiers with the held-out cases . 
	</s>
	

	<s id="127">
		 If the error rate increases or its reduction is below a threshold break the loop ; else go to step 1 . 
	</s>
	

	<s id="128">
		 Output : Use the optimal classifier to label the test cases Figure 2. Collaborative bootstrapping algorithm 3.4 Classification Confidence Measurement Classification confidence is the metric used to measure the correctness of each labeled case automatically ( see Step 2 in Figure 2 ) . 
	</s>
	

	<s id="129">
		 The desirable metric should satisfy two principles : • It should be able to measure the uncertainty/ certainty of the output of the classifiers ; and • It should be easy to calculate . 
	</s>
	

	<s id="130">
		 We adopt entropy , i.e. an information theory based criterion , for this purpose . 
	</s>
	

	<s id="131">
		 Let x be the classi- fied object , and C = { c1 , c2 , c3 , ... , cn } the set of output . 
	</s>
	

	<s id="132">
		 x is classified as ci with the probability p(ci | x ) i =1,2,3 , .. , n . 
	</s>
	

	<s id="133">
		 The entropy of the output is then calculated as : n e(C | x)=^^p(ci | x ) log p(ci | x ) ( 9 ) i = 1 Once p(ci | x ) is known , the entropy can be deter- mined . 
	</s>
	

	<s id="134">
		 These parameters can be easily determined in PDT , as each incoming case is classified into each class with a probability . 
	</s>
	

	<s id="135">
		 However , the incoming cases in NBC are grouped into one class which is assigned the highest score . 
	</s>
	

	<s id="136">
		 We then have to estimate p(ci | x ) from those scores . 
	</s>
	

	<s id="137">
		 Without loss of general- ity , the probability is estimated as : p(ci | x ) = nscore(ci | x ) ( 10 ) ^ score c | x ) j where score(ci | x ) is the ranking score of x belonging to ci . 
	</s>
	

	<s id="138">
		 4 Experiment Setup and Evaluation Several experiments have been designed to evaluate the proposed learning approaches and to reveal the impact of linguistic features on learning performance . 
	</s>
	

	<s id="139">
		 700 sentences are extracted from Ta Kong Pao ( a local Hong Kong Chinese newspaper ) financial version . 
	</s>
	

	<s id="140">
		 600 cases are labeled manually and 100 left unlabeled . 
	</s>
	

	<s id="141">
		 Among those labeled , 400 are used as training data , 100 as test data and the rest as held-out data . 
	</s>
	

	<s id="142">
		 4.1 Use of Linguistic Features As Classification Attributes The impact of a temporal indicator is determined by its position in a sentence . 
	</s>
	

	<s id="143">
		 In PDT and NBC , we consider an indicator located in four positions : ( 1 ) BEFORE the first event ; ( 2 ) AFTER the first event and BEFORE the second and it modifies the first event ; ( 3 ) the same as ( 2 ) but it modifies the second event ; and ( 4 ) AFTER the second event . 
	</s>
	

	<s id="144">
		 Cases ( 2 ) and ( 3 ) are ambiguous . 
	</s>
	

	<s id="145">
		 The positions of the temporal indicators are the same . 
	</s>
	

	<s id="146">
		 But it is uncertain whether these indicators modify the first or the second event if there is no punctuation separating their roles . 
	</s>
	

	<s id="147">
		 We introduce two methods , namely NA and SAP to check if the ambiguity affects the two learning approaches . 
	</s>
	

	<s id="148">
		 N(atural) O(rder) : the temporal indicators between the two events are extracted and compared according to their occurrence in the sentences regardless which event they modify . 
	</s>
	

	<s id="149">
		 S(eparate) A(uxiliary) and P(osition) words : we try to resolve the above ambiguity with the grammatical features of the indicators . 
	</s>
	

	<s id="150">
		 In this method , we assume that an indicator modifies the first event if it is an auxiliary word ( e.g. T ) , a trend verb ( e.g. jt~* ) or a position word ( e.g. u ) ; otherwise it modifies the second event . 
	</s>
	

	<s id="151">
		 Temporal indicators are either tense/aspect or connectives ( see Section 2.2 ) . 
	</s>
	

	<s id="152">
		 Intuitively , it seems that classification could be better achieved if connective features are isolated from tense/ aspect features , allowing like to be compared with like . 
	</s>
	

	<s id="153">
		 Methods SC1 and SC2 are designed based on this assumption . 
	</s>
	

	<s id="154">
		 Table 2 shows the effect the different classification methods . 
	</s>
	

	<s id="155">
		 SC1 ( Separate Connecting words 1 ) : it separates conjunctions and verbs relating to causality from others . 
	</s>
	

	<s id="156">
		 They are assumed to contribute to discourse structure ( intra- or inter-sentence structure ) , and the others contribute to the tense/aspect expressions for each individual event . 
	</s>
	

	<s id="157">
		 They are built into 2 separate attributes , one for each event . 
	</s>
	

	<s id="158">
		 j=1 SC2 ( Separate Connecting words 2 ) : it is the same as SC1 except that it combines the connecting word pairs ( i.e. as a single pattern ) into one attribute . 
	</s>
	

	<s id="159">
		 EC ( Event Class ) : it takes event classes into consideration . 
	</s>
	

	<s id="160">
		 Method Accuracy PDT NBC NO 82.00 % 81.00 % SAP 82.20 % 81.50 % SAP +SC1 80.20 % 78.00 % SAP +SC2 81.70 % 79.20 % SAP +EC 85.70 % 82.25 % Table 2 . 
	</s>
	

	<s id="161">
		 Effect of encoding linguistic features in the dif- ferent ways 4.2 Impact of Individual Features From linguistic perspectives , 13 features ( see Table 1 ) are useful for relative relation resolution . 
	</s>
	

	<s id="162">
		 To examine the impact of each individual feature , we feed a single linguistic feature to the PDT learning algorithm one at a time and study the accuracy of the resultant classifier . 
	</s>
	

	<s id="163">
		 The experimental results are given in Table 3 . 
	</s>
	

	<s id="164">
		 It shows that event classes have greatest accuracy , followed by conjunctions in the second place , and adverbs in the third . 
	</s>
	

	<s id="165">
		 Feature Accuracy Feature Accuracy PT 50.5 % VA 56.5 % VS 54 % C 62 % VC 54 % U 51.5 % TR 50.5 % T 57.2 % P 52.2 % D 61.7 % PS 58.7 % EC 68.2 % VS 51.2 % None 50.5 % Table 3 . 
	</s>
	

	<s id="166">
		 Impact of individual linguistic features 4.3 Discussions Analysis of the results in Tables 2 and 3 reveals some linguistic insights : 1 . 
	</s>
	

	<s id="167">
		 In a situation where temporal indicators appear between two events and there is no punctuation mark separating them , POS information help reduce the ambiguity . 
	</s>
	

	<s id="168">
		 Compared with NO , SAP shows a slight improvement from 82 % to 82.2 % . 
	</s>
	

	<s id="169">
		 But the improvement seems trivial and is not as good as our prediction . 
	</s>
	

	<s id="170">
		 This might due to the small percent of such cases in the corpus . 
	</s>
	

	<s id="171">
		 Separating conjunctions and verbs relating to causality from others is ineffective . 
	</s>
	

	<s id="172">
		 This reveals the complexity of Chinese in connecting expressions . 
	</s>
	

	<s id="173">
		 It is because other words ( such as adverbs , proposition and position words ) also serve such a function . 
	</s>
	

	<s id="174">
		 Meanwhile , experiments based on SC1 and SC2 suggest that the connecting ex- pressions generally involve more than one word or phrase . 
	</s>
	

	<s id="175">
		 Although the words in a connecting expression are separated in a sentence , the action is indeed interactive . 
	</s>
	

	<s id="176">
		 It would be more useful to regard them as one attribute . 
	</s>
	

	<s id="177">
		 3. The effect of event classification is striking . 
	</s>
	

	<s id="178">
		 Taking this feature into account , the accuracies of both PDT and NB improved significantly . 
	</s>
	

	<s id="179">
		 As a matter of fact , different event classes may introduce different relations even if they are constrained by the same temporal indicators . 
	</s>
	

	<s id="180">
		 4.4 Collaborative Bootstrapping Table 4 presents the evaluation results of the four different classification approaches . 
	</s>
	

	<s id="181">
		 DM is the default model , which classifies all incoming cases as the most likely class . 
	</s>
	

	<s id="182">
		 It is used as evaluation baseline . 
	</s>
	

	<s id="183">
		 Compare with DM , PDT and NBC show improvement in accuracy ( i.e. above 60 % improvement ) . 
	</s>
	

	<s id="184">
		 And CB in turn outperforms PDT and NBC . 
	</s>
	

	<s id="185">
		 This proves that using unlabeled data to boost the performance of the two classifiers is effective . 
	</s>
	

	<s id="186">
		 Approach Accuracy Close test Open test DM 50.50 % 55.00 % NBC 82.25 % 72.00 % PDT 85.70 % 74.00 % CB 88.70 % 78.00 % Table 4 . 
	</s>
	

	<s id="187">
		 Evaluation of NBC , PDT and CB approaches 5 Conclusions Relative temporal relation resolution received growing attentions in recent years . 
	</s>
	

	<s id="188">
		 It is important for many natural language processing applications , such as information extraction and machine translation . 
	</s>
	

	<s id="189">
		 This topic , however , has not been well studied , especially in Chinese . 
	</s>
	

	<s id="190">
		 In this paper , we propose a model for relative temporal relation resolution in Chinese . 
	</s>
	

	<s id="191">
		 Our model combines linguistic knowledge and machine learning approaches . 
	</s>
	

	<s id="192">
		 Two learning approaches , namely probabilistic decision tree ( PDT ) and naive Bayesian classifier ( NBC ) and 13 linguistic features are employed . 
	</s>
	

	<s id="193">
		 Due to the limited labeled cases , we also propose a collaborative bootstrapping technique to improve learning performance . 
	</s>
	

	<s id="194">
		 The experimental results show that our approaches are encouraging . 
	</s>
	

	<s id="195">
		 To our knowledge , this is the first attempt of collaborative bootstrapping , which involves two heterogeneous classifiers , in NLP application . 
	</s>
	

	<s id="196">
		 This lays down the main contribution of our research . 
	</s>
	

	<s id="197">
		 In this pilot work , temporal indicators are selected based on linguistic knowledge . 
	</s>
	

	<s id="198">
		 It is time-consuming and could be error-prone . 
	</s>
	

	<s id="199">
		 This suggests two directions for future studies . 
	</s>
	

	<s id="200">
		 We will try to automate or at least semi-automate feature selection process . 
	</s>
	

	<s id="201">
		 An- other future work worth investigating is temporal indicator clustering . 
	</s>
	

	<s id="202">
		 There are two methods we could investigate , i.e. clustering the recognized indicators which occur in training corpus according to co-occurrence information or grouping them into two semantic roles , one related to tense/aspect expressions and the other to connecting expressions between two events . 
	</s>
	

	<s id="203">
		 Acknowledgements The work presented in this paper is partially supported by Research Grants Council of Hong Kong ( RGC reference number PolyU5085/02E ) and CUHK Strategic Grant ( account number 4410001 ) . 
	</s>
	

	<s id="204">
		 References Allen J. , 1981 . 
	</s>
	

	<s id="205">
		 An Interval-based Represent Action of Temporal Knowledge . 
	</s>
	

	<s id="206">
		 In Proceedings of 7th International Joint Conference on Artificial Intelligence , pages 221-226 . 
	</s>
	

	<s id="207">
		 Los Altos , CA . 
	</s>
	

	<s id="208">
		 Blum , A. and Mitchell T. , 1998 . 
	</s>
	

	<s id="209">
		 Combining Labeled and Unlabeled Data with Co-Training . 
	</s>
	

	<s id="210">
		 In Proceedings of the Eleventh Annual Conference on Computational Learning Theory , Madison , Wisconsin , pages 92-100 Bruce B. , 1972 . 
	</s>
	

	<s id="211">
		 A Model for Temporal References and its Application in Question-Answering Program . 
	</s>
	

	<s id="212">
		 Artificial Intelligence , 3(1):1-25 . 
	</s>
	

	<s id="213">
		 Collins M. and Singer Y , 1999 . 
	</s>
	

	<s id="214">
		 Unsupervised Models for Named Entity Classification . 
	</s>
	

	<s id="215">
		 In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora , pages 189-196 . 
	</s>
	

	<s id="216">
		 University of Maryland . 
	</s>
	

	<s id="217">
		 Dorr B. and Gaasterland T. , 2002 . 
	</s>
	

	<s id="218">
		 Constraints on the Generation of Tense , Aspect , and Connecting Words from Temporal Expressions . 
	</s>
	

	<s id="219">
		 ( submitted to JAIR ) Hitzeman J. , Moens M. and Grover C. , 1995 . 
	</s>
	

	<s id="220">
		 Algorithms for Analyzing the Temporal Structure of Discourse . 
	</s>
	

	<s id="221">
		 In Proceedings of the 7th European Meeting of the Association for Computational Linguistics , pages 253-260 . 
	</s>
	

	<s id="222">
		 Dublin , Ireland . 
	</s>
	

	<s id="223">
		 Lascarides A. , Asher N. and Oberlander J. , 1992 . 
	</s>
	

	<s id="224">
		 Inferring Discourse Relations in Context . 
	</s>
	

	<s id="225">
		 In Proceedings of the 30th Meeting of the Association for Computational Linguistics , pages 1-8 , Newark , Del. Li W.J. and Wong K.F. , 2002 . 
	</s>
	

	<s id="226">
		 A Word-based Approach for Modeling and Discovering Temporal Relations Embedded in Chinese Sentences , ACM Transaction on Asian Language Processing , 1(3):173-206 . 
	</s>
	

	<s id="227">
		 Ma J. and Knight B. , 1994 . 
	</s>
	

	<s id="228">
		 A General Temporal Theory . 
	</s>
	

	<s id="229">
		 The Computer Journal , 37(2):114- 123 . 
	</s>
	

	<s id="230">
		 Màntaras L. , 1991 . 
	</s>
	

	<s id="231">
		 A Distance-based Attribute Selection Measure for Decision Tree Induction . 
	</s>
	

	<s id="232">
		 Machine Learning , 6(1) : 81–92 . 
	</s>
	

	<s id="233">
		 Màrquez L. , Padró L. and Rodríguez H. , 2000 . 
	</s>
	

	<s id="234">
		 A Machine Learning Approach to POS Tagging . 
	</s>
	

	<s id="235">
		 Machine Learning , 39(1):59-91 . 
	</s>
	

	<s id="236">
		 Kluwer Academic Publishers . 
	</s>
	

	<s id="237">
		 Partee , B. , 1984 . 
	</s>
	

	<s id="238">
		 Nominal and Temporal Anaphora . 
	</s>
	

	<s id="239">
		 Linguistics and Philosophy , 7(3):287-324 . 
	</s>
	

	<s id="240">
		 Quinlan J. , 1993 . 
	</s>
	

	<s id="241">
		 C4.5 Programs for Machine Learning . 
	</s>
	

	<s id="242">
		 Morgan Kauman Press . 
	</s>
	

	<s id="243">
		 Reichenbach H. , 1947 . 
	</s>
	

	<s id="244">
		 Elements of Symbolic Logic . 
	</s>
	

	<s id="245">
		 Berkeley CA , University of California Press . 
	</s>
	

	<s id="246">
		 Siegel E. and McKeown K. , 2000 . 
	</s>
	

	<s id="247">
		 Learning Methods to Combine Linguistic Indicators : Improving Aspectual Classification and Revealing Linguistic Insights . 
	</s>
	

	<s id="248">
		 Computational Linguistics , 26(4) : 595- 627 . 
	</s>
	

	<s id="249">
		 Wiebe , J.M. , O'Hara , T.P. , Ohrstrom-Sandgren , T. and McKeever , K.J , 1998 . 
	</s>
	

	<s id="250">
		 An Empirical Approach to Temporal Reference Resolution . 
	</s>
	

	<s id="251">
		 Journal of Artificial Intelligence Research , 9:247-293 . 
	</s>
	

	<s id="252">
		 Wong F. , Li W. , Yuan C. , etc. , 2002 . 
	</s>
	

	<s id="253">
		 Temporal Representation and Classification in Chinese . 
	</s>
	

	<s id="254">
		 International Journal of Computer Processing of Oriental Languages , 15(2):211-230 . 
	</s>
	

	<s id="255">
		 Yarowsky D. , 1994 . 
	</s>
	

	<s id="256">
		 Decision Lists for Lexical Ambiguity Resolution : Application to the Accent Restoration in Spanish and French . 
	</s>
	

	<s id="257">
		 In Proceeding of the 32rd Annual Meeting of ACL , San Francisco , CA . 
	</s>
	

	<s id="258">
		 Zhou X. , Dillon T. , 1991 . 
	</s>
	

	<s id="259">
		 A Statistical-heuristic Feature Selection Criterion for Decision Tree Induction . 
	</s>
	

	<s id="260">
		 IEEE Transaction on Pattern Analysis and Machine Intelligence , 13(8) : 834-841 . 
	</s>
	


</acldoc>
