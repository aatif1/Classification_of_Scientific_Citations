<?xml version="1.0" encoding="iso-8859-1"?>
<acldoc acl_id="P04-3024">
	

	<s id="1">
		 A New Feature Selection Score for Multinomial Naive Bayes Text Classification Based on KL-Divergence Karl-Michael Schneider Department of General Linguistics University of Passau 94032 Passau , Germany schneide@phil.uni-passau.de Abstract We define a new feature selection score for text classification based on the KL-divergence between the distribution of words in training documents and their classes . 
	</s>
	

	<s id="2">
		 The score favors words that have a similar distribution in documents of the same class but different distributions in documents of different classes . 
	</s>
	

	<s id="3">
		 Experiments on two standard data sets indicate that the new method outperforms mutual information , especially for smaller categories . 
	</s>
	

	<s id="4">
		 1 Introduction Text classification is the assignment of predefined categories to text documents . 
	</s>
	

	<s id="5">
		 Text classification has many applications in natural language processing tasks such as E-mail filtering , prediction of user preferences and organization of web content . 
	</s>
	

	<s id="6">
		 The Naive Bayes classifier is a popular machine learning technique for text classification because it performs well in many domains , despite its simplicity 
		<ref citStr="Domingos and Pazzani , 1997" id="1" label="CEPF" position="1200">
			( Domingos and Pazzani , 1997 )
		</ref>
		 . 
	</s>
	

	<s id="7">
		 Naive Bayes assumes a stochastic model of document generation . 
	</s>
	

	<s id="8">
		 Using Bayes’ rule , the model is inverted in order to predict the most likely class for a new document . 
	</s>
	

	<s id="9">
		 We assume that documents are generated according to a multinomial event model 
		<ref citStr="McCallum and Nigam , 1998" id="2" label="CEPF" position="1507">
			( McCallum and Nigam , 1998 )
		</ref>
		 . 
	</s>
	

	<s id="10">
		 Thus a document is represented as a vector di = ( xi1 ... xil Vl ) of word counts where V is the vocabulary and each xit E { 0 , 1 , 2 , ... } indicates how often wt occurs in di . 
	</s>
	

	<s id="11">
		 Given model parameters p(wt Icj ) and class prior probabilities p(cj ) and assuming independence of the words , the most likely class for a document di is computed as c* ( di ) = argmax p(cj)p(dIcj ) j ( 1 ) p(wt I cj)n(wt,di) where n(wt , di ) is the number of occurrences of wt in di . 
	</s>
	

	<s id="12">
		 p(wtIcj) and p(cj) are estimated from training documents with known classes , using maximum likelihood estimation with a Laplacean prior : ( 2 ) IV I + Etvl1 Edicc , n(wt , di ) p(cj) = IcjI ( 3 ) ECl j~=1 Icj~I It is common practice to use only a subset of the words in the training documents for classification to avoid overfitting and make classification more efficient . 
	</s>
	

	<s id="13">
		 This is usually done by assigning each word a score f ( wt ) that measures its usefulness for classification and selecting the N highest scored words . 
	</s>
	

	<s id="14">
		 One of the best performing scoring functions for feature selection in text classification is mutual information 
		<ref citStr="Yang and Pedersen , 1997" id="3" label="CEPF" position="2691">
			( Yang and Pedersen , 1997 )
		</ref>
		 . 
	</s>
	

	<s id="15">
		 The mutual information between two random variables , MI(X ; Y ) , measures the amount of information that the value of one variable gives about the value of the other 
		<ref citStr="Cover and Thomas , 1991" id="4" label="CEPF" position="2898">
			( Cover and Thomas , 1991 )
		</ref>
		 . 
	</s>
	

	<s id="16">
		 Note that in the multinomial model , the word variable W takes on values from the vocabulary V . 
	</s>
	

	<s id="17">
		 In order to use mutual information with a multinomial model , one defines new random variables Wt E { 0 , 1 } with p(Wt = 1 ) = p(W = wt ) 
		<ref citStr="McCallum and Nigam , 1998" id="5" label="CEPF" position="3155">
			( McCallum and Nigam , 1998 
		</ref>
		<ref citStr="Rennie , 2001" id="6" label="CEPF" position="3183">
			; Rennie , 2001 )
		</ref>
		 . 
	</s>
	

	<s id="18">
		 Then the mutual information between a word wt and the class variable C is Lp(x , cj)log p p (()(c)) ( 4 ) where p(x , cj ) and p(x) are short for p(Wt = x , cj ) and p(Wt = x ) . 
	</s>
	

	<s id="19">
		 p(x,cj) , p(x) and p(cj) are estimated from the training documents by counting how often wt occurs in each class . 
	</s>
	

	<s id="20">
		 2 Naive Bayes and KL-Divergence There is a strong connection between Naive Bayes and KL-divergence ( Kullback-Leibler divergence , relative entropy ) . 
	</s>
	

	<s id="21">
		 KL-divergence measures how = argmax j lVl p(cj ) H t=1 p(wt I cj ) = 1 + Edicc , n(wt , di ) lCl MI(Wt ; C ) = L j=1 X=0,1 much one probability distribution is different from another 
		<ref citStr="Cover and Thomas , 1991" id="7" label="CERF" position="3895">
			( Cover and Thomas , 1991 )
		</ref>
		 . 
	</s>
	

	<s id="22">
		 It is defined ( for discrete distributions ) by L p(x) log p(x) ( 5 ) KL(p , q ) = q(x) x By viewing a document as a probability distribution over words , Naive Bayes can be interpreted in an information-theoretic framework 
		<ref citStr="Dhillon et al. , 2002" id="8" label="CERF" position="4156">
			( Dhillon et al. , 2002 )
		</ref>
		 . 
	</s>
	

	<s id="23">
		 Let p(wt Id ) = n(wt,d)/IdI . 
	</s>
	

	<s id="24">
		 Taking logarithms and dividing by the length of d , ( 1 ) can be rewritten as c* ( d ) a higher score . 
	</s>
	

	<s id="25">
		 By removing words with a lower score from the vocabulary , the training documents of each class become more similar to each other , and therefore , also to the class , in terms of word distribution . 
	</s>
	

	<s id="26">
		 This leads to more homogeneous classes . 
	</s>
	

	<s id="27">
		 Assuming that the test documents and training documents come from the same distribution , the similarity between the test documents and their respective classes will be increased as well , thus resulting in higher classification accuracy . 
	</s>
	

	<s id="28">
		 We now make this more precise . 
	</s>
	

	<s id="29">
		 Let S = { d1 , ... , dISI } be the set of training documents , and denote the class of di with c(di) . 
	</s>
	

	<s id="30">
		 The average KLdivergence for a word wt between the training documents and their classes is given by = argmax log p(cj) + IVI n(wt , d ) log p(wt I cj ) j L t=1 = argmax 1 LIVI p(wt Id ) log p(wt I cj ) j IdI logp(cj) + t=1 ( 6 ) Adding the entropy of p(W I d ) , we get c* ( d ) = argmax 1 LIVI p(wt I d ) log p(wt I d ) j IdI logp(cj) — t=1 p(wtIcj) = argmin 1 j KL(p(WI d),p(W Icj ) ) —IdI logp(cj) ( 7 ) This means that Naive Bayes assigns to a document d the class which is “most similar” to d in terms of the distribution of words . 
	</s>
	

	<s id="31">
		 Note also that the prior probabilities are usually dominated by document probabilities except for very short documents . 
	</s>
	

	<s id="32">
		 3 Feature Selection using KL-Divergence We define a new scoring function for feature selection based on the following considerations . 
	</s>
	

	<s id="33">
		 In the previous section we have seen that Naive Bayes assigns a document d the class c* such that the “distance” between d and c* is minimized . 
	</s>
	

	<s id="34">
		 A classification error occurs when a test document is closer to some other class than to its true class , in terms of KL-divergence . 
	</s>
	

	<s id="35">
		 We seek to define a scoring function such that words whose distribution in the individual training documents of a class is much different from the distribution in the class ( according to ( 2 ) ) receive a lower score , while words with a similar distribution in all training documents of the same class receive dzES ( 8 ) One problem with ( 8 ) is that in addition to the conditional probabilities p(wt I cj ) for each word and each class , the computation considers each individual document , thus resulting in a time requirement of O ( I S I ).1 In order to avoid this additional complexity , instead of KLt(S) we use an approxima- tion KLt(S) , which is based on the following two assumptions : ( i ) the number of occurrences of wt is the same in all documents that contain wt , ( ii ) all documents in the same class cj have the same length . 
	</s>
	

	<s id="36">
		 Let Njt be the number of documents in cj that contain wt , and let ˜pd ( wt I cj ) = p(wt I cj ) Icj I ( 9 ) be the average probability of wt in those documents in cj that contain wt ( if wt does not occur in cj , set ˜pd ( wt I cj ) = 0 ) . 
	</s>
	

	<s id="37">
		 Then KLt ( S ) reduces to 1 ICI pd ( wt I cj ) KLt(S) = ISI LNjtpd ( wt Icj)log p(wtIcj) j=1 ( 10 ) Plugging in ( 9 ) and ( 3 ) and defining q(wt I cj ) = Njt /I cj I , we get ICI ~KLt(S) = — L p(cj)p(wtIcj)log q(wtIcj) . 
	</s>
	

	<s id="38">
		 ( 11 ) j=1 Note that computing KLt(S) only requires a statistics of the number of words and documents for each ' Note that KLt(S) cannot be computed simultaneously with p(wticj) in one pass over the documents in ( 2 ) : KLt(S) requires p(wt | cj ) when each document is considered , but computing the latter needs iterating over all documents itself . 
	</s>
	

	<s id="39">
		 KLt(S) = IsI L KL(p(wtIdi),p(wtIc(di))) . 
	</s>
	

	<s id="40">
		 class , not per document . 
	</s>
	

	<s id="41">
		 Thus ~KLt ( 5 ) can be computed in O ( I C I ) . 
	</s>
	

	<s id="42">
		 Typically , ICI is much smaller than I5I . 
	</s>
	

	<s id="43">
		 Another important thing to note is the following . 
	</s>
	

	<s id="44">
		 By removing words with an uneven distribution in the documents of the same class , not only the documents in the class , but also the classes themselves may become more similar , which reduces the ability to distinguish between different classes . 
	</s>
	

	<s id="45">
		 Let p(wt) be the number of occurrences of wt in all training documents , divided by the total number of words , q(wt) = ff'1 Nyt/I 5I and define MI KL dKL 1 0.8 0.6 0.4 0.2 010 100 1000 10000 100000 ~Kt ( 5 ) = —p(wt) log q ( wt ) . 
	</s>
	

	<s id="46">
		 ( 12 ) ~Kt ( 5 ) can be interpreted as an approximation of the average divergence of the distribution of wt in the individual training documents from the global distribution ( averaged over all training documents in all classes ) . 
	</s>
	

	<s id="47">
		 If wt is independent of the class , then ~Kt ( 5 ) = ~KLt(5) . 
	</s>
	

	<s id="48">
		 The difference between the two is a measure of the increase in homogeneity of the training documents , in terms of the distribution of wt , when the documents are clustered in their true classes . 
	</s>
	

	<s id="49">
		 It is large if the distribution of wt is similar in the training documents of the same class but dissimilar in documents of different classes . 
	</s>
	

	<s id="50">
		 In analogy to mutual information , we define our new scoring function as the difference KL(wt) = ~Kt(5) — ~KLt(5) . 
	</s>
	

	<s id="51">
		 ( 13 ) We also use a variant of KL , denoted dKL , where p(wt) is estimated according to ( 14 ) : lCl p~ ( wt ) = L p(cy)p(wtIcy) ( 14 ) y=1 and p(wt Icy ) is estimated as in ( 2 ) . 
	</s>
	

	<s id="52">
		 4 Experiments We compare KL and dKL to mutual information , using two standard data sets : 20 Newsgroups2 and Reuters 21578.3 In tokenizing the data , only words consisting of alphabetic characters are used after conversion to lower case . 
	</s>
	

	<s id="53">
		 In addition , all numbers are mapped to a special token NUM . 
	</s>
	

	<s id="54">
		 For 20 Newsgroups we remove the newsgroup headers and use a stoplist consisting of the 100 most frequent words of 2http://www.ai.mit.edu/—j rennie/20Newsgroups/ 3http://www.daviddlewis.com/resources/testcollections/ reuters21578/ Vocabulary Size Figure 1 : Classification accuracy for 20 Newsgroups . 
	</s>
	

	<s id="55">
		 The curves have small error bars . 
	</s>
	

	<s id="56">
		 the British National Corpus.4 We use the ModApte split of Reuters 21578 ( Apt´e et al. , 1994 ) and use only the 10 largest classes . 
	</s>
	

	<s id="57">
		 The vocabulary size is 111868 words for 20 Newsgroups and 22430 words for Reuters . 
	</s>
	

	<s id="58">
		 Experiments with 20 Newsgroups are performed with 5-fold cross-validation , using 80 % of the data for training and 20 % for testing . 
	</s>
	

	<s id="59">
		 We build a single classifier for the 20 classes and vary the number of selected words from 20 to 20000 . 
	</s>
	

	<s id="60">
		 Figure 1 compares classification accuracy for the three scoring functions . 
	</s>
	

	<s id="61">
		 dKL slightly outperforms mutual information , especially for smaller vocabulary sizes . 
	</s>
	

	<s id="62">
		 The difference is statistically significant for 20 to 200 words at the 99 % confidence level , and for 20 to 2000 words at the 95 % confidence level , using a one-tailed paired t-test . 
	</s>
	

	<s id="63">
		 For the Reuters dataset we build a binary classifier for each of the ten topics and set the number of positively classified documents such that precision equals recall . 
	</s>
	

	<s id="64">
		 Precision is the percentage of positive documents among all positively classified documents . 
	</s>
	

	<s id="65">
		 Recall is the percentage of positive documents that are classified as positive . 
	</s>
	

	<s id="66">
		 In Figures 2 and 3 we report microaveraged and macroaveraged recall for each number of selected words . 
	</s>
	

	<s id="67">
		 Microaveraged recall is the percentage of all positive documents ( in all topics ) that are classified as positive . 
	</s>
	

	<s id="68">
		 Macroaveraged recall is the average of the recall values of the individual topics . 
	</s>
	

	<s id="69">
		 Microaveraged recall gives equal weight to the documents and thus emphasizes larger topics , while macroaveraged recall gives equal weight to the topics and thus emphasizes smaller topics more than microav- 4 http : //www. itri.brighton . 
	</s>
	

	<s id="70">
		 ac.uk/—Adam.Kilgarriff/bnc- readme.html Vocabulary Size Figure 2 : Microaveraged recall on Reuters at break- even point . 
	</s>
	

	<s id="71">
		 Vocabulary Size Figure 3 : Macroaveraged recall on Reuters at break- even point . 
	</s>
	

	<s id="72">
		 eraged recall . 
	</s>
	

	<s id="73">
		 Both KL and dKL achieve slightly higher values for microaveraged recall than mutual information , for most vocabulary sizes ( Fig . 
	</s>
	

	<s id="74">
		 2 ) . 
	</s>
	

	<s id="75">
		 KL performs best at 20000 words with 90.1 % microaveraged recall , compared to 89.3 % for mutual information . 
	</s>
	

	<s id="76">
		 The largest improvement is found for dKL at 100 words with 88.0 % , compared to 86.5 % for mutual information . 
	</s>
	

	<s id="77">
		 For smaller categories , the difference between the KL-divergence based scores and mutual information is larger , as indicated by the curves for macroaveraged recall ( Fig . 
	</s>
	

	<s id="78">
		 3 ) . 
	</s>
	

	<s id="79">
		 KL yields the highest recall at 20000 words with 82.2 % , an increase of 3.9 % compared to mutual information with 78.3 % , whereas dKL has its largest value at 100 words with 78.8 % , compared to 76.1 % for mutual information . 
	</s>
	

	<s id="80">
		 We find the largest improvement at 5000 words with 5.6 % for KL and 2.9 % for dKL , compared to mutual information . 
	</s>
	

	<s id="81">
		 5 Conclusion By interpreting Naive Bayes in an information theoretic framework , we derive a new scoring method for feature selection in text classification , based on the KL-divergence between training documents and their classes . 
	</s>
	

	<s id="82">
		 Our experiments show that it outperforms mutual information , which was one of the best performing methods in previous studies 
		<ref citStr="Yang and Pedersen , 1997" id="9" label="CJPN" position="13659">
			( Yang and Pedersen , 1997 )
		</ref>
		 . 
	</s>
	

	<s id="83">
		 The KL-divergence based scores are especially effective for smaller categories , but additional experiments are certainly required . 
	</s>
	

	<s id="84">
		 In order to keep the computational cost low , we use an approximation instead of the exact KLdivergence . 
	</s>
	

	<s id="85">
		 Assessing the error introduced by this approximation is a topic for future work . 
	</s>
	

	<s id="86">
		 References Chidanand Apt´e , Fred Damerau , and Sholom M. Weiss . 
	</s>
	

	<s id="87">
		 1994. Towards language independent automated learning of text categorization models . 
	</s>
	

	<s id="88">
		 In Proc . 
	</s>
	

	<s id="89">
		 17th ACM SIGIR Conference on Research and Development in Information Retrieval ( SIGIR ’94 ) , pages 23–30 . 
	</s>
	

	<s id="90">
		 Thomas M. Cover and Joy A. Thomas . 
	</s>
	

	<s id="91">
		 1991. Elements of Information Theory . 
	</s>
	

	<s id="92">
		 John Wiley , New York . 
	</s>
	

	<s id="93">
		 Inderjit S. Dhillon , Subramanyam Mallela , and Rahul Kumar . 
	</s>
	

	<s id="94">
		 2002. Enhanced word clustering for hierarchical text classification . 
	</s>
	

	<s id="95">
		 In Proc . 
	</s>
	

	<s id="96">
		 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 191– 200 . 
	</s>
	

	<s id="97">
		 Pedro Domingos and Michael Pazzani . 
	</s>
	

	<s id="98">
		 1997. On the optimality of the simple bayesian classifier under zero-one loss . 
	</s>
	

	<s id="99">
		 Machine Learning , 29:103– 130. Andrew McCallum and Kamal Nigam . 
	</s>
	

	<s id="100">
		 1998. A comparison of event models for Naive Bayes text classification . 
	</s>
	

	<s id="101">
		 In Learning for Text Categorization : Papers from the AAAI Workshop , pages 41– 48 . 
	</s>
	

	<s id="102">
		 AAAI Press . 
	</s>
	

	<s id="103">
		 Technical Report WS-98-05 . 
	</s>
	

	<s id="104">
		 Jason D. M. Rennie . 
	</s>
	

	<s id="105">
		 2001. Improving multi-class text classification with Naive Bayes . 
	</s>
	

	<s id="106">
		 Master’s thesis , Massachusetts Institute of Technology . 
	</s>
	

	<s id="107">
		 Yiming Yang and Jan O. Pedersen . 
	</s>
	

	<s id="108">
		 1997. A comparative study on feature selection in text categorization . 
	</s>
	

	<s id="109">
		 In Proc . 
	</s>
	

	<s id="110">
		 14th International Conference on Machine Learning ( ICML-97 ) , pages 412– 420 . 
	</s>
	

	<s id="111">
		 0.710 100 1000 10000 100000 MI KL dKL 1 0.95 0.75 0.9 0.85 0.8 0.710 100 1000 10000 100000 1 0.95 0.9 0.85 0.8 0.75 MI KL dKL 
	</s>
	


</acldoc>
