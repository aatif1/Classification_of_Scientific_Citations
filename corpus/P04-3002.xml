<?xml version="1.0" encoding="iso-8859-1"?>
<acldoc acl_id="P04-3002">
	

	<s id="1">
		 Improving Domain-Specific Word Alignment for Computer Assisted Translation WU Hua , WANG Haifeng Toshiba ( China ) Research and Development Center 5/F. , Tower W2 , Oriental Plaza No. 1 , East Chang An Ave. , Dong Cheng District Beijing , China , 100738 { wuhua , wanghaifeng}@rdc.toshiba.com.cn Abstract This paper proposes an approach to improve word alignment in a specific domain , in which only a small-scale domain-specific corpus is available , by adapting the word alignment information in the general domain to the specific domain . 
	</s>
	

	<s id="2">
		 This approach first trains two statistical word alignment models with the large-scale corpus in the general domain and the small-scale corpus in the specific domain respectively , and then improves the domain-specific word alignment with these two models . 
	</s>
	

	<s id="3">
		 Experimental results show a significant improvement in terms of both alignment precision and recall . 
	</s>
	

	<s id="4">
		 And the alignment results are applied in a computer assisted translation system to improve human translation efficiency . 
	</s>
	

	<s id="5">
		 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation ( SMT ) 
		<ref citStr="Brown et al. , 1993" id="1" label="CEPF" position="1216">
			( Brown et al. , 1993 )
		</ref>
		 . 
	</s>
	

	<s id="6">
		 In previous alignment methods , some researchers modeled the alignments with different statistical models 
		<ref citStr="Wu , 1997" id="2" label="CJPN" position="1334">
			( Wu , 1997 
		</ref>
		<ref citStr="Och and Ney , 2000" id="3" label="CJPN" position="1346">
			; Och and Ney , 2000 
		</ref>
		<ref citStr="Cherry and Lin , 2003" id="4" label="CJPN" position="1367">
			; Cherry and Lin , 2003 )
		</ref>
		 . 
	</s>
	

	<s id="7">
		 Some researchers use similarity and association measures to build alignment links 
		<ref citStr="Ahrenberg et al. , 1998" id="5" label="CJPN" position="1486">
			( Ahrenberg et al. , 1998 
		</ref>
		<ref citStr="Tufis and Barbu , 2002" id="6" label="CJPN" position="1512">
			; Tufis and Barbu , 2002 )
		</ref>
		 . 
	</s>
	

	<s id="8">
		 However , All of these methods require a large-scale bilingual corpus for training . 
	</s>
	

	<s id="9">
		 When the large-scale bilingual corpus is not available , some researchers use existing dictionaries to improve word alignment 
		<ref citStr="Ker and Chang , 1997" id="7" label="CJPF" position="1794">
			( Ker and Chang , 1997 )
		</ref>
		 . 
	</s>
	

	<s id="10">
		 However , few works address the problem of domain-specific word alignment when neither the large-scale domain-specific bilingual corpus nor the domain-specific translation dictionary is available . 
	</s>
	

	<s id="11">
		 This paper addresses the problem of word alignment in a specific domain , where only a small domain-specific corpus is available . 
	</s>
	

	<s id="12">
		 In the domain-specific corpus , there are two kinds of words . 
	</s>
	

	<s id="13">
		 Some are general words , which are also frequently used in the general domain . 
	</s>
	

	<s id="14">
		 Others are domain-specific words , which only occur in the specific domain . 
	</s>
	

	<s id="15">
		 In general , it is not quite hard to obtain a large-scale general bilingual corpus while the available domain-specific bilingual corpus is usually quite small . 
	</s>
	

	<s id="16">
		 Thus , we use the bilingual corpus in the general domain to improve word alignments for general words and the corpus in the specific domain for domain-specific words . 
	</s>
	

	<s id="17">
		 In other words , we will adapt the word alignment information in the general domain to the specific domain . 
	</s>
	

	<s id="18">
		 In this paper , we perform word alignment adaptation from the general domain to a specific domain ( in this study , a user manual for a medical system ) with four steps . 
	</s>
	

	<s id="19">
		 ( 1 ) We train a word alignment model using the large-scale bilingual corpus in the general domain ; ( 2 ) We train another word alignment model using the small-scale bilingual corpus in the specific domain ; ( 3 ) We build two translation dictionaries according to the alignment results in ( 1 ) and ( 2 ) respectively ; ( 4 ) For each sentence pair in the specific domain , we use the two models to get different word alignment results and improve the results according to the translation dictionaries . 
	</s>
	

	<s id="20">
		 Experimental results show that our method improves domain-specific word alignment in terms of both precision and recall , achieving a 21.96 % relative error rate reduction . 
	</s>
	

	<s id="21">
		 The acquired alignment results are used in a generalized translation memory system ( GTMS , a kind of computer assisted translation systems ) 
		<ref citStr="Simard and Langlais , 2001" id="8" label="OEPF" position="3915">
			( Simard and Langlais , 2001 )
		</ref>
		 . 
	</s>
	

	<s id="22">
		 This kind of system facilitates the re-use of existing translation pairs to translate documents . 
	</s>
	

	<s id="23">
		 When translating a new sentence , the system tries to provide the pre-translated examples matched with the input and recommends a translation to the human translator , and then the translator edits the suggestion to get a final translation . 
	</s>
	

	<s id="24">
		 The conventional TMS can only recommend translation examples on the sentential level while GTMS can work on both sentential and sub-sentential levels by using word alignment results . 
	</s>
	

	<s id="25">
		 These GTMS are usually employed to translate various documents such as user manuals , computer operation guides , and mechanical operation manuals . 
	</s>
	

	<s id="26">
		 2 Word Alignment Adaptation 2.1 Bi-directional Word Alignment In statistical translation models 
		<ref citStr="Brown et al. , 1993" id="9" label="CJPN" position="4755">
			( Brown et al. , 1993 )
		</ref>
		 , only one-to-one and more-to-one word alignment links can be found . 
	</s>
	

	<s id="27">
		 Thus , some multi-word units cannot be correctly aligned . 
	</s>
	

	<s id="28">
		 In order to deal with this problem , we perform translation in two directions ( English to Chinese , and Chinese to English ) as described in 
		<ref citStr="Och and Ney , 2000" id="10" label="CERF" position="5067">
			( Och and Ney , 2000 )
		</ref>
		 . 
	</s>
	

	<s id="29">
		 The GIZA++ toolkit 1 is used to perform statistical word alignment . 
	</s>
	

	<s id="30">
		 For the general domain , we use SG1 and SG2 to represent the alignment sets obtained with English as the source language and Chinese as the target language or vice versa . 
	</s>
	

	<s id="31">
		 For alignment links in both sets , we use i for English words and j for Chinese words . 
	</s>
	

	<s id="32">
		 SG1 = { ( Aj , j ) | Aj = { aj } , aj &gt;_ 0 } SG2 = { ( i , Ai ) |Ai = { ai } , ai &gt;_ 0 } Where , ak ( k = i , j ) is the position of the source word aligned to the target word in position k . 
	</s>
	

	<s id="33">
		 The set Ak(k=i,j) indicates the words aligned to the same source word k . 
	</s>
	

	<s id="34">
		 For example , if a Chinese word in position j is connect to an English word in position i , then aj = i . 
	</s>
	

	<s id="35">
		 And if a Chinese word in position j is connect to English words in position i and k , then Aj = { i , k Based on the above two alignment sets , we obtain their intersection set , union set 2 and subtraction set . 
	</s>
	

	<s id="36">
		 Intersection : SG = SG1 ^ SG2 Union : PG = SG1 ^ SG2 Subtraction : MG = PG ^ SG For the specific domain , we use SFl and SF2 to represent the word alignment sets in the two directions . 
	</s>
	

	<s id="37">
		 The symbols SF , PF and MF represents the intersection set , union set and the subtraction set , respectively . 
	</s>
	

	<s id="38">
		 2.2 Translation Dictionary Acquisition When we train the statistical word alignment model with a large-scale bilingual corpus in the general domain , we can get two word alignment results for the training data . 
	</s>
	

	<s id="39">
		 By taking the intersection of the two word alignment results , we build a new alignment set . 
	</s>
	

	<s id="40">
		 The alignment links in this intersection set are extended by iteratively adding 1 It is located at http://www.isi.edu/~och/GIZA++.html 2 In this paper , the union operation does not remove the replicated elements . 
	</s>
	

	<s id="41">
		 For example , if set one includes two elements { 1 , 2 } and set two includes two elements { 1 , 3 } , then the union of these two sets becomes { 1 , 1 , 2 , 3 } . 
	</s>
	

	<s id="42">
		 word alignment links into it as described in 
		<ref citStr="Och and Ney , 2000" id="11" label="CERF" position="7166">
			( Och and Ney , 2000 )
		</ref>
		 . 
	</s>
	

	<s id="43">
		 Based on the extended alignment links , we build an English to Chinese translation dictionary D1 with translation probabilities . 
	</s>
	

	<s id="44">
		 In order to filter some noise caused by the error alignment links , we only retain those translation pairs whose translation probabilities are above a threshold ^1 or co-occurring frequencies are above a threshold ^2 . 
	</s>
	

	<s id="45">
		 When we train the IBM statistical word alignment model with a limited bilingual corpus in the specific domain , we build another translation dictionary D2 with the same method as for the dictionary D1 . 
	</s>
	

	<s id="46">
		 But we adopt a different filtering strategy for the translation dictionary D2 . 
	</s>
	

	<s id="47">
		 We use log-likelihood ratio to estimate the association strength of each translation pair because 
		<ref citStr="Dunning ( 1993 )" id="12" label="CEPF" position="7960">
			Dunning ( 1993 )
		</ref>
		 proved that log-likelihood ratio performed very well on small-scale data . 
	</s>
	

	<s id="48">
		 Thus , we get the translation dictionary D2 by keeping those entries whose log-likelihood ratio scores are greater than a threshold ^3 . 
	</s>
	

	<s id="49">
		 2.3 Word Alignment Adaptation Algorithm Based on the bi-directional word alignment , we define SI as SI = SG ^ SF and UG as UG = PG ^ PF ^ SI . 
	</s>
	

	<s id="50">
		 The word alignment links in the set SI are very reliable . 
	</s>
	

	<s id="51">
		 Thus , we directly accept them as correct links and add them into the final alignment set WA . 
	</s>
	

	<s id="52">
		 Input : Alignment set SIand UG Output : Updated alignment set WA Figure 1 . 
	</s>
	

	<s id="53">
		 Word Alignment Adaptation Algorithm ( 1 ) For alignment links in SI , we directly add them into the final alignment set WA . 
	</s>
	

	<s id="54">
		 ( 2 ) For each English word i in the UG , we firstfind its different alignment links , and then do the following : a ) If there are alignment links found in dictionary D1 , add the link with the largest probability to WA . 
	</s>
	

	<s id="55">
		 b ) Otherwise , if there are alignment links found in dictionary D2 , add the link with the largest log-likelihood ratio score to WA . 
	</s>
	

	<s id="56">
		 c ) If both a ) and b ) fail , but three links select the same target words for the English word i , we add this link into WA . 
	</s>
	

	<s id="57">
		 d ) Otherwise , if there are two different links for this word : one target is a single word , and the other target is a multi-word unit and the words in the multi-word unit have no link in , add this multi-word alignment link to . 
	</s>
	

	<s id="58">
		 WA WA . 
	</s>
	

	<s id="59">
		 } For each source word in the set uG , there are two to four different alignment links . 
	</s>
	

	<s id="60">
		 We first use translation dictionaries to select one link among them . 
	</s>
	

	<s id="61">
		 We first examine the dictionary D1 and then D2 to see whether there is at least an alignment link of this word included in these two dictionaries . 
	</s>
	

	<s id="62">
		 If it is successful , we add the link with the largest probability or the largest log-likelihood ratio score to the final set WA . 
	</s>
	

	<s id="63">
		 Otherwise , we use two heuristic rules to select word alignment links . 
	</s>
	

	<s id="64">
		 The detailed algorithm is described in Figure 1 . 
	</s>
	

	<s id="65">
		 Figure 2 . 
	</s>
	

	<s id="66">
		 Alignment Example Figure 2 shows an alignment result obtained with the word alignment adaptation algorithm . 
	</s>
	

	<s id="67">
		 For example , for the English word “x-ray” , we have two different links in UG . 
	</s>
	

	<s id="68">
		 One is ( x-ray , X ) and the other is ( x-ray , X^^ ) . 
	</s>
	

	<s id="69">
		 And the single Chinese words “^” and “^” have no alignment links in the set WA . 
	</s>
	

	<s id="70">
		 According to the rule d ) , we select the link ( x-ray , X ^^ ) . 
	</s>
	

	<s id="71">
		 The Chinese sentences in both the training set and the testing set are automatically segmented into words . 
	</s>
	

	<s id="72">
		 In order to exclude the effect of the segmentation errors on our alignment results , we correct the segmentation errors in our testing set . 
	</s>
	

	<s id="73">
		 The alignments in the testing set are manually annotated , which includes 1,478 alignment links . 
	</s>
	

	<s id="74">
		 3.2 Overall Performance We use evaluation metrics similar to those in 
		<ref citStr="Och and Ney , 2000" id="13" label="CERF" position="11050">
			( Och and Ney , 2000 )
		</ref>
		 . 
	</s>
	

	<s id="75">
		 However , we do not classify alignment links into sure links and possible links . 
	</s>
	

	<s id="76">
		 We consider each alignment as a sure link . 
	</s>
	

	<s id="77">
		 If we use SG to represent the alignments identified by the proposed methods and SC to denote the reference alignments , the methods to calculate the precision , recall , and f-measure are shown in Equation ( 1 ) , ( 2 ) and ( 3 ) . 
	</s>
	

	<s id="78">
		 According to the definition of the alignment error rate ( AER ) in 
		<ref citStr="Och and Ney , 2000" id="14" label="CERF" position="11536">
			( Och and Ney , 2000 )
		</ref>
		 , AER can be calculated with Equation ( 4 ) . 
	</s>
	

	<s id="79">
		 Thus , the higher the f-measure is , the lower the alignment error rate is . 
	</s>
	

	<s id="80">
		 Thus , we will only give precision , recall and AER values in the experimental results . 
	</s>
	

	<s id="81">
		 precision = |SG ^SC | | SG | ( 1 ) SG ^SC | | |SC | ( 2 ) 3 Evaluation recall = We compare our method with three other methods . 
	</s>
	

	<s id="82">
		 The first method “Gen+Spec” directly combines the corpus in the general domain and in the specific domain as training data . 
	</s>
	

	<s id="83">
		 The second method “Gen” only uses the corpus in the general domain as training data . 
	</s>
	

	<s id="84">
		 The third method “Spec” only uses the domain-specific corpus as training data . 
	</s>
	

	<s id="85">
		 With these training data , the three methods can get their own translation dictionaries . 
	</s>
	

	<s id="86">
		 However , each of them can only get one translation dictionary . 
	</s>
	

	<s id="87">
		 Thus , only one of the two steps a ) and b ) in Figure 1 can be applied to these methods . 
	</s>
	

	<s id="88">
		 The difference between these three methods and our method is that , for each word , our method has four candidate alignment links while the other three methods only has two candidate alignment links . 
	</s>
	

	<s id="89">
		 Thus , the steps c ) and d ) in Figure 1 should not be applied to these three methods . 
	</s>
	

	<s id="90">
		 3.1 Training and Testing Data We have a sentence aligned English-Chinese bilingual corpus in the general domain , which includes 320,000 bilingual sentence pairs , and a sentence aligned English-Chinese bilingual corpus in the specific domain ( a medical system manual ) , which includes 546 bilingual sentence pairs . 
	</s>
	

	<s id="91">
		 From this domain-specific corpus , we randomly select 180 pairs as testing data . 
	</s>
	

	<s id="92">
		 The remained 366 pairs are used as domain-specific training data . 
	</s>
	

	<s id="93">
		 Method Precision Recall AER Ours 0.8363 0.7673 0.1997 Gen+Spec 0.8276 0.6758 0.2559 Gen 0.8668 0.6428 0.2618 Spec 0.8178 0.4769 0.3974 Table 1 . 
	</s>
	

	<s id="94">
		 Word Alignment Adaptation Results We get the alignment results shown in Table 1 by setting the translation probability threshold to ^1 = 0.1 , the co-occurring frequency threshold to ^2 = 5 and log-likelihood ratio score to ^3 = 50 . 
	</s>
	

	<s id="95">
		 From the results , it can be seen that our approach performs the best among others , achieving much higher recall and comparable precision . 
	</s>
	

	<s id="96">
		 It also achieves a 21.96 % relative error rate reduction compared to the method “Gen+Spec” . 
	</s>
	

	<s id="97">
		 This indicates that separately modeling the general words and domain-specific words can effectively improve the word alignment in a specific domain . 
	</s>
	

	<s id="98">
		 2* | | SG ^ SC = fmeasure ( 3 ) SG | | +|SC| AER =1^2*| SG ^SC | =1 | SG | +|SC| fmeasure ( 4 ) 4 Computer Assisted Translation System A direct application of the word alignment result to the GTMS is to get translations for sub-sequences in the input sentence using the pre-translated examples . 
	</s>
	

	<s id="99">
		 For each sentence , there are many sub-sequences . 
	</s>
	

	<s id="100">
		 GTMS tries to find translation examples that match the longest sub-sequences so as to cover as much of the input sentence as possible without overlapping . 
	</s>
	

	<s id="101">
		 Figure 3 shows a sentence translated on the sub-sentential level . 
	</s>
	

	<s id="102">
		 The three panels display the input sentence , the example translations and the translation suggestion provided by the system , respectively . 
	</s>
	

	<s id="103">
		 The input sentence is segmented to three parts . 
	</s>
	

	<s id="104">
		 For each part , the GTMS finds one example to get a translation fragment according to the word alignment result . 
	</s>
	

	<s id="105">
		 By combining the three translation fragments , the GTMS produces a correct translation suggestion “~~~jk)~ CT 49~~o ” Without the word alignment information , the conventional TMS cannot find translations for the input sentence because there are no examples closely matched with it . 
	</s>
	

	<s id="106">
		 Thus , word alignment information can improve the translation accuracy of the GTMS , which in turn reduces editing time of the translators and improves translation efficiency . 
	</s>
	

	<s id="107">
		 Figure 3 . 
	</s>
	

	<s id="108">
		 A Snapshot of the Translation System 5 Conclusion This paper proposes an approach to improve domain-specific word alignment through alignment adaptation . 
	</s>
	

	<s id="109">
		 Our contribution is that our approach improves domain-specific word alignment by adapting word alignment information from the general domain to the specific domain . 
	</s>
	

	<s id="110">
		 Our approach achieves it by training two alignment models with a large-scale general bilingual corpus and a small-scale domain-specific corpus . 
	</s>
	

	<s id="111">
		 Moreover , with the training data , two translation dictionaries are built to select or modify the word alignment links and further improve the alignment results . 
	</s>
	

	<s id="112">
		 Experimental results indicate that our approach achieves a precision of 83.63 % and a recall of 76.73 % for word alignment on a user manual of a medical system , resulting in a relative error rate reduction of 21.96 % . 
	</s>
	

	<s id="113">
		 Furthermore , the alignment results are applied to a computer assisted translation system to improve translation efficiency . 
	</s>
	

	<s id="114">
		 Our future work includes two aspects . 
	</s>
	

	<s id="115">
		 First , we will seek other adaptation methods to further improve the domain-specific word alignment results . 
	</s>
	

	<s id="116">
		 Second , we will use the alignment adaptation results in other applications . 
	</s>
	

	<s id="117">
		 References Lars Ahrenberg , Magnus Merkel and Mikael Andersson . 
	</s>
	

	<s id="118">
		 1998. A Simple Hybrid Aligner for Generating Lexical Correspondences in Parallel Tests . 
	</s>
	

	<s id="119">
		 In Proc . 
	</s>
	

	<s id="120">
		 of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics , pages 29-35 . 
	</s>
	

	<s id="121">
		 Peter F. Brown , Stephen A. Della Pietra , Vincent J. Della Pietra and Robert L. Mercer . 
	</s>
	

	<s id="122">
		 1993. The Mathematics of Statistical Machine Translation : Parameter Estimation . 
	</s>
	

	<s id="123">
		 Computational Linguistics , 19(2) : 263-311 . 
	</s>
	

	<s id="124">
		 Colin Cherry and Dekang Lin . 
	</s>
	

	<s id="125">
		 2003. A Probability Model to Improve Word Alignment . 
	</s>
	

	<s id="126">
		 In Proc . 
	</s>
	

	<s id="127">
		 of the 41st Annual Meeting of the Association for Computational Linguistics , pages 88-95 . 
	</s>
	

	<s id="128">
		 Ted Dunning . 
	</s>
	

	<s id="129">
		 1993 . 
	</s>
	

	<s id="130">
		 Accurate Methods for the Statistics of Surprise and Coincidence . 
	</s>
	

	<s id="131">
		 Computational Linguistics , 19(1) : 61-74 . 
	</s>
	

	<s id="132">
		 Sue J. Ker , Jason S. Chang . 
	</s>
	

	<s id="133">
		 1997. A Class-based Approach to Word Alignment . 
	</s>
	

	<s id="134">
		 Computational Linguistics , 23(2) : 313-343 . 
	</s>
	

	<s id="135">
		 Franz Josef Och and Hermann Ney . 
	</s>
	

	<s id="136">
		 2000. Improved Statistical Alignment Models . 
	</s>
	

	<s id="137">
		 In Proc . 
	</s>
	

	<s id="138">
		 of the 38th Annual Meeting of the Association for Computational Linguistics , pages 440-447 . 
	</s>
	

	<s id="139">
		 Michel Simard and Philippe Langlais . 
	</s>
	

	<s id="140">
		 2001. Sub-sentential Exploitation of Translation Memories . 
	</s>
	

	<s id="141">
		 In Proc . 
	</s>
	

	<s id="142">
		 of MT Summit VIII , pages 335-339 . 
	</s>
	

	<s id="143">
		 Dan Tufis and Ana Maria Barbu . 
	</s>
	

	<s id="144">
		 2002. Lexical Token Alignment : Experiments , Results and Application . 
	</s>
	

	<s id="145">
		 In Proc . 
	</s>
	

	<s id="146">
		 of the Third International Conference on Language Resources and Evaluation , pages 458-465 . 
	</s>
	

	<s id="147">
		 Dekai Wu . 
	</s>
	

	<s id="148">
		 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora . 
	</s>
	

	<s id="149">
		 Computational Linguistics , 23(3) : 377-403 . 
	</s>
	


</acldoc>
