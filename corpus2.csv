data,label
"['\n\t\t Optimization in Multimodal Interpretation Joyce Y. Chai* Pengyu Hong+ Michelle X. Zhou\x87 Zahar Prasov* *Computer Science and Engineering Michigan State University East Lansing , MI 48824 { jchai@cse.msu.edu , prasovz@cse.msu.edu } Abstract In a multimodal conversation , the way users communicate with a system depends on the available interaction channels and the situated context ( e.g. , conversation focus , visual feedback ) . \n\t', '\n\t\t These dependencies form a rich set of constraints from various perspectives such as temporal alignments between different modalities , coherence of conversation , and the domain semantics . \n\t', '\n\t\t There is strong evidence that competition and ranking of these constraints is important to achieve an optimal interpretation . \n\t', '\n\t\t Thus , we have developed an optimization approach for multimodal interpretation , particularly for interpreting multimodal references . \n\t', '\n\t\t A preliminary evaluation indicates the effectiveness of this approach , especially for complex user inputs that involve multiple referring expressions in a speech utterance and multiple gestures . \n\t', '\n\t\t 1 Introduction Multimodal systems provide a natural and effective way for users to interact with computers through multiple modalities such as speech , gesture , and gaze \n\t\t']",Positive
['\n\t\t Since the first appearance of \x93Put-That-There\x94 system \n\t\t'],Positive
"['\n\t\t One important aspect of building multimodal systems is multimodal interpretation , which is a process that identifies the meanings of user inputs . \n\t', '\n\t\t \x87Intelligent Multimedia Interaction IBM T. J. Watson Research Ctr. Hawthorne , NY 10532 mzhou@us.ibm.com In a multimodal conversation , the way users communicate with a system depends on the available interaction channels and the situated context ( e.g. , conversation focus , visual feedback ) . \n\t', '\n\t\t These dependencies form a rich set of constraints from various aspects ( e.g. , semantic , temporal , and contextual ) . \n\t', '\n\t\t A correct interpretation can only be attained by simultaneously considering these constraints . \n\t', '\n\t\t In this process , two issues are important : first , a mechanism to combine information from various sources to form an overall interpretation given a set of constraints ; and second , a mechanism that achieves the best interpretation among all the possible alternatives given a set of constraints . \n\t', '\n\t\t The first issue focuses on the fusion aspect , which has been well studied in earlier work , for example , through unification- based approaches \n\t\t']",Positive
"['\n\t\t This paper focuses on the second issue of optimization . \n\t', '\n\t\t As in natural language interpretation , there is strong evidence that competition and ranking of constraints is important to achieve an optimal interpretation for multimodal language processing . \n\t', '\n\t\t We have developed a graph-based optimization approach for interpreting multimodal references . \n\t', '\n\t\t This approach achieves an optimal interpretation by simultaneously applying semantic , temporal , and contextual constraints . \n\t', '\n\t\t A preliminary evaluation indicates the effectiveness of this approach , particularly for complex user inputs that involve multiple referring expressions in a speech utterance and multiple gestures . \n\t', '\n\t\t In this paper , we first describe the necessities for optimization in multimodal interpretation , then present our graph- based optimization approach and discuss how our approach addresses key principles in Optimality Theory used for natural language interpretation \n\t\t']",Positive
"['\n\t\t +Department of Statistics Harvard University Cambridge , MA 02138 hong@stat.harvard.edu 2 Necessities for Optimization in Multimodal Interpretation In a multimodal conversation , the way a user interacts with a system is dependent not only on the available input channels ( e.g. , speech and gesture ) , but also upon his/her conversation goals , the state of the conversation , and the multimedia feedback from the system . \n\t', '\n\t\t In other words , there is a rich context that involves dependencies from many different aspects established during the interaction . \n\t', '\n\t\t Interpreting user inputs can only be situated in this rich context . \n\t', '\n\t\t For example , the temporal relations between speech and gesture are important criteria that determine how the information from these two modalities can be combined . \n\t', '\n\t\t The focus of attention from the prior conversation shapes how users refer to those objects , and thus , influences the interpretation of referring expressions . \n\t', '\n\t\t Therefore , we need to simultaneously consider the temporal relations between the referring expressions and the gestures , the semantic constraints specified by the referring expressions , and the contextual constraints from the prior conversation . \n\t', '\n\t\t It is important to have a mechanism that supports competition and ranking among these constraints to achieve an optimal interpretation , in particular , a mechanism to allow constraint violation and support soft constraints . \n\t', '\n\t\t We use temporal constraints as an example to illustrate this viewpoint1 . \n\t', '\n\t\t The temporal constraints specify whether multiple modalities can be combined based on their temporal alignment . \n\t', '\n\t\t In earlier work , the temporal constraints are empirically determined based on user studies \n\t\t']",Negative
"['\n\t\t For example , in the unification- based approach \n\t\t']",Negative
"['\n\t\t This is a hard constraint that has to be satisfied in order for the unification to take place . \n\t', '\n\t\t If a given input does not satisfy these hard constraints , the unification fails . \n\t', '\n\t\t In our user studies , we found that , although the majority of user temporal alignment behavior may satisfy pre-defined temporal constraints , there are 1 We implemented a system using real estate as an application domain . \n\t', '\n\t\t The user can interact with a map using both speech and gestures to retrieve information . \n\t', '\n\t\t All the user studies mentioned in this paper were conducted using this system . \n\t', '\n\t\t Speech First Gesture First Total Non-overlap 7 % 45 % 52 % Overlap 8 % 40 % 48 % Total 15 % 85 % 100 % Table 1 : Overall temporal relations between speech and gesture Figure 1 : Temporal relations between speech and gesture for individual users some exceptions . \n\t', '\n\t\t Table 1 shows the percentage of different temporal relations collected from our user studies . \n\t', '\n\t\t The rows indicate whether there is an overlap between speech referring expressions and their accompanied gestures . \n\t', '\n\t\t The columns indicate whether the speech ( more precisely , the referring expressions ) or the gesture occurred first . \n\t', '\n\t\t Consistent with the previous findings \n\t\t']",Positive
"['\n\t\t However , in 15 % of the cases the speech referring expressions were uttered before the gesture occurred . \n\t', '\n\t\t Among those cases , 8 % had an overlap between the referring expressions and the gesture and 7 % had no overlap . \n\t', '\n\t\t Furthermore , as shown in \n\t\t']",Positive
"['\n\t\t Figure 1 shows the temporal alignments from seven individual users in our study . \n\t', '\n\t\t User 2 and User 6 maintained a consistent behavior in that User 2\x92s speech referring expressions always overlapped with gestures and User 6\x92s gesture always occurred ahead of the speech expressions . \n\t', '\n\t\t The other five users exhibited varied temporal alignment between speech and gesture during the interaction . \n\t', '\n\t\t It will be difficult for a system using pre-defined temporal constraints to anticipate and accommodate all these different behaviors . \n\t', '\n\t\t Therefore , it is desirable to have a mechanism that Non-overlap Speech First Non-overlap Gesture First Overlap Speech First Overlap Gesture First 1 0.8 0.6 0.4 0.2 0 User 1 2 3 4 5 6 7 allows violation of these constraints and support soft or graded constraints . \n\t', '\n\t\t 3 A Graph-based Optimization Approach To address the necessities described above , we developed an optimization approach for interpreting multimodal references using graph matching . \n\t', '\n\t\t The graph representation captures both salient entities and their inter-relations . \n\t', '\n\t\t The graph matching is an optimization process that finds the best matching between two graphs based on constraints modeled as links or nodes in these graphs . \n\t', '\n\t\t This type of structure and process is especially useful for interpreting multimodal references . \n\t', '\n\t\t One graph can represent all the referring expressions and their inter-relations , and the other graph can represent all the potential referents . \n\t', '\n\t\t The question is how to match them together to achieve a maximum compatibility given a particular context . \n\t', '\n\t\t 3.1 Overview Graph-based Representation Attribute Relation Graph ( ARG ) \n\t\t']",Positive
"['\n\t\t An ARG consists of a set of nodes that are connected by a set of edges . \n\t', '\n\t\t Each node represents an entity , which in our case is either a referring expression to be resolved or a potential referent . \n\t', '\n\t\t Each node encodes the properties of the corresponding entity including : \x95 Semantic information that indicates the semantic type , the number of potential referents , and the specific attributes related to the Speech : Compare this house , the green house and the brown one Figure 2 : An example of a referring graph corresponding entity ( e.g. , extracted from the referring expressions ) . \n\t', '\n\t\t \x95 Temporal information that indicates the time when the corresponding entity is introduced into the discourse ( e.g. , uttered or gestured ) . \n\t', '\n\t\t Each edge represents a set of relations between two entities . \n\t', '\n\t\t Currently we capture temporal relations and semantic type relations . \n\t', '\n\t\t A temporal relation indicates the temporal order between two related entities during an interaction , which may have one of the following values : \x95 Precede : Node A precedes Node B if the entity represented by Node A is introduced into the discourse before the entity represented by Node B. \x95 Concurrent : Node A is concurrent with Node B if the entities represented by them are referred to or mentioned simultaneously . \n\t', '\n\t\t \x95 Non-concurrent : Node A is non-concurrent with Node B if their corresponding objects/references cannot be referred/mentioned simultaneously . \n\t', '\n\t\t \x95 Unknown : The temporal order between two entities is unknown . \n\t', '\n\t\t It may take the value of any of the above . \n\t', '\n\t\t A semantic type relation indicates whether two related entities share the same semantic type . \n\t', '\n\t\t It currently takes the following discrete values : Same , Different , and Unknown . \n\t', '\n\t\t It could be beneficial in the future to consider a continuous function measuring the rate of compatibility instead . \n\t', '\n\t\t Specially , two graphs are generated . \n\t', '\n\t\t One graph , called the referring graph , captures referring expressions from speech utterances . \n\t', '\n\t\t For example , suppose a user says Compare this house , the green house , and the brown one . \n\t', '\n\t\t Figure 2 show a referring graph that represents three referring expressions from this speech input . \n\t', '\n\t\t Each node captures the semantic information such as the semantic type ( i.e. , Semantic Type ) , the attribute ( Color ) , the number ( Number ) of the potential referents , as well as the temporal information about when this referring expression is uttered ( BeginTime and EndTime ) . \n\t', '\n\t\t Each edge captures the semantic ( e.g. , SemanticTypeRelation ) and temporal relations ( e.g. , TemporalRelation ) between the referring expressions . \n\t', '\n\t\t In this case , since the green house is uttered before the brown one , there is a temporal Precede relationship between these two expressions . \n\t', '\n\t\t Furthermore , according to our heuristic that objects-to-be-compared should share the same semantic type , therefore , the SemanticTypeRelation between two nodes is set to Same . \n\t', '\n\t\t SemanticType : House Number. : 1 Attribute : Color = $ Green BeginTime : 32244242ms EndTime : ... \n\t', '\n\t\t SemanticTypeRelation : Same TemporalRelation : Precede Direction : Node 2 -> Node 3 ... ... Node 2 the green house Node 1 this house Node 3 the brown one Gesture : Point to one position and point to another position Chappaqua First pointing Second pointing Conversation Context Semantic Type Relation : Diff Temporal relation : Same Direction : Ossining Object ID : MLS2365478 SemanticType : House Attribute : Color = $ Brown BeginTime : 32244292 ms SelectionProb : 0.65 ... ... Graph-matching Process Given these graph representations , interpreting multimodal references becomes a graph-matching problem . \n\t', '\n\t\t The goal is to find the best match between a referring graph ( Gs ) and a referent graph ( Gr ) . \n\t', '\n\t\t Suppose \x95 A referring graph Gs = ^{^m} , {^mn}^ , where { ^m } are nodes and { ^mn } are edges connecting nodes ^m and ^n . \n\t', '\n\t\t Nodes in Gs are named referring nodes . \n\t', '\n\t\t \x95 A referent graph Gr = ^{ax} , {rxy}^ , where { ax } are nodes and { rxy } are edges connecting nodes ax and ay . \n\t', '\n\t\t Nodes in Gr are named referent nodes . \n\t', '\n\t\t The following equation finds a match that achieves the maximum compatibility between Gr and Gs : Q(Gr , , Gs ^x ^y ^m ^P(ax n P(a y , ^n )EdgeSim ( rte , Figure 3 : An example of referent graph ) ^x ^P(ax,am)NodeSim(ax,am)+ , ^m ) ( 1 ) In Equation(1) , Q(Gr,Gs) measures the degree of the overall matchbetweenthe referentgraphand the referringgraph . \n\t', '\n\t\t P(ax , ^m ) is the matching probabilitybetween anode ax in the referentgraph and anode ^m in the referringgraph . \n\t', '\n\t\t The overall compatibilitydepends on the similarities between nodes ( NodeSim ) and the similarities between edges ( EdgeSim ) . \n\t', '\n\t\t The function NodeSim(ax,^m) measures the similarity betweenareferentnode ax and areferring node ^m by combiningsemantic constraints andtemporal constraints . \n\t', '\n\t\t The function EdgeSim(rxy,^mn) measures the similarity between rxy and ^mn , whichdepends onthe semantic and temporal constraints ofthe correspondingedges . \n\t', '\n\t\t These functions are described indetail in the next section . \n\t', '\n\t\t We use the graduatedassignmentalgorithm \n\t\t']",Positive
"['\n\t\t The algorithmfirstinitializes P(ax,^m) and then iterativelyupdates the values of P(ax,^m) until itconverges . \n\t', '\n\t\t Whenthe algorithm converges , P(ax,^m) gives the matching probabilities betweenthe referentnode ax and the referringnode ^m thatmaximizes the overall compatibilityfunction . \n\t', '\n\t\t Given this probability matrix , the systemis able to assign the most probable referent(s) to each referring expression . \n\t', '\n\t\t 3.2 Similarity Functions As showninEquation ( 1 ) , the overall compatibility between areferringgraphand a referentgraphdepends onthe node similari ty ) , ^mn Similarly , the second graph , called the referent graph , represents all potential referents from multiple sources ( e.g. , from the last conversation , gestured by the user , etc ) . \n\t', '\n\t\t Each node captures the semantic and temporal information about a potential referent ( e.g. , the time when the potential referent is selected by a gesture ) . \n\t', '\n\t\t Each edge captures the semantic and temporal relations between two potential referents . \n\t', '\n\t\t For instance , suppose the user points to one position and then points to another position . \n\t', '\n\t\t The corresponding referent graph is shown in Figure 3 . \n\t', '\n\t\t The objects inside the first dashed rectangle correspond to the potential referents selected by the first pointing gesture and those inside the second dashed rectangle correspond to the second pointing gesture . \n\t', '\n\t\t Each node also contains a probability that indicates the likelihood of its corresponding object being selected by the gesture . \n\t', '\n\t\t Furthermore , the salient objects from the prior conversation are also included in the referent graph since they could also be the potential referents ( e.g. , the rightmost dashed rectangle in Figure 32 ) . \n\t', '\n\t\t To create these graphs , we apply a grammar- based natural language parser to process speech inputs and a gesture recognition component to process gestures . \n\t', '\n\t\t The details are described in \n\t\t']",Positive
"['\n\t\t 2 Each node from the conversation context is linked to every node corresponding to the first pointing and the second pointing . \n\t', '\n\t\t function and the edge similarity function . \n\t', '\n\t\t Next we give a detailed account of how we defined these functions . \n\t', '\n\t\t Our focus here is not on the actual definitions of those functions ( since they may vary for different applications ) , but rather a mechanism that leads to competition and ranking of constraints . \n\t', '\n\t\t Node Similarity Function Given a referring expression ( represented as am in the referring graph ) and a potential referent ( represented as ax in the referent graph ) , the node similarity function is defined based on the semantic and temporal information captured in ax and am through a set of individual compatibility functions : NodeSim(ax,am) = Id(ax,am) SemType(ax,am) rlk Attrk(ax,am) Temp(ax,am) Currently , in our system , the specific return values for these functions are empirically determined through iterative regression tests . \n\t', '\n\t\t Id(a,,,am) captures the constraint of the compatibilities between identifiers specified in ax and am . \n\t', '\n\t\t It indicates that the identifier of the potential referent , as expressed in a referring expression , should match the identifier of the true referent . \n\t', '\n\t\t This is particularly useful for resolving proper nouns . \n\t', '\n\t\t For example , if the referring expression is house number eight , then the correct referent should have the identifier number eight . \n\t', '\n\t\t We currently define this constraint as follows : Id(ax,am) = 0 if the object identities of ax and am are different . \n\t', '\n\t\t Id(ax,am) = 100 if they are the same . \n\t', '\n\t\t Id(ax,am) = 1 if at least one of the identities of ax and am is unknown . \n\t', '\n\t\t The different return values enforce that a large reward is given to the case where the identifiers from the referring expressions match the identifiers from the potential referents . \n\t', '\n\t\t SemType(a,,,am) captures the constraint of semantic type compatibility between ax and am . \n\t', '\n\t\t It indicates that the semantic type of a potential referent as expressed in the referring expression should match the semantic type of the correct referent . \n\t', '\n\t\t We define the following : SemType(ax,am) = 0 if the semantic types of ax and am are different . \n\t', '\n\t\t SemType(ax,am) = 1 if they are the same . \n\t', '\n\t\t SemType(ax,am) = 0.5 if at least one of the semantic types of ax and am is unknown . \n\t', '\n\t\t Note that the return value given to the case where semantic types are the same ( i.e. , \x931\x94 ) is much lower than that given to the case where identifiers are the same ( i.e. , \x93100\x94 ) . \n\t', '\n\t\t This was designed to support constraint ranking . \n\t', '\n\t\t Our assumption is that the constraint on identifiers is more important than the constraint on semantic types . \n\t', '\n\t\t Because identifiers are usually unique , the corresponding constraint is a greater indicator of node matching if the identifier expressed from a referring expression matches the identifier of a potential referent . \n\t', '\n\t\t Attrk(a,,,am) captures the domain specific constraint concerning a particular semantic feature ( indicated by the subscription k ) . \n\t', '\n\t\t This constraint indicates that the expected features of a potential referent as expressed in a referring expression should be compatible with features associated with the true referent . \n\t', '\n\t\t For example , in the referring expression the Victorian house , the style feature is Victorian . \n\t', '\n\t\t Therefore , an object can only be a possible referent if the style of that object is Victorian . \n\t', '\n\t\t Thus , we define the following : Ak(ax,am) = 1 if both ax and am share the kth feature with the same value . \n\t', '\n\t\t Ak(ax,am) = 0 if both ax and am have the feature k and the values of the feature k are not equal . \n\t', '\n\t\t Otherwise , when the kth feature is not present in either ax or am , then Ak ( ax,am ) = 0.1 . \n\t', '\n\t\t Note that these feature constraints are dependent on the specific domain model for a particular application . \n\t', '\n\t\t Temp(a,,,am) captures the temporal constraint between a referring expression am and a potential referent ax . \n\t', '\n\t\t As discussed in Section 2 , a hard constraint concerning temporal relations between referring expressions and gestures will be incapable of handling the flexibility of user temporal alignment behavior . \n\t', '\n\t\t Thus the temporal constraint in our approach is a graded constraint , which is defined as follows : |BeginTime(ax) - BeginTime(am) 2000 This constraint indicates that the closer a referring expression and a potential referent in terms of their temporal alignment ( regardless of the absolute precedence relationship ) , the more compatible they are . \n\t', '\n\t\t Edge Similarity Function The edge similarity function measures the compatibility of relations held between referring expressions ( i.e. , an edge ymn in the referring graph ) Temp(ax,aj = exp(- ) and relations between the potential referents ( i.e. , an edge rxy in the referent graph ) . \n\t', '\n\t\t It is defined by two individual compatibility functions as follows : EdgeSim(rxy , ym\x84 ) = SemType(rxy , ym\x84 ) Temp(rxy , ym\x84 ) SemType(rxy , ymn ) encodes the semantic type compatibility between an edge in the referring graph and an edge in the referent graph . \n\t', '\n\t\t It is defined in Table 2 . \n\t', '\n\t\t This constraint indicates that the relation held between referring expressions should be compatible with the relation held between two correct referents . \n\t', '\n\t\t For example , consider the utterance How much is this green house and this blue house . \n\t', '\n\t\t This utterance indicates that the referent to the first expression this green house should share the same semantic type as the referent to the second expression this blue house . \n\t', '\n\t\t As shown in Table 2 , if the semantic type relations of rxy and ym\x84 are the same , SemType(rxy , ym\x84 ) returns 1 . \n\t', '\n\t\t If they are different , SemType(rxy , ym\x84 ) returns zero . \n\t', '\n\t\t If either rxy or ym\x84 is unknown , then it returns 0.5 . \n\t', '\n\t\t Temp(rxy , ymn ) captures the temporal compatibility between an edge in the referring graph and an edge in the referent graph . \n\t', '\n\t\t It is defined in Table 3 . \n\t', '\n\t\t This constraint indicates that the temporal relationship between two referring expressions ( in one utterance ) should be compatible with the relations of their corresponding referents as they are introduced into the context ( e.g. , through gesture ) . \n\t', '\n\t\t The temporal relation between referring expressions ( i.e. , ym\x84 ) is either Precede or Concurrent . \n\t', '\n\t\t If the temporal relations of rxy and ym\x84 are the same , then Temp(rxy , ym\x84 ) returns 1 . \n\t', '\n\t\t Because potential references could come from prior conversation , even if rxy and ym\x84 are not the same , the function does not return zero when ym\x84 is Precede . \n\t', '\n\t\t Next , we discuss how these definitions and the process of graph matching address optimization , in particular , with respect to key principles of Optimality Theory for natural language interpretation . \n\t', '\n\t\t 3.3 Optimality Theory Optimality Theory ( OT ) is a theory of language and grammar , developed by Alan Prince and Paul Smolensky \n\t\t']",Positive
"['\n\t\t In Optimality Theory , a grammar consists of a set of well-formed constraints . \n\t', '\n\t\t These constraints are applied simultaneously to identify linguistic SemType(rxy , ym\x84 ) rxy Same Different Unknown ym\x84 Same 1 0 0.5 Different 0 1 0.5 Unknown 0.5 0.5 0.5 Table 2 : Definition of SemType(rxy , ym\x84 ) Temp(rxy , ym\x84 ) rxy Preceding Concurrent Non-concurrent Unknown ym\x84 Precede 1 0.5 0.7 0.5 Concurrent 0 1 0 0.5 Table 3 : Definition of Temp(rxy , ym\x84 ) structures . \n\t', '\n\t\t Optimality Theory does not restrict the content of the constraints \n\t\t']",Positive
"['\n\t\t An innovation of Optimality Theory is the conception of these constraints as soft , which means violable and conflicting . \n\t', '\n\t\t The interpretation that arises for an utterance within a certain context maximizes the degree of constraint satisfaction and is consequently the best alternative ( hence , optimal interpretation ) among the set of possible interpretations . \n\t', '\n\t\t The key principles or components of Optimality Theory can be summarized as the following three components \n\t\t']",Positive
"['\n\t\t 2 ) From the set of candidate output , Evaluator selects the optimal output for that input . \n\t', '\n\t\t 3 ) There is a strict dominance in term of the ranking of constraints . \n\t', '\n\t\t Constraints are absolute and the ranking of the constraints is strict in the sense that outputs that have at least one violation of a higher ranked constraint outrank outputs that have arbitrarily many violations of lower ranked constraints . \n\t', '\n\t\t Although Optimality Theory is a grammar-based framework for natural language processing , its key principles can be applied to other representations . \n\t', '\n\t\t At a surface level , our approach addresses these main principles . \n\t', '\n\t\t First , in our approach , the matching matrix P(ax,am) captures the probabilities of all the possible matches between a referring node am and a referent node ax . \n\t', '\n\t\t The matching process updates these probabilities iteratively . \n\t', '\n\t\t This process corresponds to the Generator component in Optimality Theory . \n\t', '\n\t\t Second , in our approach , the satisfaction or violation of constraints is implemented via return values of compatibility functions . \n\t', '\n\t\t These G1 : No Gesture G2 : One Gesture G3 : Multi- Gestures Total Num S1:No referring expression 1(1) , 3(1) , 0 4(2) , 0(0) 0(0) 0(0) S2 : One referring expression 6(4) , 96(89) , 8(7) , 110(90) , 5(2) 58(21) 11(2) 74(25) S3 : Multiple referring expressions 0(0) , 3(1) , 12(8) , 15(9) , 1(0) 7(1) 8(0) 16(1) Total Num 7(5) , 102(91) , 20(15) , 129(111) 6(2) 65(22) 19(2) 90(26) Table 4 : Evaluation Results . \n\t', '\n\t\t In each entry form \x93a(b) , c(d)\x94 , \x93a\x94 indicates the number of inputs in which the referring expressions were correctly recognized by the speech recognizer ; \x93b\x94 indicates the number of inputs in which the referring expressions were correctly recognized and were correctly resolved ; \x93c\x94 indicates the number of inputs in which the referring expressions were not correctly recognized ; \x93d\x94 indicates the number of inputs in which the referring expressions also were not correctly recognized , but were correctly resolved . \n\t', '\n\t\t The sum of \x93a\x94 and \x93c\x94 gives the total number of inputs with a particular combination of speech and gesture . \n\t', '\n\t\t constraints can be violated during the matching process . \n\t', '\n\t\t For example , functions Id(ax,an) , SenType(ax,an) , and Attrk(ax,an) return zero if the corresponding intended constraints are violated . \n\t', '\n\t\t In this case , the overall similarity function will return zero . \n\t', '\n\t\t However , because of the iterative updating nature of the matching algorithm , the system will still find the most optimal match as a result of the matching process even some constraints are violated . \n\t', '\n\t\t Furthermore , A function that never returns zero such as Tenp(ax , an ) in the node similarity function implements a gradient constraint in Optimality Theory . \n\t', '\n\t\t Given these compatibility functions , the graph-matching algorithm provides an optimization process to find the best match between two graphs . \n\t', '\n\t\t This process corresponds to the Evaluator component of Optimality Theory . \n\t', '\n\t\t Third , in our approach , different compatibility functions return different values to address the Constraint Ranking component in Optimality Theory . \n\t', '\n\t\t For example , as discussed earlier , once ax and an share the same identifier , Id(ax,an) returns 100 . \n\t', '\n\t\t If ax and an share the same semantic type , SenType(ax,an) returns 1 . \n\t', '\n\t\t Here , we consider the compatibility between identifiers is more important than the compatibility between semantic types . \n\t', '\n\t\t However , currently we have not yet addressed the strict dominance aspect of Optimality Theory . \n\t', '\n\t\t 3.4 Evaluation We conducted several user studies to evaluate the performance of this approach . \n\t', '\n\t\t Users could interact with our system using both speech and deictic gestures . \n\t', '\n\t\t Each subject was asked to complete five tasks . \n\t', '\n\t\t For example , one task was to find the cheapest house in the most populated town . \n\t', '\n\t\t Data from eleven subjects was collected and analyzed . \n\t', '\n\t\t Table 4 shows the evaluation results of 219 inputs . \n\t', '\n\t\t These inputs were categorized in terms of the number of referring expressions in the speech input and the number of gestures in the gesture inputs . \n\t', '\n\t\t Out of the total 219 inputs , 137 inputs had their referents correctly interpreted . \n\t', '\n\t\t For the remaining 82 inputs in which the referents were not correctly identified , the problem did not come from the approach itself , but rather from other sources such as speech recognition and language understanding errors . \n\t', '\n\t\t These were two major error sources , which were accounted for 55 % and 20 % of total errors respectively \n\t\t']",Positive
"['\n\t\t In our studies , the majority of user references were simple in that they involved only one referring expression and one gesture as in earlier findings \n\t\t']",Positive
"['\n\t\t It is trivial for our approach to handle these simple inputs since the size of the graph is usually very small and there is only one node in the referring graph . \n\t', '\n\t\t However , we did find 23 % complex inputs ( the row S3 and the column G3 in Table 4 ) , which involved multiple referring expressions from speech utterances and/or multiple gestures . \n\t', '\n\t\t Our optimization approach is particularly effective to interpret these complex inputs by simultaneously considering semantic , temporal , and contextual constraints . \n\t', '\n\t\t 4 Conclusion As in natural language interpretation addressed by Optimality Theory , the idea of optimizing constraints is beneficial and there is evidence in favor of competition and constraint ranking in multimodal language interpretation . \n\t', '\n\t\t We developed a graph-based approach to address optimization for multimodal interpretation ; in particular , interpreting multimodal references . \n\t', '\n\t\t Our approach simultaneously applies temporal , semantic , and contextual constraints together and achieves the best interpretation among all alternatives . \n\t', '\n\t\t Although currently the referent graph corresponds to gesture input and conversation context , it can be easily extended to incorporate other modalities such as gaze inputs . \n\t', '\n\t\t We have only taken an initial step to investigate optimization for multimodal language processing . \n\t', '\n\t\t Although preliminary studies have shown the effectiveness of the optimization approach based on graph matching , this approach also has its limitations . \n\t', '\n\t\t The graph-matching problem is a NP complete problem and it can become intractable once the size of the graph is increased . \n\t', '\n\t\t However , we have not experienced the delay of system responses during real-time user studies . \n\t', '\n\t\t This is because most user inputs were relatively concise ( they contained no more than four referring expressions ) . \n\t', '\n\t\t This brevity limited the size of the graphs and thus provided an opportunity for such an approach to be effective . \n\t', '\n\t\t Our future work will address how to extend this approach to optimize the overall interpretation of user multimodal inputs . \n\t', '\n\t\t Acknowledgements This work was partially supported by grant IIS0347548 from the National Science Foundation and grant IRGP-03-42111 from Michigan State University . \n\t', '\n\t\t The authors would like to thank John Hale and anonymous reviewers for their helpful comments and suggestions . \n\t', '\n\t\t References Bolt , R.A. 1980 . \n\t', '\n\t\t Put that there : Voice and Gesture at the Graphics Interface . \n\t', '\n\t\t Computer Graphics , 14(3) : 262-270 . \n\t', '\n\t\t Blutner , R. , 1998 . \n\t', '\n\t\t Some Aspects of Optimality In Natural Language Interpretation . \n\t', '\n\t\t Journal of Semantics , 17 , 189-216 . \n\t', '\n\t\t Cassell , J. , Bickmore , T. , Billinghurst , M. , Campbell , L. , Chang , K. , Vilhjalmsson , H. and Yan , H. 1999 . \n\t', '\n\t\t Embodiment in Conversational Interfaces : Rea . \n\t', ""\n\t\t In Proceedings of the CHI'99 Conference , 520-527 . \n\t"", '\n\t\t Chai , J. , Prasov , Z , and Hong , P. 2004b . \n\t', '\n\t\t Performance Evaluation and Error Analysis for Multimodal Reference Resolution in a Conversational System . \n\t', '\n\t\t Proceedings of HLTNAACL 2004 ( Companion Volumn ) . \n\t', '\n\t\t Chai , J. Y. , Hong , P. , and Zhou , M. X. 2004a . \n\t', '\n\t\t A Probabilistic Approach to Reference Resolution in Multimodal User Interfaces , Proceedings of 9th International Conference on Intelligent User Interfaces ( IUI ) : 70-77 . \n\t', '\n\t\t Chai , J. , Pan , S. , Zhou , M. , and Houck , K. 2002 . \n\t', '\n\t\t Context- based Multimodal Interpretation in Conversational Systems . \n\t', '\n\t\t Fourth International Conference on Multimodal Interfaces . \n\t', '\n\t\t Cohen , P. , Johnston , M. , McGee , D. , Oviatt , S. , Pittman , J. , Smith , I. , Chen , L. , and Clow , J. 1996 . \n\t', '\n\t\t Quickset : Multimodal Interaction for Distributed Applications . \n\t', '\n\t\t Proceedings of ACM Multimedia . \n\t', '\n\t\t Eisner , Jason . \n\t', '\n\t\t 1997. Efficient Generation in Primitive Optimality Theory . \n\t', '\n\t\t Proceedings of ACL\x9297 . \n\t', '\n\t\t Gold , S. and Rangarajan , A. 1996 . \n\t', '\n\t\t A Graduated Assignment Algorithm for Graph-matching . \n\t', '\n\t\t IEEE Trans . \n\t', '\n\t\t Pattern Analysis and Machine Intelligence , vol. 18 , no . \n\t', '\n\t\t 4. Gustafson , J. , Bell , L. , Beskow , J. , Boye J. , Carlson , R. , Edlund , J. , Granstrom , B. , House D. , and Wiren , M. 2000 . \n\t', '\n\t\t AdApt \x96 a Multimodal Conversational Dialogue System in an Apartment Domain . \n\t', '\n\t\t Proceedings of 6th International Conference on Spoken Language Processing ( ICSLP ) . \n\t', '\n\t\t Johnston , M , Cohen , P. , McGee , D. , Oviatt , S. , Pittman , J. and Smith , I. 1997 . \n\t', '\n\t\t Unification-based Multimodal Integration , Proceedings of ACL\x9297 . \n\t', '\n\t\t Johnston , M. 1998 . \n\t', '\n\t\t Unification-based Multimodal Parsing , Proceedings of COLING -ACL\x9298 . \n\t', '\n\t\t Johnston , M. and Bangalore , S. 2000 . \n\t', '\n\t\t Finite-state Multimodal Parsing and Understanding . \n\t', '\n\t\t Proceedings of COLING\x9200 . \n\t', '\n\t\t Johnston , M. , Bangalore , S. , Visireddy G. , Stent , A. , Ehlen , P. , Walker , M. , Whittaker , S. , and Maloor , P. 2002 . \n\t', '\n\t\t MATCH : An Architecture for Multimodal Dialog Systems , Proceedings of ACL\x9202 , Philadelphia , 376-383 . \n\t', '\n\t\t Kehler , A. 2000 . \n\t', '\n\t\t Cognitive Status and Form of Reference in Multimodal Human-Computer Interaction , Proceedings of AAAI\x9201 , 685-689 . \n\t', '\n\t\t Koons , D. B. , Sparrell , C. J. and Thorisson , K. R. 1993 . \n\t', '\n\t\t Integrating Simultaneous Input from Speech , Gaze , and Hand Gestures . \n\t', '\n\t\t In Intelligent Multimedia Interfaces , M. Maybury , Ed . \n\t', '\n\t\t MIT Press : Menlo Park , CA . \n\t', '\n\t\t Neal , J. G. , and Shapiro , S. C. 1991 . \n\t', '\n\t\t Intelligent Multimedia Interface Technology . \n\t', '\n\t\t In Intelligent User Interfaces , J. Sullivan & S. Tyler , Eds . \n\t', '\n\t\t ACM : New York . \n\t', '\n\t\t Oviatt , S. L. 1996 . \n\t', '\n\t\t Multimodal Interfaces for Dynamic Interactive Maps . \n\t', ""\n\t\t In Proceedings of Conference on Human Factors in Computing Systems : CHI '96 , 95-102 . \n\t"", '\n\t\t Oviatt , S. , DeAngeli , A. , and Kuhn , K. , 1997 . \n\t', ""\n\t\t Integration and Synchronization of Input Modes during Multimodal Human-Computer Interaction , In Proceedings of Conference on Human Factors in Computing Systems : CHI '97 . \n\t"", '\n\t\t Oviatt , S. , Coulston , R. , Tomko , S. , Xiao , B. , Bunsford , R. Wesson , M. , and Carmichael , L. 2003 . \n\t', '\n\t\t Toward a Theory of Organized Multimodal Integration Patterns during Human- Computer Interaction . \n\t', '\n\t\t In Proceedings of Fifth International Conference on Multimodal Interfaces , 44-51 . \n\t', '\n\t\t Prince , A. and Smolensky , P. 1993 . \n\t', '\n\t\t Optimality Theory . \n\t', '\n\t\t Constraint Interaction in Generative Grammar . \n\t', '\n\t\t ROA 537. http://roa.rutgers.edu/view.php3?id=845 . \n\t', '\n\t\t Stent , A. , J. Dowding , J. M. Gawron , E. O. Bratt , and R. Moore . \n\t', '\n\t\t 1999. The Commandtalk Spoken Dialog System . \n\t', '\n\t\t Proceedings of ACL\x9299 , 183\x96190 . \n\t', '\n\t\t Tsai , W.H. and Fu , K.S. 1979 . \n\t', '\n\t\t Error-correcting Isomorphism of Attributed Relational Graphs for Pattern Analysis . \n\t', '\n\t\t IEEE Transactions on Systems , Man and Cybernetics. , vol. 9 . \n\t', '\n\t\t Wahlster , W. , 1998 . \n\t', '\n\t\t User and Discourse Models for Multimodal Communication . \n\t', '\n\t\t Intelligent User Interfaces , M. Maybury and W. Wahlster ( eds . \n\t', '\n\t\t ) , 359-370 . \n\t', '\n\t\t Wu , L. , Oviatt , S. , and Cohen , P. 1999 . \n\t', '\n\t\t Multimodal Integration \x96 A Statistical View , IEEE Transactions on Multimedia , Vol. 1 , No. 4 , 334-341 . \n\t', '\n\t\t Constructivist Development of Grounded Construction Grammars Luc Steels University of Brussels ( VUB AI Lab ) SONY Computer Science Lab - Paris 6 Rue Amyot , 75005 Paris steels@arti.vub.ac.be Abstract The paper reports on progress in building computational models of a constructivist approach to language development . \n\t', '\n\t\t It introduces a formalism for construction grammars and learning strategies based on invention , abduction , and induction . \n\t', '\n\t\t Examples are drawn from experiments exercising the model in situated language games played by embodied artificial agents . \n\t', '\n\t\t 1 Introduction The constructivist approach to language learning proposes that \x94children acquire linguistic competence ( ... ) only gradually , beginning with more concrete linguistic structures based on particular words and morphemes , and then building up to more abstract and productive structures based on various types of linguistic categories , schemas , and constructions.\x94 \n\t\t']",Positive
"['\n\t\t The approach furthermore assumes that language development is ( i ) grounded in cognition because prior to ( or in a co-development with language ) there is an understanding and conceptualisation of scenes in terms of events , objects , roles that objects play in events , and perspectives on the event , and ( ii ) grounded in communication because language learning is intimately embedded in interactions with specific communicative goals . \n\t', '\n\t\t In contrast to the nativist position , defended , for example , by Pinker \n\t\t']",Positive
"['\n\t\t Rather , semantic and syntactic categories as well as the way they are linked is built up in a gradual developmental process , starting from quite specific \x91verb-island constructions\x92 . \n\t', '\n\t\t Although the constructivist approach appears to explain a lot of the known empirical data about child language acquisition , there is so far no worked out model that details how constructivist language development works concretely , i.e. what kind of computational mechanisms are implied and how they work together to achieve adult ( or even child ) level competence . \n\t', '\n\t\t Moreover only little work has been done so far to build computational models for handling the sort of \x92construction grammars\x92 assumed by this approach . \n\t', '\n\t\t Both challenges inform the research discussed in this paper . \n\t', '\n\t\t 2 Abductive Learning In the constructivist literature , there is often the implicit assumption that grammatical development is the result of observational learning , and several research efforts are going on to operationalise this approach for acquiring grounded lexicons and grammars ( see e.g. \n\t\t']",Positive
"['\n\t\t The agents are given pairs with a real world situation , as perceived by the sensori-motor apparatus , and a language utterance . \n\t', '\n\t\t For example , an image of a ball is shown and at the same time a stretch of speech containing the word \x93ball\x94 . \n\t', '\n\t\t Based on a generalisation process that uses statistical pattern recognition algorithms or neural networks , the learner then gradually extracts what is common between the various situations in which the same word or construction is used , thus progressively building a grounded lexicon and grammar of a language . \n\t', '\n\t\t The observational learning approach has had some success in learning words for objects and acquiring simple grammatical constructions , but there seem to be two inherent limitations . \n\t', '\n\t\t First , there is the well known poverty of the stimulus argument , widely accepted in linguistics , which says that there is not enough data in the sentences normally available to the language learner to arrive at realistic lexicons and grammars , let alone learn at the same time the categorisations and conceptualisations of the world implied by the language . \n\t', '\n\t\t This has lead many linguists to adopt the nativist position mentioned earlier . \n\t', '\n\t\t The nativist position could in principle be integrated in an observational learning framework by introducing strong biases on the generalisation process , incorporating the constraints of universal grammar , but it has been difficult to identify and operationalise enough of these constraints to do concrete experiments in realistic settings . \n\t', '\n\t\t Second , observational learning assumes that the language system ( lexicon and grammar ) exists as a fixed static system . \n\t', '\n\t\t However , observations of language in use shows that language users constantly align their language conventions to suit the purposes of specific conversations \n\t\t']",Positive
"['\n\t\t Natural languages therefore appear more to be like complex adaptive systems , similar to living systems that constantly adapt and evolve . \n\t', '\n\t\t This makes it difficult to rely exclusively on statistical generalisation . \n\t', '\n\t\t It does not capture the inherently creative nature of language use . \n\t', '\n\t\t This paper explores an alternative approach , which assumes a much more active stance from language users based on the Peircian notion of abduction \n\t\t']",Positive
"['\n\t\t The speaker first attempts to use constructions from his existing inventory to express whatever he wants to express . \n\t', '\n\t\t However when that fails or is judged unsatisfactory , the speaker may extend his existing repertoire by inventing new constructions . \n\t', '\n\t\t These new constructions should be such that there is a high chance that the hearer may be able to guess their meaning . \n\t', '\n\t\t The hearer also uses as much as possible constructions stored in his own inventory to make sense of what is being said . \n\t', '\n\t\t But when there are unknown constructions , or the meanings do not fit with the situation being talked about , the hearer makes an educated guess about what the meaning of the unknown language constructions could be , and adds them as new hypotheses to his own inventory . \n\t', '\n\t\t Abductive constructivist learning hence relies crucially on the fact that both agents have sufficient common ground , share the same situation , have established joint attention , and share communicative goals . \n\t', '\n\t\t Both speaker and hearer use themselves as models of the other in order to guess how the other one will interpret a sentence or why the speaker says things in a particular way . \n\t', '\n\t\t Because both speaker and hearer are taking risks making abductive leaps , a third activity is needed , namely induction , not in the sense of statistical generalisation as in observational learning but in the sense of Peirce \n\t\t']",Positive
"['\n\t\t When a construction leads to a successful interaction , there is some evidence that this construction is ( or could become ) part of the set of conventions adopted by the group , and language users should therefore prefer it in the future . \n\t', '\n\t\t When the construction fails , the language user should avoid it if alternatives are available . \n\t', '\n\t\t Implementing these visions of language learning and use is obviously an enormous challenge for computational linguistics . \n\t', '\n\t\t It requires not only cognitive and communicative grounding , but also grammar formalisms and associated parsing and production algorithms which are extremely flexible , both from the viewpoint of getting as far as possible in the interpretation or production process despite missing rules or incompatibilities in the inventories of speaker and hearer , and from the viewpoint of supporting continuous change . \n\t', '\n\t\t 3 Language Games The research reported here uses a methodological approach which is quite common in Artificial Life research but still relatively novel in ( computational ) linguistics : Rather than attempting to develop simulations that generate natural phenomena directly , as one does when using Newton\x92s equations to simulate the trajectory of a ball falling from a tower , we engage in computational simulations and robotic experiments that create ( new ) artificial phenomena that have some of the characteristics of natural phenomena and hence are seen as explaining them . \n\t', '\n\t\t Specifically , we implement artificial agents with components modeling certain cognitive operations ( such as introducing a new syntactic category , computing an analogy between two events , etc. ) , and then see what language phenomena result if these agents exercise these components in embodied situated language games . \n\t', '\n\t\t This way we can investigate very precisely what causal factors may underly certain phenomena and can focus on certain aspects of ( grounded ) language use without having to face the vast full complexity of real human languages . \n\t', '\n\t\t A survey of work which follows a similar methodology is found in \n\t\t']",Positive
"['\n\t\t The artificial agents used in the experiments driving our research observe real-world scenes through their cameras . \n\t', '\n\t\t The scenes consist of interactions between puppets , as shown in figure 1 . \n\t', '\n\t\t These scenes enact common events like movement of people and objects , actions such as push or pull , give or take , etc. . \n\t', '\n\t\t In order to achieve the cognitive grounding assumed in constructivist language learning , the scenes are processed by a battery of relatively standard machine vision algorithms that segment objects based on color and movement , track objects in real-time , and compute a stream of low- level features indicating which objects are touching , in which direction objects are moving , etc. . \n\t', '\n\t\t These low-level features are input to an event- recognition system that uses an inventory of hierarchical event structures and matches them against the data streaming in from low-level vision , similar to the systems described in \n\t\t']",Positive
"['\n\t\t Figure 1 : Scene enacted with puppets so that typical interactions between humans involving agency can be perceived and described . \n\t', '\n\t\t In order to achieve the communicative grounding required for constructivist learning , agents go through scripts in which they play various language games , similar to the setups described in \n\t\t']",Positive
"['\n\t\t These language games are deliberately quite similar to the kind of scenes and interactions used in a lot of child language research . \n\t', '\n\t\t A language game is a routinised interaction between two agents about a shared situation in the world that involves the exchange of symbols . \n\t', '\n\t\t Agents take turns playing the role of speaker and hearer and give each other feedback about the outcome of the game . \n\t', '\n\t\t In the game further used in this paper , one agent describes to another agent an event that happened in the most recently experienced scene . \n\t', '\n\t\t The game succeeds if the hearer agrees that the event being described occurred in the recent scene . \n\t', '\n\t\t 4 The Lexicon Visual processing and event recognition results in a world model in the form of a series of facts describing the scene . \n\t', '\n\t\t To play the description game , the speaker selects one event as the topic and then seeks a series of facts which discriminate this event and its objects against the other events and objects in the context . \n\t', '\n\t\t We use a standard predicate calculus-style representation for meanings . \n\t', '\n\t\t A semantic structure consists of a set of units where each unit has a referent , which is the object or event to which the unit draws attention , and a meaning , which is a set of clauses constraining the referent . \n\t', '\n\t\t A semantic structure with one unit is for example written down as follows : [1]unit1 ev1 fall(ev1,true) , fall- 1(ev1,obj 1 ) , ball(obj 1 ) where unit1 is the unit , ev1 the referent , and fall(ev1 , true ) , fall- 1(ev1,obj1) , ball(obj1) the meaning . \n\t', '\n\t\t The different arguments of an event are decomposed into different predicates . \n\t', '\n\t\t For example , for \x93John gives a book to Mary\x94 , there would be four clauses : give(ev1,true) for the event itself , give-1(ev1 , John ) , for the one who gives , give-2(ev1,book1) , for the object given , and give-3(ev1,Mary) , for the recipient . \n\t', '\n\t\t This representation is more flexible and makes it possible to add new components ( like the manner of an event ) at any time . \n\t', '\n\t\t Syntactic structures mirror semantic structures . \n\t', '\n\t\t They also consist of units and the name of units are shared with semantic structures so that cross- reference between them is straightforward . \n\t', '\n\t\t The form aspects of the sentence are represented in a declarative predicate calculus style , using the units as arguments . \n\t', '\n\t\t For example , the following unit is constrained as introducing the string \x93fall\x94 : [ 2 ] unit1 string(unit1 , \x93fall\x94 ) The rule formalism we have developed uses ideas from several existing formalisms , particularly unification grammars and is most similar to the Embodied Construction Grammars proposed in \n\t\t']",Positive
"['\n\t\t Lexical rules link parts of semantic structure with parts of syntactic structure . \n\t', '\n\t\t All rules are reversable . \n\t', '\n\t\t When producing , the left side of a rule is matched against the semantic structure and , if there is a match , the right side is unified with the syntactic structure . \n\t', '\n\t\t Conversely when parsing , the right side is matched against the syntactic structure and the left side unified with the semantic structure . \n\t', '\n\t\t Here is a lexical entry for the word \x94fall\x94 . \n\t', '\n\t\t [ 3 ] ?unit ?ev fall(?ev,?state) , fall- 1(?ev,?obj) ?unit string(?unit,\x93fall\x94) It specifies that a unit whose meaning is fall(?ev,?state) , fall- 1(?ev,?obj) is expressed with the string \x93fall\x94 . \n\t', '\n\t\t Variables are written down with a question mark in front . \n\t', '\n\t\t Their scope is restricted to the structure or rule in which they appear and rule application often implies the renaming of certain variables to take care of the scope constraints . \n\t', '\n\t\t Here is a lexical entry for \x93ball\x94 : [ 4 ] ?unit ?obj ball(?obj) ?unit string(?unit,\x93ball\x94) Lexicon lookup attempts to find the minimal set of rules that covers the total semantic structure . \n\t', '\n\t\t New units may get introduced ( both in the syntactic and semantic structure ) if the meaning of a unit is broken down in the lexicon into more than one word . \n\t', '\n\t\t Thus , the original semantic structure in [ 1 ] results after the application of the two rules [ 3 ] and [ 4 ] in the following syntactic and semantic structures : [5]unit1 ev1 fall(ev1,true) , fall- 1(ev1,obj1) unit2 obj 1 ball(obj 1 ) \x97\x96 unit1 string(unit1 , \x93fall\x94 ) unit2 string(unit2 , \x93ball\x94 ) If this syntactic structure is rendered , it produces the utterance \x93fall ball\x94 . \n\t', '\n\t\t No syntax is implied yet . \n\t', '\n\t\t In the reverse direction , the parser starts with the two units forming the syntactic structure in [ 5 ] and application of the rules produces the following semantic structure : [ 6 ] unit1 ?ev fall(?ev,?state) , fall- 1(?ev,?obj) unit2 ?obj 1 ball(?obj 1 ) The semantic structure in [ 6 ] now contains variables for the referent of each unit and for the various predicate-arguments in their meanings . \n\t', '\n\t\t The interpretation process matches these variables against the facts in the world model . \n\t', '\n\t\t If a single consistent series of bindings can be found , then interpretation is successful . \n\t', '\n\t\t For example , assume that the facts in the meaning part of [ 1 ] are in the world model then matching [ 6 ] against them results in the bindings : [ 7 ] ?ev/ev1 , ?state/true , ?obj/obj 1 , ?obj 1/obj 1 When the same word or the same meaning is covered by more than one rule , a choice needs to be made . \n\t', '\n\t\t Competing rules may develop if an agent invented a new word for a particular meaning but is later confronted with another word used by somebody else for the same meaning . \n\t', '\n\t\t Every rule has a score and in production and parsing , rules with the highest score are preferred . \n\t', '\n\t\t When the speaker performs lexicon lookup and rules were found to cover the complete semantic structure , no new rules are needed . \n\t', '\n\t\t But when some part is uncovered , the speaker should create a new rule . \n\t', '\n\t\t We have experimented so far with a simple strategy where agents lump together the uncovered facts in a unit and create a brand new word , consisting of a randomly chosen configuration of syllables . \n\t', '\n\t\t For example , if no word for ball(obj1) exists yet to cover the semantic structure in [ 1 ] , a new rule such as [ 4 ] can be constructed by the speaker and subsequently used . \n\t', '\n\t\t If there is no word at all for the whole semantic structure in [ 1 ] , a single word covering the whole meaning will be created , giving the effect of holophrases . \n\t', '\n\t\t The hearer first attempts to parse as far as possible the given sentence , and then interprets the resulting semantic structure , possibly using joint attention or other means that may help to find the intended interpretation . \n\t', '\n\t\t If this results in a unique set of bindings , the language game is deemed successful . \n\t', '\n\t\t But if there were parts of the sentence which were not covered by any rule , then the hearer can use abductive learning . \n\t', '\n\t\t The first critical step is to guess as well as possible the meaning of the unknown word(s) . \n\t', '\n\t\t Thus suppose the sentence is \x93fall ball\x94 , resulting in the semantic structure : [ 8 ] unit1 ?ev fall(?ev,?state) , fall- 1(?ev,?obj) If this structure is matched , bindings for ?ev and ?obj are found . \n\t', '\n\t\t The agent can now try to find the possible meaning of the unknown word \x93ball\x94 . \n\t', '\n\t\t He can assume that this meaning must somehow help in the interpretation process . \n\t', '\n\t\t He therefore conceptualises the same way as if he would be the speaker and constructs a distinctive description that draws attention to the event in question , for example by constraining the referent of ?obj with an additional predicate . \n\t', '\n\t\t Although there are usually several ways in which obj 1 differs from other objects in the context . \n\t', '\n\t\t There is a considerable chance that the predicate ball is chosen and hence ball(?obj) is abductively inferred as the meaning of \x93ball\x94 resulting in a rule like [ 4 ] . \n\t', '\n\t\t Agents use induction to test whether the rules they created by invention and abduction have been adopted by the group . \n\t', '\n\t\t Every rule has a score , which is local to each agent . \n\t', '\n\t\t When the speaker or hearer has success with a particular rule , its score is increased and the score of competing rules is decreased , thus implementing lateral inhibition . \n\t', '\n\t\t When there is a failure , the score of the rule that was used is decreased . \n\t', '\n\t\t Because the agents prefer rules with the highest score , there is a positive feedback in the system . \n\t', '\n\t\t The more a word is used for a particular meaning , the more success that word will have . \n\t', '\n\t\t Figure 2 : Winner-take-all effect in words competing for same meaning . \n\t', '\n\t\t The x-axis plots language games and the y-axis the use frequency . \n\t', '\n\t\t Scores rise in all the agents for these words and so progressively we see a winner-take-all effect with one word dominating for the expression of a particular meaning ( see figure 2 ) . \n\t', '\n\t\t Many experiments have by now been performed showing that this kind of lateral inhibition dynamics allows a population of agents to negotiate a shared inventory of form- meaning pairs for content words \n\t\t']",Positive
"['\n\t\t 5 Syntactisation The reader may have noticed that the semantic structure in [ 6 ] resulting from parsing the sentence \x93fall ball\x94 , includes two variables which will both get bound to the same object , namely ?obj , introduced by the predicate fall- 1(?ev,?obj) , and ?obj 1 , introduced by the predicate ball(?obj1) . \n\t', '\n\t\t We say that in this case ?obj and ?obj 1 form an equality . \n\t', '\n\t\t Just from parsing the two words , the hearer cannot know that the object involved in the fall event is the same as the object introduced by ball . \n\t', '\n\t\t He can only figure this out when looking at the scene ( i.e. the world model ) . \n\t', '\n\t\t In fact , if there are several balls in the scene and only one of them is falling , there is no way to know which object is intended . \n\t', '\n\t\t And even if the hearer can figure it out , it is still desirable that the speaker should provide extra-information about equalities to optimise the hearer\x92s interpretation efforts . \n\t', '\n\t\t A major thesis of the present paper is that resolving equivalences between variables is the main motor for the introduction of syntax . \n\t', '\n\t\t To achieve it , the agents could , as a first approximation , use rules like the following one , to be applied after all lexical rules have been applied : [ 9 ] ?unit1 ?ev1 fall- 1(?ev1,?obj2) ?unit2 ?obj2 ball(?obj2) ?unit2 string(?unit2 , \x94ball\x94 ) This rule is formally equivalent to the lexical rules discussed earlier in the sense that it links parts of a semantic structure with parts of a syntactic structure . \n\t', '\n\t\t But now more than one unit is involved . \n\t', '\n\t\t Rule [ 9 ] will do the job , because when unifying its right side with the semantic structure ( in parsing ) ?obj2 unifies with the variables ?obj ( supplied by \x94fall\x94 ) and ?obj 1 ( supplied by \x94ball\x94 ) and this forces them to be equivalent . \n\t', '\n\t\t Note that ?unit1 in [ 9 ] only contains those parts of the original meaning that involve the variables which need to be made equal . \n\t', '\n\t\t The above rule works but is completely specific to this case . \n\t', '\n\t\t It is an example of the ad hoc \x91verb-island\x92 constructions reported in an early stage of child language development . \n\t', '\n\t\t Obviously it is much more desirable to have a more general rule , which can be achieved by introducing syntactic and semantic categories . \n\t', '\n\t\t A semantic category ( such as agent , perfective , countable , male ) is a categorisation of a conceptual relation , which is used to constrain the semantic side of grammatical rules . \n\t', '\n\t\t A syntactic category ( such as noun , verb , nominative ) is a categorisation of a word or a group of words , which can be used to constrain the syntactic side of grammatical rules . \n\t', '\n\t\t A rule using categories can be formed by taking rule [ 9 ] above and turning all predicates or content words into semantic or syntactic categories . \n\t', '\n\t\t [ 10 ] ?unit1 ?ev1 semcat1(?ev1,?obj2) ?unit2 ?obj2 semcat2(?obj2) ?unit1 syncat1 ( ?unit1 ) ?unit2 syncat2(?unit2) The agent then needs to create sem-rules to categorise a predicate as belonging to a semantic category , as in : [ 11 ] ?unit1 ?ev1 fall- 1(?ev1,?obj2) ?unit1 ?ev1 semcat1(?ev1,?obj1) and syn-rules to categorise a word as belonging to a syntactic category , as in : [ 12 ] ?unit1 string(?unit1,\x94fall\x94) ?unit1 ?ev1 syncat1(?unit1) These rules have arrows going only in one direction because they are only applied in one way . \n\t', ""\n\t\t ' During production , the sem-rules are applied first , then the lexical rules , next the syn-rules and then the gram- ' Actually if word morphology is integrated , syn-rules need to be bi-directional , but this topic is not discussed further here due to space limitations . \n\t"", '\n\t\t ?unit1 string(?unit1 , \x94fall\x94 ) matical rules . \n\t', '\n\t\t In parsing , the lexical rules are applied first ( in reverse direction ) , then the syn-rules and the sem-rules , and only then the grammatical rules ( in reverse direction ) . \n\t', '\n\t\t The complete syntactic and semantic structures for example [ 9 ] look as follows : [ 13 ] unit1 ?ev1 fall(?ev1,?state) , fall- 1(?ev1,?obj) , semcat1 ( ?ev 1,?obj ) unit2 ?obj 1 ball(?obj 1 ) , semcat2(?obj 1 ) \x97\x96 unit1 string(unit1 , \x93fall\x94 ) , syncat-1 ( unit 1 ) unit2 string(unit2 , \x93ball\x94 ) , syncat-2(unit2) The right side of rule [ 10 ] matches with this syntactic structure , and if the left side of rule [ 10 ] is unified with the semantic structure in [ 13 ] the variable ?obj2 unifies with ?obj and ?obj 1 , thus resolving the equality before semantic interpretation ( matching against the world model ) starts . \n\t', '\n\t\t How can language users develop such rules ? \n\t', '\n\t\t The speaker can detect equalities that need to be resolved by re-entrance : Before rendering a sentence and communicating it to the hearer , the speaker re- parses his own sentence and interprets it against the facts in his own world model . \n\t', '\n\t\t If the resulting set of bindings contains variables that are bound to the same object after interpretation , then these equalities are candidates for the construction of a rule and new syntactic and semantic categories are made as a side effect . \n\t', '\n\t\t Note how the speaker uses himself as a model of the hearer and fixes problems that the hearer might otherwise encounter . \n\t', '\n\t\t The hearer can detect equalities by first interpreting the sentence based on the constructions that are already part of his own inventory and the shared situation and prior joint attention . \n\t', '\n\t\t These equalities are candidates for new rules to be constructed by the hearer , and they again involve the introduction of syntactic and semantic categories . \n\t', '\n\t\t Note that syntactic and semantic categories are always local to an agent . \n\t', '\n\t\t The same lateral inhibition dynamics is used for grammatical rules as for lexical rules , and so is also a positive feedback loop leading to a winner-take-all effect for grammatical rules . \n\t', '\n\t\t 6 Hierarchy Natural languages heavily use categories to tighten rule application , but they also introduce additional syntactic markings , such as word order , function words , affixes , morphological variation of word forms , and stress or intonation patterns . \n\t', '\n\t\t These markings are often used to signal to which category certain words belong . \n\t', '\n\t\t They can be easily incorporated in the formalism developed so far by adding additional descriptors of the units in the syntactic structure . \n\t', '\n\t\t For example , rule [ 10 ] can be expanded with word order constraints and the introduction of a particle \x93ba\x94 : [ 14 ] ?unit1 ?ev1 semcat1(?ev1,?obj2) ?unit2 ?obj2 semcat2(?obj2) ?unit1 syncat1 ( ?unit1 ) ?unit2 syncat2(?unit2) ?unit3 string ( ?unit3 , \x93ba\x94 ) ?unit4 syn-subunits ( ?unit1 , ?unit2 , ?unit3 ) , preceeds(?unit2 , ?unit3 ) Note that it was necessary to introduce a superunit ?unit4 in order to express the word order constraints between the ba-particle and the unit that introduces the object . \n\t', '\n\t\t Applying this rule as well as the synrules and sem-rules discussed earlier to the semantic structure in [ 5 ] yields : [ 13 ] unit1 ev1 fall(ev1,true) , fall- 1(ev1,obj) , semcat1(ev1,obj) unit2 obj 1 ball(obj 1 ) , semcat2(obj 1 ) \x97\x96 unit1 string(unit1,\x93fall\x94) , syncat-1(unit1) unit2 string(unit2 , \x93ball\x94 ) , syncat-2(unit2) unit3 string(unit3 , \x93ba\x94 ) unit4 syn-subunits( unit1,unit2,unit3 ) , preceeds ( unit2,unit3 ) When this syntactic structure is rendered , it produces \x94fall ball ba\x94 , or equivalently \x94ball ba fall\x94 , because only the order between \x93ball\x94 and \x93ba\x94 is constrained . \n\t', '\n\t\t Obviously the introduction of additional syntactic features makes the learning of grammatical rules more difficult . \n\t', '\n\t\t Natural languages appear to have meta-level strategies for invention and abduction . \n\t', '\n\t\t For example , a language ( like Japanese ) tends to use particles for expressing the roles of objects in events and this usage is a strategy both for inventing the expression of a new relation and for guessing what the use of an unknown word in the sentence might be . \n\t', '\n\t\t Another language ( like Swahili ) uses morphological variations similar to Latin for the same purpose and thus has ended up with a rich set of affixes . \n\t', '\n\t\t In our experiments so far , we have implemented such strategies directly , so that invention and abduction is strongly constrained . \n\t', '\n\t\t We still need to work out a formalism for describing these strategies as meta- rules and research the associated learning mechanisms . \n\t', '\n\t\t Figure 3 : The graph shows the dependency structure as well as the phrase-structure emerging through the application of multiple rules When the same word participates in several rules , we automatically get the emergence of hierarchical structures . \n\t', '\n\t\t For example , suppose that two predicates are used to draw attention to obj 1 in [ 5 ] : ball and red . \n\t', '\n\t\t If the lexicon has two separate words for each predicate , then the initial semantic structure would introduce different variables so that the meaning after parsing \x94fall ball ba red\x94 would be : [ 15 ] fall(?ev,?state) , fall-1(?ev,?obj) , ball ( ?obj ) , red(?obj2) To resolve the equality between ?obj and ?obj2 , the speaker could create the following rule : [ 14 ] ?unit1 ?obj semcat3(?obj) ?unit2 ?obj semcat4(?obj) ?unit1 syncat3(?unit1) ?unit2 syncat4(?unit2) ?unit3 syn-subunits ( unit1,unit2 ) , pre- ceeds(unit1,unit2) The predicate ball is declared to belong to semcat4 and the word \x93ball\x94 to syncat4 . \n\t', '\n\t\t The predicate red belongs to semcat3 and the word \x93red\x94 to syncat3 . \n\t', '\n\t\t Rendering the syntactic structure after application of this rule gives the sentence \x94fall red ball ba\x94 . \n\t', '\n\t\t A hierarchical structure ( figure 3 ) emerges because \x93ball\x94 participates in two rules . \n\t', '\n\t\t 7 Re-use Agents obviously should not invent new conventions from scratch every time they need one , but rather use as much as possible existing categorisations and hence existing rules . \n\t', '\n\t\t This simple economy principle quickly leads to the kind of syntagmatic and paradigmatic regularities that one finds in natural grammars . \n\t', '\n\t\t For example , if the speaker wants to express that a block is falling , no new semantic or syntactic categories or linking rules are needed but block can simply be declared to belong to semcat4 and \x93block\x94 to syncat3 and rule [ 14 ] applies . \n\t', '\n\t\t Re-use should be driven by analogy . \n\t', '\n\t\t In one of the largest experiments we have carried out so far , agents had a way to compute the similarity between two event-structures by pairing the primitive operations making up an event . \n\t', '\n\t\t For example , a pick-up action is decomposed into : an object moving into the direction of another stationary object , the first object then touching the second object , and next the two objects moving together in ( roughly ) the opposite direction . \n\t', '\n\t\t A put-down action has similar sub- events , except that their ordering is different . \n\t', '\n\t\t The roles of the objects involved ( the hand , the object being picked up ) are identical and so their grammatical marking could be re-used with very low risk of being misunderstood . \n\t', '\n\t\t When a speaker reuses a grammatical marking for a particular semantic category , this gives a strong hint to the hearer what kind of analogy is expected . \n\t', '\n\t\t By using these invention and abduction strategies , semantic categories like agent or patient gradually emerged in the artificial grammars . \n\t', '\n\t\t Figure 4 visualises the result of this experiment ( after 700 games between 2 agents taking turns ) . \n\t', '\n\t\t The x-axis ( randomly ) ranks the different predicate-argument relations , the y-axis their markers . \n\t', '\n\t\t Without re-use , every argument would have its own marker . \n\t', '\n\t\t Now several markers ( such as \x93va\x94 or \x93zu\x94 ) cover more than one relation . \n\t', '\n\t\t Figure 4 : More compact grammars result from reuse based on semantic analogies . \n\t', '\n\t\t 8 Conclusions The paper reports significant steps towards the computational modeling of a constructivist approach to language development . \n\t', '\n\t\t It has introduced aspects of a construction grammar formalism that is designed to handle the flexibility required for emergent developing grammars . \n\t', '\n\t\t It also proposed that invention , abduction , and induction are necessary and sufficient for language learning . \n\t', '\n\t\t Much more technical work remains to be done but already significant experimental results have been obtained with embod- ied agents playing situated language games . \n\t', '\n\t\t Most of the open questions concern under what circumstances syntactic and semantic categories should be re-used . \n\t', '\n\t\t Research funded by Sony CSL with additional funding from ESF-OMLL program , EU FET-ECAgents and CNRS OHLL . \n\t', '\n\t\t References Bergen , B.K. and N.C. Chang . \n\t', '\n\t\t 2003. Embodied Construction Grammar in Simulation-Based Language Understanding . \n\t', '\n\t\t TR 02-004 , ICSI , Berkeley . \n\t', '\n\t\t Cangelosi , and D. Parisi 2003 . \n\t', '\n\t\t Simulating the Evo- lution of Language . \n\t', '\n\t\t Springer-Verlag , Berlin . \n\t', '\n\t\t Clark , H. and S. Brennan 1991 . \n\t', '\n\t\t Grounding in communication . \n\t', '\n\t\t In : Resnick , L. J. Levine and S. Teasley ( eds . \n\t', '\n\t\t ) Perspectives on Socially Shared Cognition . \n\t', '\n\t\t APA Books , Washington . \n\t', '\n\t\t p. 127-149 . \n\t', '\n\t\t Fann , K.T. 1970 . \n\t', '\n\t\t Peirce\x92s Theory of Abduction Martinus Nijhoff , The Hague . \n\t', '\n\t\t Roy , D. 2001 . \n\t', '\n\t\t Learning Visually Grounded Words and Syntax of Natural Spoken Language . \n\t', '\n\t\t Evolution of communication 4(1). Pinker , S. 1998 . \n\t', '\n\t\t Learnability and Cognition : The acquisition of Argument Structure . \n\t', '\n\t\t The MIT Press , Cambridge Ma . \n\t', '\n\t\t Steels , L. 2003 Evolving grounded communication for robots . \n\t', '\n\t\t Trends in Cognitive Science . \n\t', '\n\t\t Volume 7 , Issue 7 , July 2003 , pp. 308-312 . \n\t', '\n\t\t Steels , L. and J-C. Baillie 2003 . \n\t', '\n\t\t Shared Grounding ofEvent Descriptions by Autonomous Robots . \n\t', '\n\t\t Journal of Robotics and Autonomous Systems 43 , 2003 , pp. 163-173 . \n\t', '\n\t\t Tomasello , M. and P.J. Brooks 1999 . \n\t', '\n\t\t Early syntactic development : A Construction Grammar approach In : Barrett , M. ( ed . \n\t', '\n\t\t ) ( 1999 ) The Development of Language Psychology Press , London . \n\t', '\n\t\t pp. 161-190 . \n\t', '\n\t\t A distributional model of semantic context effects in lexical processing Scott McDonald Department of Psychology and Institute for Adaptive and Neural Computation University of Edinburgh Edinburgh , Scotland , UK Chris Brew Department of Linguistics and Center for Cognitive Science The Ohio State University Columbus , Ohio USA Abstract One of the most robust findings of experimental psycholinguistics is that the context in which a word is presented influences the effort involved in processing that word . \n\t', '\n\t\t We present a computational model of contextual facilitation based on word co-occurrence vectors , and empirically validate the model through simulation of three representative types of context manipulation : single word priming , multiple-priming and contextual constraint . \n\t', '\n\t\t The aim of our study is to find out whether special-purpose mechanisms are necessary in order to capture the pattern of the experimental results . \n\t', '\n\t\t 1 Introduction In psycholinguistics , lexical access is the process of retrieving a word from the mental lexicon using perceptual and contextual information . \n\t', '\n\t\t In everyday life , the point of this process is to facilitate communication . \n\t', '\n\t\t Many different experimental methodologies have been brought to bear on the study of this process , including visual and auditory lexical decision tasks ( e.g. , Meyer & Schvaneveldt , 1971 ; Moss , Ostrin , Tyler & Marslen-Wilson , 1995 ) , event-related brain potentials ( e.g. , Brown , Hagoort & Chwilla , 2000 ) , and the recording of eye movements during normal reading . \n\t', '\n\t\t The extensive literature concerned with contextual influences on lexical processing divides into three main strands : ( 1 ) lexical priming ( single- word contexts , where the prime-target relation is semantic or associative in nature ) ; ( 2 ) multiple priming ( two or more individual lexical primes ) ; and ( 3 ) contextual constraint ( the set of primes is structured by linguistic relationships with one another ) . \n\t', '\n\t\t Because these effects are robust and apparently automatic , researchers often seek explanations in terms of low-level mechanisms such as spreading activation , compound-cue models ( Ratcliff & McKoon , 1988 ) , and distributed neural network models ( Cree , McRae & McNorgan , 1999 ; Plaut , 1995 ) . \n\t', '\n\t\t When these relatively simple models fail to cover every aspect of the behavioral data , one response has been to develop theories that meld several mechanisms ( Keefe & Neely , 1990 ) . \n\t', '\n\t\t Another response is to prefer simplicity over detailed explanatory power . \n\t', '\n\t\t \n\t\t']",Positive
"[""\n\t\t We present a model even simpler than Plaut and Booth 's . \n\t"", '\n\t\t We demonstrate that distributional information available from the linguistic environment \x97 information about word usage that is inherent in large language corpora \x97 can capture salient aspects of a range of data from the literature . \n\t', '\n\t\t It is not necessary to invoke distinct mechanisms for the different priming settings . \n\t', '\n\t\t Furthermore , we did not need to vary the independently tunable parameters of our algorithm in order to obtain our results . \n\t', '\n\t\t The same model has been used in simulations of eye movement behavior during reading \n\t\t']",Positive
"['\n\t\t 2 Distributional models The normal setting for speech processing is an environment in which acoustic cues are unreliable or absent , so it makes sense for the hearer to draw upon available resources in order to maximize the chances of successful comprehension . \n\t', '\n\t\t Such resources include any prior knowledge that the hearer might have about what the speaker will say next . \n\t', '\n\t\t One way to encode prior knowledge is to construct probabilistically weighted hypotheses about the meaning of upcoming words . \n\t', '\n\t\t Our model , which we call the ICE model ( for Incremental Construction of semantic Expectations ) , is of this type . \n\t', '\n\t\t Specifically , it maintains a vector of probabilities as its representation of the current best guess about the likely location in semantic space of the upcoming word . \n\t', '\n\t\t We use the semantic space defined by the 500 most frequent content words of the spoken portion of the British National Corpus ( BNCspoken ) . \n\t', '\n\t\t When a word is observed , the system updates its meaning representation to reflect the newly arrived information . \n\t', '\n\t\t The update mechanism , which uses standard multivariate distributions from Bayesian statistics , is designed to give greater weight to recent words than to those far in the past . \n\t', '\n\t\t A number of studies have tried to uncover correlations between the similarity structure of word vectors and measurable indicators of human performance , such as lexical priming ( e.g. , Lund , Burgess & Atchley , 1995 ; McDonald & Lowe , 1998 ) and semantic similarity ratings \n\t\t']",Positive
"[""\n\t\t The same representations also play a role in simulations of children 's vocabulary acquisition and synonym choice tests ( Landauer & Dumais , 1997 ) . \n\t"", '\n\t\t All of these studies rely on the basic assumption that word vectors can function as convenient proxies for more highly articulated semantic representations . \n\t', '\n\t\t Our primary claim is that word vectors also provide a compact and perspicuous account of priming phenomena normally ascribed to a multitude of mechanisms . \n\t', '\n\t\t 2.1 The ICE model We use a vector-based representation of the "" best-guess "" hypothesis about context . \n\t', '\n\t\t cy C x Figure 1. Distributional representations of ""sake"",""make"",""nin"" and the null context . \n\t', '\n\t\t The vector representations in Figure 1 encode the number of times the window five words to either side of the target word is discovered to include each of two context words cx and ( Figure 1 shows the semantic space as having two dimensions , rather than the 500 actually used in our simulations . \n\t', '\n\t\t ) The representations for the real words are formed by examining the distribution of context words in the neighborhood of these target words , while the representation of the null context is derived from the distribution of context words over the corpus as a whole . \n\t', '\n\t\t "" Sake "" is shown as having a distributional representation far from that of the null context , while "" make "" and "" run "" are relatively close . \n\t', '\n\t\t Therefore we predict that it will be harder to process "" sake "" in the null context than it is to process the other words . \n\t', '\n\t\t The account of priming : in single word priming , we need two moves within the semantic space . \n\t', '\n\t\t Consider the case of the word "" metal "" being primed by the word "" bronze "" , as shown in Figure 2 . \n\t', ""\n\t\t The first step moves the system 's hypothesis away from the null context . \n\t"", '\n\t\t The resulting intermediate position is shown as the diagonal of the quadrilateral linking "" metal "" to the origin and the null context . \n\t', '\n\t\t c Figure 2 . \n\t', '\n\t\t A distributional account of priming . \n\t', '\n\t\t In the second step , the system needs to move from the intermediate position to the final position , which is the vector representation of the target word . \n\t', '\n\t\t The relative entropy between the intermediate position and the target distribution is our simulation of the effort expended by the lexical processor in understanding the word . \n\t', '\n\t\t The model : Our model is Bayesian ; the vectors shown in the diagrams are summaries of its ongoing estimates of the underlying high- dimensional probability distribution that gives rise to the observed distribution of co- occurring context words . \n\t', '\n\t\t This licenses the use of relative entropy , which we employ as the primary dependent variable in our simulations of semantic context effects . \n\t', '\n\t\t Because every distribution that we consider involves a contribution from a highly unspecific distribution associated with the null context , there are no zeroes in the distributions , and relative entropy can be used directly , with no need for smoothing . \n\t', '\n\t\t The distributions : We can simulate meanings using multinomials \x97 computing relative entropy by comparing entries in the 500- dimensional vectors associated with the context words \x97 but to model the dynamic processes involved in semantic priming we need to represent more than just the maximum of the likelihood . \n\t', '\n\t\t We also want to model the extent to which the lexical processor is committed to the hypothesis that the target will be found in the location that we expect . \n\t', '\n\t\t For reasons of simplicity we prefer distributions that have convenient analytical properties and concise parametric representations . \n\t', '\n\t\t One such is the Dirichlet distribution , which is widely used in Bayesian statistics ( Gelman , Carlin , Stern & Rubin , 1995 ) . \n\t', '\n\t\t We begin with prior information expressed in the form of a Dirichlet , then update it with data drawn from a multinomial . \n\t', '\n\t\t The resulting posterior distribution is also a Dirichlet , albeit one whose parameters have been adjusted to better fit the recently observed data . \n\t', '\n\t\t This closure property ( known in the statistical literature as conjugacy ) is crucial to our application , since it allows us to model both prior and posterior hypotheses in the same way . \n\t', '\n\t\t The difference between the Dirichlet and the multinomial is that the latter is parameterized by a vector of probabilities , subject to the constraint that the sum must be zero , while the Dirichlet is specified by a vector of arbitrary real-valued weights , subject to no such constraint . \n\t', '\n\t\t It represents both a direction in semantic space and the number of "" virtual samples "" on which the estimate of that direction is based . \n\t', ""\n\t\t It can therefore be used in priming simulations to represent both the current best guess about the upcoming word 's position in semantic space and the strength with which this belief is held . \n\t"", '\n\t\t We need to decide how the balance will be struck between the prior and the incoming new word , and we need to implement some discounting strategy to prevent the weight given to the prior from increasing without limit and overwhelming the incoming data . \n\t', '\n\t\t Figure 3 : Discounting To avoid this , we first add together the vectors corresponding to the two words , then shrink the result , as shown in Figure 3 . \n\t', '\n\t\t The sum of the two word vectors is the full diagonal of the quadrilateral , while the shrunken version is just the bold part of the diagonal . \n\t', '\n\t\t Model parameters : The ICE model has two free parameters . \n\t', '\n\t\t The first parameter determines how much weight should be given to prior information . \n\t', '\n\t\t Recall that the ICE model forms its probabilistically weighted hypotheses by integrating prior knowledge ( derived from previous words in the context ) with new data ( the currently encountered word ) . \n\t', '\n\t\t For example , if the sum of the prior weights is 1000 , and the results of 100 new "" multinomial trials "" are recorded , prior knowledge is deemed ten times more important to the outcome than the newly arrived evidence . \n\t', '\n\t\t After every update we scale the total prior weight so that it is constant . \n\t', '\n\t\t This produces a straightforward discounting of old information , and is the simplest approach that we could find that has this biologically plausible property . \n\t', '\n\t\t We set the total prior weight parameter to 2000 by maximizing the predictive probability of a small corpus ( see McDonald , 2000 , for details ) . \n\t', '\n\t\t The second parameter is the scheme for determining the weight to be given to the incoming word . \n\t', '\n\t\t We could have given words weight in proportion to their frequency , but that would have given undue weight to frequent words . \n\t', '\n\t\t We therefore used a fixed size sample , setting the sample size parameter to 500 . \n\t', '\n\t\t Thus , our model weights prior context as four times more significant than the incoming word . \n\t', '\n\t\t We have tested the sensitivity of our results to variations in this parameter , and the results are not significantly impacted by any but the largest changes . \n\t', '\n\t\t Although there are certainly other conceivable discounting schemes , this one is simple , robust , and easy to apply . \n\t', '\n\t\t This tells us how to generate the vectors for the intermediate stage of the priming process , producing new positions in the semantic space . \n\t', '\n\t\t We compare this positions using relative entropy , just as if they were ordinary multinomials . \n\t', '\n\t\t Although it is only an approximation , it works well , as the results below demonstrate . \n\t', '\n\t\t The account of multiple priming : with the Dirichlet-based simulation of priming in hand , the simulation of multiple priming is easy . \n\t', '\n\t\t We just need three steps instead of two . \n\t', '\n\t\t Reaction time modeling : Our Bayesian measure is only one of the components that would be needed in a full mechanistic model of human reaction time ( RT ) behavior . \n\t', '\n\t\t To do justice to the richness of RT data , one would need to model not only the effects of informational context but also those of time pressure and experimental setup . \n\t', '\n\t\t Our model could be used , for example , to parameterize a diffusion model ( Ratcliff & Smith , 2004 ) . \n\t', '\n\t\t 3 Simulations We used the same model settings for three experiments , of which two are reported here . \n\t', '\n\t\t The third is simulation of a contextual constraint study by Altarriba , Kroll , \n\t\t']",Positive
"['\n\t\t This is described in a longer version of the present paper ( McDonald & Brew , 2002 ) . \n\t', ""\n\t\t 3.1 Simulation 1 : single-word priming The first test of the ICE model was to simulate the results of Hodgson 's ( 1991 ) single-word lexical priming study . \n\t"", '\n\t\t We tested the hypothesis that a minimal priming context \x97 a single word \x97 would have a reliable effect on the amount of information conveyed by the target word , and that this effect would pattern with the human behavioral data . \n\t', '\n\t\t Specifically , we predicted that a related prime word ( such as value ) would reduce the relative entropy of a target word ( like worth ) , compared with an unrelated prime ( such as tolerate ) . \n\t', '\n\t\t The difference in ICE values resulting from the divergent influence of the related and unrelated prime words on the form of the posterior distribution was expected to correspond to the difference in lexical decision response times reported by Hodgson ( 1991 , Experiment 1 ) . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t Hodgson found equivalent priming effects for all six types of lexical relation , indicating that priming was not restricted to particular types of prime-target relation , such as the category member stimuli employed by the majority of semantic priming studies . \n\t', '\n\t\t Method From the 144 original prime-target pairs listed in Hodgson ( 1991 , Appendix ) , 48 were removed because either the prime or the target word ( or both ) had a lexeme frequency of less than 25 occurrences in the BNC-spoken . \n\t', '\n\t\t The reliability of co-occurrence vector representations decreases with word frequency ( McDonald & Shillcock , 2001 ) , making it preferable to refrain from collecting statistics for low- frequency words . \n\t', '\n\t\t The number of items remaining in each Lexical Relation condition after frequency thresholding is displayed in Table 1 . \n\t', '\n\t\t The ICE value for each Related prime-target combination was calculated using the model parameter settings detailed earlier . \n\t', '\n\t\t The corresponding value for each Unrelated item was computed as the mean of the ICE values for the target word paired with each of the other primes in the Lexical Relation condition . \n\t', ""\n\t\t ' For example , each Unrelated datapoint in the Antonym condition was computed as the mean of 15 ICE values . \n\t"", '\n\t\t Results and Discussion We conducted a two-way analysis of variance on the simulated priming data generated by the ICE model . \n\t', '\n\t\t The factors were Lexical Relation ( antonyms , synonyms , conceptual associates , phrasal associates , category co-ordinates , superordinate-subordinates ) and Context ( related , unrelated ) . \n\t', '\n\t\t ICE values for each cell of the design are presented in Table 1. ( ICE values can be considered analogous to reaction times , the smaller the value , the shorter the RT ) . \n\t', '\n\t\t As expected , there was a main effect of Context : collapsing across all types of Lexical Relation , relative entropy was significantly less when the target is preceded by a related prime than when it is preceded by an unrelated prime : F(1,90)=71.63 , MSE=0.0037 , p<0.001 . \n\t', '\n\t\t There was no main effect of Lexical Relation : F(5,90)<1 , and importantly , no evidence for a Lexical Relation x Context interaction : F(5,90)<1 . \n\t', '\n\t\t Separate ANOVAs conducted for each type of Relation showed consistent , reliable priming effects for all six relations As was the case for human subjects , Context did not interact with Lexical Relation . \n\t', '\n\t\t There is no evidence here for different mechanisms for Because the unrelated primes corresponding to each target word were not supplied in \n\t\t']",Positive
"['\n\t\t An alternative would be to select a prime word at random from the other items in the same condition to serve as the unrelated prime ; both methods give the same results . \n\t', '\n\t\t the different types of word-to-word relations . \n\t', '\n\t\t We know that ICE is using nothing but distributional information , and it could be that human subjects are doing the same . \n\t', '\n\t\t 3.2 Simulation 2 : multiple priming Simulation 1 demonstrated that single-word lexical priming can be modeled as the influence of the local linguistic context on the quantity of information conveyed by a word about its contextual behavior . \n\t', '\n\t\t In Simulation 2 , we submit the ICE model to a more stringent test : the lexical priming situation where more than one prime word is presented before the target . \n\t', '\n\t\t The multiple priming paradigm \x97 the procedure by which two or more lexical primes precede the target word \x97 is a natural extension of the single-word priming task . \n\t', '\n\t\t Multiple priming can be seen as occupying the middle ground between the lexical priming and contextual constraint paradigms . \n\t', '\n\t\t In multiple priming experiments , the prime words are presented as unstructured lists , but in contextual constraint studies , whole sentences are presented in their original order , and the usual cues to syntactic structure are present . \n\t', '\n\t\t Despite the fact that multiple primes do not form a syntactically coherent unit , research by \n\t\t']",Positive
"['\n\t\t Balota and Paul were interested in how multiple primes \x97 construed as independent sources of spreading activation \x97 influenced target word processing . \n\t', '\n\t\t Using two-word contexts , they separately manipulated the relatedness of each prime to the target word ; this procedure allowed additive priming effects to be accurately measured . \n\t', '\n\t\t In their first experiment , they demonstrated that the multiple- prime advantage was additive : the facilitation obtained in the two-related-primes condition ( RR ) was equivalent to the sum of the facilitation for the one-related-prime conditions ( UR and RU ) . \n\t', '\n\t\t ( See Table 2 for sample stimuli ) . \n\t', '\n\t\t Because they found evidence for simple additivity using a range of prime presentation durations and both lexical decision and naming as response tasks ( Balota & Paul , 1996 , Experiments 1-5 ) , the authors state that "" ... we believe that contextual constraints can produce simple additive influences on target processing . \n\t', '\n\t\t "" ( p. 839 ) . \n\t', ""\n\t\t In terms of the ICE model , two related prime words would need to constrain the processor 's expectations about the meaning of the target to a greater degree than a single related prime in order to simulate the multiple- prime advantage . \n\t"", '\n\t\t Table 1 : Mean ICE Values ( bits ) for Related and Unrelated Primes and Simulated Priming Effect ( Difference ) for Six Types of Lexical Relation Lexical Relation N Context Related Unrelated Effect Semantic Antonym 16 1.133 1.230 0.097 Synonym 11 0.673 0.736 0.063 Associate Conceptual 17 1.086 1.172 0.086 Phrasal 20 1.095 1.153 0.058 Category Coordinates 18 1.165 1.239 0.074 Super-subordinates 14 1.073 1.140 0.067 Table 2 . \n\t', '\n\t\t Results of the Simulation of ( Balota and Paul 1996 , Experiment 1 ) , with Mean Lexical Deci- sion Response Times ( RT ) and Amount of Priming ( Priming ) Condition Prime-1 Prime-2 Target ICE ( bits ) RT Priming ( msec ) ( msec ) Homograph targets RR game drama play 0.895 601 34 UR lip drama play 0.970 618 17 RU game tuna play 0.932 630 5 UU lip tuna play 1.011 635 Category label targets RR hate rage emotion 1.095 606 34 UR author rage emotion 1.151 616 24 RU hate design emotion 1.114 627 13 UU author design emotion 1.193 640 Note : R=related prime , U=Imrelated prime . \n\t', ""\n\t\t Method The design was identical to that of Balota and Paul 's Experiment 1 . \n\t"", '\n\t\t This was a 2 x 4 mixed factors design , with Type of Target ( homograph , category label ) as the between-items factor , and Prime Type ( RR , UR , RU , UU ) as the within- items factor . \n\t', '\n\t\t Preparation of the lexical stimuli was very similar to the procedure carried out in Simulation 1. Inflected stimuli were first converted to their canonical forms , and items containing target or related prime words that did not meet the 25- occurrence frequency threshold were removed . \n\t', '\n\t\t Unrelated prime words that failed to meet the frequency threshold were replaced with unrelated primes randomly chosen from the set of discarded items . \n\t', '\n\t\t From the 106 original homograph items , 69 could be used in the simulation . \n\t', '\n\t\t Out of the 94 original category stimuli , 39 met the frequency criterion . \n\t', '\n\t\t ( See Table 2 for sample materials ) . \n\t', '\n\t\t We computed ICE values for each target word when preceded by each of the four Prime Types . \n\t', '\n\t\t Model parameter settings were identical to those used in Simulation 1 . \n\t', '\n\t\t Results and Discussion As in Simulation 1 , facilitation was simulated by a reduction in relative entropy in one of the Related prime conditions ( RR , RU and UR ) , compared with the UU ( two-unrelated-primes ) condition . \n\t', '\n\t\t Facilitation was apparent for all three Related conditions . \n\t', '\n\t\t The size of the context effect was 0.110 bits for the RR condition , 0.041 bits for the UR condition , and 0.079 bits for the RU condition . \n\t', '\n\t\t These differences in mean ICE value were verified by an analysis of variance , which revealed a main effect of Prime Type , F(3,306)=40.53 , MSE=0.0058 , p<0.001 . \n\t', '\n\t\t There was no reliable effect of Target Type . \n\t', '\n\t\t The pattern of results was closely comparable to the human data . \n\t', '\n\t\t As expected , the strongest context effect was observed in the RR condition , which was larger than the effects in both the UR and RU conditions . \n\t', '\n\t\t This result replicates the multiple-prime advantage reported by Balota and Paul . \n\t', '\n\t\t The results of the ICE simulation did not match the human data completely ; specifically , the context effect for the RU targets was larger than for the HR targets , whereas the pattern observed in human subjects was the opposite . \n\t', '\n\t\t This difference between the RU and HR conditions was statistically reliable : planned comparisons ( with suitable alpha corrections ) confirmed that all four conditions differed reliably from one other , at the a=0.05 level of significance . \n\t', '\n\t\t We investigated further . \n\t', '\n\t\t Briefly , it appears that the discrepancy may be an artifact of the particular choice of experimental materials . \n\t', '\n\t\t The larger simulated priming effect for the RU condition was probably due to the differences between the Prime-1 words and the Prime-2 words . \n\t', '\n\t\t 4 Conclusions and future work Our approach is simple , and involves few tunable parameters , and so lends itself to exploratory work and to the generation of clear and testable hypotheses . \n\t', '\n\t\t It is straightforward , given a large corpus and a sufficiently precise working hypothesis , to create sets of stimulus materials that should produce context effects , and to test them using human participants . \n\t', '\n\t\t Because of the multiplicity of relevant linking relations evidenced by Hodgson , 1991 , this would be harder to do in a spreading activation framework . \n\t', '\n\t\t Another avenue for exploration is to use the combination of ICE and the refined lexical relations encoded in WordNet to create materials that would allow a larger scale replication of the results of \n\t\t']",Positive
"[""\n\t\t Such replication is independently desirable , since new reaction times would address the potential objection that we have unintentionally tuned our method to Hodgson 's data . \n\t"", '\n\t\t In the same vein , since our distributional methods provide a cheap and easy tool for exploratory studies , we intend to look more closely at the reasons for the discrepancies between our results and those of \n\t\t']",Positive
"['\n\t\t The present simulations show that a range of contextual effects can be subsumed under the same distributional mechanism , and that no task specific tuning of the parameters is necessary . \n\t', '\n\t\t Our model is computationally efficient and usable on a large scale to mine corpora for potentially interesting experimental materials . \n\t', ""\n\t\t Acknowledgements Financial support for the first author 's Ph.D studies came from the National Sciences and Engineering Research Council of Canada , the Overseas Research Student Awards Scheme , the Sir Ernest Cassels Education Trust , and the Institute for Adaptive and Neural Computation . \n\t"", '\n\t\t The work of preparing this paper was funded in part by NSF Career Grant 0347799 to the second author . \n\t', '\n\t\t We are grateful to the Clippers discussion group at the Ohio State University for the opportunity to present and discuss this work from a computational perspective . \n\t', '\n\t\t Gail McKoon provided particularly useful feedback . \n\t', '\n\t\t References Altarriba , J. , Kroll , J. , Sholl , A. & Rayner , K. 1996 . \n\t', '\n\t\t The influence of lexical and conceptual constraints on reading mixed-language sentences : Evidence from eye fixations and naming times . \n\t', '\n\t\t Memory & Cognition , 24 , 477-492 . \n\t', '\n\t\t Balota , D.A. , & Paul , S.T. ( 1996 ) . \n\t', '\n\t\t Summation of activation : Evidence from multiple primes that converge and diverge within semantic memory . \n\t', '\n\t\t Journal of Experimental Psychology : Learning , Memory , and Cognition , 22 , 827-845 . \n\t', '\n\t\t Brown , C. M. , Hagoort , P. & Chwilla , D. J. ( 2000 ) . \n\t', '\n\t\t An event-related brain potential analysis of visual word priming effects . \n\t', '\n\t\t Brain and Language , 72 , 158-190 . \n\t', '\n\t\t Cree , G. S. , McRae , K. & McNorgan , C. ( 1999 ) . \n\t', '\n\t\t An attractor model of lexical conceptual processing : Simulating semantic priming . \n\t', '\n\t\t Cognitive Science , 23,371-414 . \n\t', '\n\t\t Gelman , A. , Carlin , J. B. , Stern , H. S. & Rubin , D. B. ( 1995 ) . \n\t', '\n\t\t Bayesian data analysis . \n\t', '\n\t\t London : Chapman & Hall . \n\t', '\n\t\t Hodgson , J. M. ( 1991 ) . \n\t', '\n\t\t Informational constraints on pre-lexical priming . \n\t', '\n\t\t Language and Cognitive Processes , 6 , 169-205 . \n\t', '\n\t\t Keefe , D. E. & Neely , J. H. ( 1990 ) . \n\t', '\n\t\t Semantic priming in the pronunciation task : The role of prospective prime-generated expectancies . \n\t', '\n\t\t Memory & Cognition , 18 , 289-298 . \n\t', '\n\t\t Landauer , T. K. & Dumais , S. T. ( 1997 ) . \n\t', ""\n\t\t A solution to Plato 's problem : the Latent Semantic Analysis theory of acquisition , induction , and representation of knowledge . \n\t"", '\n\t\t Psychological Review , 104 , 211-240 . \n\t', '\n\t\t Lund , K. & Burgess , C. ( 1996 ) . \n\t', '\n\t\t Producing high- dimensional semantic spaces from lexical co- occurrence . \n\t', '\n\t\t Behavior Research Methods , Instruments , & Computers , 28 , 203-208 McDonald , S. ( 2000 ) . \n\t', '\n\t\t Environmental determinants of lexical processing effort . \n\t', '\n\t\t PhD dissertation , University of Edinburgh . \n\t', '\n\t\t McDonald , S. & Brew , C. ( 2002 ) . \n\t', '\n\t\t A distributional model of semantic context effects in lexical processing . \n\t', '\n\t\t Cogprints . \n\t', '\n\t\t McDonald , S. & Brew , C. ( 2001 ) . \n\t', '\n\t\t A rational analysis of semantic processing by the left cerebral hemisphere . \n\t', '\n\t\t First Workshop on Cognitively Plausible Models of Semantic Processing ( SEMPRO-2001 ) Edinburgh . \n\t', '\n\t\t July 31 , 2001 McDonald , S. & Lowe , W. ( 1998 ) . \n\t', '\n\t\t Modelling functional priming and the associative boost . \n\t', '\n\t\t In Proceedings of the 20th Annual Conference of the Cognitive Science Society ( pp. 667-680 ) . \n\t', '\n\t\t Mahwah , NJ : Erlbaum . \n\t', '\n\t\t McDonald , S. A. & Shillcock , R. C. ( 2001 ) . \n\t', '\n\t\t Rethinking the word frequency effect : the neglected role of distributional information in lexical processing . \n\t', '\n\t\t Language and Speech , 44 , 295-323 . \n\t', '\n\t\t McKoon , G. & Ratcliff , R. ( 1992 ) . \n\t', '\n\t\t Spreading activation versus compound cue accounts of priming : Mediated priming revisited . \n\t', '\n\t\t Journal of Experimental Psychology : Learning , Memory , and Cognition , 18 , 1155-1172 . \n\t', '\n\t\t Meyer , D. & Schvaneveldt , R. ( 1971 ) . \n\t', '\n\t\t Facilitation in recognizing pairs of words : Evidence of a dependence between retrieval operations . \n\t', '\n\t\t Journal of Experimental Psychology , 90 , 227-234 . \n\t', '\n\t\t Moss , H. E. , Ostrin , R. K. , Tyler , L. K. & MarslenWilson , W. D. ( 1995 ) . \n\t', '\n\t\t Accessing different types of lexical semantic information : Evidence from priming . \n\t', '\n\t\t Journal of Experimental Psychology : Learning , Memory , and Cognition , 21 , 863-883 . \n\t', '\n\t\t Neely , J. H. ( 1991 ) . \n\t', '\n\t\t Semantic priming effects in visual word recognition : a selective review of current findings and theories . \n\t', '\n\t\t In D. Besner & G. W. Humphrey ( Eds . \n\t', '\n\t\t ) Basic processes in reading : Visual word recognition ( pp. 264-336 ) . \n\t', '\n\t\t Hillsdale , NJ : Erlbaum . \n\t', '\n\t\t Plaut , D. C. & Booth , J. R. ( 2000 ) . \n\t', '\n\t\t Individual and developmental differences in semantic priming : Empirical and computational support for a single mechanism account of lexical processing , Psychological Review , 107 , 786-823 Ratcliff , R. & McKoon , G. ( 1988 ) . \n\t', '\n\t\t A retrieval theory of priming in memory . \n\t', '\n\t\t Psychological Review , 95 , 385-408 . \n\t', '\n\t\t Ratcliff , R. & Smith , P.L. ( 2004 ) . \n\t', '\n\t\t A comparison of sequential sampling models for two-choice reaction time . \n\t', '\n\t\t Psychological Review , 111 , 333-367 Redington , M. , Chater , N. & Finch , S. ( 1998 ) . \n\t', '\n\t\t Distributional information : a powerful cue for acquiring syntactic categories . \n\t', '\n\t\t Cognitive Science , 22 , 425- 469. \n\t', '\n\t\t Analysis of Mixed Natural and Symbolic Language Input in Mathematical Dialogs Magdalena Wolska Ivana Kruijff-Korbayov´a Fachrichtung Computerlinguistik Universiti t des Saarlandes , Postfach 15 1150 66041 Saarbr¨ucken , Germany magda,korbay @coli.uni-sb.de Abstract Discourse in formal domains , such as mathematics , is characterized by a mixture of telegraphic natural language and embedded (semi-)formal symbolic mathematical expressions . \n\t', '\n\t\t We present language phenomena observed in a corpus of dialogs with a simulated tutorial system for proving theorems as evidence for the need for deep syntactic and semantic analysis . \n\t', '\n\t\t We propose an approach to input understanding in this setting . \n\t', '\n\t\t Our goal is a uniform analysis of inputs of different degree of verbalization : ranging from symbolic alone to fully worded mathematical expressions . \n\t', '\n\t\t 1 Introduction Our goal is to develop a language understanding module for a flexible dialog system tutoring mathematical problem solving , in particular , theorem proving ( Benzm¨uller et al. , 2003a).1 As empirical findings in the area of intelligent tutoring show , flexible natural language dialog supports active learning \n\t\t']",Positive
"['\n\t\t However , little is known about the use of natural language in dialog setting in formal domains , such as mathematics , due to the lack of empirical data . \n\t', '\n\t\t To fill this gap , we collected a corpus of dialogs with a simulated tutorial dialog system for teaching proofs in naive set theory . \n\t', '\n\t\t An investigation of the corpus reveals various phenomena that present challenges for such input understanding techniques as shallow syntactic analysis combined with keyword spotting , or statistical methods , e.g. , Latent Semantic Analysis , which are commonly employed in ( tutorial ) dialog systems . \n\t', ""\n\t\t The prominent characteristics of the language in our corpus include : ( i ) tight interleaving of natural and symbolic language , ( ii ) varying degree of natural language verbalization of the formal mathematical ' This work is carried out within the DIALOG project : a col- laboration between the Computer Science and Computational Linguistics departments of the Saarland University , within the Collaborative Research Center on Resource -Adaptive Cognitive Processes , SFB 378 ( www.coli.uni-sb.de/ sfb378 ) . \n\t"", '\n\t\t content , and ( iii ) informal and/or imprecise reference to mathematical concepts and relations . \n\t', '\n\t\t These phenomena motivate the need for deep syntactic and semantic analysis in order to ensure correct mapping of the surface input to the underlying proof representation . \n\t', '\n\t\t An additional methodological desideratum is to provide a uniform treatment of the different degrees of verbalization of the mathematical content . \n\t', '\n\t\t By designing one grammar which allows a uniform treatment of the linguistic content on a par with the mathematical content , one can aim at achieving a consistent analysis void of example-based heuristics . \n\t', '\n\t\t We present such an approach to analysis here . \n\t', '\n\t\t The paper is organized as follows : In Section 2 , we summarize relevant existing approaches to input analysis in ( tutorial ) dialog systems on the one hand and analysis of mathematical discourse on the other . \n\t', '\n\t\t Their shortcomings with respect to our setting become clear in Section 3 where we show examples of language phenomena from our dialogs . \n\t', '\n\t\t In Section 4 , we propose an analysis methodology that allows us to capture any mixture of natural and mathematical language in a uniform way . \n\t', '\n\t\t We show example analyses in Section 5 . \n\t', '\n\t\t In Section 6 , we conclude and point out future work issues . \n\t', '\n\t\t 2 Related work Language understanding in dialog systems , be it with text or speech interface , is commonly performed using shallow syntactic analysis combined with keyword spotting . \n\t', '\n\t\t Tutorial systems also successfully employ statistical methods which compare student responses to a model built from pre- constructed gold-standard answers \n\t\t']",Positive
"['\n\t\t This is impossible for our dialogs , due to the presence of symbolic mathematical expressions . \n\t', '\n\t\t Moreover , the shallow techniques also remain oblivious of such aspects of discourse meaning as causal relations , modality , negation , or scope of quantifiers which are of crucial importance in our setting . \n\t', '\n\t\t When precise understanding is needed , tutorial systems either use menu- or template-based input , or use closed-questions to elicit short answers of little syntactic variation \n\t\t']",Positive
"['\n\t\t However , this conflicts with the preference for flexible dialog in active learning \n\t\t']",Positive
"['\n\t\t With regard to interpreting mathematical texts , \n\t\t']",Positive
"['\n\t\t However , the language in our dialogs is more informal : natural language and symbolic mathematical expressions are mixed more freely , there is a higher degree and more variety of verbalization , and mathematical objects are not properly introduced . \n\t', '\n\t\t Moreover , both above approaches rely on typesetting and additional information that identifies mathematical symbols , formulae , and proof steps , whereas our input does not contain any such information . \n\t', '\n\t\t Forcing the user to delimit formulae would reduce the flexibility of the system , make the interface harder to use , and might not guarantee a clean separation of the natural language and the non-linguistic content anyway . \n\t', '\n\t\t 3 Linguistic data In this section , we first briefly describe the corpus collection experiment and then present the common language phenomena found in the corpus . \n\t', '\n\t\t 3.1 Corpus collection 24 subjects with varying educational background and little to fair prior mathematical knowledge participated in a Wizard-of-Oz experiment ( Benzm¨uller et al. , 2003b ) . \n\t', '\n\t\t In the tutoring session , they were asked to prove 3 theorems2 : ( iii ) . \n\t', '\n\t\t To encourage dialog with the system , the subjects were instructed to enter proof steps , rather than complete proofs at once . \n\t', '\n\t\t Both the subjects and the tutor were free in formulating their turns . \n\t', '\n\t\t Buttons were available in the interface for inserting mathematical symbols , while literals were typed on the keyboard . \n\t', '\n\t\t The dialogs were typed in German . \n\t', '\n\t\t The collected corpus consists of 66 dialog log- files , containing on average 12 turns . \n\t', '\n\t\t The total number of sentences is 1115 , of which 393 are student sentences . \n\t', '\n\t\t The students\x92 turns consisted on average of 1 sentence , the tutor\x92s of 2 . \n\t', '\n\t\t More details on the corpus itself and annotation efforts that guide the development of the system components can be found in \n\t\t']",Positive
"['\n\t\t 2 stands for set complement and for power set . \n\t', '\n\t\t 3.2 Language phenomena To indicate the overall complexity of input understanding in our setting , we present an overview of common language phenomena in our dialogs.3 In the remainder of this paper , we then concentrate on the issue of interleaved natural language and mathematical expressions , and present an approach to processing this type of input . \n\t', '\n\t\t Interleaved natural language and formulae Mathematical language , often semi-formal , is interleaved with natural language informally verbalizing proof steps . \n\t', '\n\t\t In particular , mathematical expressions ( or parts thereof ) may lie within the scope of quantifiers or negation expressed in natural language : A auch [ ] A B ist von C ( A B ) [ ... is of ... ] ( da ja A B= ) [ ( because A B= ) ] B enthaelt kein x A [ B contains no x A ] For parsing , this means that the mathematical content has to be identified before it is interpreted within the utterance . \n\t', '\n\t\t Imprecise or informal naming Domain relations and concepts are described informally using imprecise and/or ambiguous expressions . \n\t', '\n\t\t A enthaelt B [ A contains B ] A muss in B sein [ A must be in B ] where contain and be in can express the domain relation of either subset or element ; B vollstaendig ausserhalb von A liegen muss , also im Komplement von A [ B has to be entirely outside ofA , so in the complement ofA ] dann sind A und B ( vollkommen ) verschieden , haben keine gemeinsamen Elemente [ then A and B are ( completely ) different , have no common elements ] where be outside of and be different are informal descriptions of the empty intersection of sets . \n\t', '\n\t\t To handle imprecision and informality , we constructed an ontological knowledge base containing domain-specific interpretations of the predicates \n\t\t']",Positive
"['\n\t\t Discourse deixis Anaphoric expressions refer deictically to pieces of discourse : der obere Ausdruck [ the above term ] der letzte Satz [ the last sentence ] Folgerung aus dem Obigen [ conclusion from the above ] aus der regel in der zweiten Zeile [ from the rule in the second line ] 3As the tutor was also free in wording his turns , we include observations from both student and tutor language behavior . \n\t', '\n\t\t In the presented examples , we reproduce the original spelling . \n\t', '\n\t\t ( i ) ; ( ii ) ; In our domain , this class of referring expressions also includes references to structural parts of terms and formulae such as \x93the left side\x94 or \x93the inner parenthesis\x94 which are incomplete specifications : the former refers to a part of an equation , the latter , metonymic , to an expression enclosed in parenthesis . \n\t', '\n\t\t Moreover , these expressions require discourse referents for the sub-parts of mathematical expressions to be available . \n\t', '\n\t\t Generic vs. specific reference Generic and specific references can appear within one utterance : where \x93a power set\x94 is a generic reference , whereas \x93 \x94 is a specific reference to a subset of a specific instance of a power set introduced earlier . \n\t', '\n\t\t Co-reference4 Co-reference phenomena specific to informal mathematical discourse involve ( parts of ) mathematical expressions within text . \n\t', '\n\t\t Da , wenn sein soll , Element von sein muss . \n\t', '\n\t\t Und wenn sein soll , muss auch Element von sein . \n\t', '\n\t\t [ Because if it should be that must be an element of . \n\t', '\n\t\t And if it should be that it must be an element of as well . \n\t', '\n\t\t ] Entities denoted with the same literals may or may not co-refer : DeMorgan-Regel-2 besagt : = ) = dem Begriff [ DeMorgan-Regel-2 means : ) In this case : e.g. = the term = the term ] Informal descriptions of proof-step actions Sometimes , \x93actions\x94 involving terms , formulae or parts thereof are verbalized before the appropriate formal operation is performed : Wende zweimal die DeMorgan-Regel an [ I\x92m applying DeMorgan rule twice ] damit kann ich den oberen Ausdruck wie folgt schreiben : ... [ given this I can write the upper term as follows : ... ] The meaning of the \x93action verbs\x94 is needed for the interpretation of the intended proof-step . \n\t', '\n\t\t Metonymy Metonymic expressions are used to refer to structural sub-parts of formulae , resulting in predicate structures acceptable informally , yet incompatible in terms of selection restrictions . \n\t', '\n\t\t Dann gilt fuer die linke Seite , wenn ^ , der Begriff A B dann ja schon dadrin und ist somit auch Element davon [ Then for the left hand side it holds that ... , the term A B is already there , and so an element of it ] 4To indicate co-referential entities , we inserted the indices which are not present in the dialog logfi les . \n\t', '\n\t\t where the predicate hold , in this domain , normally takes an argument of sort CONST , TERM or FORMULA , rather than LOCATION ; de morgan regel 2 auf beide komplemente angewendet [ de morgan rule 2 applied to both complements ] where the predicate apply takes two arguments : one of sort RULE and the other of sort TERM or FORMULA , rather than OPERATION ON SETS . \n\t', '\n\t\t In the next section , we present our approach to a uniform analysis of input that consists of a mixture of natural language and mathematical expressions . \n\t', '\n\t\t 4 Uniform input analysis strategy The task of input interpretation is two-fold . \n\t', '\n\t\t Firstly , it is to construct a representation of the utterance\x92s linguistic meaning . \n\t', '\n\t\t Secondly , it is to identify and separate within the utterance : ( i ) parts which constitute meta-communication with the tutor , e.g. : Ich habe die Aufgabenstellung nicht verstanden . \n\t', '\n\t\t [ I don\x92t understand what the task is . \n\t', '\n\t\t ] ( ii ) parts which convey domain knowledge that should be verified by a domain reasoner ; for example , the entire utterance ist laut deMorgan-1 [ ... is , according to deMorgan-1 , ... ] can be evaluated ; on the other hand , the domain reasoner\x92s knowledge base does not contain appropriate representations to evaluate the correctness of using , e.g. , the focusing particle \x93also\x94 , as in : Wenn A = B , dann ist A auch und B. [ IfA = B , then A is also and B. ] Our goal is to provide a uniform analysis of inputs of varying degrees of verbalization . \n\t', '\n\t\t This is achieved by the use of one grammar that is capable of analyzing utterances that contain both natural language and mathematical expressions . \n\t', '\n\t\t Syntactic categories corresponding to mathematical expressions are treated in the same way as those of linguistic lexical entries : they are part of the deep analysis , enter into dependency relations and take on semantic roles . \n\t', '\n\t\t The analysis proceeds in 2 stages : 1 . \n\t', '\n\t\t After standard pre-processing,5 mathematical expressions are identified , analyzed , categorized , and substituted with default lexicon entries encoded in the grammar ( Section 4.1 ) . \n\t', '\n\t\t 5 Standard pre-processing includes sentence and word tokenization , ( spelling correction and ) morphological analysis , part-of-speech tagging . \n\t', '\n\t\t Potenzmenge enthaelt alle Teilmengen , also auch ( A B ) [ A power set contains all subsets hence also(A B ) ] In diesem Fall : z.B. = dem Begriff 2 . \n\t', '\n\t\t Next , the input is syntactically parsed , and a representation of its linguistic meaning is constructed compositionally along with the parse ( Section 4.2 ) . \n\t', '\n\t\t The obtained linguistic meaning representation is subsequently merged with discourse context and interpreted by consulting a semantic lexicon of the domain and a domain-specific knowledge base ( Section 4.3 ) . \n\t', '\n\t\t If the syntactic parser fails to produce an analysis , a shallow chunk parser and keyword-based rules are used to attempt partial analysis and build a partial representation of the predicate-argument structure . \n\t', '\n\t\t In the next sections , we present the procedure of constructing the linguistic meaning of syntactically well-formed utterances . \n\t', '\n\t\t 4.1 Parsing mathematical expressions The task of the mathematical expression parser is to identify mathematical expressions . \n\t', '\n\t\t The identified mathematical expressions are subsequently verified as to syntactic validity and categorized . \n\t', '\n\t\t Implementation Identification of mathematical expressions within word-tokenized text is performed using simple indicators : single character tokens ( with the characters and standing for power set and set complement respectively ) , mathematical symbol unicodes , and new-line characters . \n\t', '\n\t\t The tagger converts the infix notation used in the input into an expression tree from which the following information is available : surface sub-structure ( e.g. , \x93left side\x94 of an expression , list of sub-expressions , list of bracketed sub-expressions ) and expression type based on the top level operator ( e.g. , CONST , TERM , FORMULA 0 FORMULA ( formula missing left argument ) , etc. ) . \n\t', '\n\t\t For example , the expression ) is represented by the formula tree in Fig . \n\t', '\n\t\t 1. The bracket subscripts indicate the operators heading sub-formulae enclosed in parenthesis . \n\t', '\n\t\t Given the expression\x92s top node operator , = , the expression is of type formula , its \x93left side\x94 is the expression , the list of bracketed sub-expressions includes : A B , C D , , etc. Evaluation We have conducted a preliminary evaluation of the mathematical expression parser . \n\t', '\n\t\t Both the student and tutor turns were included to provide more data for the evaluation . \n\t', '\n\t\t Of the 890 mathematical expressions found in the corpus ( 432 in the student and 458 in the tutor turns ) , only 9 were incorrectly recognized . \n\t', '\n\t\t The following classes of errors were detected:6 3 . \n\t', '\n\t\t K((A B ) ( C D ) ) = K(A ? \n\t', '\n\t\t B ) ? \n\t', '\n\t\t K(C ? \n\t', '\n\t\t D ) K((A B ) ( C D ) ) = K(A ? \n\t', '\n\t\t B ) ? \n\t', '\n\t\t K(C ? \n\t', '\n\t\t D ) 4 . \n\t', '\n\t\t Gleiches gilt mit D ( K(C D ) ) ( K(A B ) ) Gleiches gilt mit D ( K(C D ) ) ( K(A B ) ) [ The same holds with ... ] The examples in ( 1 ) and ( 2 ) have to do with parentheses . \n\t', '\n\t\t In ( 1 ) , the student actually omitted them . \n\t', '\n\t\t The remedy in such cases is to ask the student to correct the input . \n\t', '\n\t\t In ( 2 ) , on the other hand , no parentheses are missing , but they are ambiguous between mathematical brackets and parenthetical statement markers . \n\t', '\n\t\t The parser mistakenly included one of the parentheses with the mathematical expressions , thereby introducing an error . \n\t', '\n\t\t We could include a list of mathematical operations allowed to be verbalized , in order to include the logical connective in ( 2a ) in the tagged formula. . \n\t', '\n\t\t But ( 2b ) shows that this simple solution would not remedy the problem overall , as there is no pattern as to the amount and type of linguistic material accompanying the formulae in parenthesis . \n\t', '\n\t\t We are presently working on ways to identify the two uses of parentheses in a pre-processing step . \n\t', '\n\t\t In ( 3 ) the error is caused by a non-standard character , \x93?\x94 , found in the formula. . \n\t', '\n\t\t In ( 4 ) the student omitted punctuation causing the character \x93D\x94 to be interpreted as a nonstandard literal for naming an operation on sets . \n\t', '\n\t\t 4.2 Deep analysis The task of the deep parser is to produce a domain- independent linguistic meaning representation of syntactically well-formed sentences and fragments . \n\t', '\n\t\t By linguistic meaning ( LM ) , we understand the dependency-based deep semantics in the sense of the Prague School notion of sentence meaning as employed in the Functional Generative Description 6Incorrect tagging is shown along with the correct result below it , following an arrow . \n\t', '\n\t\t = A B C D A B C D Figure 1 : Tree representation of the formula ) b. ( da ja A B= ) )(da ja A B= ) P((A C ) ( B C))=PC ( A B ) P((A C ) ( B C ) ) =PC 1. ( A B ) 2. a. . \n\t', '\n\t\t ( A U und B U ) und B U ( A U ( FGD ) \n\t\t']",Positive
"['\n\t\t It represents the literal meaning of the utterance rather than a domain-specific interpretation.7 In FGD , the central frame unit of a sentence/clause is the head verb which specifies the tectogrammatical relations ( TRs ) of its dependents ( participants ) . \n\t', '\n\t\t Further distinction is drawn into inner participants , such as Actor , Patient , Addressee , and free modifications , such as Location , Means , Direction . \n\t', '\n\t\t Using TRs rather than surface grammatical roles provides a generalized view of the correlations between domain-specific content and its linguistic realization . \n\t', '\n\t\t We use a simplified set of TRs based on ( Haji^cov´a et al. , 2000 ) . \n\t', '\n\t\t One reason for simplification is to distinguish which relations are to be understood metaphorically given the domain sub-language . \n\t', '\n\t\t In order to allow for ambiguity in the recognition of TRs , we organize them hierarchically into a taxonomy . \n\t', '\n\t\t The most commonly occurring relations in our context , aside from the inner participant roles of Actor and Patient , are Cause , Condition , and Result- Conclusion ( which coincide with the rhetorical relations in the argumentative structure of the proof ) , for example : Da [ A gilt ] CAUSE , alle x , die in A sind sind nicht in B [ As A applies , all x that are in A are not in B ] Wenn [ A ] COND , dann A B= [ IfA , thenA B= ] Da gilt , [ alle x , die in A sind sind nicht in B ] RES Wenn A , dann [ A B= ] RES Other commonly found TRs include Norm- Criterion , e.g. [ nach deMorgan-Regel-2 ] NORM ist = ... ) [ according to De Morgan rule 2 it holds that ... ] ist [ laut DeMorgan-1 ] NORM ( ) [ ... equals , according to De Morgan rule1 , ... ] We group other relations into sets of HasProperty , GeneralRelation ( for adjectival and clausal modification ) , and Other ( a catch-all category ) , for example : dann muessen alla A und B [ in C ] PROP-LOC enthalten sein [ then all A and B have to be contained in C ] Alle x , [ die in B sind ] GENREL . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t [ All x that are in B ... ] alle elemente [ aus A ] PROP-FROM sind in enthalten [ all elements from A are contained in ] Aus A U B folgt [ mit A B= ] OTHER , B U A. [ From A U B follows with A B= , that B U A ] 7LM is conceptually related to logical form , however , differs in coverage : while it does operate on the level of deep semantic roles , such aspects of meaning as the scope of quantifi ers or interpretation of plurals , synonymy , or ambiguity are not resolved . \n\t', '\n\t\t where PROP-LOC denotes the HasProperty relation of type Location , GENREL is a general relation as in complementation , and PROP-FROM is a HasProperty relation of type Direction-From or From-Source . \n\t', '\n\t\t More details on the investigation into tectogrammatical relations that build up linguistic meaning of informal mathematical text can be found in ( Wolska and Kruijff-Korbayov´a , 2004a ) . \n\t', '\n\t\t Implementation The syntactic analysis is performed using openCCG8 , an open source parser for Multi-Modal Combinatory Categorial Grammar ( MMCCG ) . \n\t', '\n\t\t MMCCG is a lexicalist grammar formalism in which application of combinatory rules is controlled though context-sensitive specification of modes on slashes \n\t\t']",Positive
"['\n\t\t The linguistic meaning , built in parallel with the syntax , is represented using Hybrid Logic Dependency Semantics ( HLDS ) , a hybrid logic representation which allows a compositional , unification-based construction of HLDS terms with CCG \n\t\t']",Positive
"['\n\t\t An HLDS term is a relational structure where dependency relations between heads and dependents are encoded as modal relations . \n\t', '\n\t\t The syntactic categories for a lexical entry FORMULA , corresponding to mathematical expressions of type \x93formula\x94 , are , , and . \n\t', '\n\t\t For example , in one of the readings of \x93B enthaelt \x93enthaelt\x94 represents the meaning contain taking dependents in the relations Actor and Patient , shown schematically in Fig . \n\t', '\n\t\t 2. enthalten:contain Figure 2 : Tectogrammatical representation of the utterance \x93B enthaelt \x94 [ B contains ] . \n\t', '\n\t\t FORMULA represents the default lexical entry for identified mathematical expressions categorized as \x93formula\x94 ( cf. . \n\t', '\n\t\t Section 4.1 ) . \n\t', '\n\t\t The LM is represented by the following HLDS term : @h1(contain ACT ( f1 FORMULA:B ) PAT ( f2 FORMULA : ) where h1 is the state where the proposition contain is true , and the nominals f1 and f2 represent dependents of the head contain , which stand in the tectogrammatical relations Actor and Patient , respectively . \n\t', '\n\t\t It is possible to refer to the structural sub-parts of the FORMULA type expressions , as formula subparts are identified by the tagger , and discourse ref- 8http://openccg.sourceforge.net FORMULA : ACT PAT FORMULA : \x94 , erents are created for them and stored with the discourse model . \n\t', '\n\t\t We represent the discourse model within the same framework of hybrid modal logic . \n\t', '\n\t\t Nominals of the hybrid logic object language are atomic formulae that constitute a pointing device to a particular place in a model where they are true . \n\t', '\n\t\t The satisfaction operator , @ , allows to evaluate a formula at the point in the model given by a nominal ( e.g. the formula @ evaluates at the point i ) . \n\t', '\n\t\t For discourse modeling , we adopt the hybrid logic formalization of the DRT notions in ( Kruijff , 2001 ; Kruijff and Kruijff-Korbayov´a , 2001 ) . \n\t', '\n\t\t Within this formalism , nominals are interpreted as discourse referents that are bound to propositions through the satisfaction operator . \n\t', '\n\t\t In the example above , f1 and f2 represent discourse referents for FORMULA:B and FORMULA : , respectively . \n\t', '\n\t\t More technical details on the formalism can be found in the aforementioned publications . \n\t', '\n\t\t 4.3 Domain interpretation The linguistic meaning representations obtained from the parser are interpreted with respect to the domain . \n\t', '\n\t\t We are constructing a domain ontology that reflects the domain reasoner\x92s knowledge base , and is augmented to allow resolution of ambiguities introduced by natural language . \n\t', '\n\t\t For example , the previously mentioned predicate contain represents the semantic relation of Containment which , in the domain of naive set theory , is ambiguous between the domain relations ELEMENT , SUBSET , and PROPER SUBSET . \n\t', '\n\t\t The specializations of the ambiguous semantic relations are encoded in the ontology , while a semantic lexicon provides interpretations of the predicates . \n\t', '\n\t\t At the domain interpretation stage , the semantic lexicon is consulted to translate the tectogrammatical frames of the predicates into the semantic relations represented in the domain ontology . \n\t', '\n\t\t More details on the lexical-semantic stage of interpretation can be found in ( Wolska and KruijffKorbayov´a , 2004b ) , and more details on the domain ontology are presented in \n\t\t']",Positive
"['\n\t\t For example , for the predicate contain , the lexicon contains the following facts : contain( , ) ( SUBFORMULA , embedding ) [ \x92a Patient of type FORMULA is a subformula embedded within a FORMULA in the Actor relation with respect to the head contain\x92 ] contain( ) CONTA&MENT(container , containee ) [ \x92the Containment relation involves a predicate contain and its Actor and Patient dependents , where the Actor and Patient are the container and containee parameters respectively\x92 ] Translation rules that consult the ontology expand the meaning of the predicates to all their alterna- tive domain-specific interpretations preserving argument structure . \n\t', '\n\t\t As it is in the capacity of neither sentence-level nor discourse-level analysis to evaluate the correctness of the alternative interpretations , this task is delegated to the Proof Manager ( PM ) . \n\t', '\n\t\t The task of the PM is to : ( A ) communicate directly with the theorem prover;9 ( B ) build and maintain a representation of the proof constructed by the student ; 10 ( C ) check type compatibility of proof-relevant entities introduced as new in discourse ; ( D ) check consistency and validity of each of the interpretations constructed by the analysis module , with the proof context ; ( E ) evaluate the proof-relevant part of the utterance with respect to completeness , accuracy , and relevance . \n\t', '\n\t\t 5 Example analysis In this section , we illustrate the mechanics of the approach on the following examples . \n\t', '\n\t\t ( 1 ) B enthaelt kein [ B contains no ] ( 2 ) A B A B ( 3 ) A enthaelt keinesfalls Elemente , die in B sind . \n\t', '\n\t\t [ A contains no elements that are also in B ] Example ( 1 ) shows the tight interaction of natural language and mathematical formulae . \n\t', '\n\t\t The intended reading of the scope of negation is over a part of the formula following it , rather than the whole formula. . \n\t', '\n\t\t The analysis proceeds as follows . \n\t', '\n\t\t The formula tagger first identifies the formula x A and substitutes it with the generic entry FORMULA represented in the lexicon . \n\t', '\n\t\t If there was no prior discourse entity for \x93B\x94 to verify its type , the type is ambiguous between CONST , TERM , and FORMULA.11 The sentence is assigned four alternative readings : ( i ) \x93CONST contains no FORMULA\x94 , ( ii ) \x93TERM contains no FORMULA\x94 , ( iii ) \x93FORMULA contains no FORMULA\x94 , ( iv ) \x93CONST contains no CONST 0 FORMULA\x94 . \n\t', '\n\t\t The last reading is obtained by partitioning an entity of type FORMULA in meaningful ways , taking into account possible interaction with preceding modifiers . \n\t', '\n\t\t Here , given the quantifier \x93no\x94 , the expression x A has been split into its surface parts 9We are using a version of MEGA adapted for assertion- level proving \n\t\t']",Positive
"['\n\t\t 10The discourse content representation is separated from the proof representation , however , the corresponding entities must be co-indexed in both . \n\t', '\n\t\t 11In prior discourse , there may have been an assignment B:= , where is a formula , in which case , B would be known from discourse context to be of type FORMULA ( similarly for term assignment ) ; by CONST we mean a set or element variable such as A , x denoting a set A or an element x respectively . \n\t', '\n\t\t enthalten:contain enthalten:contain FORMULA : ACT no RESTR FORMULA : PAT Figure 3 : Tectogrammatical representation of the utterance \x93B enthaelt kein \x94 [ B contains no Figure 4 : Tectogrammatical representation of the utterance \x93B enthaelt kein \x94 [ B con- tains no ] . \n\t', '\n\t\t as follows : [ x ] [ A ] .12[x] has been substituted with a generic lexical entry CONST , and [ A ] with a symbolic entry for a formula missing its left argument ( cf. . \n\t', '\n\t\t Section 4.1 ) . \n\t', '\n\t\t The readings ( i ) and ( ii ) are rejected because of sortal incompatibility . \n\t', '\n\t\t The linguistic meanings of readings ( iii ) and ( iv ) are presented in Fig . \n\t', '\n\t\t 3 and Fig . \n\t', '\n\t\t 4 , respectively . \n\t', '\n\t\t The corresponding HLDS representations are : 13 \x97 for \x93FORMULA contains no FORMULA\x94 : s:(@kl(kein RESTR f2 BODY ( el enthalten ACT ( fl FORMULA ) PAT f2 ) ) @f2(FORMULA)) [ \x91formula B embeds no subformula x A\x92 ] \x97 for \x93CONST contains no CONST 0 FORMULA\x94 : s:(@kl(kein RESTR xl BODY ( el enthalten ACT ( c1 CONST ) PAT xl ) ) @x1 ( CONST HASPROP ( x2 0 FORMULA ) ) ) [ \x91B contains no x such that x is an element ofA\x92 ] Next , the semantic lexicon is consulted to translate these readings into their domain interpretations . \n\t', '\n\t\t The relevant lexical semantic entries were presented in Section 4.3 . \n\t', '\n\t\t Using the linguistic meaning , the semantic lexicon , and the ontology , we obtain four interpretations paraphrased below : \x97 for \x93FORMULA contains no FORMULA\x94 : ( 1.1 ) \x92it is not the case that PAT , the formula , x A , is a subformula of ACT , the formula B\x92 ; \x97 for \x93CONST contains no CONST 0 FORMULA\x94 : 12 There are other ways of constituent partitioning of the formula at the top level operator to separate the operator and its arguments : [ x ] [ ] [ A ] and [ x ] [ A ] . \n\t', '\n\t\t Each of the partitions obtains its appropriate type corresponding to a lexical entry available in the grammar ( e.g. , the [ x ] chunk is of type FORMULA 0 for a formula missing its right argument ) . \n\t', '\n\t\t Not all the readings , however , compose to form a syntactically and semantically valid parse of the given sentence . \n\t', '\n\t\t 13 Irrelevant parts of the meaning representation are omitted ; glosses of the hybrid formulae are provided . \n\t', '\n\t\t in GENREL ACT CONST : LOC Figure 5 : Tectogrammatical representation of the utterance \x93A enthaelt keinesfalls Elemente , die auch in B sind.\x94 [ A contains no elements that are also in B. ] . \n\t', '\n\t\t ( 1.2a ) \x92it is not the case that PAT , the constant x , ACT , B , and x A\x92 , ( 1.2b ) \x92it is not the case that PAT , the constant x , ACT , B , and x A\x92 , ( 1.2c ) \x92it is not the case that PAT , the constant x , ACT , B , and x A\x92 . \n\t', '\n\t\t The interpretation ( 1.1 ) is verified in the discourse context with information on structural parts of the discourse entity \x93B\x94 of type formula , while ( 1.2a-c ) are translated into messages to the PM and passed on for evaluation in the proof context . \n\t', '\n\t\t Example ( 2 ) contains one mathematical formula. . \n\t', '\n\t\t Such utterances are the simplest to analyze : The formulae identified by the mathematical expression tagger are passed directly to the PM . \n\t', '\n\t\t Example ( 3 ) shows an utterance with domain- relevant content fully linguistically verbalized . \n\t', '\n\t\t The analysis of fully verbalized utterances proceeds similarly to the first example : the mathematical expressions are substituted with the appropriate generic lexical entries ( here , \x93A\x94 and \x93B\x94 are substituted with their three possible alternative readings : CONST , TERM , and FORMULA , yielding several readings \x93CONST contains no elements that are also in CONST\x94 , \x93TERM contains no elements that are also in TERM\x94 , etc. ) . \n\t', '\n\t\t Next , the sentence is analyzed by the grammar . \n\t', '\n\t\t The semantic roles of Actor and Patient associated with the verb \x93contain\x94 are taken by \x93A\x94 and \x93elements\x94 respectively ; quantifier \x93no\x94 is in the relation Restrictor with \x93A\x94 ; the relative clause is in the GeneralRelation with \x93elements\x94 , etc. . \n\t', '\n\t\t The linguistic meaning of the utterance in example ( 3 ) is shown in Fig . \n\t', '\n\t\t 5. Then , the semantic lexicon and the ontology are consulted to translate the linguistic meaning into its domain-specific interpretations , which are in this case very similar to the ones of example ( 1 ) . \n\t', '\n\t\t 6 Conclusions and Further Work Based on experimentally collected tutorial dialogs on mathematical proofs , we argued for the use of deep syntactic and semantic analysis . \n\t', '\n\t\t We presented an approach that uses multimodal CCG with hy- CONST : ACT no RESTR elements PAT ] . \n\t', '\n\t\t enthalten:contain CONST : ACT no RESTR CONST : PAT 0 FORMULA : GENREL brid logic dependency semantics , treating natural and symbolic language on a par , thus enabling uniform analysis of inputs with varying degree of formal content verbalization . \n\t', '\n\t\t A preliminary evaluation of the mathematical expression parser showed a reasonable result . \n\t', '\n\t\t We are incrementally extending the implementation of the deep analysis components , which will be evaluated as part of the next Wizard-of-Oz experiment . \n\t', '\n\t\t One of the issues to be addressed in this context is the treatment of ill-formed input . \n\t', '\n\t\t On the one hand , the system can initiate a correction subdialog in such cases . \n\t', '\n\t\t On the other hand , it is not desirable to go into syntactic details and distract the student from the main tutoring goal . \n\t', '\n\t\t We therefore need to handle some degree of ill-formed input . \n\t', '\n\t\t Another question is which parts of mathematical expressions should have explicit semantic representation . \n\t', '\n\t\t We feel that this choice should be motivated empirically , by systematic occurrence of natural language references to parts of mathematical expressions ( e.g. , \x93the left/right side\x94 , \x93the parenthesis\x94 , and \x93the inner parenthesis\x94 ) and by the syntactic contexts in which they occur ( e.g. , the partitioning [x][ A ] seems well motivated in \x93B contains no x A\x94 ; [ x ] is a constituent in \x93x of complement of B.\x94 ) We also plan to investigate the interaction of modal verbs with the argumentative structure of the proof . \n\t', '\n\t\t For instance , the necessity modality is compatible with asserting a necessary conclusion or a prerequisite condition ( e.g. , \x93A und B muessen disjunkt sein.\x94 [ A and B must be disjoint . \n\t', '\n\t\t ] ) . \n\t', '\n\t\t This introduces an ambiguity that needs to be resolved by the domain reasoner . \n\t', '\n\t\t References J. M. Baldridge and G.J. M. Kruijff . \n\t', '\n\t\t 2002. Coupling CCG with hybrid logic dependency semantics . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 40th Annual Meeting of the Association for Computational Linguistics ( ACL ) , Philadelphia PA . \n\t', '\n\t\t pp. 319\x96326 . \n\t', '\n\t\t J. M. Baldridge and G.J. M. Kruijff . \n\t', '\n\t\t 2003. Multi-modal combinatory categorial grammar . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 10th Annual Meeting of the European Chapter of the Association for Computational Linguistics ( EACL\x9203 ) , Budapest , Hungary . \n\t', '\n\t\t pp. 211 \x96218. J. Baur . \n\t', '\n\t\t 1999. Syntax und Semantik mathematischer Texte . \n\t', '\n\t\t Diplomarbeit , Fachrichtung Computerlinguistik , Universit ¨at des Saarlandes , Saarbr¨ucken , Germany . \n\t', '\n\t\t C. Benzm ¨uller , A. Fiedler , M. Gabsdil , H. Horacek , I. KruijffKorbayov´a , M. Pinkal , J. Siekmann , D. Tsovaltzi , B. Q. Vo , and M. Wolska . \n\t', '\n\t\t 2003a . \n\t', '\n\t\t Tutorial dialogs on mathematical proofs . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ofIJCAI\x9203 Workshop on Knowledge Representation and Automated Reasoning for E-Learning Systems , Acapulco , Mexico . \n\t', '\n\t\t C. Benzm ¨uller , A. Fiedler , M. Gabsdil , H. Horacek , I. KruijffKorbayov´a , M. Pinkal , J. Siekmann , D. Tsovaltzi , B. Q. Vo , and M. Wolska . \n\t', '\n\t\t 2003b . \n\t', '\n\t\t A Wizard-of-Oz experiment for tutorial dialogues in mathematics . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the AIED\x9203 Workshop on Advanced Technologies for Mathematics Education , Sydney , Australia . \n\t', '\n\t\t pp. 471\x96481 . \n\t', '\n\t\t M. Glass . \n\t', '\n\t\t 2001. Processing language input in the CIRCSIMTutor intelligent tutoring system . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 10th AIED Conference , San Antonio , TX . \n\t', '\n\t\t pp. 210\x96221 . \n\t', '\n\t\t A. Graesser , P. Wiemer-Hastings , K. Wiemer-Hastings , D. Harter , and N. Person . \n\t', '\n\t\t 2000. Using latent semantic analysis to evaluate the contributions of students in autotutor . \n\t', '\n\t\t Interactive Learning Environments , 8:2 . \n\t', '\n\t\t pp. 129\x96147 . \n\t', '\n\t\t E. Haji^cov´a , J. Panevov´a , and P. Sgall . \n\t', '\n\t\t 2000. A manual for tec- togrammatical tagging of the Prague Dependency Treebank . \n\t', '\n\t\t TR-2000-09 , Charles University , Prague , Czech Republic . \n\t', '\n\t\t H. Horacek and M. Wolska . \n\t', '\n\t\t 2004 . \n\t', '\n\t\t Interpreting Semi-Formal Utterances in Dialogs about Mathematical Proofs . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 9th International Conference on Application of Natural Language to Information Systems ( NLDB\x9204 ) , Salford , Manchester , Springer . \n\t', '\n\t\t To appear . \n\t', '\n\t\t G.J.M. Kruijff and I. Kruijff-Korbayov´a . \n\t', '\n\t\t 2001. A hybrid logic formalization of information structure sensitive discourse interpretation . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 4th International Conference on Text , Speech and Dialogue ( TSD\x922001 ) , ^Zelezn´a Ruda , Czech Republic . \n\t', '\n\t\t pp. 31\x9638 . \n\t', '\n\t\t G.J.M. Kruijff . \n\t', '\n\t\t 2001. A Categorial -Modal Logical Architecture of Informativity : Dependency Grammar Logic & Information Structure . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t Thesis , Institute of Formal and Applied Linguistics ( ´UFAL ) , Faculty of Mathematics and Physics , Charles University , Prague , Czech Republic . \n\t', '\n\t\t J. Moore . \n\t', '\n\t\t 1993. What makes human explanations effective ? \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 15th Annual Conference ofthe Cognitive Science Society , Hillsdale , NJ . \n\t', '\n\t\t pp. 131\x96136 . \n\t', '\n\t\t P. Sgall , E. Haji^cov´a , and J. Panevov´a . \n\t', '\n\t\t 1986. The meaning of the sentence in its semantic and pragmatic aspects . \n\t', '\n\t\t Reidel Publishing Company , Dordrecht , The Netherlands . \n\t', '\n\t\t Q.B. Vo , C. Benzm ¨uller , and S. Autexier . \n\t', '\n\t\t 2003. Assertion Application in Theorem Proving and Proof Planning . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the International Joint Conference on Artificial Intelligence ( IJCAI ) . \n\t', '\n\t\t Acapulco , Mexico . \n\t', '\n\t\t M. Wolska and I. Kruijff-Korbayov´a . \n\t', '\n\t\t 2004a . \n\t', '\n\t\t Building a dependency-based grammar for parsing informal mathematical discourse . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 7th International Conference on Text , Speech and Dialogue ( TSD\x9204 ) , Brno , Czech Republic , Springer . \n\t', '\n\t\t To appear . \n\t', '\n\t\t M. Wolska and I. Kruijff-Korbayov´a . \n\t', '\n\t\t 2004b . \n\t', '\n\t\t Lexical- Semantic Interpretation of Language Input in Mathematical Dialogs . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the ACL Workshop on Text Meaning and Interpretation , Barcelona , Spain . \n\t', '\n\t\t To appear . \n\t', '\n\t\t M. Wolska , B. Q. Vo , D. Tsovaltzi , I. Kruijff-Korbayov´a , E. Karagjosova , H. Horacek , M. Gabsdil , A. Fiedler , C. Benzm ¨uller , 2004 . \n\t', '\n\t\t An annotated corpus of tutorial dialogs on mathematical theorem proving . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of 4th International Conference On Language Resources and Evaluation ( LREC\x9204 ) , Lisbon , Portugal . \n\t', '\n\t\t pp. 1007\x961010 . \n\t', '\n\t\t C. Zinn. 2003 . \n\t', '\n\t\t A Computational Framework for Understanding Mathematical Discourse . \n\t', '\n\t\t In Logic Journal of the IGPL , 11:4 , pp. 457\x96484 , Oxford University Press . \n\t', '\n\t\t A TAG-based noisy channel model of speech repairs Mark Johnson Eugene Charniak Brown University Brown University Providence , RI 02912 Providence , RI 02912 mj@cs.brown.edu ec@cs.brown.edu Abstract This paper describes a noisy channel model of speech repairs , which can identify and correct repairs in speech transcripts . \n\t', '\n\t\t A syntactic parser is used as the source model , and a novel type of TAG-based transducer is the channel model . \n\t', '\n\t\t The use of TAG is motivated by the intuition that the reparandum is a \x93rough copy\x94 of the repair . \n\t', '\n\t\t The model is trained and tested on the Switchboard disfluency-annotated corpus . \n\t', '\n\t\t 1 Introduction Most spontaneous speech contains disfluencies such as partial words , filled pauses ( e.g. , \x93uh\x94 , \x93um\x94 , \x93huh\x94 ) , explicit editing terms ( e.g. , \x93I mean\x94 ) , parenthetical asides and repairs . \n\t', '\n\t\t Of these repairs pose particularly difficult problems for parsing and related NLP tasks . \n\t', '\n\t\t This paper presents an explicit generative model of speech repairs and shows how it can eliminate this kind of disfluency . \n\t', '\n\t\t While speech repairs have been studied by psycholinguists for some time , as far as we know this is the first time a probabilistic model of speech repairs based on a model of syntactic structure has been described in the literature . \n\t', '\n\t\t Probabilistic models have the advantage over other kinds of models that they can in principle be integrated with other probabilistic models to produce a combined model that uses all available evidence to select the globally optimal analysis . \n\t', '\n\t\t \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
['\n\t\t The previous work most closely related to the current work is \n\t\t'],Positive
"['\n\t\t There are two innovations in this paper . \n\t', '\n\t\t First , we demonstrate that using a syntactic parser-based language model \n\t\t']",Positive
"['\n\t\t Second , we show how Tree Adjoining Grammars ( TAGs ) can be used to provide a precise formal description and probabilistic model of the crossed dependencies occurring in speech repairs . \n\t', '\n\t\t The rest of this paper is structured as follows . \n\t', '\n\t\t The next section describes the noisy channel model of speech repairs and the section after that explains how it can be applied to detect and repair speech repairs . \n\t', '\n\t\t Section 4 evaluates this model on the Penn 3 disfluency-tagged Switchboard corpus , and section 5 concludes and discusses future work . \n\t', '\n\t\t 2 A noisy channel model of repairs We follow \n\t\t']",Positive
"['\n\t\t Figure 1 shows these three parts for a typical repair . \n\t', '\n\t\t Most current probabilistic language models are based on HMMs or PCFGs , which induce linear or tree-structured dependencies between words . \n\t', '\n\t\t The relationship between reparandum and repair seems to be quite different : the repair is a \x93rough copy\x94 of the reparandum , often incorporating the same or very similar words in roughly the same word order . \n\t', '\n\t\t That is , they seem to involve \x93crossed\x94 dependencies between the reparandum and the repair , shown in Figure 1. Languages with an unbounded number of crossed dependencies cannot be described by a context-free or finite- state grammar , and crossed dependencies like these have been used to argue natural languages ... a flight to Boston , uh , I mean , to Denver on Friday ... ~N11 J ~N11 J ~N11 J Reparandum Interregnum Repair Figure 1 : The structure of a typical repair , with crossing dependencies between reparandum and repair . \n\t', '\n\t\t Figure 2 : The \x93helical\x94 dependency structure induced by the generative model of speech repairs for the repair depicted in Figure 1. a flight to Denver on Friday to Boston are not context-free \n\t\t']",Positive
"['\n\t\t Mildly context-sensitive grammars , such as Tree Adjoining Grammars ( TAGs ) and Combinatory Categorial Grammars , can describe such crossing dependencies , and that is why TAGs are used here . \n\t', '\n\t\t Figure 2 shows the combined model\x92s dependency structure for the repair of Figure 1 . \n\t', '\n\t\t Interestingly , if we trace the temporal word string through this dependency structure , aligning words next to the words they are dependent on , we obtain a \x93helical\x94 type of structure familiar from genome models , and in fact TAGs are being used to model genomes for very similar reasons . \n\t', '\n\t\t The noisy channel model described here involves two components . \n\t', '\n\t\t A language model defines a probability distribution P(X) over the source sentences X , which do not contain repairs . \n\t', '\n\t\t The channel model defines a conditional probability distribution P(YIX) of surface sentences Y , which may contain repairs , given source sentences . \n\t', '\n\t\t In the work reported here , X is a word string and Y is a speech transcription not containing punctuation or partial words . \n\t', '\n\t\t We use two language models here : a bigram language model , which is used in the search process , and a syntactic parser-based language model \n\t\t']",Positive
"['\n\t\t Because the language model is responsible for generating the well-formed sentence X , it is reasonable to expect that a language model that can model more global properties of sentences will lead to better performance , and the results presented here show that this is the case . \n\t', '\n\t\t The channel model is a stochastic TAG-based transducer ; it is responsible for generating the repairs in the transcript Y , and it uses the ability of TAGs to straightforwardly model crossed dependencies . \n\t', '\n\t\t 2.1 Informal description Given an observed sentence Y we wish to find the most likely source sentence ~X , where : X~ = argmax P(XIY) = argmaxP(YIX)P(Y) . \n\t', '\n\t\t X X This is the same general setup that is used in statistical speech recognition and machine translation , and in these applications syntax- based language models P(Y) yield state-of-the- art performance , so we use one such model here . \n\t', '\n\t\t The channel model P(YIX) generates sentences Y given a source X . \n\t', '\n\t\t A repair can potentially begin before any word of X . \n\t', '\n\t\t When a repair has begun , the channel model incrementally processes the succeeding words from the start of the repair . \n\t', '\n\t\t Before each succeeding word either the repair can end or else a sequence of words can be inserted in the reparandum . \n\t', '\n\t\t At the end of each repair , a ( possibly null ) interregnum is appended to the reparandum . \n\t', '\n\t\t The intuition motivating the channel model design is that the words inserted into the reparandum are very closely related those in the repair . \n\t', '\n\t\t Indeed , in our training data over 60 % of the words in the reparandum are exact copies of words in the repair ; this similarity is strong evidence of a repair . \n\t', '\n\t\t The channel model is designed so that exact copy reparandum words will have high probability . \n\t', '\n\t\t We assume that X is a substring of Y , i.e. , that the source sentence can be obtained by deleting words from Y , so for a fixed observed sentence there are only a finite number of possible source sentences . \n\t', '\n\t\t However , the number of source sentences grows exponentially with the length of Y , so exhaustive search is probably infeasible . \n\t', '\n\t\t TAGs provide a systematic way of formalizing the channel model , and their polynomial- time dynamic programming parsing algorithms can be used to search for likely repairs , at least when used with simple language models like a bigram language model . \n\t', '\n\t\t In this paper we first identify the 20 most likely analysis of each sentence using the TAG channel model together with a bigram language model . \n\t', '\n\t\t Then each of these analysis is rescored using the TAG channel model and a syntactic parser based language model . \n\t', '\n\t\t The TAG channel model\x92s analysis do not reflect the syntactic structure of the sentence being analyzed ; instead they encode the crossed dependencies of the speech repairs . \n\t', '\n\t\t If we want to use TAG dynamic programming algorithms to efficiently search for repairs , it is necessary that the intersection ( in language terms ) of the TAG channel model and the language model itself be describable by a TAG . \n\t', '\n\t\t One way to guarantee this is to use a finite state language model ; this motivates our use of a bigram language model . \n\t', '\n\t\t On the other hand , it seems desirable to use a language model that is sensitive to more global properties of the sentence , and we do this by reranking the initial analysis , replacing the bigram language model with a syntactic parser based model . \n\t', '\n\t\t We do not need to intersect this parser based language model with our TAG channel model since we evaluate each analysis separately . \n\t', '\n\t\t 2.2 The TAG channel model The TAG channel model defines a stochastic mapping of source sentences X into observed sentences Y . \n\t', '\n\t\t There are several ways to define transducers using TAGs such as \n\t\t']",Positive
"['\n\t\t The TAG defines a language whose vocabulary is the set of pairs ( U{0} ) x ( U{0} ) , where E is the vocabulary of the observed sentences Y . \n\t', '\n\t\t A string Z in this language can be interpreted as a pair of strings ( Y , X ) , where Y is the concatenation of the projection of the first components of Z and X is the concatenation of the projection of the second components . \n\t', '\n\t\t For example , the string Z = a : a flight:flight to:0 Boston:0 uh:0 I:0 mean:0 to:to Denver:Denver on:on Friday:Friday corresponds to the observed string Y = a flight to Boston uh I mean to Denver on Friday and the source string X = a flight to Denver on Friday . \n\t', '\n\t\t Figure 3 shows the TAG rules used to generate this example . \n\t', '\n\t\t The nonterminals in this grammar are of the form N , , ,. , R,,,\x84 : , , , . \n\t', '\n\t\t and I , where w , , is a word appearing in the source string and wy is a word appearing in the ob- served string . \n\t', '\n\t\t Informally , the N , , , . \n\t', '\n\t\t nonterminals indicate that the preceding word w , , was an- alyzed as not being part of a repair , while the R,,,\x84:,,,.that the preceding words wy and w , , were part of a repair . \n\t', '\n\t\t The nonterminal I generates words in the interregnum of a repair . \n\t', '\n\t\t Encoding the preceding words in the TAGs nonterminals permits the channel model to be sensitive to lexical properties of the preceding words . \n\t', '\n\t\t The start symbol is N $ , where ` $ \x92 is a distinguished symbol used to indicate the beginning and end of sentences . \n\t', '\n\t\t 2.3 Estimating the repair channel model from data The model is trained from the disfluency and POS tagged Switchboard corpus on the LDC Penn tree bank III CD-ROM ( specifically , the files under dysfl/dps/swbd ) . \n\t', '\n\t\t This version of the corpus annotates the beginning and ending positions of repairs as well as fillers , editing terms , asides , etc. , which might serve as the interregnum in a repair . \n\t', '\n\t\t The corpus also includes punctuation and partial words , which are ignored in both training and evaluation here since we felt that in realistic applications these would not be available in speech recognizer output . \n\t', '\n\t\t The transcript of the example of Figure 1 would look something like the following : a/DT flight/NN [ to/IN Boston/NNP + { F uh/UH } { E I/PRP mean/VBP } to/IN Denver/NNP ] on/IN Friday/NNP In this transcription the repair is the string from the opening bracket \x93[\x94 to the interruption point \x93+\x94 ; the interregnum is the sequence of braced strings following the interregnum , and the repair is the string that begins at the end of the interregnum and ends at the closing bracket \x93 ] \x94 . \n\t', ""\n\t\t The interregnum consists of the braced to:to Friday:Friday NFriday ( a1 ) Nwant a:a Nab 1 \x97 Pn(repairla) ( a2 ) Na flight:flight Rflight:flight Ij Pn(repairl flight ) ( a3 ) NDenver on : on Non 1 1\x97 Pn(repairlon) ( a5 ) I uh I I mean PZ ( uh I mean ) ( 01 ) Rflight:flight to:0 Rto:to R* ` flight:flight to:to Pr ( copy lflight , flight ) ( 02 ) Rto:to Boston:0 RBoston:Denver R*Denver:Denver ' to:to Pr(substlto , to)Pr(Bostonlsubst , to , Denver ) ( 03 ) RBoston:Denver R* Boston:Denver NDenver j Pr(nonrepl Boston , Denver ) R*Boston,Denver tomorrow : tomorrow Pr ( del lBoston , Denver ) Nwant a : a Na R* Boston,Denver Pr ( i ns l Boston , Denver ) Pr ( tomorrow l ins , Boston , Denver ) flight:flight Rflight:flight to:0 Rto:to Boston:0 RBoston:Denver RBoston:Denver Denver:Denver on:on Non . \n\t"", '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t NDenver Rto:to Rflight:flight I uh:0 I I:0 mean:0 Figure 3 : The TAG rules used to generate the example shown in Figure 1 and their respective weights , and the corresponding derivation and derived trees . \n\t', '\n\t\t expressions immediately following the interruption point . \n\t', '\n\t\t We used the disfluency tagged version of the corpus for training rather than the parsed version because the parsed version does not mark the interregnum , but we need this information for training our repair channel model . \n\t', '\n\t\t Testing was performed using data from the parsed version since this data is cleaner , and ( 04 ) RBoston,Denver RBoston,tomorrow ( 05 ) RBoston,Denver tomorrow:0 Rtomorrow,Denver . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t a1 a2 a5 01 02 03 a3 a4 . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t it enables a direct comparison with earlier work . \n\t', '\n\t\t We followed \n\t\t']",Positive
"['\n\t\t We now describe how the weights on the TAG productions described in subsection 2.2 are estimated from this training data . \n\t', '\n\t\t In order to estimate these weights we need to know the TAG derivation of each sentence in the training data . \n\t', '\n\t\t In order to uniquely determine this we need the not just the locations of each reparandum , interregnum and repair ( which are annotated in the corpus ) but also the crossing dependencies between the reparandum and repair words , as indicated in Figure 1 . \n\t', '\n\t\t We obtain these by aligning the reparandum and repair strings of each repair using a minimum-edit distance string aligner with the following alignment costs : aligning identical words costs 0 , aligning words with the same POS tag costs 2 , an insertion or a deletion costs 4 , aligning words with POS tags that begin with the same letter costs 5 , and an arbitrary substitution costs 7 . \n\t', '\n\t\t These costs were chosen so that a substitution will be selected over an insertion followed by a deletion , and the lower cost for substitutions involving POS tags beginning with the same letter is a rough and easy way of establishing a preference for aligning words whose POS tags come from the same broad class , e.g. , it results in aligning singular and plural nouns , present and past participles , etc. . \n\t', '\n\t\t While we did not evaluate the quality of the alignments since they are not in themselves the object of this exercise , they seem to be fairly good . \n\t', '\n\t\t From our training data we estimate a number of conditional probability distributions . \n\t', '\n\t\t These estimated probability distributions are the linear interpolation of the corresponding empirical distributions from the main sub-corpus using various subsets of conditioning variables ( e.g. , bigram models are mixed with unigram models , etc. ) using Chen\x92s bucketing scheme \n\t\t']",Positive
"['\n\t\t As is commonly done in language modelling , the interpolation coefficients are determined by maximizing the likelihood of the held out data counts using EM . \n\t', '\n\t\t Special care was taken to ensure that all distributions over words ranged over ( and assigned non-zero probability to ) every word that occurred in the train ing corpora ; this turns out to be important as the size of the training data for the different distributions varies greatly . \n\t', '\n\t\t The first distribution is defined over the words in source sentences ( i.e. , that do not contain reparandums or interregnums ) . \n\t', '\n\t\t P,,,(repairlW) is the probability of a repair beginning after a word W in the source sentence X ; it is estimated from the training sentences with reparandums and interregnums removed . \n\t', '\n\t\t Here and in what follows , W ranges over E U 1$1 , where ` $ \x92 is a distinguished beginning-ofsentence marker . \n\t', '\n\t\t For example , P , , , ( repair lflight ) is the probability of a repair beginning after the word flight . \n\t', '\n\t\t Note that repairs are relatively rare ; in our training data P,,,(repair) Pz~ 0.02 , which is a fairly strong bias against repairs . \n\t', '\n\t\t The other distributions are defined over aligned reparandum/repair strings , and are estimated from the aligned repairs extracted from the training data . \n\t', '\n\t\t In training we ignored all overlapping repairs ( i.e. , cases where the reparandum of one repair is the repair of another ) . \n\t', '\n\t\t ( Naturally , in testing we have no such freedom . \n\t', '\n\t\t ) We analyze each repair as consisting of n aligned word pairs ( we describe the interregnum model later ) . \n\t', '\n\t\t MZ is the ith reparandum word and RZ is the corresponding repair word , so both of these range over E U ~0~ . \n\t', '\n\t\t We define M0 and R0 to be source sentence word that preceded the repair ( which is ` $ \x92 if the repair begins at the beginning of a sen- tence ) . \n\t', ""\n\t\t We define M ' Z and R'Z to be the last non-0 reparandum and repair words respectively , i.e. , M ' Z = MZ if MZ =~ 0 and M ' Z = M'Z_1 oth- erwise . \n\t"", '\n\t\t Finally , TZ , i = 1 ... n + 1 , which indicates the type of repair that occurs at posi- tion i , ranges over { copy , subst , ins , del , nonrep } , where T,,,+1 = nonrep ( indicating that the repair has ended ) , and for i = 1 ... n , TZ = copy if MZ=RZ,TZ=ins ifRZ=0,TZ=del ifMZ=0 and TZ = su bst otherwise . \n\t', '\n\t\t The distributions we estimate from the aligned repair data are the following . \n\t', ""\n\t\t P , ( TZ l M'Z_ 1 , R'Z_ 1 ) is the probability of seeing repair type TZ following the reparandum word M'Z_1 and repair word R'Z_1 ; e.g. , P , ( non rep lBoston , Denver ) is the probability of the repair ending when Boston is the last reparandum word and Denver is the last repair word . \n\t"", ""\n\t\t P,(MZlTZ = ins,M'Z_1,R'Z ) is the probability that MZ is the word that is inserted into the reparandum ( i.e. , RZ = 0 ) given that some word is substituted , and that the preceding reparan dum and repair words are M'Z_1 and R'Z . \n\t"", '\n\t\t For example Pr(tomorrowlins , Boston , Denver ) is the probability that the word tomorrow is inserted into the reparandum after the words Boston and Denver , given that some word is inserted . \n\t', ""\n\t\t Pr(MZlTZ = subst , M'Z_1 , R'Z ) is the prob- ability that MZ is the word that is substi- tuted in the reparandum for R'Z , given that some word is substituted . \n\t"", '\n\t\t For example , Pr(Bostonlsubst , to , Denver ) is the probability that Boston is substituted for Denver , given that some word is substituted . \n\t', '\n\t\t Finally , we also estimated a probability distribution PZ(W) over interregnum strings as follows . \n\t', '\n\t\t Our training corpus annotates what we call interregnum expressions , such as uh and I mean . \n\t', '\n\t\t We estimated a simple unigram distribution over all of the interregnum expressions observed in our training corpus , and also extracted the empirical distribution of the number of interregnum expressions in each repair . \n\t', '\n\t\t Interregnums are generated as follows . \n\t', '\n\t\t First , the number k of interregnum expressions is chosen using the empirical distribution . \n\t', '\n\t\t Then k interregnum expressions are independently generated from the unigram distribution of interregnum expressions , and appended to yield the interregnum string W . \n\t', '\n\t\t The weighted TAG that constitutes the channel model is straight forward to define using these conditional probability distributions . \n\t', '\n\t\t Note that the language model generates the source string X . \n\t', '\n\t\t Thus the weights of the TAG rules condition on the words in X , but do not generate them . \n\t', '\n\t\t There are three different schema defining the initial trees of the TAG . \n\t', '\n\t\t These correspond to analyzing a source word as not beginning a repair ( e.g. , a1 and a3 in Figure 3 ) , analyzing a source word as beginning a repair ( e.g. , a2 ) , and generating an interregnum ( e.g. , a5 ) . \n\t', '\n\t\t Auxiliary trees generate the paired reparandum/repair words of a repair . \n\t', '\n\t\t There are five different schema defining the auxiliary trees corresponding to the five different values that TZ can take . \n\t', ""\n\t\t Note that the nonterminal Rm,r expanded by the auxiliary trees is annotated with the last reparandum and repair words M'Z_1 and R'Z_1 respectively , which makes it possible to condition the rule\x92s weight on these words . \n\t"", ""\n\t\t Auxiliary trees of the form ( 01 ) generate reparandum words that are copies of the corresponding repair words ; the weight on such trees is Pr(copylM'Z_1 , R'Z_1 ) . \n\t"", ""\n\t\t Trees of the form ( 02 ) substitute a reparandum word for a repair word ; their weight is Pr(substlM'Z_1 , R'Z_1)Pr(MZlsubst , M'Z_1 , R'Z ) . \n\t"", ""\n\t\t Trees of the form ( 03 ) end a repair ; their weight is Pr(nonrepl , M'Z_1 , R'Z_1 ) . \n\t"", ""\n\t\t Auxiliary trees of the form ( 03 ) end a repair ; they are weighted Pr(nonreplM'Z_1 , R'Z_1 ) . \n\t"", ""\n\t\t Auxiliary trees of the form ( 04 ) permit the repair word R'Z_1 to be deleted in the reparandum ; the weight of such a tree is Pr(dellM'Z_1,R'Z_1) . \n\t"", ""\n\t\t Finally , auxiliary trees of the form ( 05 ) generate a reparandum word MZ is inserted ; the weight of such a tree is Pr(inslM'Z_1 , R'Z_1)Pr(MZlins , M'Z_1 , R'Z_1 ) . \n\t"", '\n\t\t 3 Detecting and repairing speech repairs The TAG just described is not probabilistic ; informally , it does not include the probability costs for generating the source words . \n\t', '\n\t\t However , it is easy to modify the TAG so it does include a bigram model that does generate the source words , since each nonterminal encodes the preceding source word . \n\t', '\n\t\t That is , we multiply the weights of each TAG production given earlier that introduces a source word RZ by Pn ( RZ l RZ_1 ) . \n\t', '\n\t\t The resulting stochastic TAG is in fact exactly the intersection of the channel model TAG with a bigram language model . \n\t', '\n\t\t The standard n5 bottom-up dynamic programming parsing algorithm can be used with this stochastic TAG . \n\t', '\n\t\t Each different parse of the observed string Y with this grammar corresponds to a way of analyzing Y in terms of a hypothetical underlying sentence X and a number of different repairs . \n\t', '\n\t\t In our experiments below we extract the 20 most likely parses for each sentence . \n\t', '\n\t\t Since the weighted grammar just given does not generate the source string X , the score of the parse using the weighted TAG is P ( Y l X ) . \n\t', '\n\t\t This score multiplied by the probability P(X) of the source string using the syntactic parser based language model , is our best estimate of the probability of an analysis . \n\t', '\n\t\t However , there is one additional complication that makes a marked improvement to the model\x92s performance . \n\t', '\n\t\t Recall that we use the standard bottom-up dynamic programming TAG parsing algorithm to search for candidate parses . \n\t', '\n\t\t This algorithm has n5 running time , where n is the length of the string . \n\t', '\n\t\t Even though our sentences are often long , it is extremely unlikely that any repair will be longer than , say , 12 words . \n\t', '\n\t\t So to increase processing speed we only compute analyses for strings of length 12 or less . \n\t', '\n\t\t For every such substring that can be analyzed as a repair we calculate the repair odds , i.e. , the probability of generating this substring as a repair divided by the probability of generating this substring via the non-repair rules , or equivalently , the odds that this substring constitutes a repair . \n\t', '\n\t\t The substrings with high repair odds are likely to be repairs . \n\t', '\n\t\t This more local approach has a number of advantages over computing a global analysis . \n\t', '\n\t\t First , as just noted it is much more efficient to compute these partial analyses rather than to compute global analyses of the entire sentence . \n\t', '\n\t\t Second , there are rare cases in which the same substring functions as both repair and reparandum ( i.e. , the repair string is itself repaired again ) . \n\t', '\n\t\t A single global analysis would not be able to capture this ( since the TAG channel model does not permit the same substring to be both a reparandum and a repair ) , but we combine these overlapping repair substring analyses in a post-processing operation to yield an analysis of the whole sentence . \n\t', '\n\t\t ( We do insist that the reparandum and interregnum of a repair do not overlap with those of any other repairs in the same analysis ) . \n\t', '\n\t\t 4 Evaluation This section describes how we evaluate our noisy model . \n\t', '\n\t\t As mentioned earlier , following \n\t\t']",Positive
"['\n\t\t However , our test data differs from theirs in that in this test we deleted all partial words and punctuation from the data , as this results in a more realistic test situation . \n\t', '\n\t\t Since the immediate goal of this work is to produce a program that identifies the words of a sentence that belong to the reparandum of a repair construction ( to a first approximation these words can be ignored in later processing ) , our evaluation focuses on the model\x92s performance in recovering the words in a reparandum . \n\t', '\n\t\t That is , the model is used to classify each word in the sentence as belonging to a reparandum or not , and all other additional structure produced by the model is ignored . \n\t', '\n\t\t We measure model performance using standard precision p , recall r and f-score f , measures . \n\t', '\n\t\t If n , is the number of reparandum words the model correctly classified , nt is the number of true reparandum words given by the manual annotations and nm is the number of words the model predicts to be reparandum words , then the precision is n,/nm , recall is n,/nt , and f is 2pr/(p + r ) . \n\t', '\n\t\t For comparison we include the results of running the word-by-word classifier described in \n\t\t']",Positive
"['\n\t\t We also provide results for our noisy channel model using a bigram language model and a second trigram model where the twenty most likely analyses are rescored . \n\t', '\n\t\t Finally we show the results using the parser language model . \n\t', ""\n\t\t CJ01 ' Bigram Trigram Parser Precision 0.951 0.776 0.774 0.820 Recall 0.631 0.736 0.763 0.778 F-score 0.759 0.756 0.768 0.797 The noisy channel model using a bigram language model does a slightly worse job at identifying reparandum and interregnum words than the classifier proposed in \n\t\t""]",Positive
"['\n\t\t Replacing the bigram language model with a trigram model helps slightly , and parser- based language model results in a significant performance improvement over all of the others . \n\t', '\n\t\t 5 Conclusion and further work This paper has proposed a novel noisy channel model of speech repairs and has used it to identify reparandum words . \n\t', '\n\t\t One of the advantages of probabilistic models is that they can be integrated with other probabilistic models in a principled way , and it would be interesting to investigate how to integrate this kind of model of speech repairs with probabilistic speech recognizers . \n\t', '\n\t\t There are other kinds of joint models of reparandum and repair that may produce a better reparandum detection system . \n\t', '\n\t\t We have experimented with versions of the models described above based on POS bi-tag dependencies rather than word bigram dependencies , but with results very close to those presented here . \n\t', '\n\t\t Still , more sophisticated models may yield better performance . \n\t', '\n\t\t It would also be interesting to combine this probabilistic model of speech repairs with the word classifier approach of \n\t\t']",Positive
"['\n\t\t That approach may do so well because many speech repairs are very short , involving only one or two words \n\t\t']",Positive
"['\n\t\t On the other hand , the probabilistic model of repairs explored here seems to be most successful in identifying long repairs in which the reparandum and repair are similar enough to be unlikely to have been generated independently . \n\t', '\n\t\t Since the two approaches seem to have different strengths , a combined model may outperform both of them . \n\t', '\n\t\t References Eugene Charniak and Mark Johnson . \n\t', '\n\t\t 2001 . \n\t', '\n\t\t Edit detection and parsing for transcribed speech . \n\t', '\n\t\t In Proceedings of the 2nd Meeting of the North American Chapter of the Association for Computational Linguistics , pages 118\x96126 . \n\t', '\n\t\t The Association for Computational Linguistics . \n\t', '\n\t\t Eugene Charniak . \n\t', '\n\t\t 2001. Immediate-head pars- ing for language models . \n\t', '\n\t\t In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics . \n\t', '\n\t\t The Association for Computational Linguistics . \n\t', '\n\t\t Stanley F. Chen and Joshua Goodman . \n\t', '\n\t\t 1998. An empirical study of smoothing techniques for language modeling . \n\t', '\n\t\t Technical Report TR10-98 , Center for Research in Computing Technology , Harvard University . \n\t', '\n\t\t Peter A. Heeman and James F. Allen . \n\t', '\n\t\t 1999. Speech repairs , intonational phrases , and discourse markers : Modeling speaker\x92s utterances in spoken dialogue . \n\t', '\n\t\t Computational Linguistics , 25(4):527\x96571 . \n\t', '\n\t\t Stuart M. Shieber and Yves Schabes . \n\t', '\n\t\t 1990. Synchronous tree-adjoining grammars . \n\t', '\n\t\t In Proceedings of the 13th International Conference on Computational Linguistics ( COLING 1990 ) , pages 253\x96258 . \n\t', '\n\t\t Stuart M. Shieber . \n\t', '\n\t\t 1985. Evidence against the Context-Freeness of natural language . \n\t', '\n\t\t Linguistics and Philosophy , 8(3):333\x96344 . \n\t', '\n\t\t Elizabeth Shriberg and Andreas Stolcke . \n\t', '\n\t\t 1998. How far do speakers back up in repairs ? \n\t', '\n\t\t a quantitative model . \n\t', '\n\t\t In Proceedings of the International Conference on Spoken Language Processing , volume 5 , pages 2183\x962186 , Sydney , Australia . \n\t', '\n\t\t Elizabeth Shriberg . \n\t', '\n\t\t 1994 . \n\t', '\n\t\t Preliminaries to a Theory of Speech Disfluencies . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , University of California , Berkeley . \n\t', '\n\t\t Attention Shifting for Parsing Speech ^ Keith Hall Department of Computer Science Brown University Providence , RI 02912 kh@cs.brown.edu Mark Johnson Department of Cognitive and Linguistic Science Brown University Providence , RI 02912 Mark Johnson@Brown.edu Abstract We present a technique that improves the efficiency of word-lattice parsing as used in speech recognition language modeling . \n\t', '\n\t\t Our technique applies a probabilistic parser iteratively where on each iteration it focuses on a different subset of the word- lattice . \n\t', '\n\t\t The parser\x92s attention is shifted towards word-lattice subsets for which there are few or no syntactic analyses posited . \n\t', '\n\t\t This attention-shifting technique provides a six-times increase in speed ( measured as the number of parser analyses evaluated ) while performing equivalently when used as the first-stage of a multi-stage parsing-based language model . \n\t', '\n\t\t 1 Introduction Success in language modeling has been dominated by the linear n-gram for the past few decades . \n\t', '\n\t\t A number of syntactic language models have proven to be competitive with the n-gram and better than the most popular n-gram , the trigram \n\t\t']",Positive
"['\n\t\t Language modeling for speech could well be the first real problem for which syntactic techniques are useful . \n\t', '\n\t\t VP:ate VB NP IN NP:plate IN NP:fork John ate the pizza on a plate with a fork . \n\t', '\n\t\t Figure 1 : An incomplete parse tree with head-word annotations . \n\t', '\n\t\t One reason that we expect syntactic models to perform well is that they are capable of modeling long-distance dependencies that simple n-gram * This research was supported in part by NSF grants 9870676 and 0085940. models cannot . \n\t', '\n\t\t For example , the model presented by Chelba and Jelinek \n\t\t']",Positive
['\n\t\t The model presented by Charniak \n\t\t'],Positive
['\n\t\t While there are n-gram models that attempt to extend the left-context window through the use of caching and skip models \n\t\t'],Negative
"['\n\t\t Figure 1 presents a simple example to illustrate the nature of long-distance dependencies . \n\t', '\n\t\t Using a syntactic model such as the the Structured Language Model \n\t\t']",Positive
"['\n\t\t Consider the problem of disambiguating between ... plate with a fork and ... plate with effort . \n\t', '\n\t\t The syntactic model captures the semantic relationship between the words ate andfork . \n\t', '\n\t\t The syntactic structure allows us to find lexical contexts for which there is some semantic relationship ( e.g. , predicate- argument ) . \n\t', '\n\t\t Unfortunately , syntactic language modeling techniques have proven to be extremely expensive in terms of computational effort . \n\t', '\n\t\t Many employ the use of string parsers ; in order to utilize such techniques for language modeling one must preselect a set of strings from the word-lattice and parse each of them separately , an inherently inefficient procedure . \n\t', '\n\t\t Of the techniques that can process word-lattices directly , it takes significant computation to achieve the same levels of accuracy as then \x96best reranking method . \n\t', '\n\t\t This computational cost is the result of increasing the search space evaluated with the syntactic model ( parser ) ; the larger space resulting from combining the search for syntactic structure with the search for paths in the word-lattice . \n\t', '\n\t\t In this paper we propose a variation of a probabilistic word-lattice parsing technique that increases PP:with PP:on Figure 2 : A partial word-lattice from the NIST HUB-1 dataset . \n\t', '\n\t\t it/51.59 to/0 13 7 outline/2.573 strategy/0 a/71.30 the/115.3 strategy/0 11 </s>/0 12/0 outline/0 9 of/115.4 outlines/7.140 0 yesterday/0 1 and/4.004 in/14.73 tuesday/0 to/0 5 3 14 2 tuesday/0 4 two/8.769 6 to/0.000 outlaw/83.57 outlines/10.71 8 outlined/8.027 outline/0 in/0 10 outlined/12.58 efficiency while incurring no loss of language modeling performance ( measured as Word Error Rate \x96 WER ) . \n\t', '\n\t\t In \n\t\t']",Positive
"['\n\t\t The first stage is a PCFG word-lattice parser that generates a set of candidate parses over strings in a word-lattice , while the second stage rescores these candidate edges using a lexicalized syntactic language model \n\t\t']",Positive
"['\n\t\t Under this paradigm , the first stage is not only responsible for selecting candidate parses , but also for selecting paths in the word-lattice . \n\t', '\n\t\t Due to computational and memory requirements of the lexicalized model , the second stage parser is capable of rescoring only a small subset of all parser analyses . \n\t', '\n\t\t For this reason , the PCFG prunes the set of parser analyses , thereby indirectly pruning paths in the word lattice . \n\t', '\n\t\t We propose adding a meta-process to the first- stage that effectively shifts the selection of word- lattice paths to the second stage ( where lexical information is available ) . \n\t', '\n\t\t We achieve this by ensuring that for each path in the word-lattice the first-stage parser posits at least one parse . \n\t', '\n\t\t 2 Parsing speech word-lattices P(A,W) = P(AIW)P(W) ( 1 ) The noisy channel model for speech is presented in Equation 1 , where A represents the acoustic data extracted from a speech signal , and W represents a word string . \n\t', '\n\t\t The acoustic model P(AIW) assigns probability mass to the acoustic data given a word string and the language model P(W) defines a distribution over word strings . \n\t', '\n\t\t Typically the acoustic model is broken into a series of distributions conditioned on individual words ( though these are based on false independence assumptions ) . \n\t', '\n\t\t P(AIw1 ... wi ... wn ) = ~n P(AI wi ) ( 2 ) i=1 The result of the acoustic modeling process is a set of string hypotheses ; each word of each hypothesis is assigned a probability by the acoustic model . \n\t', '\n\t\t Word-lattices are a compact representation of output of the acoustic recognizer ; an example is presented in Figure 2 . \n\t', '\n\t\t The word-lattice is a weighted directed acyclic graph where a path in the graph corresponds to a string predicted by the acoustic recognizer . \n\t', '\n\t\t The ( sum ) product of the ( log ) weights on the graph ( the acoustic probabilities ) is the probability of the acoustic data given the string . \n\t', '\n\t\t Typically we want to know the most likely string given the acoustic data . \n\t', '\n\t\t arg max P(W IA ) ( 3 ) = arg max P ( A , W ) = argmaxP(AIW)P(W) In Equation 3 we use Bayes\x92 rule to find the optimal string given P(AIW) , the acoustic model , and P(W) , the language model . \n\t', '\n\t\t Although the language model can be used to rescore1 the word-lattice , it is typically used to select a single hypothesis . \n\t', '\n\t\t We focus our attention in this paper to syntactic language modeling techniques that perform complete parsing , meaning that parse trees are built upon the strings in the word-lattice . \n\t', '\n\t\t 2.1 n\x96best list reranking Much effort has been put forth in developing efficient probabilistic models for parsing strings \n\t\t']",Positive
"['\n\t\t Then \x96best list reranking procedure , depicted in Figure 3 , utilizes an external language model that selects a set of strings from the word-lattice . \n\t', '\n\t\t These strings are analyzed by the parser which computes a language model probability . \n\t', '\n\t\t This probability is combined 1To rescore a word-lattice , each arch is assigned a new score ( probability ) defined by a new model ( in combination with the acoustic model ) . \n\t', ""\n\t\t duh/1.385 1 6 the/0 3 2 man/0 is/0 mans/1.385 man's/1.385 man/0 4 7 10 is/0 5 early/0 early/0 surly/0 surly/0.692 surely/0 8 early/0 9 n-best list extractor w1 , ... , wi , ... , wn1 w1 , ... , wi , ... , wn2 w1 , ... , wi , ... , wn3 w1 , ... , wi , ... , wn4 w1 , ... , wi , ... , wnm ... \n\t"", '\n\t\t Language Model o1 , ... , oi , ... , on Figure 3 : n \x96best list reranking with the acoustic model probability to reranked the strings according to the joint probability P(A , W ) . \n\t', '\n\t\t There are two significant disadvantages to this approach . \n\t', '\n\t\t First , we are limited by the performance of the language model used to select then \x96best lists . \n\t', '\n\t\t Usually , the trigram model is used to select n paths through the lattice generating at most n unique strings . \n\t', '\n\t\t The maximum performance that can be achieved is limited by the performance of this extractor model . \n\t', '\n\t\t Second , of the strings that are analyzed by the parser , many will share common substrings . \n\t', '\n\t\t Much of the work performed by the parser is duplicated for these substrings . \n\t', '\n\t\t This second point is the primary motivation behind parsing word-lattices \n\t\t']",Positive
"['\n\t\t 2.2 Multi-stage parsing PCFG Parser Lexicalized Parser Figure 4 : Coarse-to-fine lattice parsing . \n\t', '\n\t\t In Figure 4 we present the general overview of a multi-stage parsing technique \n\t\t']",Positive
"['\n\t\t This process Parse word-lattice with PCFG parser 5 . \n\t', '\n\t\t Overparse , generating additional candidates 6 . \n\t', '\n\t\t Compute inside-outside probabilities 7 . \n\t', '\n\t\t Prune candidates with probability threshold Table 1 : First stage word-lattice parser is know as coarse-to-fine modeling , where coarse models are more efficient but less accurate than fine models , which are robust but computationally expensive . \n\t', '\n\t\t In this particular parsing model a PCFG best-first parser \n\t\t']",Positive
"[""\n\t\t This first stage performs overparsing which effectively allows it to generate a set of high probability candi- date parses ^ ' . \n\t"", '\n\t\t These parses are then rescored us- ing a lexicalized syntactic model \n\t\t']",Positive
"['\n\t\t Although the coarse-to-fine model may include any number of intermediary stages , in this paper we consider this two-stage model . \n\t', '\n\t\t There is no guarantee that parses favored by the second stage will be generated by the first stage . \n\t', '\n\t\t In other words , because the first stage model prunes the space of parses from which the second stage rescores , the first stage model may remove solutions that the second stage would have assigned a high probability . \n\t', '\n\t\t In \n\t\t']",Positive
"['\n\t\t The first-stage parser , Table 1 , is responsible for positing a set of candidate parses over the word- lattice . \n\t', '\n\t\t Were we to run the parser to completion it would generate all parses for all strings described by the word-lattice . \n\t', '\n\t\t As with string parsing , we stop the first stage parser early , generating a subset of all parses . \n\t', '\n\t\t Only the strings covered by complete parses are passed on to the second stage parser . \n\t', '\n\t\t This indirectly prunes the word-lattice of all word-arcs that were not covered by complete parses in the first stage . \n\t', '\n\t\t We use a first stage PCFG parser that performs a best-first search over the space of parses , which means that it depends on a heuristic \x93figure-ofmerit\x94 ( FOM ) \n\t\t']",Positive
"['\n\t\t A good FOM attempts to model the true probability of a chart edge2 P(Nij,k) . \n\t', '\n\t\t Generally , this probability is impossible to compute during the parsing process as it requires knowing both the inside and outside probabilities ( Charniak , 1993 ; Manning and Sch¨utze , 1999 ) . \n\t', '\n\t\t The FOM we describe is an approximation to the edge probability and is computed using an estimate of the inside probability times an approximation to the outside probability 3 . \n\t', '\n\t\t The inside probability ^(Nij,k) can be computed incrementally during bottom-up parsing . \n\t', '\n\t\t The normalized acoustic probabilities from the acoustic recognizer are included in this calculation . \n\t', '\n\t\t \x88^(Nij,k) ( 4 ) fwd(Tqi,j)p(Ni |T N q)p(7r i)bkwd(Tkr l ) The outside probability is approximated with a bitag model and the standard tag/category boundary model \n\t\t']",Positive
"['\n\t\t Equation 4 presents the approximation to the outside probability . \n\t', '\n\t\t Part-of-speech tags Tq and Tr are the candidate tags to the left and right of the constituent Nij,k . \n\t', '\n\t\t The fwd() and bkwd() functions are the HMM forward and backward probabilities calculated over a lattice containing the part-of-speech tag , the word , and the acoustic scores from the word-lattice to the left and right of the constituent , respectively . \n\t', '\n\t\t p(Ni | Tq ) and p(Tr| Ni ) are the boundary statistics which are estimated from training data ( details of this model can be found in \n\t\t']",Positive
"['\n\t\t FOM(Nij,k) = \x88^(Nij,k)^(Nij,k)^C(j , k ) ( 5 ) The best-first search employed by the first stage parser uses the FOM defined in Equation 5 , where ^is a normalization factor based on path length C(j , k ) . \n\t', '\n\t\t The normalization factor prevents small constituents from consistently being assigned a 2A chart edge Nij,k indicates a grammar category Ni can be constructed from nodes j to k. 3An alternative to the inside and outside probabilities are the Viterbi inside and outside probabilities \n\t\t']",Positive
['\n\t\t higher probability than larger constituents \n\t\t'],Positive
"['\n\t\t Although this heuristic works well for directing the parser towards likely parses over a string , it is not an ideal model for pruning the word-lattice . \n\t', '\n\t\t First , the outside approximation of this FOM is based on a linear part-of-speech tag model ( the bitag ) . \n\t', '\n\t\t Such a simple syntactic model is unlikely to provide realistic information when choosing a word-lattice path to consider . \n\t', '\n\t\t Second , the model is prone to favoring subsets of the word-lattice causing it to posit additional parse trees for the favored sublattice rather than exploring the remainder of the word-lattice . \n\t', '\n\t\t This second point is the primary motivation for the attention shifting technique presented in the next section . \n\t', '\n\t\t 3 Attention shifting4 We explore a modification to the multi-stage parsing algorithm that ensures the first stage parser posits at least one parse for each path in the word-lattice . \n\t', '\n\t\t The idea behind this is to intermittently shift the attention of the parser to unexplored parts of the word lattice . \n\t', '\n\t\t PCFG Word-lattice Parser Identify Used Edges Clear Agenda/ Add Edges for Unused Words yes Continue Multi-stage Parsing Figure 5 : Attention shifting parser . \n\t', '\n\t\t Figure 5 depicts the attention shifting first stage parsing procedure . \n\t', '\n\t\t A used edge is a parse edge that has non-zero outside probability . \n\t', '\n\t\t By definition of 4The notion of attention shifting is motivated by the work on parser FOM compensation presented in \n\t\t']",Positive
"['\n\t\t ~= i,l,q,r Is Agenda no Empty ? \n\t', '\n\t\t the outside probability , used edges are constituents that are part of a complete parse ; a parse is complete if there is a root category label ( e.g. , S for sentence ) that spans the entire word-lattice . \n\t', '\n\t\t In order to identify used edges , we compute the outside probabilities for each parse edge ( efficiently computing the outside probability of an edge requires that the inside probabilities have already been computed ) . \n\t', '\n\t\t In the third step of this algorithm we clear the agenda , removing all partial analyses evaluated by the parser . \n\t', '\n\t\t This forces the parser to abandon analyses of parts of the word-lattice for which complete parses exist . \n\t', '\n\t\t Following this , the agenda is populated with edges corresponding to the unused words , priming the parser to consider these words . \n\t', '\n\t\t To ensure the parser builds upon at least one of these unused edges , we further modify the parsing algorithm : \x95 Only unused edges are added to the agenda . \n\t', '\n\t\t \x95 When building parses from the bottom up , a parse is considered complete if it connects to a used edge . \n\t', '\n\t\t These modifications ensure that the parser focuses on edges built upon the unused words . \n\t', '\n\t\t The second modification ensures the parser is able to determine when it has connected an unused word with a previously completed parse . \n\t', '\n\t\t The application of these constraints directs the attention of the parser towards new edges that contribute to parse analyses covering unused words . \n\t', '\n\t\t We are guaranteed that each iteration of the attention shifting algorithm adds a parse for at least one unused word , meaning that it will take at most I |A | iterations to cover the entire lattice , where A is the set of word-lattice arcs . \n\t', '\n\t\t This guarantee is trivially provided through the constraints just described . \n\t', '\n\t\t The attention-shifting parser continues until there are no unused words remaining and each parsing iteration runs until it has found a complete parse using at least one of the unused words . \n\t', '\n\t\t As with multi-stage parsing , an adjustable parameter determines how much overparsing to perform on the initial parse . \n\t', '\n\t\t In the attention shifting algorithm an additional parameter specifies the amount of overparsing for each iteration after the first . \n\t', '\n\t\t The new parameter allows for independent control of the attention shifting iterations . \n\t', '\n\t\t After the attention shifting parser populates a parse chart with parses covering all paths in the lattice , the multi-stage parsing algorithm performs additional pruning based on the probability of the parse edges ( the product of the inside and outside probabilities ) . \n\t', '\n\t\t This is necessary in order to constrain the size of the hypothesis set passed on to the second stage parsing model . \n\t', '\n\t\t The Charniak lexicalized syntactic language model effectively splits the number of parse states ( an edges in a PCFG parser ) by the number of unique contexts in which the state is found . \n\t', '\n\t\t These contexts include syntactic structure such as parent and grandparent category labels as well as lexical items such as the head of the parent or the head of a sibling constituent \n\t\t']",Positive
"['\n\t\t State splitting on this level causes the memory requirement of the lexicalized parser to grow rapidly . \n\t', '\n\t\t Ideally , we would pass all edges on to the second stage , but due to memory limitations , pruning is necessary . \n\t', '\n\t\t It is likely that edges recently discovered by the attention shifting procedure are pruned . \n\t', '\n\t\t However , the true PCFG probability model is used to prune these edges rather than the approximation used in the FOM . \n\t', '\n\t\t We believe that by considering parses which have a relatively high probability according to the combined PCFG and acoustic models that we will include most of the analyses for which the lexicalized parser assigns a high probability . \n\t', '\n\t\t 4 Experiments The purpose of attention shifting is to reduce the amount of work exerted by the first stage PCFG parser while maintaining the same quality of language modeling ( in the multi-stage system ) . \n\t', '\n\t\t We have performed a set of experiments on the NIST \x9293 HUB\x961 word-lattices . \n\t', '\n\t\t The HUB\x961 is a collection of 213 word-lattices resulting from an acoustic recognizer\x92s analysis of speech utterances . \n\t', '\n\t\t Professional readers reading Wall Street Journal articles generated the utterances . \n\t', '\n\t\t The first stage parser is a best-first PCFG parser trained on sections 2 through 22 , and 24 of the Penn WSJ treebank \n\t\t']",Positive
"['\n\t\t Prior to training , the treebank is transformed into speech-like text , removing punctuation and expanding numerals , etc.5 Overparsing is performed using an edge pop6 multiplicative factor . \n\t', '\n\t\t The parser records the number of edge pops required to reach the first complete parse . \n\t', '\n\t\t The parser continues to parse a until multiple of the number of edge pops required for the first parse are popped off the agenda . \n\t', '\n\t\t The second stage parser used is a modified version of the Charniak language modeling parser described in \n\t\t']",Positive
"['\n\t\t We trained this parser 5Brian Roark of AT&T provided a tool to perform the speech normalization . \n\t', '\n\t\t 6An edge pop is the process of the parser removing an edge from the agenda and placing it in the parse chart . \n\t', '\n\t\t on the BLLIP99 corpus \n\t\t']",Positive
"['\n\t\t In order to compare the work done by then \x96best reranking technique to the word-lattice parser , we generated a set of n \x96best lattices . \n\t', '\n\t\t 50\x96best lists were extracted using the Chelba A* decoder7 . \n\t', '\n\t\t A 50\x96 best lattice is a sublattice of the acoustic lattice that generates only the strings found in the 50\x96best list . \n\t', '\n\t\t Additionally , we provide the results for parsing the full acoustic lattices ( although these work measurements should not be compared to those of n \x96best reranking ) . \n\t', '\n\t\t We report the amount of work , shown as the cumulative # edge pops , the oracle WER for the word-lattices after first stage pruning , and the WER of the complete multi-stage parser . \n\t', '\n\t\t In all of the word-lattice parsing experiments , we pruned the set of posited hypothesis so that no more than 30,000 local-trees are generated8 . \n\t', '\n\t\t We chose this threshold due to the memory requirements of the second stage parser . \n\t', '\n\t\t Performing pruning at the end of the first stage prevents the attention shifting parser from reaching the minimum oracle WER ( most notable in the full acoustic word-lattice experiments ) . \n\t', '\n\t\t While the attention-shifting algorithm ensures all word-lattice arcs are included in complete parses , forward-backward pruning , as used here , will eliminate some of these parses , indirectly eliminating some of the word-lattice arcs . \n\t', '\n\t\t To illustrate the need for pruning , we computed the number of states used by the Charniak lexicalized syntactic language model for 30,000 local trees . \n\t', '\n\t\t An average of 215 lexicalized states were generated for each of the 30,000 local trees . \n\t', '\n\t\t This means that the lexicalized language model , on average , computes probabilities for over 6.5 million states when provided with 30,000 local trees . \n\t', '\n\t\t Model # edge pops O-WER WER n \x96best ( Charniak ) 2.5 million 7.75 11.8 100x LatParse 3.4 million 8.18 12.0 1 0x AttShift 564,895 7.78 11.9 Table 2 : Results for n \x96best lists and n \x96best lattices . \n\t', '\n\t\t Table 2 shows the results for n \x96best list reranking and word-lattice parsing of n \x96best lattices . \n\t', '\n\t\t We recreated the results of the Charniak language model parser used for reranking in order to measure the amount of work required . \n\t', '\n\t\t We ran the first stage parser with 4-times overparsing for each string in 7The n\x96best lists were provided by Brian Roark \n\t\t']",Positive
"['\n\t\t An example local tree is NP3,8 --+ DT3,4 NN4,8 . \n\t', '\n\t\t then \x96best list . \n\t', '\n\t\t The LatParse result represents running the word-lattice parser on then \x96best lattices performing 100\x96times overparsing in the first stage . \n\t', '\n\t\t The AttShift model is the attention shifting parser described in this paper . \n\t', '\n\t\t We used 10\x96times overparsing for both the initial parse and each of the attention shifting iterations . \n\t', '\n\t\t When run on then \x96best lattice , this model achieves a comparable WER , while reducing the amount of parser work sixfold ( as compared to the regular word-lattice parser ) . \n\t', '\n\t\t Model # edge pops O-WER WER acoustic lats N/A 3.26 N/A 100x LatParse 3.4 million 5.45 13.1 1 0x AttShift 1.6 million 4.17 13.1 Table 3 : Results for acoustic lattices . \n\t', '\n\t\t In Table 3 we present the results of the word- lattice parser and the attention shifting parser when run on full acoustic lattices . \n\t', '\n\t\t While the oracle WER is reduced , we are considering almost half as many edges as the standard word-lattice parser . \n\t', '\n\t\t The increased size of the acoustic lattices suggests that it may not be computationally efficient to consider the entire lattice and that an additional pruning phase is necessary . \n\t', '\n\t\t The most significant constraint of this multi-stage lattice parsing technique is that the second stage process has a large memory requirement . \n\t', '\n\t\t While the attention shifting technique does allow the parser to propose constituents for every path in the lattice , we prune some of these constituents prior to performing analysis by the second stage parser . \n\t', '\n\t\t Currently , pruning is accomplished using the PCFG model . \n\t', '\n\t\t One solution is to incorporate an intermediate pruning stage ( e.g. , lexicalized PCFG ) between the PCFG parser and the full lexicalized model . \n\t', '\n\t\t Doing so will relax the requirement for aggressive PCFG pruning and allows for a lexicalized model to influence the selection of word-lattice paths . \n\t', '\n\t\t 5 Conclusion We presented a parsing technique that shifts the attention of a word-lattice parser in order to ensure syntactic analyses for all lattice paths . \n\t', '\n\t\t Attention shifting can be thought of as a meta-process around the first stage of a multi-stage word-lattice parser . \n\t', '\n\t\t We show that this technique reduces the amount of work exerted by the first stage PCFG parser while maintaining comparable language modeling performance . \n\t', '\n\t\t Attention shifting is a simple technique that attempts to make word-lattice parsing more efficient . \n\t', '\n\t\t As suggested by the results for the acoustic lattice experiments , this technique alone is not sufficient . \n\t', '\n\t\t Solutions to improve these results include modifying the first-stage grammar by annotating the category labels with local syntactic features as suggested in \n\t\t']",Positive
"['\n\t\t Improving the quality of the parses selected by the first stage should reduce the need for generating such a large number of candidates prior to pruning , improving efficiency as well as overall accuracy . \n\t', '\n\t\t We believe that attention shifting , or some variety of this technique , will be an integral part of efficient solutions for word-lattice parsing . \n\t', '\n\t\t References Don Blaheta and Eugene Charniak . \n\t', '\n\t\t 1999. Automatic compensation for parser figure-of-merit flaws . \n\t', '\n\t\t In Proceedings of the 37th annual meeting of the Association for Computational Linguistics , pages 513\x96518 . \n\t', '\n\t\t Robert J. Bobrow . \n\t', '\n\t\t 1990. Statistical agenda parsing . \n\t', '\n\t\t In DARPA Speech and Language Workshop , pages 222\x96224 . \n\t', '\n\t\t Sharon Caraballo and Eugene Charniak . \n\t', '\n\t\t 1998. New figures of merit for best-first probabilistic chart parsing . \n\t', '\n\t\t Computational Linguistics , 24(2):275\x96298 , June . \n\t', '\n\t\t Eugene Charniak , Don Blaheta , Niyu Ge , Keith Hall , John Hale , and Mark Johnson . \n\t', '\n\t\t 1999. BLLIP 1987\x9689 wsj corpus release 1 . \n\t', '\n\t\t LDC corpus LDC2000T43 . \n\t', '\n\t\t Eugene Charniak . \n\t', '\n\t\t 1993. Statistical Language Learning . \n\t', '\n\t\t MIT Press . \n\t', '\n\t\t Eugene Charniak . \n\t', '\n\t\t 2000. A maximum-entropyinspired parser . \n\t', '\n\t\t In Proceedings of the 2000 Conference of the North American Chapter of the Association for Computational Linguistics. , ACL , New Brunswick , NJ . \n\t', '\n\t\t Eugene Charniak . \n\t', '\n\t\t 2001. Immediate-head parsing for language models . \n\t', '\n\t\t In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics . \n\t', '\n\t\t Ciprian Chelba and Frederick Jelinek . \n\t', '\n\t\t 1998. A study on richer syntactic dependencies for structured language modeling . \n\t', '\n\t\t In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics , pages 225\x96231 . \n\t', '\n\t\t Sharon Goldwater , Eugene Charniak , and Mark Johnson . \n\t', '\n\t\t 1998. Best-first edge-based chart parsing . \n\t', '\n\t\t In 6th Annual Workshop for Very Large Corpora , pages 127\x96133 . \n\t', '\n\t\t Joshua Goodman . \n\t', '\n\t\t 1997. Global thresholding and multiple-pass parsing . \n\t', '\n\t\t In Proceedings of the Sec- ond Conference on Empirical Methods in Natural Language Processing , pages 11\x9625 . \n\t', '\n\t\t Joshua Goodman . \n\t', '\n\t\t 2001. A bit of progress in language modeling , extendend version . \n\t', '\n\t\t In Microsoft Research Technical Report MSR-TR-2001-72 . \n\t', '\n\t\t Keith Hall and Mark Johnson . \n\t', '\n\t\t 2003. Language modeling using efficient best-first bottom-up parsing . \n\t', '\n\t\t In Proceedings of IEEE Automated Speech Recognition and Understanding Workshop . \n\t', '\n\t\t Mark Johnson . \n\t', '\n\t\t 1998. PCFG models of linguistic tree representations . \n\t', '\n\t\t Computational Linguistics , 24:617\x96636 . \n\t', '\n\t\t Dan Klein and Christopher D. Manning . \n\t', '\n\t\t 2003 . \n\t', '\n\t\t Accurate unlexicalized parsing . \n\t', '\n\t\t In Proceedings of the 41st Meeting of the Association for Computational Linguistics ( ACL-03 ) . \n\t', '\n\t\t Christopher D. Manning and Hinrich Sch¨utze . \n\t', '\n\t\t 1999. Foundations of statistical natural language processing . \n\t', '\n\t\t MIT Press . \n\t', '\n\t\t Mitchell Marcus , Beatrice Santorini , and Mary Ann Marcinkiewicz . \n\t', '\n\t\t 1993. Building a large annotated corpus of english : The penn treebank . \n\t', '\n\t\t Computational Linguistics , 19:313\x96330 . \n\t', '\n\t\t Brian Roark . \n\t', '\n\t\t 2001. Probabilistic top-down parsing and language modeling . \n\t', '\n\t\t Computational Linguistics , 27(3):249\x96276 . \n\t', '\n\t\t Peng Xu , Ciprian Chelba , and Frederick Jelinek . \n\t', '\n\t\t 2002. A study on richer syntactic dependencies for structured language modeling . \n\t', '\n\t\t In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , pages 191\x96 198. \n\t', '\n\t\t Discriminative Language Modeling with Conditional Random Fields and the Perceptron Algorithm Brian Roark Murat Saraclar Michael Collins Mark Johnson AT&T Labs - Research MIT CSAIL Brown University {roark,murat}@research.att.com mcollins@csail.mit.edu Mark Johnson@Brown.edu Abstract This paper describes discriminative language modeling for a large vocabulary speech recognition task . \n\t', '\n\t\t We contrast two parameter estimation methods : the perceptron algorithm , and a method based on conditional random fields ( CRFs ) . \n\t', '\n\t\t The models are encoded as deterministic weighted finite state automata , and are applied by intersecting the automata with word-lattices that are the output from a baseline recognizer . \n\t', '\n\t\t The perceptron algorithm has the benefit of automatically selecting a relatively small feature set in just a couple of passes over the training data . \n\t', '\n\t\t However , using the feature set output from the perceptron algorithm ( initialized with their weights ) , CRF training provides an additional 0.5 % reduction in word error rate , for a total 1.8 % absolute reduction from the baseline of 39.2 % . \n\t', '\n\t\t 1 Introduction A crucial component of any speech recognizer is the language model ( LM ) , which assigns scores or probabilities to candidate output strings in a speech recognizer . \n\t', '\n\t\t The language model is used in combination with an acoustic model , to give an overall score to candidate word sequences that ranks them in order of probability or plausibility . \n\t', '\n\t\t A dominant approach in speech recognition has been to use a \x93source-channel\x94 , or \x93noisy-channel\x94 model . \n\t', '\n\t\t In this approach , language modeling is effectively framed as density estimation : the language model\x92s task is to define a distribution over the source \x96 i.e. , the possible strings in the language . \n\t', '\n\t\t Markov ( n-gram ) models are often used for this task , whose parameters are optimized to maximize the likelihood of a large amount of training text . \n\t', '\n\t\t Recognition performance is a direct measure of the effectiveness of a language model ; an indirect measure which is frequently proposed within these approaches is the perplexity of the LM ( i.e. , the log probability it assigns to some held-out data set ) . \n\t', '\n\t\t This paper explores alternative methods for language modeling , which complement the source-channel approach through discriminatively trained models . \n\t', '\n\t\t The language models we describe do not attempt to estimate a generative model P(w) over strings . \n\t', '\n\t\t Instead , they are trained on acoustic sequences with their transcriptions , in an attempt to directly optimize error-rate . \n\t', '\n\t\t Our work builds on previous work on language modeling using the perceptron algorithm , described in \n\t\t']",Positive
"['\n\t\t In particular , we explore conditional random field methods , as an alternative training method to the perceptron . \n\t', '\n\t\t We describe how these models can be trained over lat- tices that are the output from a baseline recognizer . \n\t', '\n\t\t We also give a number of experiments comparing the two approaches . \n\t', '\n\t\t The perceptron method gave a 1.3 % absolute improvement in recognition error on the Switchboard domain ; the CRF methods we describe give a further gain , the final absolute improvement being 1.8 % . \n\t', '\n\t\t A central issue we focus on concerns feature selection . \n\t', '\n\t\t The number of distinct n-grams in our training data is close to 45 million , and we show that CRF training converges very slowly even when trained with a subset ( of size 12 million ) of these features . \n\t', '\n\t\t Because of this , we explore methods for picking a small subset of the available features . \n\t', ""\n\t\t ' The perceptron algorithm can be used as one method for feature selection , selecting around 1.5 million features in total . \n\t"", '\n\t\t The CRF trained with this feature set , and initialized with parameters from perceptron training , converges much more quickly than other approaches , and also gives the optimal performance on the held-out set . \n\t', '\n\t\t We explore other approaches to feature selection , but find that the perceptron-based approach gives the best results in our experiments . \n\t', '\n\t\t While we focus on n-gram models , we stress that our methods are applicable to more general language modeling features \x96 for example , syntactic features , as explored in , e.g. , \n\t\t']",Positive
"['\n\t\t We intend to explore methods with new features in the future . \n\t', '\n\t\t Experimental results with n-gram models on 1000-best lists show a very small drop in accuracy compared to the use of lattices . \n\t', '\n\t\t This is encouraging , in that it suggests that models with more flexible features than n-gram models , which therefore cannot be efficiently used with lattices , may not be unduly harmed by their restriction to n-best lists . \n\t', '\n\t\t 1.1 Related Work Large vocabulary ASR has benefitted from discriminative estimation of Hidden Markov Model ( HMM ) parameters in the form of Maximum Mutual Information Estimation ( MMIE ) or Conditional Maximum Likelihood Estimation ( CMLE ) . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t In fact , state-of-the-art acoustic modeling , as seen , for example , at annual Switchboard evaluations , invariably includes some kind of discriminative training . \n\t', '\n\t\t Discriminative estimation of language models has also been proposed in recent years . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t are estimated by minimizing H(W |A ) , the expected uncertainty of the spoken text W , given the acoustic sequence A. \n\t\t']",Positive
['\n\t\t This work was followed up with some success by \n\t\t'],Positive
['\n\t\t \n\t\t'],Positive
['\n\t\t \n\t\t'],Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t Their algorithm first uses a classifier to predict what effect each parameter has on the error rate , and then modifies the parameters to reduce the error rate based on this prediction . \n\t', '\n\t\t 2 Linear Models , the Perceptron Algorithm , and Conditional Random Fields This section describes a general framework , global linear models , and two parameter estimation methods within the framework , the perceptron algorithm and a method based on conditional random fields . \n\t', '\n\t\t The linear models we describe are general enough to be applicable to a diverse range of NLP and speech tasks \x96 this section gives a general description of the approach . \n\t', '\n\t\t In the next section of the paper we describe how global linear models can be applied to speech recognition . \n\t', '\n\t\t In particular , we focus on how the decoding and parameter estimation problems can be implemented over lattices using finite-state techniques . \n\t', '\n\t\t 2.1 Global linear models We follow the framework outlined in Collins ( 2002 ; 2004 ) . \n\t', '\n\t\t The task is to learn a mapping from inputs x E X to outputs y E Y. We assume the following compo- nents : ( 1 ) Training examples ( xi , yi ) for i = 1 ... \n\t', '\n\t\t N. ( 2 ) A function GEN which enumerates a set of candidates GEN(x) for an input x. ( 3 ) A representation 4 ) mapping each ( x , y ) E X x Y to a feature vector 4)(x , y ) E Rd. ( 4 ) A parameter vector a¯ E Rd. . \n\t', '\n\t\t The components GEN , 4 ) and a¯ define a mapping from an input x to an output F(x) through F(x) = argmax 4)(x , y ) \x95 a¯ ( 1 ) yEGEN(x) where 4)(x , y ) \x95 a¯ is the inner product Ps as4)s(x , y ) . \n\t', '\n\t\t The learning task is to set the parameter values a¯ using the training examples as evidence . \n\t', '\n\t\t The decoding algorithm is a method for searching for the y that maximizes Eq . \n\t', '\n\t\t 1. 2.2 The Perceptron algorithm We now turn to methods for training the parameters a¯ of the model , given a set of training examples Inputs : Training examples ( xi , yi ) Initialization : Set a¯ = 0 Algorithm : Fort =1 ... \n\t', '\n\t\t T,i=1 ... \n\t', '\n\t\t N Calculate zi = argmaxzEGEN(xj) 4)(xi , z ) \x95 a¯ If(zi =~ yi ) then a¯ = a¯ + 4)(xi , yi ) \x97 4)(xi , zi ) Output : Parameters a¯ Figure 1 : A variant of the perceptron algorithm . \n\t', '\n\t\t ( x1 , y1 ) ... ( xN , yN ) . \n\t', '\n\t\t This section describes the per- ceptron algorithm , which was previously applied to language modeling in \n\t\t']",Positive
"['\n\t\t The next section describes an alternative method , based on conditional random fields . \n\t', '\n\t\t The perceptron algorithm is shown in figure 1 . \n\t', '\n\t\t At each training example ( xi , yi ) , the current best-scoring hypothesis zi is found , and if it differs from the reference yi , then the cost of each feature2 is increased by the count of that feature in zi and decreased by the count of that feature in yi . \n\t', '\n\t\t The features in the model are updated , and the algorithm moves to the next utterance . \n\t', '\n\t\t After each pass over the training data , performance on a held-out data set is evaluated , and the parameterization with the best performance on the held out set is what is ultimately produced by the algorithm . \n\t', '\n\t\t Following \n\t\t']",Positive
"['\n\t\t Say ¯ai is the parameter vector after the i\x92th example is processed on the t\x92th pass through the data in the algorithm in figure 1 . \n\t', '\n\t\t Then the averaged parameters ¯aAVG are defined as ¯aAVG = Ei,t ¯ai/NT . \n\t', '\n\t\t \n\t\t']",Positive
['\n\t\t 2.3 Conditional Random Fields Conditional Random Fields have been applied to NLP tasks such as parsing \n\t\t'],Positive
"['\n\t\t CRFs use the parameters a¯ to define a conditional distribution over the members of GEN ( x ) for a given input x : p¯^ ( y| x ) = Z(x ¯a ) exp (4)(x , y ) \x95 ¯a ) where Z(x , ¯a ) = EyCGEN(x) exp (4)(x , y ) \x95 ¯a ) is a normalization constant that depends on x and ¯a . \n\t', '\n\t\t Given these definitions , the log-likelihood of the training data under parameters a¯ is log p¯^ ( yi l xi ) [ 4)(xi , yi ) \x95 a¯ \x97 log Z(xi , ¯a ) ] ( 2 ) 2Note that here lattice weights are interpreted as costs , which changes the sign in the algorithm presented in figure 1. LL(¯a) = ~N = i=1 ~N i=1 Following \n\t\t']",Positive
"['\n\t\t The optimal parameters under this criterion are ¯^* = argmax¯^ LLR ( ¯^ ) . \n\t', '\n\t\t We use a limited memory variable metric method ( Benson and Mor´e , 2002 ) to optimize LLR . \n\t', '\n\t\t There is a general implementation of this method in the Tao/PETSc software libraries \n\t\t']",Positive
['\n\t\t This technique has been shown to be very effective in a variety of NLP tasks \n\t\t'],Positive
"['\n\t\t The main interface between the optimizer and the training data is a procedure which takes a parameter vector ^¯ as input , and in turn returns LLR(¯^) as well as the gradient of LLR at ¯^ . \n\t', '\n\t\t The derivative of the objective function with respect to a parameter ^s at parameter values ^¯ is ^ ^ a3 , D3(Xi,yi) \x97 X p¯^(y1Xi),D3(Xi,y)v \x97 2 ( 4 ) y^GEN(xi) Note that LLR ( ¯^ ) is a convex function , so that there is a globally optimal solution and the optimization method will find it . \n\t', '\n\t\t The use of the Gaussian prior term 11¯^112 /2^2 in the objective function has been found to be useful in several NLP settings . \n\t', '\n\t\t It effectively ensures that there is a large penalty for parameter values in the model becoming too large \x96 as such , it tends to control over-training . \n\t', '\n\t\t The choice of LLR as an objective function can be justified as maximum a-posteriori ( MAP ) training within a Bayesian approach . \n\t', '\n\t\t An alternative justification comes through a connection to support vector machines and other large margin approaches . \n\t', '\n\t\t SVM-based approaches use an optimization criterion that is closely related to LLR \x96 see \n\t\t']",Positive
"['\n\t\t 3 Linear models for speech recognition We now describe how the formalism and algorithms in section 2 can be applied to language modeling for speech recognition . \n\t', '\n\t\t 3.1 The basic approach As described in the previous section , linear models require definitions of X , Y , xi , yi , GEN , 4 ) and a parameter estimation method . \n\t', '\n\t\t In the language modeling setting we take X to be the set of all possible acoustic inputs ; Y is the set of all possible strings , E* , for some vocabulary E . \n\t', '\n\t\t Each xi is an utterance ( a sequence of acoustic feature-vectors ) , and GEN(xi) is the set of possible transcriptions under a first pass recognizer . \n\t', '\n\t\t ( GEN(xi) is a huge set , but will be represented compactly using a lattice \x96 we will discuss this in detail shortly ) . \n\t', '\n\t\t We take yi to be the member of GEN(xi) with lowest error rate with respect to the reference transcription of xi . \n\t', '\n\t\t All that remains is to define the feature-vector representation , 4)(x , y ) . \n\t', '\n\t\t In the general case , each component 4)i ( x , y ) could be essentially any function of the acoustic input x and the candidate transcription y . \n\t', '\n\t\t The first feature we define is 4)0 ( x , y ) as the log -probability of y given x under the lattice produced by the baseline recognizer . \n\t', '\n\t\t Thus this feature will include contributions from the acoustic model and the original language model . \n\t', '\n\t\t The remaining features are restricted to be functions over the transcription y alone and they track all n-grams up to some length ( say n = 3 ) , for example : 4)1(x , y ) = Number of times \x93the the of\x94 is seen in y At an abstract level , features of this form are introduced for all n-grams up to length 3 seen in some training data lattice , i.e. , n-grams seen in any word sequence within the lattices . \n\t', '\n\t\t In practice , we consider methods that search for sparse parameter vectors ¯^ , thus assigning many n- grams 0 weight . \n\t', '\n\t\t This will lead to more efficient algorithms that avoid dealing explicitly with the entire set of n-grams seen in training data . \n\t', '\n\t\t 3.2 Implementation using WFA We now give a brief sketch of how weighted finite-state automata ( WFA ) can be used to implement linear models for speech recognition . \n\t', '\n\t\t There are several papers describing the use of weighted automata and transducers for speech in detail , e.g. , \n\t\t']",Positive
"['\n\t\t For our purpose , a WFA A = ( E , Q , qs , F , E , ^ ) , where E is the vocabulary , Q is a ( finite ) set of states , qs E Q is a unique start state , F C Q is a set of final states , E is a ( finite ) set of transitions , and ^ : F ^ R is a function from final states to final weights . \n\t', '\n\t\t Each tran- sition e E E is a tuple e = ( l [ e ] , p[e] , n[e] , w[e] ) , where l[e] E E is a label ( in our case , words ) , p[e] E Q is the origin state of e , n[e] E Q is the destination state of e , and w[e] E R is the weight of the transition . \n\t', '\n\t\t A successful path 7r = e1 ... ej is a sequence of transitions , such that p[e1] = qs , n[ej] E F , and for 1 < k < j , n[ek_1] = p[ek] . \n\t', '\n\t\t Let IIA be the set of successful paths 7r in a WFA A . \n\t', '\n\t\t For any 7r = e1 ... ej , l[7r] = l[e1] ... l[ej] . \n\t', '\n\t\t The weights of the WFA in our case are always in the log semiring , which means that the weight of a path 7r = e1 ... ej E IIA is defined as : wA[7r] = Xj w[ek] I + ^(ej) ( 5 ) k=1 By convention , we use negative log probabilities as weights , so lower weights are better . \n\t', ""\n\t\t All WFA that we will discuss in this paper are deterministic , i.e. there are no a transitions , and for any two transitions e , e ' E E , if p[e] = p[e'] , then l[e] =~ l[e'] . \n\t"", '\n\t\t Thus , for any string w = w1 ... wj , there is at most one successful path 7r E IIA , such that 7r = e1 ... ej and for 1 < k < j , l [ ek ] = wk , i.e. l[7r] = w . \n\t', '\n\t\t The set of strings w such that there exists a 7r E IIA with l[7r] = w define a regular language LA C E. We can now define some operations that will be used in this paper . \n\t', '\n\t\t 8LLR = ~N i=1 8a3 \x95 ^A . \n\t', '\n\t\t For a set of transitions E and ^ E ]IR , define ^E = { ( l[e] , p[e] , n[e] , ^w[e] ) : e E E } . \n\t', '\n\t\t Then , for any WFA A = ( E , Q , qs , F , E , ^ ) , define ^A for ^ E ]IR as follows : ^A = ( E , Q , qs , F , ^E , ^^ ) . \n\t', ""\n\t\t \x95 A o A ' . \n\t"", ""\n\t\t The intersection of two deterministic WFAs A o A ' in the log semiring is a deterministic WFA such that LAoA ' = LA n LA ' . \n\t"", ""\n\t\t For any 7r E IIAoA ' , wAoA'[7r] = wA [ 7r1 ] + wA'[7r2] , where l[7r] = l[7r1] = l[7r2] . \n\t"", '\n\t\t \x95 BestPath(A) . \n\t', '\n\t\t This operation takes a WFA A , and returns the best scoring path 7r\x88 = argmin7rErlA wA[7r] . \n\t', '\n\t\t \x95 MinErr(A , y ) . \n\t', '\n\t\t Given a WFA A , a string y , and an error-function E(y , w ) , this operation returns 7r\x88 = argmin7rErlA E(y , l[7r] ) . \n\t', '\n\t\t This operation will generally be used with y as the reference transcription for a particular training example , and E(y , w ) as some measure of the number of errors in w when compared to y . \n\t', '\n\t\t In this case , the MinErr operation returns the path 7r E IIA such l[7r] has the smallest number of errors when compared to y. \x95 Norm(A) . \n\t', ""\n\t\t Given a WFA A , this operation yields a WFA A ' such that LA = LA ' and for every 7r E IIA there is a 7r ' E IIA ' such that l[7r] = l[7r'] and exp(\x97wA [7r]))(6) Note that E exp(\x97wNorm(A)[7r]) = 1 ( 7 ) 7rENorm(A) In other words the weights define a probability distribution over the paths . \n\t"", '\n\t\t \x95 ExpCount(A , w ) . \n\t', '\n\t\t Given a WFA A and an n-gram w , we define the expected count of w in A as ExpCount ( A , w ) = E wNorm(A) [ 7r ] C ( w , l[7r] ) 7rErlA where C ( w , l[7r] ) is defined to be the number of times the n-gram w appears in a string l[7r] . \n\t', '\n\t\t Given an acoustic input x , let L. , be a deterministic word-lattice produced by the baseline recognizer . \n\t', '\n\t\t The lattice L. , is an acyclic WFA , representing a weighted set of possible transcriptions of x under the baseline recognizer . \n\t', '\n\t\t The weights represent the combination of acoustic and language model scores in the original recognizer . \n\t', '\n\t\t The new , discriminative language model constructed during training consists of a deterministic WFA which we will denote D , together with a single parameter a0 . \n\t', '\n\t\t The parameter a0 is the weight for the log probability feature 4)0 given by the baseline recognizer . \n\t', '\n\t\t The WFA D is constructed so that LD = E^ and for all 7r E IID d wD [ 7r ] = E 4)j ( x , l [ 7r ] ) aj j=1 Recall that 4)j ( x , w ) for j > 0 is the count of the j\x92th n- gram in w , and aj is the parameter associated with that Figure 2 : Representation of a trigram model with failure transitions . \n\t', '\n\t\t n-gram . \n\t', '\n\t\t Then , by definition , a0L o D accepts the same set of strings as L , but d waoLoD[7r] = E 4)j ( x , l [ 7r ] ) aj j=0 and argmin 4)(x , l[7r] ) · a¯ = BestPath(a0L o D ) . \n\t', '\n\t\t 7rEL Thus decoding under our new model involves first producing a lattice L from the baseline recognizer ; second , scaling L with a0 and intersecting it with the discriminative language model D ; third , finding the best scoring path in the new WFA . \n\t', '\n\t\t We now turn to training a model , or more explicitly , deriving a discriminative language model ( D , a0 ) from a set of training examples . \n\t', '\n\t\t Given a training set ( xi , ri ) for i = 1 ... \n\t', '\n\t\t N , where xi is an acoustic sequence , and ri is a reference transcription , we can construct lattices Li for i = 1 ... \n\t', '\n\t\t N using the baseline recognizer . \n\t', '\n\t\t We can also derive target transcriptions yi = MinErr(Li , ri ) . \n\t', '\n\t\t The training algorithm is then a mapping from ( Li , yi ) for i = 1 ... \n\t', '\n\t\t N to a pair ( D , a0 ) . \n\t', '\n\t\t Note that the construction of the language model requires two choices . \n\t', '\n\t\t The first concerns the choice of the set of n-gram features 4)i for i = 1 ... d implemented by D . \n\t', '\n\t\t The second concerns the choice ofparameters ai for i = 0 ... d which assign weights to the n-gram features as well as the baseline feature 4)0 . \n\t', '\n\t\t Before describing methods for training a discriminative language model using perceptron and CRF algorithms , we give a little more detail about the structure of D , focusing on how n-gram language models can be implemented with finite-state techniques . \n\t', '\n\t\t 3.3 Representation of n-gram language models An n-gram model can be efficiently represented in a deterministic WFA , through the use of failure transitions \n\t\t']",Positive
"['\n\t\t Every string accepted by such an automaton has a single path through the automaton , and the weight of the string is the sum of the weights of the transitions in that path . \n\t', '\n\t\t In such a representation , every state in the automaton represents an n-gram history h , e.g. wi_2wi_1 , and there are transitions leaving the state for every word wi such that the feature hwi has a weight . \n\t', '\n\t\t There is also a failure transition leaving the state , labeled with some reserved symbol ^ , which can only be traversed if the next symbol in the input does not match any transition leaving the state . \n\t', ""\n\t\t This failure transition points to the backoff state h ' , i.e. the n-gram history h minus its initial word . \n\t"", '\n\t\t Figure 2 shows how a trigram model can be represented in such an automaton . \n\t', '\n\t\t See \n\t\t']",Positive
"[""\n\t\t w ; -2w ;-1w ; wi-1 w ; 0 w ; w;-1 w ; 0 w ; wA'[7r'] = wA [ 7r ] + log I E ¯7rErlA Note that in such a deterministic representation , the entire weight of all features associated with the word wi following history h must be assigned to the transition labeled with wi leaving the state h in the automaton . \n\t"", '\n\t\t For example , if h = wi_2wi_1 , then the trigram wi_2wi_1wi is a feature , as is the bigram wi_1wi and the unigram wi . \n\t', '\n\t\t In this case , the weight on the transition wi leaving state h must be the sum of the trigram , bigram and unigram feature weights . \n\t', '\n\t\t If only the trigram feature weight were assigned to the transition , neither the unigram nor the bigram feature contribution would be included in the path weight . \n\t', '\n\t\t In order to ensure that the correct weights are assigned to each string , every transition encoding an order k n-gram must carry the sum of the weights for all n-gram features of orders ^ k . \n\t', ""\n\t\t To ensure that every string in E* receives the correct weight , for any n-gram hw represented explicitly in the automaton , h'w must also be represented explicitly in the automaton , even if its weight is 0 . \n\t"", '\n\t\t 3.4 The perceptron algorithm The perceptron algorithm is incremental , meaning that the language model D is built one training example at a time , during several passes over the training set . \n\t', '\n\t\t Initially , we build D to accept all strings in E* with weight 0 . \n\t', '\n\t\t For the perceptron experiments , we chose the parameter a0 to be a fixed constant , chosen by optimization on the held-out set . \n\t', '\n\t\t The loop in the algorithm in figure 1 is implemented as : Fort= 1 ... \n\t', '\n\t\t T,i=1 ... \n\t', ""\n\t\t N : \x95 Calculate zi = argmaxyEGEN(x) 4'(x , y ) · a¯ = BestPath(a0Li o D ) \x95 If zi =~ MinErr(Li , ri ) , then update the feature weights as in figure 1 ( modulo the sign , because of the use of costs ) , and modify D so as to assign the correct weight to all strings . \n\t"", '\n\t\t In addition , averaged parameters need to be stored ( see section 2.2 ) . \n\t', '\n\t\t These parameters will replace the unaveraged parameters in D once training is completed . \n\t', '\n\t\t Note that the only n-gram features to be included in D at the end of the training process are those that occur in either a best scoring path zi or a minimum error path yi at some point during training . \n\t', '\n\t\t Thus the perceptron algorithm is in effect doing feature selection as a by-product of training . \n\t', '\n\t\t Given N training examples , and T passes over the training set , O(NT) n-grams will have non-zero weight after training . \n\t', '\n\t\t Experiments in \n\t\t']",Positive
"['\n\t\t Thus O(NT) can be very small compared to the full number of n-grams seen in all training lattices . \n\t', '\n\t\t In our experiments , the perceptron method chose around 1.4 million n-grams with non-zero weight . \n\t', '\n\t\t This compares to 43.65 million possible n-grams seen in the training data . \n\t', '\n\t\t This is a key contrast with conditional random fields , which optimize the parameters of a fixed feature set . \n\t', '\n\t\t Feature selection can be critical in our domain , as training and applying a discriminative language model over all n-grams seen in the training data ( in either correct or incorrect transcriptions ) may be computationally very demanding . \n\t', '\n\t\t One training scenario that we will consider will be using the output of the perceptron algorithm ( the averaged parameters ) to provide the feature set and the initial feature weights for use in the CRF algorithm . \n\t', '\n\t\t This leads to a model which is reasonably sparse , but has the benefit of CRF training , which as we will see gives gains in performance . \n\t', ""\n\t\t 3.5 Conditional Random Fields The CRF methods that we use assume a fixed definition of the n-gram features 4'i for i = 1 ... d in the model . \n\t"", '\n\t\t In the experimental section we will describe a number of ways of defining the feature set . \n\t', '\n\t\t The optimization methods we use begin at some initial setting for ¯a , and then search for the parameters ¯a* which maximize LLR(¯a) as defined in Eq . \n\t', '\n\t\t 3. The optimization method requires calculation of LLR(¯a) and the gradient of LLR(¯a) for a series of values for ¯a . \n\t', ""\n\t\t The first step in calculating these quantities is to take the parameter values ¯a , and to construct an acceptor D which accepts all strings in E* , such that d wD [ 7r ] = E 4'j ( x , l[7r])aj j=1 For each training lattice Li , we then construct a new lattice L'i = Norm(a0Li o D ) . \n\t"", ""\n\t\t The lattice L'i represents ( in the log domain ) the distribution p¯^(ylxi) over strings y E GEN(xi) . \n\t"", ""\n\t\t The value of logp¯^(yilxi) for any i can be computed by simply taking the path weight of 7r such that l[7r] = yi in the new lattice L'i . \n\t"", '\n\t\t Hence computation of LLR(¯a) in Eq . \n\t', '\n\t\t 3 is straightforward . \n\t', ""\n\t\t Calculating the n-gram feature gradients for the CRF optimization is also relatively simple , once L'i has been constructed . \n\t"", '\n\t\t From the derivative in Eq . \n\t', '\n\t\t 4 , for each i = 1 ... \n\t', ""\n\t\t N , j = 1 ... d the quantity 4'j ( xi , yi ) ^ X p¯^(yl xi ) 4'j(xi,y) ( 8 ) yEGEN(x;) must be computed . \n\t"", '\n\t\t The first term is simply the number of times the j\x92th n-gram feature is seen in yi . \n\t', ""\n\t\t The second term is the expected number of times that the j\x92th n-gram is seen in the acceptor L'i . \n\t"", ""\n\t\t If the j\x92 th n-gram is w1 ... w , , , , then this can be computed as ExpCount(L'i , w1 ... w , , , ) . \n\t"", '\n\t\t The GRM library , which was presented in \n\t\t']",Positive
"['\n\t\t The one non-ngram feature weight that is being estimated is the weight a0 given to the baseline ASR negative log probability . \n\t', '\n\t\t Calculation of the gradient of LLR with respect to this parameter again requires calculation of the term in Eq . \n\t', '\n\t\t 8 for j = 0 and i = 1 ... \n\t', ""\n\t\t N. Com- putation of PyEGEN(x;) p¯^ ( yl xi)4'0 ( xi , y ) turns out to be not as straightforward as calculating n-gram expectations . \n\t"", ""\n\t\t To do so , we rely upon the fact that 4'0 ( xi , y ) , the negative log probability of the path , decomposes to the sum of negative log probabilities of each transition in the path . \n\t"", '\n\t\t We index each transition in the lattice Li , and store its negative log probability under the baseline model . \n\t', '\n\t\t We can then calculate the required gradient from L~i , by calculating the expected value in L~i of each indexed transition in Li . \n\t', '\n\t\t We found that an approximation to the gradient of a0 , however , performed nearly identically to this exact gradient , while requiring substantially less computation . \n\t', '\n\t\t Let wn1 be a string of n words , labeling a path in word- lattice L~i . \n\t', '\n\t\t For brevity , let Pi(wn1 ) = p~^ ( wn1 I xi ) be the conditional probability under the current model , and let Qi ( wn1 ) be the probability of wn1 in the normalized baseline ASR lattice Norm(Li) . \n\t', '\n\t\t Let Li be the set of strings in the language defined by Li . \n\t', '\n\t\t Then we wish to compute Ei for i = 1 ... \n\t', '\n\t\t N , where Ei = X Pi ( wn1 )logQi(wn1 ) wn1 ELi X= X Pi(wn1 ) logQi(wklwk^1 1 ) ( 9 ) wn1 ELi k=1 ... n The approximation is to make the following Markov assumption : Ei , .. X X Pi(wn1 ) log Qi(wklwk-2) wn1 ELi k=1 ... n = X ExpCount(G~i , xyz ) log Qi(zlxy)(10) xyzESi where Si is the set of all trigrams seen in Li . \n\t', '\n\t\t The term log Qi(zIxy) can be calculated once before training for every lattice in the training set ; the ExpCount term is calculated as before using the GRM library . \n\t', '\n\t\t We have found this approximation to be effective in practice , and it was used for the trials reported below . \n\t', '\n\t\t When the gradients and conditional likelihoods are collected from all of the utterances in the training set , the contributions from the regularizer are combined to give an overall gradient and objective function value . \n\t', '\n\t\t These values are provided to the parameter estimation routine , which then returns the parameters for use in the next iteration . \n\t', '\n\t\t The accumulation of gradients for the feature set is the most time consuming part of the approach , but this is parallelizable , so that the computation can be divided among many processors . \n\t', '\n\t\t 4 Empirical Results We present empirical results on the Rich Transcription 2002 evaluation test set ( rt02 ) , which we used as our development set , as well as on the Rich Transcription 2003 Spring evaluation CTS test set ( rt03 ) . \n\t', '\n\t\t The rt02 set consists of 6081 sentences ( 63804 words ) and has three subsets : Switchboard 1 , Switchboard 2 , Switchboard Cellular . \n\t', '\n\t\t The rt03 set consists of 9050 sentences ( 76083 words ) and has two subsets : Switchboard and Fisher . \n\t', '\n\t\t We used the same training set as that used in \n\t\t']",Positive
"['\n\t\t The training set consists of 276726 transcribed utterances ( 3047805 words ) , with an additional 20854 utterances ( 249774 words ) as held out data . \n\t', '\n\t\t For 40 39.5 Baseline recognizer Perceptron , Feat=PL , Lattice Perceptron , Feat=PN , N=1000 CRF , a = oo , Feat=PL , Lattice CRF , a = 0.5 , Feat=PL , Lattice CRF , a = 0.5 , Feat=PN , N=1000 37.5 370 500 1000 Iterations over training Figure 3 : Word error rate on the rt02 eval set versus training iterations for CRF trials , contrasted with baseline recognizer performance and perceptron performance . \n\t', '\n\t\t Points are at every 20 iterations . \n\t', '\n\t\t Each point ( x,y ) is the WER at the iteration with the best objective function value in the interval ( x-20,x ] . \n\t', '\n\t\t each utterance , a weighted word-lattice was produced , representing alternative transcriptions , from the ASR system . \n\t', '\n\t\t From each word-lattice , the oracle best path was extracted , which gives the best word-error rate from among all of the hypotheses in the lattice . \n\t', '\n\t\t The oracle word-error rate for the training set lattices was 12.2 % . \n\t', '\n\t\t We also performed trials with 1000-best lists for the same training set , rather than lattices . \n\t', '\n\t\t The oracle score for the 1000-best lists was 16.7 % . \n\t', '\n\t\t To produce the word-lattices , each training utterance was processed by the baseline ASR system . \n\t', '\n\t\t However , these same utterances are what the acoustic and language models are built from , which leads to better performance on the training utterances than can be expected when the ASR system processes unseen utterances . \n\t', '\n\t\t To somewhat control for this , the training set was partitioned into 28 sets , and baseline Katz backoff trigram models were built for each set by including only transcripts from the other 27 sets . \n\t', '\n\t\t Since language models are generally far more prone to overtrain than standard acoustic models , this goes a long way toward making the training conditions similar to testing conditions . \n\t', '\n\t\t There are three baselines against which we are comparing . \n\t', '\n\t\t The first is the ASR baseline , with no reweighting from a discriminatively trained n-gram model . \n\t', '\n\t\t The other two baselines are with perceptron-trained n-gram model re-weighting , and were reported in \n\t\t']",Positive
"['\n\t\t The first of these is for a pruned-lattice trained trigram model , which showed a reduction in word error rate ( WER ) of 1.3 % , from 39.2 % to 37.9 % on rt02 . \n\t', '\n\t\t The second is for a 1000-best list trained trigram model , which performed only marginally worse than the lattice- trained perceptron , at 38.0 % on rt02 . \n\t', '\n\t\t 4.1 Perceptron feature set We use the perceptron-trained models as the starting point for our CRF algorithm : the feature set given to the CRF algorithm is the feature set selected by the perceptron algorithm ; the feature weights are initialized to those of the averaged perceptron . \n\t', '\n\t\t Figure 3 shows the performance of our three baselines versus three trials of 39 38.5 38 Iterations over training Figure 4 : Word error rate on the rt02 eval set versus training iterations for CRF trials , contrasted with baseline recognizer performance and perceptron performance . \n\t', '\n\t\t Points are at every 20 iterations . \n\t', '\n\t\t Each point ( x,y ) is the WER at the iteration with the best objective function value in the interval ( x-20,x ] . \n\t', '\n\t\t the CRF algorithm . \n\t', '\n\t\t In the first two trials , the training set consists of the pruned lattices , and the feature set is from the perceptron algorithm trained on pruned lattices . \n\t', '\n\t\t There were 1.4 million features in this feature set . \n\t', '\n\t\t The first trial set the regularizer constant u = oo , so that the algorithm was optimizing raw conditional likelihood . \n\t', '\n\t\t The second trial is with the regularizer constant u = 0.5 , which we found empirically to be a good parameterization on the held-out set . \n\t', '\n\t\t As can be seen from these results , regularization is critical . \n\t', '\n\t\t The third trial in this set uses the feature set from the perceptron algorithm trained on 1000-best lists , and uses CRF optimization on these on these same 1000-best lists . \n\t', '\n\t\t There were 0.9 million features in this feature set . \n\t', '\n\t\t For this trial , we also used u = 0.5 . \n\t', '\n\t\t As with the percep- tron baselines , the n-best trial performs nearly identically with the pruned lattices , here also resulting in 37.4 % WER . \n\t', '\n\t\t This may be useful for techniques that would be more expensive to extend to lattices versus n-best lists ( e.g. models with unbounded dependencies ) . \n\t', '\n\t\t These trials demonstrate that the CRF algorithm can do a better job of estimating feature weights than the perceptron algorithm for the same feature set . \n\t', '\n\t\t As mentioned in the earlier section , feature selection is a by-product of the perceptron algorithm , but the CRF algorithm is given a set of features . \n\t', '\n\t\t The next two trials looked at selecting feature sets other than those provided by the perceptron algorithm . \n\t', '\n\t\t 4.2 Other feature sets In order for the feature weights to be non-zero in this approach , they must be observed in the training set . \n\t', '\n\t\t The number of unigram , bigram and trigram features with non-zero observations in the training set lattices is 43.65 million , or roughly 30 times the size of the perceptron feature set . \n\t', '\n\t\t Many of these features occur only rarely with very low conditional probabilities , and hence cannot meaningfully impact system performance . \n\t', '\n\t\t We pruned this feature set to include all unigrams and bigrams , but only those trigrams with an expected count of greater than 0.01 in the training set . \n\t', '\n\t\t That is , to be included , a Trial Iter rt02 rt03 ASR Baseline - 39.2 38.2 Perceptron , Lattice - 37.9 36.9 Perceptron , N-best - 38.0 37.2 CRF , Lattice , Percep Feats ( 1.4M ) 769 37.4 36.5 CRF , N-best , Percep Feats ( 0.9M ) 946 37.4 36.6 CRF , Lattice , 0 = 0.01 ( 12M ) 2714 37.6 36.5 CRF , Lattice , 0 = 0.9 ( 1.5M ) 1679 37.5 36.6 Table 1 : Word-error rate results at convergence iteration for various trials , on both Switchboard 2002 test set ( rt02 ) , which was used as the dev set , and Switchboard 2003 test set ( rt03 ) . \n\t', '\n\t\t trigram must occur in a set of paths , the sum of the conditional probabilities of which must be greater than our threshold 0 = 0.01 . \n\t', '\n\t\t This threshold resulted in a feature set of roughly 12 million features , nearly 10 times the size of the perceptron feature set . \n\t', '\n\t\t For better comparability with that feature set , we set our thresholds higher , so that trigrams were pruned if their expected count fell be- low 0 = 0.9 , and bigrams were pruned if their expected count fell below 0 = 0.1 . \n\t', '\n\t\t We were concerned that this may leave out some of the features on the oracle paths , so we added back in all bigram and trigram features that occurred on oracle paths , giving a feature set of 1.5 million features , roughly the same size as the perceptron feature set . \n\t', '\n\t\t Figure 4 shows the results for three CRF trials versus our ASR baseline and the perceptron algorithm baseline trained on lattices . \n\t', '\n\t\t First , the result using the perceptron feature set provides us with a WER of 37.4 % , as previously shown . \n\t', '\n\t\t The WER at convergence for the big feature set ( 12 million features ) is 37.6 % ; the WER at convergence for the smaller feature set ( 1.5 million features ) is 37.5 % . \n\t', '\n\t\t While both of these other feature sets converge to performance close to that using the perceptron features , the number of iterations over the training data that are required to reach that level of performance are many more than for the perceptron-initialized feature set . \n\t', '\n\t\t Table 1 shows the word-error rate at the convergence iteration for the various trials , on both rt02 and rt03 . \n\t', '\n\t\t All of the CRF trials are significantly better than the perceptron performance , using the Matched Pair Sentence Segment test for WER included with SCTK ( NIST , 2000 ) . \n\t', '\n\t\t On rt02 , the N-best and perceptron initialized CRF trials were were significantly better than the lattice perceptron at p < 0.001 ; the other two CRF trials were significantly better than the lattice perceptron at p < 0.01 . \n\t', '\n\t\t On rt03 , the N-best CRF trial was significantly better than the lat- tice perceptron at p < 0.002 ; the other three CRF tri- als were significantly better than the lattice perceptron at p < 0.001 . \n\t', '\n\t\t Finally , we measured the time of a single iteration over the training data on a single machine for the perceptron algorithm , the CRF algorithm using the approximation to the gradient of ao , and the CRF algorithm using an exact gradient of ao . \n\t', '\n\t\t Table 2 shows these times in hours . \n\t', '\n\t\t Because of the frequent update of the weights in the model , the perceptron algorithm is more expensive than the CRF algorithm for a single iteration . \n\t', '\n\t\t Further , the CRF algorithm is parallelizable , so that most of the work of an 40 39.5 Baseline recognizer Perceptron , Feat=PL , Lattice CRF , a = 0.5 , Feat=PL , Lattice CRF , ^ = 0.5 , Feat=E , 0=0.01 CRF , ^ = 0.5 , Feat=E , 0=0.9 39 38.5 38 37.5 370 500 1000 1500 2000 2500 Features Percep CRF approx exact Lattice , Percep Feats ( 1.4M ) 7.10 1.69 3.61 N-best , Percep Feats ( 0.9M ) 3.40 0.96 1.40 Lattice , ^ = 0.01(12M) - 2.24 4.75 Table 2 : Time ( in hours ) for one iteration on a single Intel Xeon 2.4Ghz processor with 4GB RAM . \n\t', '\n\t\t iteration can be shared among multiple processors . \n\t', '\n\t\t Our most common training setup for the CRF algorithm was parallelized between 20 processors , using the approximation to the gradient . \n\t', '\n\t\t In that setup , using the 1 .4M feature set , one iteration of the perceptron algorithm took the same amount of real time as approximately 80 iterations of CRF . \n\t', '\n\t\t 5 Conclusion We have contrasted two approaches to discriminative language model estimation on a difficult large vocabulary task , showing that they can indeed scale effectively to handle this size of a problem . \n\t', '\n\t\t Both algorithms have their benefits . \n\t', '\n\t\t The perceptron algorithm selects a relatively small subset of the total feature set , and requires just a couple of passes over the training data . \n\t', '\n\t\t The CRF algorithm does a better job of parameter estimation for the same feature set , and is parallelizable , so that each pass over the training set can require just a fraction of the real time of the perceptron algorithm . \n\t', '\n\t\t The best scenario from among those that we investigated was a combination of both approaches , with the output of the perceptron algorithm taken as the starting point for CRF estimation . \n\t', '\n\t\t As a final point , note that the methods we describe do not replace an existing language model , but rather complement it . \n\t', '\n\t\t The existing language model has the benefit that it can be trained on a large amount of text that does not have speech transcriptions . \n\t', '\n\t\t It has the disadvantage of not being a discriminative model . \n\t', '\n\t\t The new language model is trained on the speech transcriptions , meaning that it has less training data , but that it has the advantage of discriminative training \x96 and in particular , the advantage of being able to learn negative evidence in the form of negative weights on n-grams which are rarely or never seen in natural language text ( e.g. , \x93the of\x94 ) , but are produced too frequently by the recognizer . \n\t', '\n\t\t The methods we describe combines the two language models , allowing them to complement each other . \n\t', '\n\t\t References Cyril Allauzen , Mehryar Mohri , and Brian Roark . \n\t', '\n\t\t 2003. Generalized algorithms for constructing language models . \n\t', '\n\t\t In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics , pages 40\x9647 . \n\t', '\n\t\t Satish Balay , William D. Gropp , Lois Curfman McInnes , and Barry F. Smith . \n\t', '\n\t\t 2002. Petsc users manual . \n\t', '\n\t\t Technical Report ANL-95/1 1 - Revision 2.1.2 , Argonne National Laboratory . \n\t', '\n\t\t Satanjeev Banerjee , Jack Mostow , Joseph Beck , and Wilson Tam . \n\t', '\n\t\t 2003. Improving language models by learning from speech recognition errors in a reading tutor that listens . \n\t', '\n\t\t In Proceedings of the Second International Conference on Applied Artificial Intelligence , Fort Panhala , Kolhapur , India . \n\t', '\n\t\t Steven J. Benson and Jorge J. Mor´e . \n\t', '\n\t\t 2002. A limited memory variable metric method for bound constrained minimization . \n\t', '\n\t\t Preprint ANL/ACSP909-0901 , Argonne National Laboratory . \n\t', '\n\t\t Steven J. Benson , Lois Curfman McInnes , Jorge J. Mor´e , and Jason Sarich . \n\t', '\n\t\t 2002. Tao users manual . \n\t', '\n\t\t Technical Report ANL/MCS-TM242-Revision 1.4 , Argonne National Laboratory . \n\t', '\n\t\t Zheng Chen , Kai-Fu Lee , and Ming Jing Li . \n\t', '\n\t\t 2000. Discriminative training on language model . \n\t', '\n\t\t In Proceedings of the Sixth International Conference on Spoken Language Processing ( ICSLP ) , Beijing , China . \n\t', '\n\t\t Michael Collins . \n\t', '\n\t\t 2002. Discriminative training methods for hidden markov models : Theory and experiments with perceptron algorithms . \n\t', '\n\t\t In Proceedings of the Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1\x968 . \n\t', '\n\t\t Michael Collins . \n\t', '\n\t\t 2004. Parameter estimation for statistical parsing models : Theory and practice of distribution-free methods . \n\t', '\n\t\t In Harry Bunt , John Carroll , and Giorgio Satta , editors , New Developments in Parsing Technology . \n\t', '\n\t\t Kluwer . \n\t', '\n\t\t Yoav Freund and Robert Schapire . \n\t', '\n\t\t 1999. Large margin classification using the perceptron algorithm . \n\t', '\n\t\t Machine Learning , 3(37):277\x96296 . \n\t', '\n\t\t Frederick Jelinek . \n\t', '\n\t\t 1995. Acoustic sensitive language modeling . \n\t', '\n\t\t Technical report , Center for Language and Speech Processing , Johns Hopkins University , Baltimore , MD . \n\t', '\n\t\t Mark Johnson , Stuart Geman , Steven Canon , Zhiyi Chi , and Stefan Riezler . \n\t', '\n\t\t 1999. Estimators for stochastic \x93unification-based\x94 grammars . \n\t', '\n\t\t In Proceedings ofthe 37th Annual Meeting of the Association for Computational Linguistics , pages 535\x96541 . \n\t', '\n\t\t Sanjeev Khudanpur and Jun Wu . \n\t', '\n\t\t 2000. Maximum entropy techniques for exploiting syntactic , semantic and collocational dependencies in language modeling . \n\t', '\n\t\t Computer Speech and Language , 14(4):355\x96 372 . \n\t', '\n\t\t Hong-Kwang Jeff Kuo , Eric Fosler-Lussier , Hui Jiang , and Chin- Hui Lee . \n\t', '\n\t\t 2002. Discriminative training of language models for speech recognition . \n\t', '\n\t\t In Proceedings of the International Conference on Acoustics , Speech , and Signal Processing ( ICASSP ) , Orlando , Florida . \n\t', '\n\t\t John Lafferty , Andrew McCallum , and Fernando Pereira . \n\t', '\n\t\t 2001. Conditional random fields : Probabilistic models for segmenting and labeling sequence data . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ICML , pages 282\x96289 , Williams College , Williamstown , MA , USA . \n\t', '\n\t\t Robert Malouf . \n\t', '\n\t\t 2002. A comparison of algorithms for maximum entropy parameter estimation . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t CoNLL , pages 49\x9655 . \n\t', '\n\t\t Andrew McCallum and Wei Li . \n\t', '\n\t\t 2003. Early results for named entity recognition with conditional random fields , feature induction and web-enhanced lexicons . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t CoNLL . \n\t', '\n\t\t Mehryar Mohri , Fernando C. N. Pereira , and Michael Riley . \n\t', '\n\t\t 2002. Weighted finite-state transducers in speech recognition . \n\t', '\n\t\t Computer Speech and Language , 16(1):69\x9688 . \n\t', '\n\t\t NIST . \n\t', '\n\t\t 2000. Speech recognition scoring toolkit ( sctk ) version 1.2c . \n\t', '\n\t\t Available athttp://www.nist.gov/speech/tools . \n\t', '\n\t\t David Pinto , Andrew McCallum , Xing Wei , and W. Bruce Croft . \n\t', '\n\t\t 2003. Table extraction using conditional random fields . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ACM SIGIR . \n\t', '\n\t\t Adwait Ratnaparkhi , Salim Roukos , and R. Todd Ward . \n\t', '\n\t\t 1994. A maximum entropy model for parsing . \n\t', '\n\t\t In Proceedings of the International Conference on Spoken Language Processing ( ICSLP ) , pages 803\x96806 . \n\t', '\n\t\t Brian Roark , Murat Saraclar , and Michael Collins . \n\t', '\n\t\t 2004. Corrective language modeling for large vocabulary ASR with the perceptron algorithm . \n\t', '\n\t\t In Proceedings of the International Conference on Acoustics , Speech , and Signal Processing ( ICASSP ) , pages 749\x96752 . \n\t', '\n\t\t Fei Sha and Fernando Pereira . \n\t', '\n\t\t 2003. Shallow parsing with conditional random fields . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t HLT-NAACL , Edmonton , Canada . \n\t', '\n\t\t A. Stolcke and M. Weintraub . \n\t', '\n\t\t 1998. Discriminitive language modeling . \n\t', '\n\t\t In Proceedings ofthe 9th Hub-5 Conversational Speech Recognition Workshop . \n\t', '\n\t\t A. Stolcke , H. Bratt , J. Butzberger , H. Franco , V. R. Rao Gadde , M. Plauche , C. Richey , E. Shriberg , K. Sonmez , F. Weng , and J. Zheng . \n\t', '\n\t\t 2000. The SRI March 2000 Hub-5 conversational speech transcription system . \n\t', '\n\t\t In Proceedings of the NIST Speech Transcription Workshop . \n\t', '\n\t\t Hanna Wallach . \n\t', '\n\t\t 2002. Efficient training of conditional random fields . \n\t', '\n\t\t Master\x92s thesis , University of Edinburgh . \n\t', '\n\t\t P.C. Woodland and D. Povey . \n\t', '\n\t\t 2000. Large scale discriminative training for speech recognition . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ISCAITRWASR2000 , pages 7\x9616 . \n\t', '\n\t\t Statistical Modeling for Unit Selection in Speech Synthesis Cyril Allauzen and Mehryar Mohri and Michael Riley* AT&T Labs \x96 Research 180 Park Avenue , Florham Park , NJ 07932 , USA { allauzen , mohri , riley}@research.att.com Abstract Traditional concatenative speech synthesis systems use a number of heuristics to define the target and concatenation costs , essential for the design of the unit selection component . \n\t', '\n\t\t In contrast to these approaches , we introduce a general statistical modeling framework for unit selection inspired by automatic speech recognition . \n\t', '\n\t\t Given appropriate data , techniques based on that framework can result in a more accurate unit selection , thereby improving the general quality of a speech synthesizer . \n\t', '\n\t\t They can also lead to a more modular and a substantially more efficient system . \n\t', '\n\t\t We present a new unit selection system based on statistical modeling . \n\t', '\n\t\t To overcome the original absence of data , we use an existing high-quality unit selection system to generate a corpus of unit sequences . \n\t', '\n\t\t We show that the concatenation cost can be accurately estimated from this corpus using a statistical n-gram language model over units . \n\t', '\n\t\t We used weighted automata and transducers for the representation of the components of the system and designed a new and more efficient composition algorithm making use of string potentials for their combination . \n\t', '\n\t\t The resulting statistical unit selection is shown to be about 2.6 times faster than the last release of the AT&T Natural Voices Product while preserving the same quality , and offers much flexibility for the use and integration of new and more complex components . \n\t', '\n\t\t 1 Motivation A concatenative speech synthesis system \n\t\t']",Positive
"['\n\t\t The first component , the text- analysis frontend , takes text as input and outputs a sequence of feature vectors that characterize the acoustic signal to synthesize . \n\t', '\n\t\t The first element of each of these vectors is the predicted phone or half- phone ; other elements are features such as the phonetic context , acoustic features ( e.g. , pitch , duration ) , or prosodic features . \n\t', '\n\t\t * This author\x92s new address is : Google , Inc , 1440 Broadway , New York , NY 10018 , riley@google . \n\t', '\n\t\t com . \n\t', '\n\t\t The second component , unit selection , determines in a set of recorded acoustic units corresponding to phones \n\t\t']",Positive
['\n\t\t The final component produces an acoustic signal from the unit sequence chosen by unit selection using simple concatenation or other methods such as PSOLA \n\t\t'],Positive
"['\n\t\t Unit selection is performed by defining two cost functions : the target cost that estimates how the features of a recorded unit match the specified feature vector and the concatenation cost that estimates how well two units will be perceived to match when appended . \n\t', '\n\t\t Unit selection then consists of finding , given a specified sequence of feature vectors , the unit sequence that minimizes the sum of these two costs . \n\t', '\n\t\t The target and concatenation cost functions have traditionally been formed from a variety of heuristic or ad hoc quality measures based on features of the audio and text . \n\t', '\n\t\t In this paper , we follow a different approach : our goal is a system based purely on statistical modeling . \n\t', '\n\t\t The starting point is to assume that we have a training corpus of utterances labeled with the appropriate unit sequences . \n\t', '\n\t\t Specifically , for each training utterance , we assume available a sequence of feature vectors f = f i ... f , , and the corresponding units u = ui ... u , , that should be used to synthesize this utterance . \n\t', '\n\t\t We wish to estimate from this corpus two probability distributions , P(f Iu ) and P(u) . \n\t', '\n\t\t Given these estimates , we can perform unit selection on a novel utterance using : P(uIf) ( 1 ) ( \x97 log P(f I u ) \x97 log P(u)) ( 2 ) Equation 1 states that the most likely unit sequence is selected given the probabilistic model used . \n\t', '\n\t\t Equation 2 follows from the definition of conditional probability and that P(f ) is fixed for a given utterance . \n\t', '\n\t\t The two terms appearing in Equation 2 can be viewed as the statistical counterparts u = argmax U = argmin U of the target and concatenation costs in traditional unit selection . \n\t', '\n\t\t The statistical framework just outlined is similar to the one used in speech recognition \n\t\t']",Positive
"['\n\t\t We also use several techniques that have been very successfully applied to speech recognition . \n\t', '\n\t\t For instance , in this paper , we show how \x97log P(u) ( the concatenation cost ) can be accu- rately estimated using a statistical n-gram language model over units . \n\t', '\n\t\t Two questions naturally arise . \n\t', '\n\t\t ( a ) How can we collect a training corpus for building a statistical model ? \n\t', '\n\t\t Ideally , the training corpus could be human-labeled , as in speech recognition and other natural language processing tasks . \n\t', '\n\t\t But this seemed impractical given the size of the unit inventory , the number of utterances needed for good statistical estimates , and our limited resources . \n\t', '\n\t\t Instead , we chose to use a training corpus generated by an existing high-quality unit selection system , that of the AT&T Natural Voices Product . \n\t', '\n\t\t Of course , building a statistical model on that output can , at best , only match the quality of the original . \n\t', '\n\t\t But , it can serve as an exploratory trial to measure the quality of our statistical modeling . \n\t', '\n\t\t As we will see , it can also result in a synthesis system that is significantly faster and modular than the original since there are well-established algorithms for representing and optimizing statistical models of the type we will employ . \n\t', '\n\t\t To further simplify the problem , we will use the existing traditional target costs , providing statistical estimates only of the concate- nation costs ( \x97 log P(u)) . \n\t', '\n\t\t ( b ) What are the benefits of a statistical modeling approach ? \n\t', '\n\t\t ( 1 ) High-quality cost functions . \n\t', '\n\t\t One issue with traditional unit selection systems is that their cost functions are the result of the following compromise : they need to be complex enough to have a perceptual meaning but simple enough to be computed efficiently . \n\t', '\n\t\t With our statistical modeling approach , the labeling phase could be performed offline by a highly accurate unit selection system , potentially slow and complex , while the run-time statistical system could still be fast . \n\t', '\n\t\t Moreover , if we had audio available for our training corpus , we could exploit that in the initial labeling phase for the design of the unit selection system . \n\t', '\n\t\t ( 2 ) Weighted finite-state transducer representation . \n\t', '\n\t\t In addition to the already mentioned synthesis speed and the opportunity of high-quality measures in the initial offline labeling phase , another benefit of this approach is that it leads to a natural represen- tation by weighted transducers , and hence enables us to build a unit selection system using general and flexible representations and methods already in use for speech recognition , e.g. , those found in the FSM \n\t\t']",Positive
['\n\t\t Other unit selection systems based on weighted transducers were also proposed in \n\t\t'],Positive
"['\n\t\t ( 3 ) Unit selection algorithms and speed-up . \n\t', '\n\t\t We present a new unit selection system based on statistical modeling . \n\t', '\n\t\t We used weighted automata and transducers for the representation of the components of the system and designed a new and efficient composition algorithm making use of string potentials for their combination . \n\t', '\n\t\t The resulting statistical unit selection is shown to be about 2.6 times faster than the last release of the AT&T Natural Voices Product while preserving the same quality , and offers much flexibility for the use and integration of new and more complex components . \n\t', '\n\t\t 2 Unit Selection Methods 2.1 Overview of a Traditional Unit Selection System This section describes in detail the cost functions used in the AT&T Natural Voices Product that we will use as the baseline in our experimental results , see \n\t\t']",Positive
"['\n\t\t In this system , unit selection is based on \n\t\t']",Positive
"['\n\t\t Let U be the set of recorded units . \n\t', '\n\t\t Two cost functions are defined : the target cost Ct(fi , ui ) is used to estimate the mismatch between the features of the feature vector fi and the unit ui ; the concatenation cost Cc(ui , uj ) is used to estimate the smoothness of the acoustic signal when concatenating the units ui and uj . \n\t', '\n\t\t Given a sequence f = f1 ... fn of feature vectors , unit selection can then be formulated as the problem of finding the sequence of units u = u1 ... un that minimizes these two costs : u = argmin uEUn In practice , not all unit sequences of a given length are considered . \n\t', '\n\t\t A preselection method such as the one proposed by \n\t\t']",Positive
"['\n\t\t The computation of the target cost can be split in two parts : the context cost Cp that is the component of the target cost corresponding to the phonetic context , and the feature cost Cf that corresponds the ( ~n Ct(fi , ui ) + ~n Cc(ui-1 , ui ) ) i=1 i=2 other components of the target cost : Ct(fi , ui ) = Cp(fi , ui ) + Cf ( fi , ui ) ( 3 ) For each phonetic context ^ of length 5 , a list L(^) of the units that are the most frequently used in the phonetic context ^ is computed . \n\t', '\n\t\t For each feature vector fi in f , the candidate units for fi are computed in the following way . \n\t', '\n\t\t Let ^i be the 5-phone context of fi in f . \n\t', ""\n\t\t The context costs between fi and all the units in the preselection list of the phonetic context ^i are computed and the M units with the best context cost are selected : Ui = M-best(Cp(fi,ui)) uiEL(^i) The feature costs between fi and the units in Ui are then computed and the N units with the best target cost are selected : U'i = N-best(Cp(fi , ui ) + Cf ( fi , ui ) ) uiEUi The unit sequence u verifying : u = argmin uEU~1...U~n is determined using a classical Viterbi search . \n\t"", ""\n\t\t Thus , for each position i , the N2 concatenation costs between the units in U'i and U'i+1 need to be computed . \n\t"", '\n\t\t The caching method for concatenation costs proposed in \n\t\t']",Positive
"['\n\t\t 2.2 Statistical Modeling Approach Our statistical modeling approach was described in Section 1 . \n\t', '\n\t\t As already mentioned , our general approach would consists of deriving both the target cost \x97 log P(f I u ) and the concatenation cost \x97 log P(u) from appropriate training data using general statistical methods . \n\t', '\n\t\t To simplify the problem , we will use the existing target cost provided by the traditional unit selection system and concentrate on the problem of estimating the concatenation cost . \n\t', '\n\t\t We used the unit selection system presented in the previous section to generate a large corpus of more than 8M unit sequences , each unit corresponding to a unique recorded halfphone . \n\t', '\n\t\t This corpus was used to build an n-gram statistical language model using Katz backoff smoothing technique \n\t\t']",Positive
"['\n\t\t This model provides us with a new cost function , the grammar cost Cg , defined by : Cg(ukIu1 ... uk-1 ) = \x97log(P(ukIu1 ... uk-1 ) ) where P is the probability distribution estimated by our model . \n\t', '\n\t\t We used this new cost function to replace both the concatenation and context costs used in the traditional approach . \n\t', '\n\t\t Unit selection then consists of finding the unit sequence u such that : ( Cf(fi , ui)+Cg(uiI ui-k ... ui-1 ) ) In this approach , rather than using a preselection method such as that of \n\t\t']",Positive
"['\n\t\t 3 Representation by Weighted Finite-State Transducers An important advantage of the statistical framework we introduced for unit selection is that the resulting components can be naturally represented by weighted finite-state transducers . \n\t', '\n\t\t This casts unit selection into a familiar schema , that of a Viterbi decoder applied to a weighted transducer . \n\t', '\n\t\t 3.1 Weighted Finite-State Transducers We give a brief introduction to weighted finite-state transducers . \n\t', '\n\t\t We refer the reader to \n\t\t']",Positive
"['\n\t\t A weightedfinite-state transducer T is an 8-tuple T = ( E , A , Q , I , F , E , ^ , ^ ) where E is the finite input alphabet of the transducer , A is the finite output alphabet , Q is a finite set of states , I C_ Q the set of initial states , F C_ Q the set of final states , E C_ Q x ( E U { E } ) x ( A U { E } ) x R x Q a fi- nite set of transitions , ^ : I \x97* R the initial weight function , and ^ : F \x97* R the final weight function mapping F to R . \n\t', '\n\t\t In our statistical framework , the weights can be interpreted as log-likelihoods , thus there are added along a path . \n\t', '\n\t\t Since we use the standard Viterbi approximation , the weight associated by T to a pair of strings ( x , y ) E E* x A* is given by : [T](x , y ) = ^ER(Ii~y,F) ^[p[7r]] + w[7r] + ^[n[7r]] where R(I , x , y , F ) denotes the set of paths from an initial state p E I to a final state q E F with input label x and output label y , w [ 7r ] the weight of the path 7r , ^ [ p [ 7r ] ] the initial weight of the origin state of 7r , and ^ [ n [ 7r ] ] the final weight of its destination . \n\t', '\n\t\t A Weighted automaton A = ( E , Q , I , F , E , ^ , ^ ) is defined in a similar way by simply omitting the output ( or input ) labels . \n\t', '\n\t\t We denote by ^2(T) the ( ~n Ct(fi , ui ) + ~n Cc(ui-1 , ui ) ) i=1 i=2 u = argmin uE Un ~n i=1 Figure 1 : ( a ) Weighted automaton T1 . \n\t', '\n\t\t ( b ) Weighted transducer T2 . \n\t', '\n\t\t ( c ) T1 o T2 , the result of the composition of T1 and T2 . \n\t', '\n\t\t weighted automaton obtained from T by removing its input labels . \n\t', '\n\t\t A general composition operation similar to the composition of relations can be defined for weighted finite-state transducers \n\t\t']",Positive
"['\n\t\t The composition of two transducers T1 and T2 is a weighted transducer denoted by T1 o T2 and defined by : QT1 o T2](x , y ) = min { QT1](x , z ) + QT2](z , y ) } There exists a simple algorithm for constructing T = T1 o T2 from T1 and T2 \n\t\t']",Positive
"['\n\t\t The states of T are identified as pairs of a state of T1 and a state of T2 . \n\t', '\n\t\t A state ( q1 , q2 ) in T1 oT2 is an initial ( final ) state if and only if q1 is an initial ( resp . \n\t', '\n\t\t final ) state of T1 and q2 is an initial ( resp . \n\t', '\n\t\t final ) state of T2 . \n\t', '\n\t\t The transitions of T are the result of matching a transition of T1 and a transition of T2 as follows : ( q1 , a , b , w1 , q~1 ) and ( q2 , b , c , w2 , q~2 ) produce the transition ((q1,q2),a,c,w1+w2,(q~1,q~2)) ( 4 ) in T . \n\t', '\n\t\t The efficiency of this algorithm was critical to that of our unit selection system . \n\t', '\n\t\t Thus , we designed an improved composition that we will describe later . \n\t', '\n\t\t Figure 1(c) gives the resulting of the composition of the weighted transducers given figure 2(a) and ( b ) . \n\t', '\n\t\t 3.2 Language Model Weighted Transducer The n-gram statistical language model we construct for unit sequences can be represented by a weighted automaton G which assigns to each sequence u its log-likelihood : QG](u) = \x97log(P(u)) . \n\t', '\n\t\t ( 5 ) according to our probability estimate P . \n\t', '\n\t\t Since a unit sequence u uniquely determines the corresponding halfphone sequence x , the n-gram statistical model equivalently defines a model of the joint distribution of P(x , u ) . \n\t', '\n\t\t G can be augmented to define a weighted transducer G\x88 assigning to pairs ( x , u ) their log-likelihoods . \n\t', '\n\t\t For any halfphone sequence x and unit sequence u , we define G\x88 by : Q \x88G ] ( x , u ) = \x97log P(u) ( 6 ) The weighted transducer G\x88 can be used to generate all the unit sequences corresponding to a specific halfphone sequence given by a finite automaton p , using composition : p o \x88G . \n\t', '\n\t\t In our case , we also wish to use the language model transducer G\x88 to limit the number of candidate unit sequences considered . \n\t', '\n\t\t We will do that by giving a strong precedence to n- grams of units that occurred in the training corpus ( see Section 4.2 ) . \n\t', '\n\t\t Example Figure 2(a) shows the bigram model G estimated from the following corpus : <s> u1 u2 u1 u2 </s> <s> u1 u3 </s> <s> u1 u3 u1 u2 </s> where ( s ) and ~/s~ are the symbols marking the start and the end of an utterance . \n\t', '\n\t\t When the unit u1 is associated to the halfphone p1 and both units u1 and u2 are associated to the halfphone p2 , the corresponding weighted halfphone-to-unit transducer G\x88 is the one shown in Figure 2(b) . \n\t', '\n\t\t 3.3 Unit Selection with Weighted Finite-State Transducers From each sequence f = f1 ... fn of feature vectors specified by the text analysis frontend , we can straightforwardly derive the halfphone sequence to be synthesized and represent it by a finite automaton p , since the first component of each feature vector fi is the corresponding halfphone . \n\t', '\n\t\t Let W be the weighted automaton obtained by composition of p with G\x88 and projection on the output : W = ^2(p o \x88G ) ( 7 ) W represents the set of candidate unit sequences with their respective grammar costs . \n\t', '\n\t\t We can then use a speech recognition decoder to search for the best sequence u since W can be thought of as the 0 a 1 b 2 c 3 d 4 ( a ) b:y 1 a:x 2 c:z 3 d:t 4 0 a:u b:v 5 6 c:w 7 a:s 8 ( b ) a:x 1 b:y 3 c:z 5 d:t 7 0 a:u 2 b:v 4 c:w 6 ( c ) <s> u3 ^/3.647 u1/0.955 u1 u3/1.871 ^/5.216. </s>/1.466 </s>/0.703 u1/0.003 u1/0.703 u3/0.921 u2/1.466 ^/4.053 ^/5.034 u2 u2/0.514 </s>/0.410 u1/1.108 </s> <s> u3 ^:^/3.647 p1:u1/0.955 u1 p2:u3/1.871 ^:^/5.21 6 . \n\t', '\n\t\t ^ : </s>/0.703^ : </s>/1.466 p1:u1/0.003 p1:u1/0.703 p2:u3/0.921 p2:u2/1.466 ^ : ^/4.053 ^ : ^/5.034 u2 p2:u2/0.514 ^ : </s>/0.410 p1:u1/1.108 </s> ( a ) ( b ) Figure 2 : ( a ) n-gram language model G for unit sequences . \n\t', '\n\t\t ( b ) Corresponding halfphone-to-unit weighted transducer \x88G . \n\t', '\n\t\t counterpart of a speech recognition transducer , f the equivalent of the acoustic features and Cf the analogue of the acoustic cost . \n\t', '\n\t\t Our decoder uses a standard beam search of W to determine the best path by computing on-the-fly the feature cost between each unit and its corresponding feature vector . \n\t', '\n\t\t Composition constitutes the most costly operation in this framework . \n\t', '\n\t\t Section 4 presents several of the techniques that we used to speed up that algorithm in the context of unit selection . \n\t', '\n\t\t 4 Algorithms 4.1 Composition with String Potentials In general , composition may create noncoaccessible states , i.e. , states that do not admit a path to a final state . \n\t', '\n\t\t These states can be removed after composition using a standard connection ( or trimming ) algorithm that removes unnecessary states . \n\t', '\n\t\t However , our purpose here is to avoid the creation of such states to save computational time . \n\t', '\n\t\t To that end , we introduce the notion of string potential at each state . \n\t', '\n\t\t Let i[7r] ( o[7r] ) be the input ( resp . \n\t', '\n\t\t output ) label of a path 7r , and denote by x n y the longest common prefix of two strings x and y . \n\t', '\n\t\t Let q be a state in a weighted transducer . \n\t', '\n\t\t The input ( output ) string potential of q is defined as the longest common prefix of the input ( resp . \n\t', '\n\t\t output ) labels of all the paths in T from q to a final state : pi(q) = / \\ i [ 7r ] 7r^^(q,F) po(q) = n o[7r] 7r^^(q,F) The string potentials of the states of T can be computed using the generic shortest-distance algorithm of \n\t\t']",Positive
"['\n\t\t They can be used in composition in the following way . \n\t', '\n\t\t We will say that two strings x and y are comparable if x is a prefix of y or y is a prefix of x . \n\t', '\n\t\t Let ( q1 , q2 ) be a state in T = T1 o T2 . \n\t', '\n\t\t Note that ( q1 , q2 ) is a coaccessible state only if the output string potential of q1 in T1 and the input string potential of q2 in T2 are comparable , i.e. , po(q1) is a prefix of pi(q2) or pi(q2) is a prefix of po(q1) . \n\t', '\n\t\t Hence , composition can be modified to create only those states for which the string potentials are compatible . \n\t', '\n\t\t As an example , state 2 = ( 1 , 5 ) of the transducer T = T1 o T2 in Figure 1 needs not be created since po(1) = bcd and pi ( 5 ) = bca are not comparable strings . \n\t', '\n\t\t The notion of string potentials can be extended to further reduce the number of non-coaccessible states created by composition . \n\t', '\n\t\t The extended input string potential of q in T , is denoted by ¯pi ( q ) and is the set of strings defined by : ¯pi(q) = pi ( q ) - ^i ( q ) ( 8 ) where ( i(q) C E and is such that for every ^ E ( i ( q ) , there exist a path 7r from q to a final state such that pi (q)^ is a prefix of the input label of 7r . \n\t', '\n\t\t The extended output string potential of q , ¯po ( q ) , is defined similarly . \n\t', '\n\t\t A state ( q1 , q2 ) in T1 o T2 is coaccessible only if ( ¯po(q1) \x95 E* ) n ( ¯pi(q2) \x95 E* ) =~ 0 ( 9 ) Using string potentials helped us substantially improve the efficiency of composition in unit selection . \n\t', '\n\t\t 4.2 Language Model Transducer \x96 Backoff As mentioned before , the transducer G\x88 represents an n-gram backoff model for the joint probability distribution P(x , u ) . \n\t', '\n\t\t Thus , backoff transitions are used in a standard fashion when G\x88 is viewed as an automaton over paired sequences ( x , u ) . \n\t', '\n\t\t Since we use G\x88 as a transducer mapping halfphone sequences to unit sequences to determine the most likely unit sequence u given a halfphone sequence x 1we need to clarify the use of the backoff transitions in the composition p o \x88G . \n\t', '\n\t\t Denote by O(V) the set of output labels of a set of transitions V . \n\t', '\n\t\t Then , the correct use derived from the definition of the backoff transitions in the joint model is as follows . \n\t', '\n\t\t At a given state s of G\x88 and for a given input halfphone a , the outgoing transitions with input a are the transitions V of s with input label a , and for each b E~ O ( V ) , the transition of the first backoff state of s with input label a and output b . \n\t', '\n\t\t For the purpose of our unit selection system , we had to resort to an approximation . \n\t', '\n\t\t This is because in general , the backoff use just outlined leads to examining , for a given halfphone , the set of all units possible at each state , which is typically quite large.2 Instead , we restricted the inspection of the backoff states in the following way within the composition po \x88G . \n\t', '\n\t\t A state s1 in p corresponds in the composed transducer p o G\x88 to a set of states ( s1 , s2 ) , s2 E S2 , where S2 is a subset of the states of \x88G . \n\t', '\n\t\t When computing the outgoing transitions of the states in ( s1 , s2 ) with input label a , the backoff transitions of a state s2 are inspected if and only if none of the states in S2 has an outgoing transition with input label a. 1This corresponds to the conditional probability P(uIx) = P(x , u)/P(x) . \n\t', '\n\t\t 2Note that more generally the vocabulary size of our statistical language models , about 400,000 , is quite large compared to the usual word-based models . \n\t', '\n\t\t 4.3 Language Model Transducer \x96 Shrinking A classical algorithm for reducing the size of an n-gram language model is shrinking using the entropy-based method of \n\t\t']",Positive
"['\n\t\t In our experiments , we used a modified version of the weighted difference method . \n\t', '\n\t\t Let w be a unit and let h be its conditioning history within the n-gram model . \n\t', ""\n\t\t For a given shrink factor ^ , the transition corresponding to the n-gram hw is removed from the weighted automaton if:^ log(P(wlh)) \x97 log(^hP(wlh')) < c(hw) ( 10 ) where h ' is the backoff sequence associated with h . \n\t"", ""\n\t\t Thus , a higher-order n-gram hw is pruned when it does not provide a probability estimate significantly different from the corresponding lower-order n-gram sequence h'w . \n\t"", '\n\t\t This standard shrinking method needs to be modified to be used in the case of our halfphone-to-unit weighted transducer model with the restriction on the traversal of the backoff transitions described in the previous section . \n\t', ""\n\t\t The shrinking methods must take into account all the transitions sharing the same input label at the state identified with h and its back- off state h ' . \n\t"", ""\n\t\t Thus , at each state identified with h in \x88G , a transition with input label x is pruned when the following condition holds : ^ P(wl h ' ) ) < c(hw) where h ' is the backoff sequence associate with h and Xxk is the set of output labels of all the outgoing transitions with input label x of the state identified with k. 5 Experimental results We used the AT&T Natural Voices Product speech synthesis system to synthesize 107,987 AP news articles , generating a large corpus of 8,731,662 unit sequences representing a total of 415,227,388 units . \n\t"", '\n\t\t We used this corpus to build several n-gram Katz backoff language models with n = 2 or 3 . \n\t', '\n\t\t Table 1 gives the size of the resulting language model weighted automata . \n\t', '\n\t\t These language models were built using the GRM Library \n\t\t']",Positive
"['\n\t\t We evaluated these models by using them to synthesize an AP news article of 1,000 words , corresponding to 8250 units or 6 minutes of synthesized speech . \n\t', '\n\t\t Table 2 gives the unit selection time ( in seconds ) taken by our new system to synthesize this AP wE log( P(wlh)) \x97 X log(^h w^Xx h~ Model No . \n\t', '\n\t\t of states No . \n\t', '\n\t\t of transitions 2-gram , unshrunken 293,935 5,003,336 3-gram , unshrunken 4,709,404 19,027,244 3-gram , y = -4 2,967,472 14,223,284 3-gram , y = -1 2,060,031 12,133,965 3-gram , y = 0 1,681,233 10,217,164 3-gram , y = 1 1,370,220 9,146,797 3-gram , y = 4 934,914 7,844,250 Table 1 : Size of the stochastic language models for different n-gram order and shrinking factor . \n\t', '\n\t\t Model composition search total time baseline system - - 4.5s 2-gram , unshrunken 2.9s 1.0s 3.9s 3-gram , unshrunken 1.2s 0.5s 1.7s 3-gram , y = -4 1.3s 0.5s 1.8s 3-gram , y = -1 1.5s 0.5s 2.0s 3-gram , y = 0 1.7s 0.5s 2.2s 3-gram , y = 1 2.1s 0.6s 2.7s 3-gram , y = 4 2.7s 0.9s 3.6s Table 2 : Computation time for each unit selection system when used to synthesize the same AP news article . \n\t', '\n\t\t news article . \n\t', '\n\t\t Experiments were run on a 1GHz Pentium III processor with 256KB of cache and 2GB of memory . \n\t', '\n\t\t The baseline system mentioned in this table is the AT&T Natural Voices Product which was also used to generate our training corpus using the concatenation cost caching method from \n\t\t']",Positive
"['\n\t\t For the new system , both the computation times due to composition and to the search are displayed . \n\t', '\n\t\t Note that the AT&T Natural Voices Product system was highly optimized for speed . \n\t', '\n\t\t In our new systems , the standard research software libraries already mentioned were used . \n\t', '\n\t\t The search was performed using the standard speech recognition Viterbi decoder from the DCD library \n\t\t']",Positive
"['\n\t\t With a trigram language model , our new statistical unit selection system was about 2.6 times faster than the baseline system . \n\t', '\n\t\t A formal test using the standard mean of opinion score ( MOS ) was used to compare the quality of the high-quality AT&T Natural Voices Product synthesizer and that of the synthesizers based on our new unit selection system with shrunken and unshrunken trigram language models . \n\t', '\n\t\t In such tests , several listeners are asked to rank the quality of each utterance from 1(worst score ) to 5 ( best ) . \n\t', '\n\t\t The MOS results of the three systems with 60 utterances tested by 21 listeners are reported in Table 3 with their correspond- Model raw score normalized score baseline system 3.54 f.20 3.09 f .22 3-gram , unshrunken 3.45 f.20 2.98 f .21 3-gram , y = -1 3.40 f.20 2.93 f .22 Table 3 : Quality testing results : we report for each system , the mean and standard error of the raw and the listener-normalized scores . \n\t', '\n\t\t ing standard error . \n\t', '\n\t\t The difference of scores between the three systems is not statistically significant ( first column ) , in particular , the absolute difference between the two best systems is less than .1 . \n\t', '\n\t\t Different listeners may rank utterances in different ways . \n\t', '\n\t\t Some may choose the full range of scores ( 1\x965 ) to rank each utterance , others may select a smaller range near 5 , near 3 , or some other range . \n\t', '\n\t\t To factor out such possible discrepancies in ranking , we also computed the listener-normalized scores ( second column of the table ) . \n\t', '\n\t\t This was done for each listener by removing the average score over the full set of utterances , dividing it by the standard deviation , and by centering it around 3 . \n\t', '\n\t\t The results show that the difference between the normalized scores of the three systems is not significantly different . \n\t', '\n\t\t Thus , the MOS results show that the three systems have the same quality . \n\t', '\n\t\t We also measured the similarity of the two best systems by comparing the number of common units they produce for each utterance . \n\t', '\n\t\t On the AP news article already mentioned , more than 75 % of the units were common . \n\t', '\n\t\t 6 Conclusion We introduced a statistical modeling approach to unit selection in speech synthesis . \n\t', '\n\t\t This approach is likely to lead to more accurate unit selection systems based on principled learning algorithms and techniques that radically depart from the heuristic methods used in the traditional systems . \n\t', '\n\t\t Our preliminary experiments using a training corpus generated by the AT&T Natural Voices Product demonstrates that statistical modeling techniques can be used to build a high-quality unit selection system . \n\t', '\n\t\t It also shows other important benefits of this approach : a substantial increase of efficiency and a greater modularity and flexibility . \n\t', '\n\t\t Acknowledgments We thank Mark Beutnagel for helping us clarify some of the details of the unit selection system in the AT&T Natural Voices Product speech synthesizer . \n\t', '\n\t\t Mark also generated the training corpora and set up the listening test used in our experiments . \n\t', '\n\t\t We also acknowledge discussions with Brian Roark about various statistical language modeling topics in the context of unit selection . \n\t', '\n\t\t References Cyril Allauzen , Mehryar Mohri , and Michael Riley . \n\t', '\n\t\t 2003. DCD Library - Decoder Library , software collection for decoding and related functions . \n\t', '\n\t\t In AT&T Labs - Research . \n\t', '\n\t\t http://www.research.att.com/sw/tools/dcd. Cyril Allauzen , Mehryar Mohri , and Brian Roark . \n\t', '\n\t\t 2004. A General Weighted Grammar Library . \n\t', '\n\t\t In Proceedings of the Ninth International Conference on Automata ( CIAA 2004 ) , Kingston , Ontario , Canada , July . \n\t', '\n\t\t http://www.research.att.com/sw/tools/grm. Jean Berstel . \n\t', '\n\t\t 1979. Transductions and Context- Free Languages . \n\t', '\n\t\t Teubner Studienbucher : Stuttgart . \n\t', '\n\t\t Mark Beutnagel , Alistair Conkie , Juergen Schroeter , and Yannis Stylianou . \n\t', '\n\t\t 1999a . \n\t', '\n\t\t The AT&T Next-Gen system . \n\t', '\n\t\t In Proceedings of the Joint Meeting ofASA , EAA and DAGA , pages 18\x9624 , Berlin , Germany . \n\t', '\n\t\t Mark Beutnagel , Mehryar Mohri , and Michael Riley . \n\t', '\n\t\t 1999b . \n\t', '\n\t\t Rapid unit selection from a large speech corpus for concatenative speech synthesis . \n\t', '\n\t\t In Proceedings of Eurospeech , volume 2 , pages 607\x96610 . \n\t', '\n\t\t Ivan Bulyko and Mari Ostendorf . \n\t', '\n\t\t 2001. Unit selection for speech synthesis using splicing costs with weighted finite-state trasnducers . \n\t', '\n\t\t In Proceedings ofEurospeech , volume 2 , pages 987\x96990 . \n\t', '\n\t\t Alistair Conkie , Mark Beutnagel , Ann Syrdal , and Philip Brown . \n\t', '\n\t\t 2000. Preselection of candidate units in a unit selection-based text-to-speech synthesis system . \n\t', '\n\t\t In Proceedings of ICSLP , volume 3 , pages 314\x96317 . \n\t', '\n\t\t Samuel Eilenberg . \n\t', '\n\t\t 1974. Automata , Languages and Machines , volume A. Academic Press . \n\t', '\n\t\t Andrew Hunt and Alan Black . \n\t', '\n\t\t 1996. Unit selection in a concatenative speech synthesis system . \n\t', '\n\t\t In Proceedings of ICASSP\x9296 , volume 1 , pages 373\x96376 , Atlanta , GA . \n\t', '\n\t\t Frederick Jelinek . \n\t', '\n\t\t 1976. Continuous speech recognition by statistical methods . \n\t', '\n\t\t IEEE Proceedings , 64(4):532\x96556 . \n\t', '\n\t\t Slava M. Katz . \n\t', '\n\t\t 1987. Estimation of probabilities from sparse data for the language model component of a speech recogniser . \n\t', '\n\t\t IEEE Transactions on Acoustic , Speech , and Signal Processing , 35(3):400\x96401 . \n\t', '\n\t\t Werner Kuich and Arto Salomaa . \n\t', '\n\t\t 1986. Semirings , Automata , Languages . \n\t', '\n\t\t Number 5 in EATCS Monographs on Theoretical Computer Science . \n\t', '\n\t\t Springer-Verlag , Berlin , Germany . \n\t', '\n\t\t Mehryar Mohri , Fernando C. N. Pereira , and Michael Riley . \n\t', '\n\t\t 1996. Weighted automata in text and speech processing . \n\t', '\n\t\t In Proceedings of the 12th European Conference on Artificial Intelligence ( ECAI 1996 ) , Workshop on Extended finite state models of language , Budapest , Hungary . \n\t', '\n\t\t John Wiley and Sons , Chichester . \n\t', '\n\t\t Mehryar Mohri , Fernando C. N. Pereira , and Michael Riley . \n\t', '\n\t\t 2000. The Design Principles of a Weighted Finite-State Transducer Library . \n\t', '\n\t\t Theoretical Computer Science , 231(1):17\x9632. http://www.research.att.com/sw/tools/fsm. Mehryar Mohri . \n\t', '\n\t\t 2002. Semiring Frameworks and Algorithms for Shortest-Distance Problems . \n\t', '\n\t\t Journal of Automata , Languages and Combinatorics , 7(3):321\x96350 . \n\t', '\n\t\t Mehryar Mohri . \n\t', '\n\t\t 2004. Weighted Finite-State Transducer Algorithms : An Overview . \n\t', '\n\t\t In Carlos Martin-Vide , Victor Mitrana , and Gheorghe Paun , editors , Formal Languages and Applications , volume 148 , VIII , 620 p. Springer , Berlin . \n\t', '\n\t\t Eric Moulines and Francis Charpentier . \n\t', '\n\t\t 1990. Pitch-synchronous waveform processing techniques for text-to-speech synthesis using di- phones . \n\t', '\n\t\t Speech Communication , 9(5-6):453\x96 467 . \n\t', '\n\t\t Fernando C. N. Pereira and Michael D. Riley . \n\t', '\n\t\t 1997. Speech Recognition by Composition of Weighted Finite Automata . \n\t', '\n\t\t In Finite-State Language Processing , pages 431\x96453 . \n\t', '\n\t\t MIT Press . \n\t', '\n\t\t Arto Salomaa and Matti Soittola. 1978 . \n\t', '\n\t\t Automata- Theoretic Aspects of Formal Power Series . \n\t', '\n\t\t Springer-Verlag : New York . \n\t', '\n\t\t Kristie Seymore and Ronald Rosenfeld . \n\t', '\n\t\t 1996. Scalable backoff language models . \n\t', '\n\t\t In Proceedings of ICSLP , volume 1 , pages 232\x96235 , Philadelphia , Pennsylvania . \n\t', '\n\t\t Andreas Stolcke . \n\t', '\n\t\t 1998. Entropy-based pruning of backoff language models . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t DARPA Broadcast News Transcription and Understanding Workshop , pages 270\x96274 . \n\t', '\n\t\t Yannis Stylianou , Thierry Dutoit , and Juergen Schroeter . \n\t', '\n\t\t 1997. Diphone conactenation using a harmonic plus noise model of speech . \n\t', '\n\t\t In Proceedings ofEurospeech . \n\t', '\n\t\t Jon Yi , James Glass , and Lee Hetherington . \n\t', '\n\t\t 2000. A flexible scalable finite-state transducer architecture for corpus-based concatenative speech synthesis . \n\t', '\n\t\t In Proceedings of ICSLP , volume 3 , pages 322\x96325 . \n\t', '\n\t\t Developing A Flexible Spoken Dialog System Using Simulation Grace Chung Corporation for National Research Initiatives 1895 Preston White Drive , Suite 100 Reston , VA , 20191 gchung@cnri.reston.va.us Abstract In this paper , we describe a new methodology to develop mixed-initiative spoken dialog systems , which is based on the extensive use of simulations to accelerate the development process . \n\t', '\n\t\t With the help of simulations , a system providing information about a database of nearly 1000 restaurants in the Boston area has been developed . \n\t', '\n\t\t The simulator can produce thousands of unique dialogs which benefit not only dialog development but also provide data to train the speech recognizer and understanding components , in preparation for real user interactions . \n\t', '\n\t\t Also described is a strategy for creating cooperative responses to user queries , incorporating an intelligent language generation capability that produces content-dependent verbal descriptions of listed items . \n\t', '\n\t\t 1 Introduction Spoken dialog systems have traditionally been difficult to instantiate because of the extensive efforts required for configuring each component from the natural language ( NL ) understanding to the domain- specific context tracking and dialog engines . \n\t', '\n\t\t This task is particularly involved when building systems that empower users with greater flexibility at the spoken input via mixed-initiative interaction \n\t\t']",Positive
"['\n\t\t The goal of this work is to assemble natural spoken dialog interfaces that enable flexible interactions through mixed initiative dialog and cooperative responses . \n\t', '\n\t\t Such interfaces aim to help users navigate large information spaces such as on-line databases . \n\t', '\n\t\t Conventional systems generally guide the users through a series of scripted prompts , either through This research is sponsored in part by SPAWAR SSC-SD . \n\t', '\n\t\t The content of this paper does not necessarily reflect the position or policy of the Government , and no official endorsement should be inferred . \n\t', '\n\t\t User : Are there any Thai restaurants ? \n\t', '\n\t\t System : Can you provide a neighborhood or city ? \n\t', '\n\t\t Boston . \n\t', '\n\t\t User : There are 21 entries found . \n\t', '\n\t\t Can you be more specific ? \n\t', '\n\t\t System : A restaurant in Dorchester . \n\t', '\n\t\t User : System : There are no Thai restaurants in Dorchester . \n\t', '\n\t\t Is there anything else I can help you with ? \n\t', '\n\t\t User : System : What about in the South End ? \n\t', '\n\t\t There are 2 entries found . \n\t', '\n\t\t Thai Village and House of Siam . \n\t', '\n\t\t Table 1 : Example human-computer dialog . \n\t', '\n\t\t a finite-state grammar or a dialog control table . \n\t', '\n\t\t In Table 1 , the system prompts present a sequence of questions in an attempt to solicit more constraints from the user , with the goal of obtaining a small data subset from the database . \n\t', '\n\t\t The system responses are generated from a set of rules that only anticipate one of a handful of situations : ( 1 ) when the set of entries returned is too large , ( 2 ) the set of entries is adequately small to enumerate , and ( 3 ) no available entries have been returned . \n\t', '\n\t\t A more flexible scenario would allow the user to browse the content by specifying one or more constraints in any order . \n\t', '\n\t\t The system should then return a succinct summary of the content upon user specification of each constraint . \n\t', '\n\t\t This would provide improved feedback to the user about the available choices so far , guards against stilted conversations with a fixed number of dialog turns for every interaction , and mitigates against repeated scenarios where user queries return no items . \n\t', '\n\t\t However , much effort is then required in configuring the numerous scenarios for users to make sequences of queries in various orders . \n\t', '\n\t\t User queries are likely to differ if the database contents shift over time , changing the frequency and availability of certain entries . \n\t', '\n\t\t Furthermore , there remains the well-known \x93chicken-andegg\x94 problem of obtaining real-user data . \n\t', '\n\t\t With no real examples of human-computer interactions , it is difficult for developers to instantiate and configure a robust system . \n\t', '\n\t\t Yet without a reasonably operational system , it is equally difficult to convince real users to generate dialogs , particularly those which achieve successful completion . \n\t', '\n\t\t Hence , the usual development process consists of multiple iterations of expensive data collections and incremental system improvements . \n\t', '\n\t\t This paper presents an alternative paradigm for designing such a spoken dialog system . \n\t', '\n\t\t Our methodology employs simulations to reduce the time and effort required to build the system . \n\t', '\n\t\t Simulations facilitate prototyping and testing of an initial version of the system that automatically produces cooperative responses to user queries . \n\t', '\n\t\t We advocate the use of a suite of simulation techniques to create large numbers of synthetic user interactions with the system , including both typed and spoken inputs , where the speech is generated using a speech synthesizer . \n\t', '\n\t\t The resulting dialogs can be used to ( 1 ) diagnose the system for any problematic interactions , ( 2 ) enable a developer to examine system responses for large numbers of possible user queries , and ( 3 ) create an initial corpus for training the language models and probabilistic NL grammar . \n\t', '\n\t\t Thus , the initial phase of development comprises simulating hundreds of dialogs and iterative refinements prior to real-user data collection . \n\t', '\n\t\t In the next sections , we first describe our spoken dialog system architecture . \n\t', '\n\t\t This is followed by a description of a simulator , which operates in concert with a language generation system to output synthetic user queries . \n\t', '\n\t\t We elaborate on how the architecture can simulate coherent dialogs , and can be tuned to simulate a cooperative or uncooperative user . \n\t', '\n\t\t Then , methods for generating cooperative responses for a restaurant information domain are described . \n\t', '\n\t\t We detail how simulations have accelerated these developments . \n\t', '\n\t\t 2 System Architecture with Simulator Figure 1 depicts a spoken dialog system architecture functioning with simulator components , which create synthetic user inputs . \n\t', '\n\t\t Simulations can be customized to generate in text or speech mode . \n\t', '\n\t\t In text mode , text utterances are treated as user inputs to the understanding components . \n\t', '\n\t\t The dialog manager creates reply frames that encode information for generating the system reply string . \n\t', '\n\t\t These are also used by the simulator for selecting a random user response in the next turn . \n\t', '\n\t\t In speech mode , synthetic waveforms are created and recognized by the speech recognizer , yielding an -best list for the understanding components . \n\t', '\n\t\t Figure 1 : A spoken dialog system architecture integrated with user simulation components . \n\t', '\n\t\t Examples and experiments in this paper are drawn from a Boston restaurant information system . \n\t', '\n\t\t Obtained from an on-line source , the content offers information for 863 restaurants , located in 106 cities in the Boston metropolitan area ( e.g. , Newton , Cambridge ) and 45 neighborhoods ( e.g. , Back Bay , South End ) . \n\t', '\n\t\t Individual restaurant entries are associated with detailed information such as cuisines , phone numbers , opening hours , credit-card acceptance , price range , handicap accessibility , and menu offerings . \n\t', '\n\t\t Additionally , latitude and longitude information for each restaurant location have been obtained . \n\t', '\n\t\t 2.1 Instantiation of a System The concept of driving the instantiation of a dialog system from the data source was described in \n\t\t']",Positive
"['\n\t\t In the following , the steps envisioned for creating an initial prototype starting with on-line content are summarized below : 1 . \n\t', '\n\t\t Combing the web for database content 2 . \n\t', '\n\t\t Identifying the relevant set of keys associated with the domain , and mapping to the information parsed from the content originator 3 . \n\t', '\n\t\t Creating an NL grammar covering possible domain queries 4 . \n\t', '\n\t\t Configuring the discourse and dialog components for an initial set of interactions 5 . \n\t', '\n\t\t Defining templates for system responses The above steps are sufficient for enabling a working prototype to communicate with the proposed simulator in text mode . \n\t', '\n\t\t The next phase will involve iteratively running simulated dialogs and refinements on the spoken dialog system , followed by Speech Recognizer Language Generation Simulation Architecture Simulator Simulated Semantic Frame Simulated User Text System Reply String NL Understanding & Context Resolution Language Generation Reply Frame Semantic Frame Dialog Management Database Simulated User Speech Waveform Synthesizer Dialog System Architecture Table 2 : Example summary frame derived from the system reply frame . \n\t', '\n\t\t examination of successive corpora of simulated dialogs . \n\t', '\n\t\t Later phases will then incorporate the speech recognition and text-to-speech components . \n\t', '\n\t\t 2.2 Simulation with User Modeling The simulator , Figure 1 , is composed of several modular components . \n\t', '\n\t\t The core simulator accepts reply frames from the dialog system , and produces a meaning representation of the next synthetic user response . \n\t', '\n\t\t A text generation component paraphrases the meaning representation into a text string . \n\t', '\n\t\t In text mode , this poses as a typed user input , whereas in speech mode , the text is passed to a synthesizer as part of a synthesize/recognize cycle . \n\t', '\n\t\t Configuring a simulation for any domain involves customizing a simple external text file to control the behavior of the domain-independent simulator module , and tailoring text generation rules to output a variety of example user input sentences from the meaning representation . \n\t', '\n\t\t One simulated dialog would commence with an initial query such as \x93what restaurants do you provide?\x94 . \n\t', '\n\t\t The synthetic user makes successive queries that constrain the search to data subsets . \n\t', '\n\t\t It may ( 1 ) continue to browse more data subsets , or ( 2 ) when a small list of data entries is in focus , choose to query attributes pertaining to one or more individual items , or ( 3 ) terminate the conversation . \n\t', '\n\t\t The entire system is run continuously through hundreds of dialogs to produce log files of user and system sentences , and dialog information for subsequent analyses . \n\t', '\n\t\t The simulator also generates generic kinds of statements such as asking for help , repeat and clearing the dialog history . \n\t', '\n\t\t 2.2.1 Generation of Semantic Frames The simulator takes input from the system- generated reply frame , and outputs a flat semantic frame , encapsulating the meaning representation of the next intended user query . \n\t', '\n\t\t The system reply frame contains the essential entities , used in the paraphrase for creating the system prompt . \n\t', '\n\t\t But also , a sub-frame , shown in Figure 2 , retains pre- Select A Key and Value c summary :count 14 :categories ( c cuisine :ordered counts ( 4 2 2 2 ... :ordered values ( \x93american\x94 \x93indian\x94 .. c price range :ordered counts ( 7 2 2 1 ) :ordered values ( \x93cheap\x94 \x93low\x94 \x93medium\x94 .. Load History Frame No Use System Reply Frame ? \n\t', '\n\t\t Set Size < N ? \n\t', '\n\t\t No Begin Yes Yes Select Database Item & Key Output Frame Terminate ? \n\t', '\n\t\t No Yes Terminate Frame Figure 2 : A schematic showing the decision making procedure for the simulator . \n\t', '\n\t\t computed counts associated with the frequency of occurrence of values for every key pertaining to the data subset within the discourse focus . \n\t', '\n\t\t During the browsing stage , the simulator randomly selects a key ( e.g , a cuisine ) from the given frame , and then makes a random selection on the value , ( e.g. , \x93Chinese.\x94 ) . \n\t', '\n\t\t The simulator may choose one or more of these key-value pairs as constraints to narrow the search . \n\t', '\n\t\t For each key , more than one value from the list of possible values may be specified , ( e.g. , querying for \x93Chinese or Japanese restaurants.\x94 ) . \n\t', '\n\t\t When querying about individual restaurants , the simulator randomly selects one restaurant entry from a small list , and then seeks to obtain the value for one key characteristic for a restaurant entry . \n\t', '\n\t\t For example , this could be a phone number or an address . \n\t', '\n\t\t Figure 2 illustrates the decision making performed by the simulator at each turn . \n\t', '\n\t\t At each decision point , the system \x93throws the dice\x94 to determine how to proceed , for example , whether to select an additional key for constraint within the same turn , and whether to persist in querying about the available attributes of the small list of restaurants or to start over . \n\t', '\n\t\t The behavior of the simulator at each decision point can be tuned from an external text file , which allows the following to be specified : Probability of combining several constraints into a single query Probability of querying a different value for a previous key versus selecting from among other keys presented by the reply frame No Output Frame Select Another Key ? \n\t', '\n\t\t Yes Probability of continued querying of the attributes of restaurants from a list of one or more restaurants Probability of the user changing his goals , hence querying with alternative constraints A simple user model is maintained by the simulator to track the key-value pairs that have already been queried in the current dialog . \n\t', '\n\t\t This tracks the dialog history so as to enable the synthetic user to further query about a previously mentioned item . \n\t', '\n\t\t It also prevents the dialog from cycling indefinitely through the same combinations of constraints , helping to make the dialog more coherent . \n\t', '\n\t\t The external configuration file can effectively tune the level of cooperative behavior for the synthetic user . \n\t', '\n\t\t If the synthetic user selects a single key- value pair from the reply frame at each turn , a non- empty and successively smaller data subset is guaranteed to result at each turn . \n\t', '\n\t\t Moreover , selections can be configured to bias towards frequencies of instance values . \n\t', '\n\t\t The basis for this stems from the hypothesis that locations populated with more restaurants are likely to be queried . \n\t', '\n\t\t That is , the statistics of the database instances can directly reflect on the distribution of user queries . \n\t', '\n\t\t For instance , users are more likely to query about , \x93Chinese restaurants in Chinatown.\x94 Hence , the output dialogs may be more suitable for training language models . \n\t', '\n\t\t Alternatively , the synthetic user may be configured to select random combinations of various keys and values from the current or stored summary frame at a turn . \n\t', '\n\t\t Under these circumstances , the subsequent database retrieval may yield no data for those particular combinations of constraints . \n\t', '\n\t\t 2.2.2 Generation of Simulated Utterances Each semantic frame is input to Genesis , a text generation module \n\t\t']",Positive
"['\n\t\t Genesis executes surface-form generation via recursive generation rules and an associated lexicon . \n\t', '\n\t\t A recent addition to Genesis is the ability to randomly generate one of several variant sentences for the same semantic frame . \n\t', '\n\t\t A developer can specify several rules for each linguistic entity allowing the generator to randomly select one . \n\t', '\n\t\t Due to the hierarchical nature of these templates , numerous output sentences can be produced from a single semantic frame , with only a few variants specified for each rule . \n\t', '\n\t\t Table 3 depicts example semantic frames and corresponding sample sentences from the simulator . \n\t', '\n\t\t In total , the full corpus of simulated sentences are generated from approximately 55 hand-written rules in the restaurants domain . \n\t', '\n\t\t These rules distinguish themselves from previous text generation tasks by the incorporation of spontaneous speech phenomena such as filled pauses and fragments . \n\t', '\n\t\t In the initial phase , this small rules set is not systematically mined from any existing corpora , but is handcrafted by the developer . \n\t', '\n\t\t However , it may be possible in future to incorporate both statistics and observations learned from real data to augment the generation rules . \n\t', '\n\t\t 2.2.3 Synthetic User Waveforms A concatenative speech synthesizer \n\t\t']",Positive
"['\n\t\t The parameters and concatenative units employed in this synthesizer were tailored for a previous domain , and therefore , the naturalness and intelligibility of the output waveforms are expected to be poor . \n\t', '\n\t\t However , the occurrence of some recognition errors may help in assessing their impact on the system . \n\t', '\n\t\t 3 Cooperative Response Strategies We have aimed to design a more cooperative spoken dialog system in two respects . \n\t', '\n\t\t First , the information is delivered so that at each turn a dynamic summary of the database items in focus is presented . \n\t', '\n\t\t Secondly , the dialog manager is augmented with a domain-independent algorithm to handle over-constrained queries . \n\t', '\n\t\t The system gives alternative suggestions that are integrated with the dynamic summaries . \n\t', '\n\t\t 3.1 Flexible System Responses Response planning is performed both in the dialog management and the language generator , Genesis . \n\t', '\n\t\t To enable flexible responses , and avoid rigid system prompts , the dialog manager accesses the database at every turn with the current set of user-specified constraints in focus . \n\t', '\n\t\t With this data subset returned , a data refinement server \n\t\t']",Positive
"['\n\t\t This is incorporated into the system reply frame as shown in Table 2 . \n\t', '\n\t\t Following this , Genesis provides a summary of the characteristics of the data set , utilizing context information provided by the dialog manager and the frequency statistics . \n\t', '\n\t\t Genesis provides control on how to summarize the data linguistically via explicit rules files . \n\t', '\n\t\t The developer can specify variables , , and which control how lists of items are summarized , separately for different classes of data . \n\t', '\n\t\t If the number of items is under , all options are enumerated . \n\t', '\n\t\t If the top frequency counts cover more than of the data , then these categories will be suggested , ( e.g. \x93Some choices are Italian Frame Example Sentences c seek I\x92m interested in some low end restaurants in Back Bay please . \n\t', '\n\t\t Inexpensive restaurants in Back Bay . \n\t', '\n\t\t :neighborhood \x93Back Bay\x94 :price range \x93low\x94 Okay a cheap restaurant in Back Bay . \n\t', '\n\t\t uh Are there any cheap restaurants in Back Bay ? \n\t', '\n\t\t c request property :property \x94hours\x94 :name \x94Emma\x92s\x94 Can you please tell me the hours for Emma\x92s ? \n\t', '\n\t\t When is Emma\x92s open ? \n\t', '\n\t\t Well what are the hours for Emma\x92s ? \n\t', '\n\t\t Okay then what are the opening hours of Emma\x92s ? \n\t', '\n\t\t Table 3 : Sample semantic frames from the simulator , along with examples of generated sentence outputs . \n\t', '\n\t\t For each example frame above , hundreds of simulated variant sentences can be obtained . \n\t', '\n\t\t and Chinese.\x94 ) . \n\t', '\n\t\t Alternatively , summaries can indicate values that are missing or common across the set , ( e.g. \x93All of them are cheap.\x94 ) . \n\t', '\n\t\t By accessing the database and then examining the data subset at each turn , the system informs the user with a concise description of the choices available at that point in the dialog . \n\t', '\n\t\t This is a more flexible alternative than following a script of prompts where in the end the user may arrive at an empty set . \n\t', '\n\t\t Moreover , we argue that performing the summary in real time yields greater robustness against changes in the database contents . \n\t', '\n\t\t 3.2 Dialog Management The domain-independent dialog manager is configurable via an external dialog control table . \n\t', '\n\t\t A set of generic functions are triggered by logical conditions specified in formal rules , where typically several rules fire in each turn . \n\t', '\n\t\t The dialog manager has been extended to handle scenarios in which the user constraints yield an empty set . \n\t', '\n\t\t The aim is to avoid simply stating that no data items were found , without providing some guidance on how the user could re-formulate his query . \n\t', '\n\t\t Domain-independent routines relax the constraints using a set of pre-defined and configurable criteria . \n\t', '\n\t\t Alternate methods for relaxing constraints are : If a geographical key has been specified , relax the value according to a geography ontology . \n\t', '\n\t\t For instance , if a particular street name has been specified , the relaxation generates a subsuming neighborhood constraint in place of the street name . \n\t', '\n\t\t If a geographical key has been specified , remove the geographical constraint and search for the nearest item that satisfies the remaining constraints . \n\t', '\n\t\t The algorithm computes the nearest item according to the central latitude/longitude coordinates of the neighborhood or city . \n\t', '\n\t\t Relax the key-value with alternative values that have been set to defaults in an external file . \n\t', '\n\t\t For instance , if a Vietnamese restaurant is not available at all , the system relaxes the query to alternative Asian cuisines . \n\t', '\n\t\t Choose the one constraint to remove that produces the smallest data subset to speak about . \n\t', '\n\t\t If no one constraint is able to produce a non- empty set , successively remove more constraints . \n\t', '\n\t\t The rationale for finding a constraint combination that produces a small data set , is to avoid suggesting very general alternatives : for instance , suggesting and summarizing the \x93337 cheap restaurants\x94 when \x93cheap fondue restaurants\x94 were requested . \n\t', '\n\t\t The routine will attempt to apply each of these relaxation techniques in turn until a non-zero data set can be attained . \n\t', '\n\t\t 4 Experiments 4.1 Simulations in Text Mode The first stage of development involved iteratively running the system in text mode and inspecting log files of the generated interactions for problems . \n\t', '\n\t\t This development cycle was particularly useful for extending the coverage of the NL parser and ensuring the proper operation of the end-to-end system . \n\t', '\n\t\t Simulations have helped diagnose initial problems overlooked in the rule-based mechanisms for context tracking ; this has served to ensure correct inheritance of attributes given the many permutations of sequences of input sentences that are possible within a single conversation . \n\t', '\n\t\t This is valuable because in such a mixed-initiative system , the user is free to change topics and specify new parameters at any time . \n\t', '\n\t\t For instance , a user may or may not follow up with suggestions for restaurants offered by the system . \n\t', '\n\t\t In fact , the user could continue to modify any of the constraints previously specified in the conversation or query any attributes for an alternate newly spoken restaurant . \n\t', '\n\t\t There are vast numbers of dialog contexts that can result , and simulations have assisted greatly in detecting problems . \n\t', '\n\t\t Furthermore , by generating many variations of possible user constraints , simulations have also helped identify initial problems in the summarization rules for system response generation . \n\t', '\n\t\t The text generation component is handcrafted and benefits largely from examples of real queries to ensure their proper operation . \n\t', '\n\t\t These kinds of problems would otherwise normally be encountered only after many user interactions have occurred . \n\t', '\n\t\t Table 4 shows a typical simulated dialog . \n\t', '\n\t\t In the interaction shown , the simulator provides one or more constraints at each turn . \n\t', '\n\t\t It also selects alternative values according to the previous chosen key . \n\t', '\n\t\t After the dialog has arrived at a small data set , the simulator randomly asks questions about individual items . \n\t', '\n\t\t During one simulation run , we completed 2000 dialogs in text mode . \n\t', '\n\t\t There were a total of 8147 input utterances , resulting in an average of 4.07 input utterances per dialog . \n\t', '\n\t\t Of the input utterances , 5446 were unique . \n\t', '\n\t\t These were generated from 3349 unique semantic frames . \n\t', '\n\t\t There were 4320 unique system replies . \n\t', '\n\t\t 4.2 Simulating Over-Constrained Queries By configuring the simulator to query with multiple constraints in single turns , we were able to synthesize over-constrained queries , in which , initially , empty data sets were retrieved . \n\t', '\n\t\t In earlier development cycles , simulated dialogs were crucial in finding combinations that yielded problematic responses due to over-constrained queries . \n\t', '\n\t\t In one run after refinements , we tested 300 dialogs whereby two or more constraints were combined in each query by the simulator . \n\t', '\n\t\t As a result , 113 queries necessitated the system to invoke the relaxation algorithm in order to present a suggested alternative data set . \n\t', '\n\t\t Under all these cases , the system gave suggestions without fail . \n\t', '\n\t\t Some examples of the replies are shown in Table 5 . \n\t', '\n\t\t 4.3 Simulation with Speech Recognition The next step is to implement the synthesize-andrecognize cycle whereby the text-to-speech output is directly piped to the recognition engine . \n\t', '\n\t\t A speech recognizer \n\t\t']",Positive
"['\n\t\t This recognizer uses class -gram models . \n\t', '\n\t\t With the training set of 8147 utterances , the perplexity of the bigram and trigram models are respectively 6.6 and 5.1 . \n\t', '\n\t\t The speech recognizer , including the restaurant names contains 2325 vocab- SIm : SYS : What restaurants do you know ? \n\t', '\n\t\t There are 863 restaurants . \n\t', '\n\t\t Some of the options are American , Pizza , and Italian . \n\t', '\n\t\t Most of them are located in Boston and Cambridge . \n\t', '\n\t\t SIm : SYS : Any restaurants in Back Bay ? \n\t', '\n\t\t There are 57 restaurants in Back Bay . \n\t', '\n\t\t Many of them are American , and Nouvelle Cuisine . \n\t', '\n\t\t SIm : SYS : So how about food in The South End ? \n\t', '\n\t\t There are 42 restaurants in The South End . \n\t', '\n\t\t They are mostly American , and Nouvelle Cuisine . \n\t', '\n\t\t Many of them are located on Columbus Avenue , and Tremont Street . \n\t', '\n\t\t SIm : SYS : I would like a moderately priced restaurant . \n\t', '\n\t\t There are 9 medium priced restaurants in The South End . \n\t', '\n\t\t Some of the options are Nouvelle Cuisine , and American . \n\t', '\n\t\t Many of them are on Tremont Street and Appleton Street . \n\t', '\n\t\t SIm : SYS : oh What about any cheap American restaurants ? \n\t', '\n\t\t There are 7 cheap American restaurants in The South End . \n\t', '\n\t\t They are located on Columbus Avenue , Tremont Street , Dartmouth Street , and Washington Street . \n\t', '\n\t\t SIm : SYS : Columbus Avenue . \n\t', '\n\t\t I have found 3 cheap American restaurants in The South End on Columbus Avenue . \n\t', '\n\t\t Their names are Charlie\x92s Sandwich Shoppe , Bob The Chefs , and Grill Fish . \n\t', '\n\t\t SIm : SYS : What is the phone number of first one ? \n\t', '\n\t\t The phone number of Charlie\x92s Sandwich Shoppe is 617 536 7669 . \n\t', '\n\t\t SIm : SYS : What is the price range for Grill Fish ? \n\t', '\n\t\t The price range of Grill Fish is cheap . \n\t', '\n\t\t Table 4 : Typical example of a simulated dialog . \n\t', '\n\t\t SIm are the simulated inputs and SYS are the system responses . \n\t', '\n\t\t ulary items . \n\t', '\n\t\t The understanding component would select one sentence of an -best list output from the speech recognizer . \n\t', '\n\t\t Presently , when a recognition error causes a hard parse failure in the understanding , the simulator is configured to output the same semantic frame as contained in the previous utterance . \n\t', '\n\t\t This will cause the text generator to output a different variant of the same query . \n\t', '\n\t\t If the parse failures occur multiple times in sequence , the simulated user aborts and terminates the dialog . \n\t', '\n\t\t 4.4 Results from Spoken Dialog System Our initial run of 36 dialogs yielded 213 sentences . \n\t', '\n\t\t To simplify dialogs , the simulator is configured to specify just one constraint at each turn . \n\t', '\n\t\t After the 1 . \n\t', '\n\t\t Cheap Restaurants on Rowes Wharf : There are no cheap restaurants on Rowes Wharf . \n\t', '\n\t\t However , there are in total 5 cheap restaurants in the Financial District . \n\t', '\n\t\t They are on Broad Street , Post Office Square , Federal Street , and Bromfield Street . \n\t', '\n\t\t 2. Cheap Thai Restaurants in Dorchester : There are no cheap Thai restaurants in Dorchester . \n\t', '\n\t\t However , there are in total 14 cheap Thai restaurants . \n\t', '\n\t\t They are predominantly in Brighton , and The South End . \n\t', '\n\t\t The nearest one is Thai Village in the South End . \n\t', '\n\t\t It is less than 3 miles away from Dorchester . \n\t', '\n\t\t 3. Expensive Asian Restaurants on Hale Street : There are no high end Asian restaurants on Hale Street . \n\t', '\n\t\t Alternatively Yanks serves high end American food . \n\t', '\n\t\t It is located at 717 Hale Street in Beverly Farms . \n\t', '\n\t\t Table 5 : Example summary responses from simulation generated over-constrained queries after relaxation of user constraints . \n\t', '\n\t\t Various schemes for relaxation are shown . \n\t', '\n\t\t ( 1 ) relaxes on the geographical location , ( 2 ) offers a nearest alternative , and ( 3 ) removes the cuisine constraint , outputting a single alternate selection . \n\t', '\n\t\t data subset has been narrowed down to six items or less , the simulator queries focus on one of the six items . \n\t', '\n\t\t For the 213 utterances , the recognition word error rate is 11.2 % , and the sentence error rate is 32.4 % . \n\t', '\n\t\t Because the synthesizer is highly domain specific and was originally trained on another domain , the synthetic waveforms were in fact highly unnatural . \n\t', '\n\t\t However , the relatively good recognition performance can be attributed to segmental units being well matched to the segment-based recognizer , an exact match to the trained -gram model and the lack of spontaneous speech phenomena such as disfluencies . \n\t', '\n\t\t These 36 dialogs were analysed by hand . \n\t', '\n\t\t All dialogs successfully arrived at some small data subset at termination , without aborting due to errors . \n\t', '\n\t\t 29 ( 80.1 % ) of the dialogs completed without errors , with the correct desired data set achieved . \n\t', '\n\t\t Of the errorful dialogs , 3 exhibited problems due to recognition errors and 4 dialogs exhibited errors in the parse and context tracking mechanisms . \n\t', '\n\t\t All the questions regarding querying of individual restaurants were answered correctly . \n\t', '\n\t\t 5 Discussion The above evaluations have been conducted on highly restricted scenarios in order to focus development on any fundamental problems that may exist in the system . \n\t', '\n\t\t In all , large numbers of synthetic dialogs have helped us identify problems that in the past would have been discovered only after data collections , and possibly after many failed dialogs with frustrated real users . \n\t', '\n\t\t The hope is that using simulation runs will improve system performance to a level such that the first collection of real user data will contain a reasonable rate of task success , ultimately providing a more useful training corpus . \n\t', '\n\t\t Having eliminated many software problems , a final real user evaluation will be more meaningful . \n\t', '\n\t\t 6 Related Work Recently , researchers have begun to address the rapid prototyping of spoken dialog applications . \n\t', '\n\t\t While some are concerned with the generation of systems from on-line content \n\t\t']",Positive
"['\n\t\t Real user simulations have been employed in other areas of software engineering . \n\t', '\n\t\t Various kinds of human-computer user interfaces can be evaluated for usability , via employing simulated human users \n\t\t']",Positive
"['\n\t\t These can range from web pages to cockpits and air traffic control systems . \n\t', '\n\t\t Simulated users have also accounted for perceptual and cognitive models . \n\t', '\n\t\t Previous work in dialog systems has addressed simulation techniques towards the goal of training and evaluation . \n\t', '\n\t\t In \n\t\t']",Positive
"['\n\t\t These simulations required collecting real-user data to build the user model . \n\t', '\n\t\t Other researchers have used simulations for the evaluation of dialog systems \n\t\t']",Positive
['\n\t\t In \n\t\t'],Negative
"['\n\t\t This was used to test alternate confirmation strategies under various recognition accuracies . \n\t', '\n\t\t Their methods did require the recording of scripted user utterances , and hence were limited in the variations of user input . \n\t', '\n\t\t Our specific goals have dealt with creating more cooperative and flexible responses in spoken dialog . \n\t', '\n\t\t The issues of mismatch between user queries and database contents have been addressed by others in database systems \n\t\t']",Positive
"['\n\t\t 7 Conclusions and Future Work The use of a simulator has greatly facilitated the development of our dialog system , with the availabil- ity of thousands of artificial dialogs . \n\t', '\n\t\t Even relatively restricted synthetic dialogs have already accelerated development . \n\t', '\n\t\t In the next phase , real user data collection will be conducted , along with full-scale evaluation . \n\t', '\n\t\t We plan to compare the efficacy of our language models built from simulated data with those trained from real user data . \n\t', '\n\t\t Future research will address issues of graceful recovery from recognition error . \n\t', '\n\t\t We believe that the framework of using simulated dialogs possibly with synthesized speech input augmented with controlled levels of additive noise can be an effective way to develop and evaluate error recovery strategies . \n\t', '\n\t\t Current methods for simulating dialogs are quite rudimentary . \n\t', '\n\t\t The text only produces certain variants that have been observed but does not respect corpus statistics , nor , in the case of synthetic speech , do they account for spontaneous speech phenomena . \n\t', '\n\t\t Improved simulations could use a set of indexed real speech waveforms invoked by the core simulator to create more realistic input . \n\t', '\n\t\t The main functionalities in the simulator software are now customizable from an external file . \n\t', '\n\t\t The simulator is domain independent and can be tailored for development of similar spoken dialog systems for browsing and navigating large databases . \n\t', '\n\t\t However further work is needed to incorporate greater configurability to the dialog flow . \n\t', '\n\t\t Increased flexibility for customizing the model of the dialog is needed to enable the software to be applied to the development of other kinds of dialog systems . \n\t', '\n\t\t 8 Acknowledgment The author wishes to thank Stephanie Seneff for her valuable feedback and the anonymous reviewers for their insightful comments and suggestions . \n\t', '\n\t\t References M. Araki and S. Doshita . \n\t', '\n\t\t 1997. Automatic evaluation environment for spoken dialog system evaluation . \n\t', '\n\t\t In Dialog Processing in Spoken Language Systems , 183\x96194 . \n\t', '\n\t\t M. Denecke et al . 2002. Rapid Prototyping for Spoken Dialog Systems . \n\t', '\n\t\t Proc . \n\t', '\n\t\t COLING , Taipei , Taiwan . \n\t', '\n\t\t M. Dzikovska et al . 2003 . \n\t', '\n\t\t Integrating linguistic and domain knowledge for spoken dialog systems in multiple domains . \n\t', '\n\t\t Proc . \n\t', '\n\t\t IJCAI , Acapulco , Mexico . \n\t', '\n\t\t J. Feng et al . 2003. Webtalk : Mining Websites for Automatically Building Dialog Systems . \n\t', '\n\t\t Proc . \n\t', '\n\t\t IEEE ASRU , Virgin Islands . \n\t', '\n\t\t G. Ferguson and J Allen . \n\t', '\n\t\t 1998. TRIPS : An Integrated Intelligent Problem-Solving Assistant . \n\t', '\n\t\t Proc . \n\t', '\n\t\t of the Fifteenth National Conference on AI ( AAAI-98 ) , 26\x9630 . \n\t', '\n\t\t Madison , WI . \n\t', '\n\t\t T. Gaasterland et al . 1992. An Overview of Cooperative Answering . \n\t', '\n\t\t Journal of Intelligent Information Systems , 1(2) , 123\x96157 . \n\t', '\n\t\t J. Glass . \n\t', '\n\t\t 2003. A Probabilistic Framework for Segment-Based Speech Recognition . \n\t', '\n\t\t Computer Speech and Language , 17 , 137\x96152 . \n\t', '\n\t\t K. Hone and C. Baber . \n\t', '\n\t\t 1995. Using a simulation method to predict the transaction time effects of applying alternative levels of constraint to user utterances within speech interactive dialogs . \n\t', '\n\t\t ESCA Workshop on Spoken Dialog Systems . \n\t', '\n\t\t B. S. Lin and L. S. Lee . \n\t', '\n\t\t 2001 . \n\t', '\n\t\t Computer-aided analysis and design for spoken dialog systems based on quantitative simulations . \n\t', '\n\t\t IEEE Trans . \n\t', '\n\t\t on Speech and Audio Processing , 9(5) , 534\x96548 . \n\t', '\n\t\t R. Lopez-Cozar et al . 2003. Assessment of dialog systems by means of a new simulation technique . \n\t', '\n\t\t Speech Communication , 40 , 387\x96407 . \n\t', '\n\t\t J. Polifroni , G. Chung and S. Seneff . \n\t', '\n\t\t 2003. Towards automatic generation of mixed-initiative dialog systems from web content . \n\t', '\n\t\t Proc . \n\t', '\n\t\t EUROSPEECH , 193\x96196 . \n\t', '\n\t\t Geneva , Switzerland . \n\t', '\n\t\t Y. Qu and N. Green . \n\t', '\n\t\t 2002. A Constraint-Based Approach for Cooperative Information-Seeking Dialog . \n\t', '\n\t\t Proc . \n\t', '\n\t\t INLG , New York . \n\t', '\n\t\t M. Riedl and R. St. Amant . \n\t', '\n\t\t 2002. Toward automated exploration of interactive systems . \n\t', '\n\t\t Proc . \n\t', '\n\t\t IUI , 135\x96142 . \n\t', '\n\t\t F. Ritter and R. Young . \n\t', '\n\t\t 2001. Embodied models as simulated users : Introduction to this special issue on using cognitive models to improve interface design . \n\t', '\n\t\t International Journal of Human- Computer Studies , 55 , 1\x9614 . \n\t', '\n\t\t K. Scheffler and S. Young . \n\t', '\n\t\t 2000. Probabilistic simulation of human-machine dialogs . \n\t', '\n\t\t Proc . \n\t', '\n\t\t ICASSP , 1217\x961220 . \n\t', '\n\t\t Istanbul , Turkey . \n\t', '\n\t\t S. Seneff et al . 1998. Galaxy-II : A Reference Architecture For Conversational System Development . \n\t', '\n\t\t Proc . \n\t', '\n\t\t ICSLP . \n\t', '\n\t\t Sydney , Australia . \n\t', '\n\t\t S. Seneff . \n\t', '\n\t\t 2002. Response Planning and Generation in the MERCURY Flight Reservation System . \n\t', '\n\t\t Computer Speech and Language 16 , 283\x96 312 . \n\t', '\n\t\t V. Zue , et al . 2000. JUPITER : A Telephone-Based Conversational Interface for Weather Information IEEE Transactions on Speech and Audio Processing , 8(1) . \n\t', '\n\t\t J. Yi et al . 2000. A flexible , scalable finite-state transducer architecture for corpus-based concatenative speech synthesis . \n\t', '\n\t\t Proc . \n\t', '\n\t\t ICSLP . \n\t', '\n\t\t Beijing , China . \n\t', '\n\t\t Data-Driven Strategies for an Automated Dialogue System Hilda HARDY , Tomek STRZALKOWSKI , Min WU ILS Institute University at Albany , SUNY 1400 Washington Ave. , SS262 Albany , NY 12222 USA hhardy|tomek|minwu@ cs.albany.edu Cristian URSU , Nick WEBB Department of Computer Science University of Sheffield Regent Court , 211 Portobello St. Sheffield S 1 4DP UK c.ursu@sheffield.ac.uk , n.webb@dcs.shef.ac.uk Alan BIERMANN , R. Bryce INOUYE , Ashley MCKENZIE Department of Computer Science Duke University P.O. Box 90129 , Levine Science Research Center , D 101 Durham , NC 27708 USA awb|rbi|armckenz@cs.duke.edu Abstract We present a prototype natural-language problem-solving application for a financial services call center , developed as part of the Amitiés multilingual human-computer dialogue project . \n\t', '\n\t\t Our automated dialogue system , based on empirical evidence from real call-center conversations , features a data- driven approach that allows for mixed system/customer initiative and spontaneous conversation . \n\t', '\n\t\t Preliminary evaluation results indicate efficient dialogues and high user satisfaction , with performance comparable to or better than that of current conversational travel information systems . \n\t', '\n\t\t 1 Introduction Recently there has been a great deal of interest in improving natural-language human-computer conversation . \n\t', '\n\t\t Automatic speech recognition continues to improve , and dialogue management techniques have progressed beyond menu-driven prompts and restricted customer responses . \n\t', '\n\t\t Yet few researchers have made use of a large body of human-human telephone calls , on which to form the basis of a data-driven automated system . \n\t', '\n\t\t The Amiti6s project seeks to develop novel technologies for building empirically induced dialogue processors to support multilingual human-computer interaction , and to integrate these technologies into systems for accessing information and services ( http://www.dcs.shef.ac . \n\t', '\n\t\t uk/nlp/amities ) . \n\t', '\n\t\t Sponsored jointly by the European Commission and the US Defense Advanced Research Projects Agency , the Amiti6s Consortium includes partners in both the EU and the US , as well as financial call centers in the UK and France . \n\t', '\n\t\t A large corpus of recorded , transcribed telephone conversations between real agents and customers gives us a unique opportunity to analyze and incorporate features of human-human dialogues into our automated system . \n\t', '\n\t\t ( Generic names and numbers were substituted for all personal details in the transcriptions . \n\t', '\n\t\t ) This corpus spans two different application areas : software support and ( a much smaller size ) customer banking . \n\t', '\n\t\t The banking corpus of several hundred calls has been collected first and it forms the basis of our initial multilingual triaging application , implemented for English , French and German \n\t\t']",Positive
"['\n\t\t The much larger software support corpus ( 10,000 calls in English and French ) is still being collected and processed and will be used to develop the next Amiti6s prototype . \n\t', '\n\t\t We observe that for interactions with structured data \x96 whether these data consist of flight information , spare parts , or customer account information \x96 domain knowledge need not be built ahead of time . \n\t', '\n\t\t Rather , methods for handling the data can arise from the way the data are organized . \n\t', '\n\t\t Once we know the basic data structures , the transactions , and the protocol to be followed ( e.g. , establish caller\x92s identity before exchanging sensitive information ) ; we need only build dialogue models for handling various conversational situations , in order to implement a dialogue system . \n\t', '\n\t\t For our corpus , we have used a modified DAMSL tag set \n\t\t']",Positive
"['\n\t\t The \x93frames\x94 or transactions in our domain are common customer-service tasks : VerifyId , ChangeAddress , InquireBalance , Lost/StolenCard and Make Payment . \n\t', '\n\t\t ( In this context \x93task\x94 and \x93transaction\x94 are synonymous . \n\t', '\n\t\t ) Each frame is associated with attributes or slots that must be filled with values in no particular order during the course of the dialogue ; for example , account number , name , payment amount , etc. 2 Related Work Relevant human-computer dialogue research efforts include the TRAINS project and the DARPA Communicator program . \n\t', '\n\t\t The classic TRAINS natural-language dialogue project \n\t\t']",Negative
"['\n\t\t The US DARPA Communicator program has been instrumental in bringing about practical implementations of spoken dialogue systems . \n\t', '\n\t\t Systems developed under this program include CMU\x92s script-based dialogue manager , in which the travel itinerary is a hierarchical composition of frames \n\t\t']",Positive
"['\n\t\t The AT&T mixed-initiative system uses a sequential decision process model , based on concepts of dialog state and dialog actions \n\t\t']",Positive
['\n\t\t MIT\x92s Mercury flight reservation system uses a dialogue control strategy based on a set of ordered rules as a mechanism to manage complex interactions \n\t\t'],Positive
"['\n\t\t CU\x92s dialogue manager is event-driven , using a set of hierarchical forms with prompts associated with fields in the forms . \n\t', '\n\t\t Decisions are based not on scripts but on current context \n\t\t']",Positive
"['\n\t\t Our data-driven strategy is similar in spirit to that of CU . \n\t', '\n\t\t We take a statistical approach , in which a large body of transcribed , annotated conversations forms the basis for task identification , dialogue act recognition , and form filling for task completion . \n\t', '\n\t\t 3 System Architecture and Components The Amitiés system uses the Galaxy Communicator Software Infrastructure \n\t\t']",Positive
"['\n\t\t Galaxy is a distributed , message-based , hub-and-spoke infrastructure , optimized for spoken dialogue systems . \n\t', '\n\t\t Figure 1. Amitiés System Architecture Components in the Amitiés system ( Figure 1 ) include a telephony server , automatic speech recognizer , natural language understanding unit , dialogue manager , database interface server , response generator , and text-to-speech conversion . \n\t', '\n\t\t 3.1 Audio Components Audio components for the Amitiés system are provided by LIMSI . \n\t', '\n\t\t Because acoustic models have not yet been trained , the current demonstrator system uses a Nuance ASR engine and TTS Vocalizer . \n\t', '\n\t\t To enhance ASR performance , we integrated static GSL ( Grammar Specification Language ) grammar classes provided by Nuance for recognizing several high-frequency items : numbers , dates , money amounts , names and yes-no statements . \n\t', '\n\t\t Training data for the recognizer were collected both from our corpus of human-human dialogues and from dialogues gathered using a text-based version of the human-computer system . \n\t', '\n\t\t Using this version we collected around 100 dialogues and annotated important domain-specific information , as in this example : \x93Hi my name is [ fname ; David ] [ lname ; Oconnor ] and my account number is [ account ; 278 one nine five].\x94 Next we replaced these annotated entities with grammar classes . \n\t', '\n\t\t We also utilized utterances from the Amitiés banking corpus \n\t\t']",Positive
"['\n\t\t These were also used for training the task identifier and the dialogue act classifier ( Section 3.3.2 ) . \n\t', '\n\t\t The training corpus for the recognizer consists of 1744 utterances totaling around 10,000 words . \n\t', '\n\t\t Using tools supplied by Nuance for building recognition packages , we created two speech recognition components : a British model in the UK and an American model at two US sites . \n\t', '\n\t\t For the text to speech synthesizer we used Nuance\x92s Vocalizer 3.0 , which supports multiple languages and accents . \n\t', '\n\t\t We integrated the Vocalizer and the ASR using Nuance\x92s speech and telephony API into a Galaxy-compliant server accessible over a telephone line . \n\t', '\n\t\t 3.2 Natural Language Understanding The goal of the language understanding component is to take the word string output of the ASR module , and identify key semantic concepts relating to the target domain . \n\t', '\n\t\t This is a specialized kind of information extraction application , and as such , we have adapted existing IE technology to this task . \n\t', '\n\t\t Nat\x92l Language Understanding Dialogue Manager Speech Recognition Telephony Server Database Server Hub Text-to-speech Conversion Response Generation Customer Database We have used a modified version of the ANNIE engine ( A Nearly-New IE system ; Cunningham et al. , 2002 ; Maynard , 2003 ) . \n\t', '\n\t\t ANNIE is distributed as the default built-in IE component of the GATE framework \n\t\t']",Positive
"['\n\t\t GATE is a pure Java-based architecture developed over the past eight years in the University of Sheffield Natural Language Processing group . \n\t', '\n\t\t ANNIE has been used for many language processing applications , in a number of languages both European and non-European . \n\t', '\n\t\t This versatility makes it an attractive proposition for use in a multilingual speech processing project . \n\t', '\n\t\t ANNIE includes customizable components necessary to complete the IE task \x96 tokenizer , gazetteer , sentence splitter , part of speech tagger and a named entity recognizer based on a powerful engine named JAPE ( Java Annotation Pattern Engine ; Cunningham et al. , 2000 ) . \n\t', '\n\t\t Given an utterance from the user , the NLU unit produces both a list of tokens for detecting dialogue acts , an important research goal inside this project , and a frame with the possible named entities specified by our application . \n\t', '\n\t\t We are interested particularly in account numbers , credit card numbers , person names , dates , amounts of money , locations , addresses and telephone numbers . \n\t', '\n\t\t In order to recognize these , we have updated the gazetteer , which works by explicit look-up tables of potential candidates , and modified the rules of the transducer engine , which attempts to match new instances of named entities based on local grammatical context . \n\t', '\n\t\t There are some significant differences between the kind of prose text more typically associated with information extraction , and the kind of text we are expecting to encounter . \n\t', '\n\t\t Current models of IE rely heavily on punctuation as well as certain orthographic information , such as capitalized words indicating the presence of a name , company or location . \n\t', '\n\t\t We have access to neither of these in the output of the ASR engine , and so had to retune our processors to data which reflected that . \n\t', '\n\t\t In addition , we created new processing resources , such as those required to spot number units and translate them into textual representations of numerical values ; for example , to take \x93twenty thousand one hundred and fourteen pounds\x94 , and produce \x93£20,114\x94 . \n\t', '\n\t\t The ability to do this is of course vital for the performance of the system . \n\t', '\n\t\t If none of the main entities can be identified from the token string , we create a list of possible fallback entities , in the hope that partial matching would help narrow the search space . \n\t', '\n\t\t For instance , if a six-digit account number is not identified , then the incomplete number recognized in the utterance is used as a fallback entity and sent to the database server for partial matching . \n\t', '\n\t\t Our robust IE techniques have proved invaluable to the efficiency and spontaneity of our data-driven dialogue system . \n\t', '\n\t\t In a single utterance the user is free to supply several values for attributes , prompted or unprompted , allowing tasks to be completed with fewer dialogue turns . \n\t', '\n\t\t 3.3 Dialogue Manager The dialogue manager identifies the goals of the conversation and performs interactions to achieve those goals . \n\t', '\n\t\t Several \x93Frame Agents\x94 , implemented within the dialogue manager , handle tasks such as verifying the customer\x92s identity , identifying the customer\x92s desired transaction , and executing those transactions . \n\t', '\n\t\t These range from a simple balance inquiry to the more complex change of address and debit-card payment . \n\t', '\n\t\t The structure of the dialogue manager is illustrated in Figure 2 . \n\t', '\n\t\t Rather than depending on a script for the progression of the dialogue , the dialogue manager takes a data-driven approach , allowing the caller to take the initiative . \n\t', '\n\t\t Completing a task depends on identifying that task and filling values in frames , but this may be done in a variety of ways : one at a time , or several at once , and in any order . \n\t', '\n\t\t For example , if the customer identifies himself or herself before stating the transaction , or even if he or she provides several pieces of information in one utterance\x97transaction , name , account number , payment amount\x97the dialogue manager is flexible enough to move ahead after these variations . \n\t', '\n\t\t Prompts for attributes , if needed , are not restricted to one at a time , but they are usually combined in the way human agents request them ; for example , city and county , expiration date and issue number , birthdate and telephone number . \n\t', '\n\t\t Figure 2. Amitiés Dialogue Manager If the system fails to obtain the necessary values from the user , reprompts are used , but no more than once for any single attribute . \n\t', '\n\t\t For the customer verification task , different attributes may be Input : from NLU via Hub ( token string language id , named entities ) Dialogue Act Classifier Frame Agent External files , domain-specific Task Execution Frame Agents Task ID Frame Agent Task info Dialogue History Response Decision Verify-Caller Frame Agent via Hub DB Server Customer Database requested . \n\t', '\n\t\t If the system fails even after reprompts , it will gracefully give up with an explanation such as , \x93I\x92m sorry , we have not been able to obtain the information necessary to update your address in our records . \n\t', '\n\t\t Please hold while I transfer you to a customer service representative.\x94 3.3.1 Task ID Frame Agent For task identification , the Amitiés team has made use of the data collected in over 500 conversations from a British call center , recorded , transcribed , and annotated . \n\t', '\n\t\t Adapting a vector- based approach reported by \n\t\t']",Positive
"['\n\t\t Tasks are represented as vectors of terms , built from the utterances requesting them . \n\t', ""\n\t\t Some examples of labeled utterances are : \x93Erm I 'd like to cancel the account cover premium that 's on my , appeared on my statement\x94 [ CancelInsurance ] and \x93Erm just to report a lost card please\x94 [ Lost/StolenCard ] . \n\t"", '\n\t\t The training process proceeds as follows : 1 . \n\t', '\n\t\t Begin with corpus of transcribed , annotated calls . \n\t', '\n\t\t 2. Document creation : For each transaction , collect raw text of callers\x92 queries . \n\t', '\n\t\t Yield : one \x93document\x94 for each transaction ( about 14 of these in our corpus ) . \n\t', '\n\t\t 3. Text processing : Remove stopwords , stem content words , weight terms by frequency . \n\t', '\n\t\t Yield : one \x93document vector\x94 for each task . \n\t', '\n\t\t 4. Compare queries and documents : Create \x93query vectors.\x94 Obtain a cosine similarity score for each query/document pair . \n\t', '\n\t\t Yield : cosine scores/routing values for each query/document pair . \n\t', '\n\t\t 5. Obtain coefficients for scoring : Use binary logistic regression . \n\t', '\n\t\t Yield : a set of coefficients for each task . \n\t', '\n\t\t Next , the Task ID Frame Agent is tested on unseen utterances or queries : 1 . \n\t', '\n\t\t Begin with one or more user queries . \n\t', '\n\t\t 2. Text processing : Remove stopwords , stem content words , weight terms ( constant weights ) . \n\t', '\n\t\t Yield : \x93query vectors\x94 . \n\t', '\n\t\t 3. Compare each query with each document . \n\t', '\n\t\t Yield : cosine similarity scores . \n\t', '\n\t\t 4. Compute confidence scores ( use training coefficients ) . \n\t', '\n\t\t Yield : confidence scores , representing the system\x92s confidence that the queries indicate the user\x92s choice of a particular transaction . \n\t', '\n\t\t Tests performed over the entire corpus , 80 % of which was used for training and 20 % for testing , resulted in a classification accuracy rate of 85 % ( correct task is one of the system\x92s top 2 choices ) . \n\t', '\n\t\t The accuracy rate rises to 93 % when we eliminate confusing or lengthy utterances , such as requests for information about payments , statements , and general questions about a customer\x92s account . \n\t', '\n\t\t These can be difficult even for human annotators to classify . \n\t', '\n\t\t 3.3.2 Dialogue Act Classifier The purpose of the DA Classifier Frame Agent is to identify a caller\x92s utterance as one or more domain-independent dialogue acts . \n\t', '\n\t\t These include Accept , Reject , Non-understanding , Opening , Closing , Backchannel , and Expression . \n\t', '\n\t\t Clearly , it is useful for a dialogue system to be able to identify accurately the various ways a person may say \x93yes\x94 , \x93no\x94 , or \x93what did you say?\x94 As with the task identifier , we have trained the DA classifier on our corpus of transcribed , labeled human-human calls , and we have used vector- based classification techniques . \n\t', '\n\t\t Two differences from the task identifier are 1 ) an utterance may have multiple correct classifications , and 2 ) a different stoplist is necessary . \n\t', '\n\t\t Here we can filter out the usual stops , including speech dysfluencies , proper names , number words , and words with digits ; but we need to include words such as yeah , uh-huh , hi , ok , thanks , pardon and sorry . \n\t', '\n\t\t Some examples of DA classification results are shown in Figure 3 . \n\t', '\n\t\t For sure , ok , the classifier returns the categories Backchannel , Expression and Accept . \n\t', '\n\t\t If the dialogue manager is looking for either Accept or Reject , it can ignore Backchannel and Expression in order to detect the correct classification . \n\t', '\n\t\t In the case of certainly not , the first word has a strong tendency toward Accept , though both together constitute a Reject act . \n\t', '\n\t\t Text : \x93sure , okay\x94 Text : \x93certainly not\x94 Back . \n\t', '\n\t\t Expresson 7 0.8 0.6 0.4 0.2 0 Cosing 0.6 0.5 0.4 0.3 0.2 0.7 0 Reject Accept Reject-part Accept Express~on Top four cosine scores Top four cosine scores 0.7 Back . \n\t', '\n\t\t 0.7 Reject 0.~ 0.6 0.5 0.4 0.3 0.2 0.7 0.~ 0.4 Expression 0.3 0.2 0.7 Axeot Expression Accept Reject-part Closing 0 0 Confidence scores Confidence scores Categories returned : Backchannel , Categories returned : Expression , Accept Reject , Accept Figure 3. DA Classification examples Our classifier performs well if the utterance is short and falls into one of the selected categories ( 86 % accuracy on the British data ) ; and it has the advantages of automatic training , domain independence , and the ability to capture a great variety of expressions . \n\t', '\n\t\t However , it can be inaccurate when applied to longer utterances , and it is not yet equipped to handle domain-specific assertions , questions , or queries about a transaction . \n\t', '\n\t\t 3.4 Database Manager Our system identifies users by matching information provided by the caller against a database of user information . \n\t', '\n\t\t It assumes that the speech recognizer will make errors when the caller attempts to identify himself . \n\t', '\n\t\t Therefore perfect matches with the database entries will be rare . \n\t', '\n\t\t Consequently , for each record in the database , we attach a measure of the probability that the record is the target record . \n\t', '\n\t\t Initially , these measures are estimates of the probability that this individual will call . \n\t', '\n\t\t When additional identifying information arrives , the system updates these probabilities using Bayes\x92 rule . \n\t', '\n\t\t Thus , the system might begin with a uniform probability estimate across all database records . \n\t', '\n\t\t If the user identifies herself with a name recognized by the machine as \x93Smith\x94 , the system will appropriately increment the probabilities of all entries with the name \x93Smith\x94 and all entries that are known to be confused with \x93Smith\x94 in proportion to their observed rate of substitution . \n\t', '\n\t\t Of course , all records not observed to be so confusable would similarly have their probabilities decreased by Bayes\x92 rule . \n\t', '\n\t\t When enough information has come in to raise the probability for some record above a threshold ( in our system 0.99 probability ) , the system assumes that the caller has been correctly identified . \n\t', '\n\t\t The designer may choose to include a verification dialog , but our decision was to minimize such interactions to shorten the calls . \n\t', '\n\t\t Our error-correcting database system receives tokens with an identification of what field each token should represent . \n\t', '\n\t\t The system processes the tokens serially . \n\t', '\n\t\t Each represents an observation made by the speech recognizer . \n\t', '\n\t\t To process a token , the system examines each record in the database and updates the probability that the record is the target record using Bayes\x92 rule : P(obs | rec ) × P(rec) P(obs) where rec is the event where the record under consideration is the target record . \n\t', '\n\t\t As is common in Bayes\x92 rule calculations , the denominator P(obs) is treated as a scaling factor , and is not calculated explicitly . \n\t', '\n\t\t All probabilities are renormalized at the end of the update of all of the records . \n\t', '\n\t\t P(rec) is the previous estimate of the probability that the record is the target record . \n\t', '\n\t\t P(obs|rec) is the probability that the recognizer returned the observation that it did given that the target record is the current record under examination . \n\t', '\n\t\t For some of the fields , such as the account number and telephone number , the user responses consist of digits . \n\t', '\n\t\t We collected data on the probability that the speech recognition system we are using mistook one digit for another and calculated the values for P(obs|rec) from the data . \n\t', '\n\t\t For fields involving place names and personal names , the probabilities were estimated . \n\t', '\n\t\t Once a record has been selected ( by virtue of its probability being greater than the threshold ) the system compares the individual fields of the record with values obtained by the speech recognizer . \n\t', '\n\t\t If the values differ greatly , as measured by their Levenshtein distance , the system returns the field name to the dialogue manager as a candidate for additional verification . \n\t', '\n\t\t If no record meets the threshold probability criterion , the system returns the most probable record to the dialogue manager , along with the fields which have the greatest Levenshtein distance between the recognized and actual values , as candidates for reprompting . \n\t', '\n\t\t Our database contains 100 entries for the system tests described in this paper . \n\t', '\n\t\t We describe the system in a more demanding environment with one million records in \n\t\t']",Positive
"['\n\t\t In that project , we required all information to be entered by spelling the items out so that the vocabulary was limited to the alphabet plus the ten digits . \n\t', '\n\t\t In the current project , with fewer names to deal with , we allowed the complete vocabulary of the domain : names , streets , counties , and so forth . \n\t', '\n\t\t 3.5 Response Generator Our current English-only system preserves the language-independent features of our original trilingual generator , storing all language- and domain-specific information in separate text files . \n\t', '\n\t\t It is a template-based system , easily modified and extended . \n\t', '\n\t\t The generator constructs utterances according to the dialogue manager\x92s specification of one or more speech acts ( prompt , request , confirm , respond , inform , backchannel , accept , reject ) , repetition numbers , and optional lists of attributes , values , and/or the person\x92s name . \n\t', '\n\t\t As far as possible , we modeled utterances after the human-human dialogues . \n\t', '\n\t\t For a more natural-sounding system , we collected variations of the utterances , which the generator selects at random . \n\t', '\n\t\t Requests , for example , may take one of twelve possible forms : Request , part 1 of 2 : Can you just confirm | Can I have | Can I take | What is | What\x92s | May I have P(rec |obs ) Request , part 2 of 2.\x95 [ list of attributes ] , [ person name ] ? \n\t', '\n\t\t | [ list of attributes ] , please ? \n\t', '\n\t\t Offers to close or continue the dialogue are similarly varied : Closing offer , part 1 of 2.\x95 Is there anything else | Anything else | Is there anything else at all Closing offer , part 2 of 2.\x95 I can do for you today ? \n\t', '\n\t\t | I can help you with today ? \n\t', '\n\t\t | I can do for you ? \n\t', '\n\t\t | I can help you with ? \n\t', '\n\t\t | you need today ? \n\t', '\n\t\t | you need ? \n\t', '\n\t\t 4 Preliminary Evaluation Ten native speakers of English , 6 female and 4 male , were asked to participate in a preliminary in- lab system evaluation ( half in the UK and half in the US ) . \n\t', '\n\t\t The Amitiés system developers were not among these volunteers . \n\t', '\n\t\t Each made 9 phone calls to the system from behind a closed door , according to scenarios designed to test various customer identities as well as single or multiple tasks . \n\t', '\n\t\t After each call , participants filled out a questionnaire to register their degree of satisfaction with aspects of the interaction . \n\t', '\n\t\t Overall call success was 70 % , with 98 % successful completions for the VerifyId and 96 % for the CheckBalance subtasks ( Figure 4 ) . \n\t', '\n\t\t \x93Failures\x94 were not system crashes but simulated transfers to a human agent . \n\t', '\n\t\t There were 5 user terminations . \n\t', '\n\t\t Average word error rates were 17 % for calls that were successfully completed , and 22 % for failed calls . \n\t', '\n\t\t Word error rate by user ranged from 11 % to 26 % . \n\t', '\n\t\t Figure 4 . \n\t', '\n\t\t Task Completion Rates Call duration was found to reflect the complexity of each scenario , where complexity is defined as the number of \x93concepts\x94 needed to complete each task . \n\t', '\n\t\t The following items are judged to be concepts : task identification ; values such as first name , last name , house number , street and phone number ; and positive or negative responses such as whether a new card is desired . \n\t', '\n\t\t Figures 5 and 6 illustrate the relationship between length of call and task complexity . \n\t', '\n\t\t It should be noted that customer verification , a task performed in every dialogue , requires a minimum of 3 personal details to be verified against a database record , but may require more in the case of recognition errors . \n\t', '\n\t\t The overall average number of turns per dialogue was 18.28 . \n\t', '\n\t\t The user spoke an average of 6.89 words per turn and the system 11.42 . \n\t', '\n\t\t User satisfaction for each call was assessed by way of a questionnaire containing five statements . \n\t', '\n\t\t These covered the clarity of the instructions , ease of doing the task , how well the system understands the caller , how well the system works , and the caller\x92s enjoyment of the system . \n\t', '\n\t\t Participants rated each on a five-point Likert scale . \n\t', '\n\t\t Summed results showed an average score of 20.45 over all users ( range 5\x9625 ; higher = stronger agreement ) . \n\t', '\n\t\t Figures 5 and 6 . \n\t', '\n\t\t Scenario Complexity ( top ) and Average Call Duration by Scenario ( in seconds ) Although user satisfaction was high , we were more interested in identifying the major problems for the callers . \n\t', '\n\t\t Users were often frustrated by recognition failures and/or unsuccessful attempts to capture values such as a new street address , county , or phone number . \n\t', '\n\t\t Sometimes the system failed to determine that the user was finished . \n\t', '\n\t\t Because the system is designed to be nonrestrictive in accepting users\x92 input , misidentifications were common . \n\t', '\n\t\t One user spoke for many by saying , \x93There were times when the system would ask me to repeat information I had not yet given to it.\x94 We plan to revise our strategy 1.20 1.00 0.80 0.60 0.40 0.20 0.00 0.85 0.70 0.57 0.98 0.96 non 0.88 ` . \n\t', '\n\t\t `` 1 2 3 4 5 6 7 8 9 Scenario 12 10 8 6 4 2 0 1 2 3 4 5 6 7 8 9 160.0 140.0 120.0 100.0 80.0 60.0 40.0 20.0 0.0 135.8 127.1 11 9.3 109.3 88.3 85.0 106.7114.0 48.4 so that we are a little more cautious in our reprompts . \n\t', '\n\t\t Occasionally , the system misidentified the user\x92s desired task and had difficulty backing off gracefully and starting the correct task . \n\t', '\n\t\t We are working on improving our recovery strategies for these cases . \n\t', '\n\t\t The following transcription of an interaction with our system illustrates the efficiency made possible when the data supplied by the caller drives the conversation : AMITIÉS : Hello , this is Am-ih-tee-eh ! \n\t', '\n\t\t What can I do for you ? \n\t', '\n\t\t USER : yes this is Stella Lang . \n\t', '\n\t\t My card number is three one six seven one four . \n\t', '\n\t\t Can you tell me the balance on my account please ? \n\t', '\n\t\t AMITIÉS : Can you just confirm the city and county ? \n\t', '\n\t\t USER : uh yes it\x92s London , Middlesex AMITIÉS : I\x92m sorry , I didn\x92t understand what you said . \n\t', '\n\t\t Can you repeat , please ? \n\t', '\n\t\t USER : London , Middlesex AMITIÉS : All right , your balance is 1753 pounds and 20 pence . \n\t', '\n\t\t Anything else I can do for you ? \n\t', ""\n\t\t USER : yes I 'd like to make a payment of fifty three pounds with my debit card please AMITIÉS : Can I have the debit card number , Miss Lang ? \n\t"", ""\n\t\t USER : sure , it 's two eight seven four six six four nine five eight nine five three eight nine one , and it expires October two thousand six AMITIÉS : What 's the issue number , please ? \n\t"", ""\n\t\t USER : one AMITIÉS : Ok , Miss Lang , I 'll send in the payment amount of 53 pounds . \n\t"", '\n\t\t Did I get that right ? \n\t', ""\n\t\t USER : yes , what 's my new balance please ? \n\t"", ""\n\t\t AMITIÉS : Ok , that 's gone through . \n\t"", '\n\t\t Your payment will be credited overnight and will reflect on your next statement . \n\t', '\n\t\t Your balance is 1700 pounds and 20 pence . \n\t', '\n\t\t Anything else I can do for you today ? \n\t', ""\n\t\t USER : no , that 's it thanks AMITIÉS : Thanks very much for your call , goodbye . \n\t"", '\n\t\t USER : bye 5 Discussion , Future Work The preliminary evaluation reported here indicates promise for an automated dialogue system such as ours , which incorporates robust techniques for information extraction , record matching , task identification , dialogue act classification , and an overall data-driven strategy . \n\t', '\n\t\t Task duration and number of turns per dialogue both appear to indicate greater efficiency and corresponding user satisfaction than many other similar systems . \n\t', '\n\t\t In the DARPA Communicator evaluation , for example , between 60 and 79 calls were made to each of 8 participating sites ( Walker , et al. , 2001 , 2002 ) . \n\t', '\n\t\t A sample scenario for a domestic round-trip flight contained 8 concepts ( airline , departure city , state , date , etc. ) . \n\t', '\n\t\t The average duration for such a call was over 300 seconds ; whereas our overall average was 104 seconds . \n\t', '\n\t\t ASR accuracy rates in 2001 were about 60 % and 75 % , for airline itineraries not completed and completed ; and task completion rates were 56 % . \n\t', '\n\t\t Our average number of user words per turn , 6.89 , is also higher than that reported for Communicator systems . \n\t', '\n\t\t This number seems to reflect lengthier responses to open prompts , responses to system requests for multiple attributes , and greater user initiative . \n\t', '\n\t\t We plan to port the system to a new domain : from telephone banking to information-technology support . \n\t', '\n\t\t As part of this effort we are again collecting data from real human-human calls . \n\t', '\n\t\t For advanced speech recognition , we hope to train our ASR on new acoustic data . \n\t', '\n\t\t We also plan to expand our dialogue act classification so that the system can recognize more types of acts , and to improve our classification reliability . \n\t', '\n\t\t 6 Acknowledgements This paper is based on work supported in part by the European Commission under the 5th Framework IST/HLT Programme , and by the US Defense Advanced Research Projects Agency . \n\t', '\n\t\t References J. Allen and M. Core . \n\t', '\n\t\t 1997. Draft of DAMSL : Dialog Act Markup in Several Layers . \n\t', '\n\t\t http://www.cs.rochester.edu/research/cisd/resour ces/damsl/ . \n\t', '\n\t\t J. Allen , L. K. Schubert , G. Ferguson , P. Heeman , Ch. L. Hwang , T. Kato , M . \n\t', '\n\t\t Light , N. G. Martin , B. W. Miller , M. Poesio , and D. R. Traum . \n\t', '\n\t\t 1995. The TRAINS Project : A Case Study in Building a Conversational Planning Agent . \n\t', '\n\t\t Journal of Experimental and Theoretical AI , 7 ( 1995 ) , 7\x9648 . \n\t', '\n\t\t Amitiés , http://www.dcs.shef.ac.uk/nlp/amities . \n\t', '\n\t\t J. Chu-Carroll and B. Carpenter . \n\t', '\n\t\t 1999. Vector- Based Natural Language Call Routing . \n\t', '\n\t\t Computational Linguistics , 25 ( 3 ) : 361\x96388 . \n\t', '\n\t\t H. Cunningham , D. Maynard , K. Bontcheva , V. Tablan . \n\t', '\n\t\t 2002. GATE : A Framework and Graphical Development Environment for Robust NLP Tools and Applications . \n\t', ""\n\t\t Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics ( ACL'02 ) , Philadelphia , Pennsylvania . \n\t"", '\n\t\t H. Cunningham and D. Maynard and V. Tablan . \n\t', '\n\t\t 2000. JAPE : a Java Annotation Patterns Engine ( Second Edition ) . \n\t', '\n\t\t Technical report CS--00--10 , University of Sheffield , Department of Computer Science . \n\t', '\n\t\t DARPA , http://www.darpa.mil/iao/Communicator.htm . \n\t', '\n\t\t H. Hardy , K. Baker , L. Devillers , L. Lamel , S. Rosset , T. Strzalkowski , C. Ursu and N. Webb . \n\t', '\n\t\t 2002. Multi-Layer Dialogue Annotation for Automated Multilingual Customer Service . \n\t', '\n\t\t Proceedings of the ISLE Workshop on Dialogue Tagging for Multi-Modal Human Computer Interaction , Edinburgh , Scotland . \n\t', '\n\t\t H. Hardy , T. Strzalkowski and M. Wu . \n\t', '\n\t\t 2003a . \n\t', '\n\t\t Dialogue Management for an Automated Multilingual Call Center . \n\t', '\n\t\t Research Directions in Dialogue Processing , Proceedings of the HLTNAACL 2003 Workshop , Edmonton , Alberta , Canada . \n\t', '\n\t\t H. Hardy , K. Baker , H. Bonneau-Maynard , L. Devillers , S. Rosset and T. Strzalkowski . \n\t', '\n\t\t 2003b . \n\t', '\n\t\t Semantic and Dialogic Annotation for Automated Multilingual Customer Service . \n\t', '\n\t\t Eurospeech 2003 , Geneva , Switzerland . \n\t', '\n\t\t R. B. Inouye , A. Biermann and A. Mckenzie . \n\t', '\n\t\t 2004. Caller Identification from Spelled-Out Personal Data Using a Database for Error Correction . \n\t', '\n\t\t Duke University Internal Report . \n\t', '\n\t\t E. Levin , S. Narayanan , R. Pieraccini , K. Biatov , E. Bocchieri , G. Di Fabbrizio , W. Eckert , S. Lee , A. Pokrovsky , M. Rahim , P. Ruscitti , and M. Walker . \n\t', '\n\t\t 2000. The AT&T-DARPA Communicator Mixed-Initiative Spoken Dialog System . \n\t', '\n\t\t ICSLP 2000 . \n\t', '\n\t\t D. Maynard . \n\t', '\n\t\t 2003. Multi-Source and Multilingual Information Extraction . \n\t', '\n\t\t Expert Update . \n\t', '\n\t\t S. Seneff , E. Hurley , R. Lau , C. Pao , P. Schmid , and V. Zue . \n\t', '\n\t\t 1998. Galaxy-II : A Reference Architecture for Conversational System Development . \n\t', '\n\t\t ICSLP 98 , Sydney , Australia . \n\t', '\n\t\t S. Seneff and J. Polifroni . \n\t', '\n\t\t 2000. Dialogue Management in the Mercury Flight Reservation System . \n\t', '\n\t\t Satellite Dialogue Workshop , ANLPNAACL , Seattle , Washington . \n\t', '\n\t\t M. Walker , J. Aberdeen , J. Boland , E. Bratt , J. Garofolo , L. Hirschman , A. Le , S. Lee , S. Narayanan , K. Papineni , B. Pellom , J. Polifroni , A. Potamianos , P. Prabhu , A. Rudnicky , G. Sanders , S. Seneff , D. Stallard and S. Whittaker . \n\t', '\n\t\t 2001. DARPA Communicator Dialog Travel Planning Systems : The June 2000 Data Collection . \n\t', '\n\t\t Eurospeech 2001 . \n\t', '\n\t\t M. Walker , A. Rudnicky , J. Aberdeen , E. Bratt , J. Garofolo , H. Hastie , A. Le , B. Pellom , A. Potamianos , R. Passonneau , R. Prasad , S. Roukos , G. Sanders , S. Seneff and D. Stallard . \n\t', '\n\t\t 2002. DARPA Communicator Evaluation : Progress from 2000 to 2001 . \n\t', '\n\t\t ICSLP 2002 . \n\t', '\n\t\t W. Ward and B. Pellom . \n\t', '\n\t\t 1999. The CU Communicator System . \n\t', '\n\t\t IEEE ASRU , pp. 341\x96 344 . \n\t', '\n\t\t W. Xu and A. Rudnicky . \n\t', '\n\t\t 2000. Task-based Dialog Management Using an Agenda . \n\t', '\n\t\t ANLP/NAACL Workshop on Conversational Systems , pp. 42\x96 47. \n\t', '\n\t\t Trainable Sentence Planning for Complex Information Presentation in Spoken Dialog Systems Amanda Stent Stony Brook University Stony Brook , NY 11794 U.S.A. stent@cs.sunysb.edu Rashmi Prasad University of Pennsylvania Philadelphia , PA 19104 U.S.A. rjprasad@linc.cis.upenn.edu Marilyn Walker University of Sheffield Sheffield S1 4DP U.K. M.A.Walker@sheffield.ac.uk Abstract A challenging problem for spoken dialog systems is the design of utterance generation modules that are fast , flexible and general , yet produce high quality output in particular domains . \n\t', '\n\t\t A promising approach is trainable generation , which uses general-purpose linguistic knowledge automatically adapted to the application domain . \n\t', '\n\t\t This paper presents a trainable sentence planner for the MATCH dialog system . \n\t', '\n\t\t We show that trainable sentence planning can produce output comparable to that of MATCH\x92s template-based generator even for quite complex information presentations . \n\t', '\n\t\t 1 Introduction One very challenging problem for spoken dialog systems is the design of the utterance generation module . \n\t', '\n\t\t This challenge arises partly from the need for the generator to adapt to many features of the dialog domain , user population , and dialog context . \n\t', '\n\t\t There are three possible approaches to generating system utterances . \n\t', '\n\t\t The first is template- based generation , used in most dialog systems today . \n\t', '\n\t\t Template-based generation enables a programmer without linguistic training to program a generator that can efficiently produce high quality output specific to different dialog situations . \n\t', '\n\t\t Its drawbacks include the need to ( 1 ) create templates anew by hand for each application ; ( 2 ) design and maintain a set of templates that work well together in many dialog contexts ; and ( 3 ) repeatedly encode linguistic constraints such as subject-verb agreement . \n\t', '\n\t\t The second approach is natural language generation ( NLG ) , which divides generation into : ( 1 ) text ( or content ) planning , ( 2 ) sentence planning , and ( 3 ) surface realization . \n\t', '\n\t\t NLG promises portability across domains and dialog contexts by using general rules for each generation module . \n\t', '\n\t\t However , the quality of the output for a particular domain , or a particular dialog context , may be inferior to that of a template- based system unless domain-specific rules are developed or general rules are tuned for the particular domain . \n\t', '\n\t\t Furthermore , full NLG may be too slow for use in dialog systems . \n\t', '\n\t\t A third , more recent , approach is trainable generation : techniques for automatically training NLG modules , or hybrid techniques that adapt NLG modules to particular domains or user groups , e.g. \n\t\t']",Positive
"['\n\t\t Open questions about the trainable approach include ( 1 ) whether the output quality is high enough , and ( 2 ) whether the techniques work well across domains . \n\t', '\n\t\t For example , the training method used in SPoT ( Sentence Planner Trainable ) , as described in \n\t\t']",Positive
['\n\t\t This paper describes trainable sentence planning for information presentation in the MATCH ( Multimodal Access To City Help ) dialog system \n\t\t'],Positive
"['\n\t\t We provide evidence that the trainable approach is feasible by showing ( 1 ) that the training technique used for SPoT can be extended to a new domain ( restaurant information ) ; ( 2 ) that this technique , previously used for information- gathering utterances , can be used for information presentations , namely recommendations and comparisons ; and ( 3 ) that the quality of the output is comparable to that of a template-based generator previously developed and experimentally evaluated with MATCH users \n\t\t']",Positive
"['\n\t\t Section 2 describes SPaRKy ( Sentence Planning with Rhetorical Knowledge ) , an extension of SPoT that uses rhetorical relations . \n\t', '\n\t\t SPaRKy consists of a randomized sentence plan generator ( SPG ) and a trainable sentence plan ranker ( SPR ) ; these are described in Sections 3 strategy : recommend items : Chanpen Thai relations justify ( nuc:1 ; sat : 2 ) ; justify ( nuc:1;sat:3 ) ; jus- tify(nuc:1;sat:4) content : 1. assert ( best ( Chanpen Thai ) ) 2. assert ( has-att ( Chanpen Thai , decor(decent))) 3. assert ( has-att ( Chanpen Thai , service(good)) 4. assert ( has-att ( Chanpen Thai , cuisine(Thai))) Figure 1 : A content plan for a recommendation for a restaurant in midtown Manhattan strategy:compare3 items : Above , Carmine\x92s relations :elaboration(1;2) ; elaboration ( 1;3 ) ; elabora- tion(1,4) ; elaboration ( 1,5 ) ; elaboration(1,6) ; elaboration(1,7) ; contrast(2;3) ; contrast(4;5) ; contrast(6;7) content : 1. assert ( exceptional ( Above , Carmine\x92s ) ) 2. assert ( has-att ( Above , decor(good))) 3. assert ( has-att ( Carmine\x92s , decor(decent))) 4. assert ( has-att ( Above , service(good))) 5. assert ( has-att ( Carmine\x92s , service(good))) 6. assert ( has-att ( Above , cuisine(New American ) ) ) 7. assert ( has-att ( Carmine\x92s , cuisine(italian))) Figure 2 : A content plan for a comparison between restaurants in midtown Manhattan and 4 . \n\t', '\n\t\t Section 5 presents the results of two experiments . \n\t', '\n\t\t The first experiment shows that given a content plan such as that in Figure 1 , SPaRKy can select sentence plans that communicate the desired rhetorical relations , are significantly better than a randomly selected sentence plan , and are on average less than 10 % worse than a sentence plan ranked highest by human judges . \n\t', '\n\t\t The second experiment shows that the quality of SPaRKy\x92s output is comparable to that of MATCH\x92s template-based generator . \n\t', '\n\t\t We sum up in Section 6 . \n\t', '\n\t\t 2 SPaRKy Architecture Information presentation in the MATCH system focuses on user-tailored recommendations and comparisons of restaurants \n\t\t']",Positive
['\n\t\t Following the bottom-up approach to text-planning described in \n\t\t'],Positive
"['\n\t\t Example content plans are shown in Figures 1 and 2 . \n\t', '\n\t\t The job of the sentence planner is to choose linguistic resources to realize a content plan and then rank the resulting alternative realizations . \n\t', '\n\t\t Figures 3 and 4 show alternative realizations for the content plans in Figures 1 and 2 . \n\t', '\n\t\t Alt Realization H SPR 2 Chanpen Thai , which is a Thai restau- rant , has decent decor . \n\t', '\n\t\t It has good service . \n\t', '\n\t\t It has the best overall quality among the selected restaurants . \n\t', '\n\t\t 3 .28 5 Since Chanpen Thai is a Thai restau- rant , with good service , and it has decent decor , it has the best overall quality among the selected restaurants . \n\t', '\n\t\t 2.5 .14 6 Chanpen Thai , which is a Thai restau- rant , with decent decor and good service , has the best overall quality among the selected restaurants . \n\t', '\n\t\t 4 .70 Figure 3 : Some alternative sentence plan realizations for the recommendation in Figure 1. H = Humans\x92 score . \n\t', '\n\t\t SPR = SPR\x92s score . \n\t', '\n\t\t Alt Realization H SPR 11 Above and Carmine\x92s offer exceptional value among the selected restaurants . \n\t', '\n\t\t Above , which is a New American restaurant , with good decor , has good service . \n\t', '\n\t\t Carmine\x92s , which is an Italian restaurant , with good service , has decent decor . \n\t', '\n\t\t 2 .73 12 Above and Carmine\x92s offer exceptional value among the selected restaurants . \n\t', '\n\t\t Above has good decor , and Carmine\x92s has decent decor . \n\t', '\n\t\t Above and Carmine\x92s 2.5 .50 have good service . \n\t', '\n\t\t Above is a New American restaurant . \n\t', '\n\t\t On the other hand , Carmine\x92s is an Italian restaurant . \n\t', '\n\t\t 13 Above and Carmine\x92s offer exceptional value among the selected restaurants . \n\t', '\n\t\t Above is a New American restaurant . \n\t', '\n\t\t It has good decor . \n\t', '\n\t\t It has good service . \n\t', '\n\t\t Carmine\x92s , which is an Italian restaurant , has decent decor and good service . \n\t', '\n\t\t 3 .67 20 Above and Carmine\x92s offer exceptional value among the selected restaurants . \n\t', '\n\t\t Carmine\x92s has decent decor but Above has good decor , and Carmine\x92s and Above have good service . \n\t', '\n\t\t Carmine\x92s is an Italian restaurant . \n\t', '\n\t\t Above , however , is a New American restaurant . \n\t', '\n\t\t 2.5 .49 25 Above and Carmine\x92s offer exceptional value among the selected restaurants . \n\t', '\n\t\t Above has good decor . \n\t', '\n\t\t Carmine\x92s is an Italian restaurant . \n\t', '\n\t\t Above has good service . \n\t', '\n\t\t Carmine\x92s has decent decor . \n\t', '\n\t\t Above is a New American restaurant . \n\t', '\n\t\t Carmine\x92s has good service . \n\t', '\n\t\t NR NR Figure 4 : Some of the alternative sentence plan realizations for the comparison in Figure 2. H = Humans\x92 score . \n\t', '\n\t\t SPR = SPR\x92s score . \n\t', '\n\t\t NR = Not generated or ranked The architecture of the spoken language generation module in MATCH is shown in Figure 5 . \n\t', '\n\t\t The dialog manager sends a high-level communicative goal to the SPUR text planner , which selects the content to be communicated using a user model and brevity constraints ( see ( Walker DIALOGUE MANAGER Communicative Goals Speech Synthesizer SYSTEM UTTERANCE Figure 5 : A dialog system with a spoken language generator et al. , 2002 ) ) . \n\t', '\n\t\t The output is a content plan for a recommendation or comparison such as those in Figures 1 and 2 . \n\t', '\n\t\t SPaRKy , the sentence planner , gets the content plan , and then a sentence plan generator ( SPG ) generates one or more sentence plans ( Figure 7 ) and a sentence plan ranker ( SPR ) ranks the generated plans . \n\t', '\n\t\t In order for the SPG to avoid generating sentence plans that are clearly bad , a content-structuring module first finds one or more ways to linearly order the input content plan using principles of entity-based coherence based on rhetorical relations \n\t\t']",Positive
"['\n\t\t It outputs a set of text plan trees ( tp-trees ) , consisting of a set of speech acts to be communicated and the rhetorical relations that hold between them . \n\t', '\n\t\t For example , the two tp-trees in Figure 6 are generated for the content plan in Figure 2 . \n\t', '\n\t\t Sentence plans such as alternative 25 in Figure 4 are avoided ; it is clearly worse than alternatives 12 , 13 and 20 since it neither combines information based on a restaurant entity ( e.g Babbo ) nor on an attribute ( e.g. decor ) . \n\t', '\n\t\t The top ranked sentence plan output by the SPR is input to the RealPro surface realizer which produces a surface linguistic utterance \n\t\t']",Positive
"['\n\t\t A prosody assignment module uses the prior levels of linguistic representation to determine the appropriate prosody for the utterance , and passes a marked- up string to the text-to-speech module . \n\t', '\n\t\t 3 Sentence Plan Generation As in SPoT , the basis of the SPG is a set of clause-combining operations that operate on tptrees and incrementally transform the elementary predicate-argument lexico-structural representations ( called DSyntS \n\t\t']",Positive
"['\n\t\t The operations are applied in a bottom-up left-to-right fashion and the resulting representation may contain one or more sentences . \n\t', '\n\t\t The application of the operations yields two parallel structures : ( 1 ) a sentence plan tree ( sp-tree ) , a binary tree with leaves labeled by the assertions from the input tp-tree , and interior nodes labeled with clause-combining operations ; and ( 2 ) one or more DSyntS trees ( d-trees ) which reflect the parallel operations on the predicate-argument representations . \n\t', ""\n\t\t We generate a random sample of possible sentence plans for each tp-tree , up to a pre- specified number of sentence plans , by randomly selecting among the operations according to a probability distribution that favors pre- ferred operations ' . \n\t"", '\n\t\t The choice of operation is further constrained by the rhetorical relation that relates the assertions to be combined , as in other work e.g. \n\t\t']",Positive
"['\n\t\t In the current work , three RST rhetorical relations \n\t\t']",Positive
"['\n\t\t We added another relation to be used during the content-structuring phase , called INFER , which holds for combinations of speech acts for which there is no rhetorical relation expressed in the content plan , as in \n\t\t']",Positive
"['\n\t\t By explicitly representing the discourse structure of the information presentation , we can generate information presentations with considerably more internal complexity than those generated in \n\t\t']",Negative
['\n\t\t The clause-combining operations are general operations similar to aggregation operations used in other research \n\t\t'],Positive
"[""\n\t\t The operations and the ' Although the probability distribution here is hand- crafted based on assumed preferences for operations such as MERGE , RELATIVE-CLAUSE and WITH-REDUCTION , it might also be possible to learn this probability distribution from the data by training in two phases . \n\t"", '\n\t\t SPUR Text Planner What to Say Sentence Planner Surface Realizer Prosody Assigner How to Say It elaboration nucleus:<1>assert-com-list_exceptional infer contrast contrast contrast nucleus:<2>assert-com-decor nucleus:<4>assert-com-service nucleus:<6>assert-com-cuisine nucleus:<3>assert-com-decor nucleus:<5>assert-com-service nucleus:<7>assert-com-cuisine elaboration nucleus:<1>assert-com-list_exceptional contrast Figure 6 : Two tp-trees for alternative 13 in Figure 4. nucleus:<5>assert-com-service nucleus:<4>assert-com-service nucleus:<2>assert-com-decor nucleus:<6>assert-com-cuisine nucleus:<3>assert-com-decor nucleus:<7>assert-com-cuisine infer infer constraints on their use are described below . \n\t', '\n\t\t MERGE applies to two clauses with identical matrix verbs and all but one identical arguments . \n\t', '\n\t\t The clauses are combined and the nonidentical arguments coordinated . \n\t', '\n\t\t For example , MERGE(Above has good service ; Carmine\x92s has good service ) yields Above and Carmine\x92s have good service . \n\t', '\n\t\t MERGE applies only for the relations INFER and CONTRAST . \n\t', '\n\t\t WITH-REDUCTION is treated as a kind of \x93verbless\x94 participial clause formation in which the participial clause is interpreted with the subject of the unreduced clause . \n\t', '\n\t\t For example , WITH-REDUCTION(Above is a New American restaurant ; Above has good decor ) yields Above is a New American restaurant , with good decor . \n\t', '\n\t\t WITH-REDUCTION uses two syntactic constraints : ( a ) the subjects of the clauses must be identical , and ( b ) the clause that undergoes the participial formation must have a have- possession predicate . \n\t', '\n\t\t In the example above , for instance , the Above is a New American restaurant clause cannot undergo participial formation since the predicate is not one of have- possession . \n\t', '\n\t\t WITH-REDUCTION applies only for the relations INFER and JUSTIFY . \n\t', '\n\t\t RELATIVE-CLAUSE combines two clauses with identical subjects , using the second clause to relativize the first clause\x92s subject . \n\t', '\n\t\t For example , RELATIVE-CLAUSE(Chanpen Thai is a Thai restaurant , with decent decor and good ser vice ; Chanpen Thai has the best overall quality among the selected restaurants ) yields Chanpen Thai , which is a Thai restaurant , with decent decor and good service , has the best overall quality among the selected restaurants . \n\t', '\n\t\t RELATIVE- CLAUSE also applies only for the relations INFER and JUSTIFY . \n\t', '\n\t\t CUE-WORD inserts a discourse connective ( one of since , however , while , and , but , and on the other hand ) , between the two clauses to be combined . \n\t', '\n\t\t CUE-WORD CONJUNCTION combines two distinct clauses into a single sentence with a coordinating or subordinating conjunction ( e.g. . \n\t', '\n\t\t Above has decent decor BUT Carmine\x92s has good decor ) , while CUE-WORD INSERTION inserts a cue word at the start of the second clause , producing two separate sentences ( e.g. Carmine\x92s is an Italian restaurant . \n\t', '\n\t\t HOWEVER , Above is a New American restaurant ) . \n\t', '\n\t\t The choice of cue word is dependent on the rhetorical relation holding between the clauses . \n\t', '\n\t\t Finally , PERIOD applies to two clauses to be treated as two independent sentences . \n\t', '\n\t\t Note that a tp-tree can have very different realizations , depending on the operations of the SPG . \n\t', '\n\t\t For example , the second tp-tree in Figure 6 yields both Alt 11 and Alt 13 in Figure 4 . \n\t', '\n\t\t However , Alt 13 is more highly rated than Alt 11 . \n\t', '\n\t\t The sp-tree and d-tree produced by the SPG for Alt 13 are shown in Figures 7 and 8 . \n\t', '\n\t\t The composite labels on the interior nodes of the sp- PERIOD\x97elaboration <1>assert-com-list\x97exceptional PERIOD\x97contrast PERIOD\x97infer RELATIVE\x97CLAUSE\x97infer PERIOD\x97infer <4>assert-com-service <7>assert-com-cuisine MERGE\x97infer <6>assert-com-cuisine <2>assert-com-decor <3>assert-come-decor <5>assert-com-service Figure 7 : Sentence plan tree ( sp-tree ) for alternative 13 in Figure 4 PERIOD Figure 8 : Dependency tree ( d-tree ) for alternative 13 in Figure 4 offer PERIOD value among exceptional restaurant selected Above_and_Carmine\x92s BE3 PERIOD HAVE1 Above HAVE1 good HAVE 1 PERIOD New_American Above restaurant Above decor Carmine\x92s BE3 decor decent AND2 service good service Carmine\x92s restaurant good Italian tree indicate the clause-combining relation selected to communicate the specified rhetorical relation . \n\t', '\n\t\t The d-tree for Alt 13 in Figure 8 shows that the SPG treats the PERIOD operation as part of the lexico-structural representation for the d-tree . \n\t', '\n\t\t After sentence planning , the d-tree is split into multiple d-trees at PERIOD nodes ; these are sent to the RealPro surface realizer . \n\t', '\n\t\t Separately , the SPG also handles referring expression generation by converting proper names to pronouns when they appear in the previous utterance . \n\t', '\n\t\t The rules are applied locally , across adjacent sequences of utterances \n\t\t']",Positive
"['\n\t\t Referring expressions are manipulated in the d-trees , either intrasententially during the creation of the sp-tree , or intersententially , if the full sp-tree contains any PERIOD operations . \n\t', '\n\t\t The third and fourth sentences for Alt 13 in Figure 4 show the conversion of a named restaurant ( Carmine\x92s ) to a pronoun . \n\t', '\n\t\t 4 Training the Sentence Plan Ranker The SPR takes as input a set of sp-trees generated by the SPG and ranks them . \n\t', '\n\t\t The SPR\x92s rules for ranking sp-trees are learned from a labeled set of sentence-plan training examples using the RankBoost algorithm \n\t\t']",Positive
"['\n\t\t Examples and Feedback : To apply Rank- Boost , a set of human-rated sp-trees are encoded in terms of a set of features . \n\t', '\n\t\t We started with a set of 30 representative content plans for each strategy . \n\t', '\n\t\t The SPG produced as many as 20 distinct sp-trees for each content plan . \n\t', '\n\t\t The sentences , realized by RealPro from these sp-trees , were then rated by two expert judges on a scale from 1 to 5 , and the ratings averaged . \n\t', '\n\t\t Each sptree was an example input for RankBoost , with each corresponding rating its feedback . \n\t', '\n\t\t Features used by RankBoost : RankBoost requires each example to be encoded as a set of real-valued features ( binary features have values 0 and 1 ) . \n\t', '\n\t\t A strength of RankBoost is that the set of features can be very large . \n\t', '\n\t\t We used 7024 features for training the SPR . \n\t', '\n\t\t These features count the number of occurrences of certain structural configurations in the sp-trees and the d-trees , in order to capture declaratively decisions made by the randomized SPG , as in \n\t\t']",Positive
"['\n\t\t The features were automatically generated using feature templates . \n\t', '\n\t\t For this experiment , we use two classes of feature : ( 1 ) Rule-features : These features are derived from the sp-trees and represent the ways in which MERGE , INFER and CUE- WORD operations are applied to the tp-trees . \n\t', '\n\t\t These feature names start with \x93rule\x94 . \n\t', '\n\t\t ( 2 ) Sent- features : These features are derived from the DSyntSs , and describe the deep-syntactic structure of the utterance , including the chosen lexemes . \n\t', '\n\t\t As a result , some may be domain specific . \n\t', '\n\t\t These feature names are prefixed with \x93sent\x94 . \n\t', '\n\t\t We now describe the feature templates used in the discovery process . \n\t', '\n\t\t Three templates were used for both sp-tree and d-tree features ; two were used only for sp-tree features . \n\t', '\n\t\t Local feature templates record structural configurations local to a particular node ( its ancestors , daughters etc. ) . \n\t', '\n\t\t Global feature templates , which are used only for sp-tree features , record properties of the entire sp-tree . \n\t', '\n\t\t We discard features that occur fewer than 10 times to avoid those specific to particular text plans . \n\t', '\n\t\t Strategy System Min Max Mean S.D. Recommend SPaRKy 2.0 5.0 3.6 .71 HUMAN 2.5 5.0 3.9 .55 RANDOM 1.5 5.0 2.9 .88 Compare2 SPaRKy 2.5 5.0 3.9 .71 HUMAN 2.5 5.0 4.4 .54 RANDOM 1.0 5.0 2.9 1.3 Compare3 SPaRKy 1.5 4.5 3.4 .63 HUMAN 3.0 5.0 4.0 .49 RANDOM 1.0 4.5 2.7 1.0 Table 1 : Summary of Recommend , Compare2 and Compare3 results ( N = 180 ) There are four types of local feature template : traversal features , sister features , ancestor features and leaf features . \n\t', '\n\t\t Local feature templates are applied to all nodes in a sp-tree or d-tree ( except that the leaf feature is not used for d-trees ) ; the value of the resulting feature is the number of occurrences of the described configuration in the tree . \n\t', '\n\t\t For each node in the tree , traversal features record the preorder traversal of the subtree rooted at that node , for all subtrees of all depths . \n\t', '\n\t\t An example is the feature \x93rule traversal assertcom-list exceptional\x94 ( with value 1 ) of the tree in Figure 7 . \n\t', '\n\t\t Sister features record all consecutive sister nodes . \n\t', '\n\t\t An example is the feature \x93rule sisters PERIOD infer RELATIVE CLAUSE infer\x94 ( with value 1 ) of the tree in Figure 7 . \n\t', '\n\t\t For each node in the tree , ancestor features record all the initial subpaths of the path from that node to the root . \n\t', '\n\t\t An example is the feature \x93rule ancestor PERIOD contrast*PERIOD infer\x94 ( with value 1 ) of the tree in Figure 7 . \n\t', '\n\t\t Finally , leaf features record all initial substrings of the frontier of the sp-tree . \n\t', '\n\t\t For example , the sp-tree of Figure 7 has value 1 for the feature \x93leaf #assert-com-list exceptional#assert-comcuisine\x94 . \n\t', '\n\t\t Global features apply only to the sptree . \n\t', '\n\t\t They record , for each sp-tree and for each clause-combining operation labeling a non- frontier node , ( 1 ) the minimal number of leaves dominated by a node labeled with that operation in that tree ( MIN ) ; ( 2 ) the maximal number of leaves dominated by a node labeled with that operation ( MAX ) ; and ( 3 ) the average number of leaves dominated by a node labeled with that operation ( AVG ) . \n\t', '\n\t\t For example , the sp-tree in Figure 7 has value 3 for \x93PERIOD infer max\x94 , value 2 for \x93PERIOD infer min\x94 and value 2.5 for \x93PERIOD infer avg\x94 . \n\t', '\n\t\t 5 Experimental Results We report two sets of experiments . \n\t', '\n\t\t The first experiment tests the ability of the SPR to select a high quality sentence plan from a population of sentence plans randomly generated by the SPG . \n\t', '\n\t\t Because the discriminatory power of the SPR is best tested by the largest possible population of sentence plans , we use 2-fold cross validation for this experiment . \n\t', '\n\t\t The second experiment compares SPaRKy to template-based generation . \n\t', '\n\t\t Cross Validation Experiment : We repeatedly tested SPaRKy on the half of the corpus of 1756 sp-trees held out as test data for each fold . \n\t', '\n\t\t The evaluation metric is the human- assigned score for the variant that was rated highest by SPaRKy for each text plan for each task/user combination . \n\t', '\n\t\t We evaluated SPaRKy on the test sets by comparing three data points for each text plan : HUMAN ( the score of the top-ranked sentence plan ) ; SPARKY ( the score of the SPR\x92s selected sentence ) ; and RANDOM ( the score of a sentence plan randomly selected from the alternate sentence plans ) . \n\t', '\n\t\t We report results separately for comparisons between two entities and among three or more entities . \n\t', '\n\t\t These two types of comparison are generated using different strategies in the SPG , and can produce text that is very different both in terms of length and structure . \n\t', '\n\t\t Table 1 summarizes the difference between SPaRKy , HUMAN and RANDOM for recommendations , comparisons between two entities and comparisons between three or more entities . \n\t', '\n\t\t For all three presentation types , a paired t-test comparing SPaRKy to HUMAN to RANDOM showed that SPaRKy was significantly better than RANDOM ( df = 59 , p < .001 ) and significantly worse than HUMAN ( df = 59 , p < .001 ) . \n\t', '\n\t\t This demonstrates that the use of a trainable sentence planner can lead to sentence plans that are significantly better than baseline ( RANDOM ) , with less human effort than programming templates . \n\t', '\n\t\t Comparison with template generation : For each content plan input to SPaRKy , the judges also rated the output of a template- based generator for MATCH . \n\t', '\n\t\t This template- based generator performs text planning and sentence planning ( the focus of the current paper ) , including some discourse cue insertion , clause combining and referring expression generation ; the templates themselves are described in \n\t\t']",Positive
"['\n\t\t Because the templates are highly tailored to this domain , this generator can be expected to perform well . \n\t', '\n\t\t Example template-based and SPaRKy outputs for a comparison between three or more items are shown in Figure 9 . \n\t', '\n\t\t Strategy System Min Max Mean S.D. Recommend Template 2.5 5.0 4.22 0.74 SPaRKy 2.5 4.5 3.57 0.59 HUMAN 4.0 5.0 4.37 0.37 Compare2 Template 2.0 5.0 3.62 0.75 SPaRKy 2.5 4.75 3.87 0.52 HUMAN 4.0 5.0 4.62 0.39 Compare3 Template 1.0 5.0 4.08 1.23 SPaRKy 2.5 4.25 3.375 0.38 HUMAN 4.0 5.0 4.63 0.35 Table 2 : Summary of template-based generation results . \n\t', '\n\t\t N = 180 Table 2 shows the mean HUMAN scores for the template-based sentence planning . \n\t', '\n\t\t A paired t-test comparing HUMAN and template-based scores showed that HUMAN was significantly better than template-based sentence planning only for compare2 ( df = 29 , t = 6.2 , p < .001 ) . \n\t', '\n\t\t The judges evidently did not like the template for comparisons between two items . \n\t', '\n\t\t A paired t-test comparing SPaRKy and template-based sentence planning showed that template-based sentence planning was significantly better than SPaRKy only for recommendations ( df = 29 , t = 3.55 , p < .01 ) . \n\t', '\n\t\t These results demonstrate that trainable sentence planning shows promise for producing output comparable to that of a template-based generator , with less programming effort and more flexibility . \n\t', '\n\t\t The standard deviation for all three template- based strategies was wider than for HUMAN or SPaRKy , indicating that there may be content-specific aspects to the sentence planning done by SPaRKy that contribute to output variation . \n\t', '\n\t\t The data show this to be correct ; SPaRKy learned content-specific preferences about clause combining and discourse cue insertion that a template-based generator can- System Realization H Template Among the selected restaurants , the fol- lowing offer exceptional overall value . \n\t', '\n\t\t 4.5 Uguale\x92s price is 33 dollars . \n\t', '\n\t\t It has good decor and very good service . \n\t', '\n\t\t It\x92s a French , Italian restaurant . \n\t', '\n\t\t Da Andrea\x92s price is 28 dollars . \n\t', '\n\t\t It has good decor and very good service . \n\t', '\n\t\t It\x92s an Italian restaurant . \n\t', '\n\t\t John\x92s Pizzeria\x92s price is 20 dollars . \n\t', '\n\t\t It has mediocre decor and decent service . \n\t', '\n\t\t It\x92s an Italian , Pizza restaurant . \n\t', '\n\t\t SPaRKy Da Andrea , Uguale , and John\x92s Pizze- ria offer exceptional value among the selected restaurants . \n\t', '\n\t\t Da Andrea is an Italian restaurant , with very good service , it has good decor , and its price is 28 dollars . \n\t', '\n\t\t John\x92s Pizzeria is an Italian , Pizza restaurant . \n\t', '\n\t\t It has decent service . \n\t', '\n\t\t It has mediocre decor . \n\t', '\n\t\t Its price is 20 dollars . \n\t', '\n\t\t 4 Uguale is a French , Italian restaurant , with very good service . \n\t', '\n\t\t It has good decor , and its price is 33 dollars . \n\t', '\n\t\t Figure 9 : Comparisons between 3 or more items , H = Humans\x92 score not easily model , but that a trainable sentence planner can . \n\t', '\n\t\t For example , Table 3 shows the nine rules generated on the first test fold which have the largest negative impact on the final RankBoost score ( above the double line ) and the largest positive impact on the final Rank- Boost score ( below the double line ) , for comparisons between three or more entities . \n\t', '\n\t\t The rule with the largest positive impact shows that SPaRKy learned to prefer that justifications involving price be merged with other information using a conjunction . \n\t', '\n\t\t These rules are also specific to presentation type . \n\t', '\n\t\t Averaging over both folds of the experiment , the number of unique features appearing in rules is 708 , of which 66 appear in the rule sets for two presentation types and 9 appear in the rule sets for all three presentation types . \n\t', '\n\t\t There are on average 214 rule features , 428 sentence features and 26 leaf features . \n\t', '\n\t\t The majority of the features are ancestor features ( 319 ) followed by traversal features ( 264 ) and sister features ( 60 ) . \n\t', '\n\t\t The remainder of the features ( 67 ) are for specific lexemes . \n\t', '\n\t\t To sum up , this experiment shows that the ability to model the interactions between domain content , task and presentation type is a strength of the trainable approach to sentence planning . \n\t', '\n\t\t 6 Conclusions This paper shows that the training technique used in SPoT can be easily extended to a new N Condition a3 1 sent anc PROPERNOUN RESTAURANT *HAVE1 > 16.5 -0.859 2 sent anc II Upper East Side*ATTR IN1* locate > 4.5 -0.852 3 sent anc PERIOD infer*PERIOD infer *PERIOD elaboration > -oo -0.542 4 rule anc assert-com-service*MERGE infer > 1.5 -0.356 5 sent tvl depth 0 BE3 > 4.5 -0.346 6 rule anc PERIOD infer*PERIOD infer *PERIOD elaboration > -oo -0.345 7 rule anc assert-com-decor*PERIOD infer *PERIOD infer*PERIOD contrast *PERIOD elaboration > -oo -0.342 8 rule anc assert-com-food quality*MERGE infer > 1.5 0.398 9 rule anc assert-com-price*CW CONJUNCTION infer*PERIOD justify 0.527 > -oo Table 3 : The nine rules generated on the first test fold which have the largest negative impact on the final RankBoost score ( above the double line ) and the largest positive impact on the final RankBoost score ( below the double line ) , for Compare3 . \n\t', '\n\t\t as represents the increment or decrement associated with satisfying the condition . \n\t', '\n\t\t domain and used for information presentation as well as information gathering . \n\t', '\n\t\t Previous work on SPoT also compared trainable sentence planning to a template-based generator that had previously been developed for the same application \n\t\t']",Positive
"['\n\t\t The evaluation results for SPaRKy ( 1 ) support the results for SPoT , by showing that trainable sentence generation can produce output comparable to template-based generation , even for complex information presentations such as extended comparisons ; ( 2 ) show that trainable sentence generation is sensitive to variations in domain application , presentation type , and even human preferences about the arrangement of particular types of information . \n\t', '\n\t\t 7 Acknowledgments We thank AT&T for supporting this research , and the anonymous reviewers for their helpful comments on this paper . \n\t', '\n\t\t References I. Langkilde . \n\t', '\n\t\t Forest-based statistical sentence generation . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t NAACL 2000 , 2000 . \n\t', '\n\t\t S. E. Brennan , M. Walker Friedman , and C. J. Pollard . \n\t', '\n\t\t A centering approach to pronouns . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 25th Annual Meeting of the ACL , Stanford , pages 155\x96162,1987 . \n\t', '\n\t\t L. Danlos . \n\t', '\n\t\t 2000. G-TAG : A lexicalized formalism for text generation inspired by tree adjoining grammar . \n\t', '\n\t\t In Tree Adjoining Grammars : Formalisms , Linguistic Analysis , and Processing . \n\t', '\n\t\t CSLI Publications . \n\t', '\n\t\t M. Johnston , S. Bangalore , G. Vasireddy , A. Stent , P. Ehlen , M. Walker , S. Whittaker , and P. Maloor . \n\t', '\n\t\t MATCH : An architecture for multimodal dialogue systems . \n\t', '\n\t\t In Annual Meeting of the ACL , 2002 . \n\t', '\n\t\t A. Knott , J. Oberlander , M. O\x92Donnell and C. Mellish . \n\t', '\n\t\t Beyond Elaboration : the interaction of relations and focus in coherent text . \n\t', '\n\t\t In Text Representation : linguistic and psycholinguistic aspects , pages 181-196 , 2001 . \n\t', '\n\t\t B. Lavoie and O. Rambow . \n\t', '\n\t\t A fast and portable realizer for text generation systems . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 3rd Conference on Applied Natural Language Processing , ANLP97 , pages 265\x96268 , 1997 . \n\t', '\n\t\t W.C. Mann and S.A. Thompson . \n\t', '\n\t\t Rhetorical structure theory : A framework for the analysis of texts . \n\t', '\n\t\t Technical Report RS-87-190 , USC/Information Sciences Institute , 1987 . \n\t', '\n\t\t D. Marcu . \n\t', '\n\t\t From local to global coherence : a bottom-up approach to text planning . \n\t', '\n\t\t In Proceedings of the National Conference on Artificial Intelligence ( AAAI\x9297 ) , 1997 . \n\t', '\n\t\t C. Mellish , A. Knott , J. Oberlander , and M. O\x92Donnell . \n\t', '\n\t\t Experiments using stochastic search for text planning . \n\t', '\n\t\t In Proceedings of INLG-98 . \n\t', '\n\t\t 1998. I. A. Mel~cuk . \n\t', '\n\t\t Dependency Syntax : Theory and Practice . \n\t', '\n\t\t SUNY , Albany , New York , 1988 . \n\t', '\n\t\t O. Rambow and T. Korelsky . \n\t', '\n\t\t Applied text generation . \n\t', '\n\t\t In Proceedings of the Third Conference on Applied Natural Language Processing , ANLP92 , pages 40\x9647 , 1992 . \n\t', '\n\t\t O. Rambow , M. Rogati and M. A. Walker . \n\t', '\n\t\t Evaluating a Trainable Sentence Planner for a Spoken Dialogue Travel System In Meeting of the ACL , 2001 . \n\t', '\n\t\t R. E. Schapire . \n\t', '\n\t\t A brief introduction to boosting . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 16th IJCAI , 1999 . \n\t', '\n\t\t D. R. Scott and C. Sieckenius de Souza . \n\t', '\n\t\t Getting the message across in RST-based text generation . \n\t', '\n\t\t In Current Research in Natural Language Generation , pages 47\x9673 , 1990 . \n\t', '\n\t\t A. Stent , M. Walker , S. Whittaker , and P. Maloor . \n\t', '\n\t\t User-tailored generation for spoken dialogue : An experiment . \n\t', '\n\t\t In Proceedings of ICSLP 2002. , 2002 . \n\t', '\n\t\t M. A. Walker , S. J. Whittaker , A. Stent , P. Maloor , J. D. Moore , M. Johnston , and G. Vasireddy . \n\t', '\n\t\t Speech-Plans : Generating evaluative responses in spoken dialogue . \n\t', '\n\t\t In Proceedings of INLG-02. , 2002 . \n\t', '\n\t\t M. Walker , O. Rambow , and M. Rogati . \n\t', '\n\t\t Training a sentence planner for spoken dialogue using boosting . \n\t', '\n\t\t Computer Speech and Language : Special Issue on Spoken Language Generation , 2002 . \n\t', '\n\t\t User Expertise Modelling and Adaptivity in a Speech-based E-mail System Kristiina JOKINEN University of Helsinki and University of Art and Design Helsinki Hämeentie 135C 00560 Helsinki kjokinen@uiah.fi Abstract This paper describes the user expertise model in AthosMail , a mobile , speech-based e-mail system . \n\t', '\n\t\t The model encodes the system\x92s assumptions about the user expertise , and gives recommendations on how the system should respond depending on the assumed competence levels of the user . \n\t', '\n\t\t The recommendations are realized as three types of explicitness in the system responses . \n\t', '\n\t\t The system monitors the user\x92s competence with the help of parameters that describe e.g. the success of the user\x92s interaction with the system . \n\t', '\n\t\t The model consists of an online and an offline version , the former taking care of the expertise level changes during the same session , the latter modelling the overall user expertise as a function of time and repeated interactions . \n\t', '\n\t\t 1 Introduction Adaptive functionality in spoken dialogue systems is usually geared towards dealing with communication disfluencies and facilitating more natural interaction ( e.g. Danieli and Gerbino , 1995 ; Litman and Pan , 1999 ; Krahmer et al , 1999 ; Walker et al , 2000 ) . \n\t', '\n\t\t In the AthosMail system \n\t\t']",Positive
"['\n\t\t The main idea is that while novice users need guidance , it would be inefficient and annoying for experienced users to be forced to listen to the same instructions every time they use the system . \n\t', '\n\t\t For instance , already \n\t\t']",Positive
"[""\n\t\t However , being able to decide when to switch from guiding a novice to facilitating an expert requires the system to be able to keep track of the user 's expertise level . \n\t"", '\n\t\t Depending on the system , the migration from one end of the expertise scale to the other may take anything from one session to an extended period of time . \n\t', '\n\t\t In some systems ( e.g. Chu-Carroll , 2000 ) , user inexperience is countered with initiative shifts towards the system , so that in the extreme case , the system leads the user from one task state to the next . \n\t', '\n\t\t This is a natural direction if the application includes tasks that can be pictured as a sequence of choices , like choosing turns from a road map when navigating towards a particular place . \n\t', '\n\t\t Examples of such a task structure include travel reservation systems , where the requested information can be given when all the relevant parameters have been collected . \n\t', '\n\t\t If , on the other hand , the task structure is flat , system initiative may not be very useful , since nothing is gained by leading the user along paths that are only one or two steps long . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t There are essentially four ways the user can learn to use a system : 1 ) by unaided trial and error , 2 ) by having a pre-use tutorial , 3 ) by trying to use the system and then asking for help when in trouble , or 4 ) by relying on advice the system gives when concluding the user is in trouble . \n\t', '\n\t\t Kamm , Litman & \n\t\t']",Negative
"['\n\t\t However , users often lack enthusiasm towards tutorials and want to proceed straight to using the system . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t She introduced various prompt design techniques , e.g. tapering which means that the system shortens the prompts for users as they gain experience with the system , and incremental prompts , which means that when a prompt is met with silence ( or a timeout occurs in a graphical interface ) , the repeated prompt will be incorporated with helpful hints or instructions . \n\t', '\n\t\t The system utterances are thus adapted online to mirror the perceived user expertise . \n\t', '\n\t\t The user model that keeps track of the perceived user expertise may be session-specific , but it could also store the information between sessions , depending on the application . \n\t', '\n\t\t A call service providing bus timetables may harmlessly assume that the user is always new to the system , but an email system is personal and the user could presumably benefit from personalized adaptations . \n\t', '\n\t\t If the system stores user modelling information between sessions , there are two paths for adaptation : the adaptations take place between sessions on the basis of observations made during earlier sessions , or the system adapts online and the resulting parameters are then passed from one session to another by means of the user model information storage . \n\t', '\n\t\t A combination of the two is also possible , and this is the chosen path for AthosMail as disclosed in section 3 . \n\t', '\n\t\t User expertise has long been the subject of user modelling in the related fields of text generation , question answering and tutorial systems . \n\t', '\n\t\t For example , \n\t\t']",Positive
"['\n\t\t Although the applications are somewhat different , we expect a fair amount of further inspiration to be forthcoming from this direction also . \n\t', '\n\t\t In this paper , we describe the AthosMail user expertise model , the Cooperativity Model , and discuss its effect on the system behaviour . \n\t', '\n\t\t The paper is organised as follows . \n\t', '\n\t\t In Section 2 we will first briefly introduce the AthosMail functionality which the user needs to familiarise herself with . \n\t', '\n\t\t Section 3 describes the user expertise model in more detail . \n\t', '\n\t\t We define the three expertise levels and the concept of DASEX ( dialogue act specific explicitness ) , and present the parameters that are used to calculate the online , session-specific DASEX values as well as offline , between-thesessions DASEX values . \n\t', ""\n\t\t We also list some of the system responses that correspond to the system 's assumptions about the user expertise . \n\t"", '\n\t\t In Section 4 , we report on the evaluation of the system\x92s adaptive responses and user errors . \n\t', '\n\t\t In Section 5 , we provide conclusions and future work . \n\t', '\n\t\t 2 System functionality AthosMail is an interactive speech-based e-mail system being developed for mobile telephone use in the project DUMAS ( Jokinen and Gambäck , 2004 ) . \n\t', '\n\t\t The research goal is to investigate adaptivity in spoken dialogue systems in order to enable users to interact with the speech-based systems in a more flexible and natural way . \n\t', '\n\t\t The practical goal of AthosMail is to give an option for visually impaired users to check their email by voice commands , and for sighted users to access their email using a mobile phone . \n\t', '\n\t\t The functionality of the test prototype is rather simple , comprising of three main functions : navigation in the mailbox , reading of messages , and deletion of messages . \n\t', '\n\t\t For ease of navigation , AthosMail makes use of automatic classification of messages by sender , subject , topic , or other relevant criteria , which is initially chosen by the system . \n\t', '\n\t\t The classification provides different "" views "" to the mailbox contents , and the user can move from one view to the next , e.g. from Paul \'s messages to Maria \'s messages , with commands like "" next "" , "" previous "" or "" first view "" , and so on . \n\t', '\n\t\t Within a particular view , the user may navigate from one message to another in a similar fashion , saying "" next "" , "" fourth message "" or "" last message "" , and so on . \n\t', '\n\t\t Reading messages is straightforward , the user may say "" read ( the message ) "" , when the message in question has been selected , or refer to another message by saying , for example , "" read the third message "" . \n\t', '\n\t\t Deletion is handled in the same way , with some room for referring expressions . \n\t', '\n\t\t The user has the option of asking the system to repeat its previous utterance . \n\t', ""\n\t\t The system asks for a confirmation when the user 's command entails something that has more potential consequences than just wasting time ( by e.g. reading the wrong message ) , namely , quitting and the deletion of messages . \n\t"", '\n\t\t AthosMail may also ask for clarifications , if the speech recognition is deemed unreliable , but otherwise the user has the initiative . \n\t', '\n\t\t The purpose of the AthosMail user model is to provide flexibility and variation in the system utterances . \n\t', '\n\t\t The system monitors the user\x92s actions in general , and especially on each possible system act . \n\t', '\n\t\t Since the user may master some part of the system functionality , while not be familiar with all commands , the system can thus provide responses tailored with respect to the user\x92s familiarity with individual acts . \n\t', '\n\t\t The user model produces recommendations for the dialogue manager on how the system should respond depending on the assumed competence levels of the user . \n\t', '\n\t\t The user model consists of different subcomponents , such as Message Prioritizing , Message Categorization and User Preference components \n\t\t']",Positive
"['\n\t\t The Cooperativity Model utilizes two parameters , explicitness and dialogue control ( i.e. initiative ) , and the combination of their values then guides utterance generation . \n\t', '\n\t\t The former is an estimate of the user\x92s competence level , and is described in the following sections . \n\t', '\n\t\t 3 User expertise modelling in AthosMail AthosMail uses a three-level user expertise scale to encode varied skill levels of the users . \n\t', ""\n\t\t The common assumption of only two classes , experts and novices , seems too simple a model which does not take into account the fact that the user 's expertise level increases gradually , and many users consider themselves neither novices nor experts but something in between . \n\t"", '\n\t\t Moreover , the users may be experienced with the system selectively : they may use some commands more often than others , and thus their skill levels are not uniform across the system functionality . \n\t', '\n\t\t A more fine-grained description of competence and expertise can also be presented . \n\t', '\n\t\t For instance , \n\t\t']",Positive
"['\n\t\t In practical dialogue systems , however , it is difficult to maintain subtle user models , and it is also difficult to define such observable facts that would allow fine-grained competence levels to be distinguished in rather simple application tasks . \n\t', '\n\t\t We have thus ended up with a compromise , and designed three levels of user expertise in our model : novice , competent , and expert . \n\t', '\n\t\t These levels are reflected in the system responses , which can vary from explicit to concise utterances depending on how much extra information the system is to give to the user in one go . \n\t', '\n\t\t As mentioned above , one of the goals of the Cooperativity model is to facilitate more natural interaction by allowing the system to adapt its utterances according to the perceived expertise level . \n\t', '\n\t\t On the other hand , we also want to validate and assess the usability of the three-level model of user expertise . \n\t', '\n\t\t While not entering into discussions about the limits of rule-based thinking ( e.g. in order to model intuitive decision making of the experts according to the Dreyfus model ) , we want to study if the designed system responses , adapted according to the assumed user skill levels , can provide useful assistance to the user in interactive situations where she is still uncertain about how to use the system . \n\t', ""\n\t\t Since the user can always ask for help explicitly , our main goal is not to study the decrease in the user 's help requests when she becomes more used to the system , but rather , to design the system responses so that they would reflect the different skill levels that the system assumes the user is on , and to get a better understanding whether the expertise levels and their reflection in the system responses is valid or not , so as to provide the best assistance for the user . \n\t"", ""\n\t\t 3.1 Dialogue act specific explicitness The user expertise model utilized in AthosMail is a collection of parameters aimed at observing telltale signals of the user 's skill level and a set of second-order parameters ( dialogue act specific explicitness DASEX , and dialogue control CTL ) that reflect what has been concluded from the first- order parameters . \n\t"", '\n\t\t Most first-order parameters are tuned to spot incoherence between new information and the current user model ( see below ) . \n\t', ""\n\t\t If there 's evidence that the user is actually more experienced than previously thought , the user expertise model is updated to reflect this . \n\t"", '\n\t\t The process can naturally proceed in the other direction as well , if the user model has been too fast in concluding that the user has advanced to a higher level of expertise . \n\t', '\n\t\t The second-order parameters affect the system behaviour directly . \n\t', '\n\t\t There is a separate experience value for each system function , which enables the system to behave appropriately even if the user is very experienced in using one function but has never used another . \n\t', '\n\t\t The higher the value , the less experienced the user ; the less experienced the user , the more explicit the manner of expression and the more additional advice is incorporated in the system utterances . \n\t', '\n\t\t The values are called DASEX , short for Dialogue Act Specific Explicitness , and their value range corresponds to the user expertise as follows : 1 = expert , 2 = competent , 3 = novice . \n\t', '\n\t\t The model comprises an online component and an offline component . \n\t', '\n\t\t The former is responsible for observing runtime events and calculating DASEX recommendations on the fly , whereas the latter makes long-time observations and , based on these , calculates default DASEX values to be used at the beginning of the next session . \n\t', '\n\t\t The offline component is , so to speak , rather conservative ; it operates on statistical event distributions instead of individual parameter values and tends to round off the extremes , trying to catch the overall learning curve behind the local variations . \n\t', '\n\t\t The components work separately . \n\t', '\n\t\t In the beginning of a new session , the current offline model of the user\x92s skill level is copied onto the online component and used as the basis for producing the DASEX recommendations , while at the end of each session , the offline component calculates the new default level on the basis of the occurred events . \n\t', '\n\t\t Figure 1 provides an illustration of the relationships between the parameters . \n\t', '\n\t\t In the next section we describe them in detail . \n\t', '\n\t\t 3.1.1 Online parameter descriptions The online component can be seen as an extension of the ideas proposed by \n\t\t']",Positive
"['\n\t\t The relative weights of the parameters are those used in our user tests , partly based on those of \n\t\t']",Positive
"['\n\t\t They will be fine-tuned according to our results . \n\t', '\n\t\t Figure 1 The functional relationships between the offline and online parameters used to calculate the DASEX values . \n\t', '\n\t\t DASEX ( dialogue act specific explicitness ) : The value is modified during sessions . \n\t', '\n\t\t Value : DDASEX ( see offline parameters ) modified by SDAI , HLP , TIM , and INT as specified in the respective parameter definitions . \n\t', '\n\t\t SDAI ( system dialogue act invoked ) : A set of parameters ( one for each system dialogue act ) that tracks whether a particular dialogue act has been invoked during the previous round . \n\t', ""\n\t\t If SDAI = ' yes ' , then DASEX -1 . \n\t"", '\n\t\t This means that when a particular system dialogue move has been instantiated , its explicitness value is decreased and will therefore be presented in a less explicit form the next time it is instantiated during the same session . \n\t', '\n\t\t HLP ( the occurrence of a help request by the user ) : The system incorporates a separate help function ; this parameter is only used to notify the offline side about the frequency of help requests . \n\t', ""\n\t\t TIM ( the occurrence of a timeout on the user 's turn ) : If TIM = ' yes ' , then DASEX +1 . \n\t"", '\n\t\t This refers to speech recognizer timeouts . \n\t', '\n\t\t INT ( occurrence of a user interruption during system turn ) : Can be either a barge-in or an interruption by telephone keys . \n\t', ""\n\t\t If INT = ' yes ' , then DASEX = 1 . \n\t"", '\n\t\t 3.1.2 Offline parameter descriptions DDASEX ( default dialogue act specific explicitness ) : Every system dialogue act has its own default explicitness value invoked at the beginning of a session . \n\t', '\n\t\t Value : DASE + GEX / 2 . \n\t', '\n\t\t GEX ( general expertise ) : General expertise . \n\t', '\n\t\t A general indicator of user expertise . \n\t', '\n\t\t Value : NSES + OHLP + OTIM / 3 . \n\t', '\n\t\t DASE ( dialogue act specific experience ) : This value is based on the number of sessions during which the system dialogue act has been invoked . \n\t', '\n\t\t There is a separate DASE value for every system dialogue act . \n\t', '\n\t\t number of sessions DASE 0-2 3 3-6 2 more than 7 1 NSES ( number of sessions ) : Based on the total number of sessions the user has used the system . \n\t', '\n\t\t number of sessions NSES 0-2 3 3-6 2 more than 7 1 OHLP ( occurrence of help requests ) : This parameter tracks whether the user has requested system help during the last 1 or 3 sessions . \n\t', '\n\t\t The HLP parameter is logged by the online component . \n\t', '\n\t\t HLP occurred during OHLP the last session 3 the last 3 sessions 2 if not 1 OTIM ( occurrence of timeouts ) : This parameter tracks whether a timeout has occurred during the last 1 or 3 sessions . \n\t', '\n\t\t The TIM parameter is logged by the online component . \n\t', '\n\t\t TIM occurred during OTIM the last session 3 the last 3 sessions 2 if not 1 3.2 DASEX-dependent surface forms Each system utterance type has three different surface realizations corresponding to the three DASEX values . \n\t', '\n\t\t The explicitness of a system utterance can thus range between [ 1 = taciturn , 2 = normal , 3 = explicit ] ; the higher the value , the more additional information the surface realization will include ( cf. Jokinen and Wilcock , 2001 ) . \n\t', '\n\t\t The value is used for choosing between the surface realizations which are generated by the presentation components as natural language utterances . \n\t', '\n\t\t The following two examples have been translated from their original Finnish forms . \n\t', '\n\t\t Example 1 : A speech recognition error ( the ASR score has been too low ) . \n\t', ""\n\t\t DASEX = 1 : I 'm sorry , I did n't understand . \n\t"", ""\n\t\t DASEX = 2 : I 'm sorry , I did n't understand . \n\t"", '\n\t\t Please speak clearly , but do not over-articulate , and speak only after the beep . \n\t', ""\n\t\t DASEX = 3 : I 'm sorry , I did n't understand . \n\t"", '\n\t\t Please speak clearly , but do not over-articulate , and speak only after the beep . \n\t', ""\n\t\t To hear examples of what you can say to the system , say ' what now ' . \n\t"", '\n\t\t Example 2 : Basic information about a message that the user has chosen from a listing of messages from a particular sender . \n\t', '\n\t\t DASEX = 1 : First message , about "" reply : sample file "" . \n\t', '\n\t\t DASEX = 2 : First message , about "" reply : sample file "" . \n\t', ""\n\t\t Say ' tell me more ' , if you want more details . \n\t"", '\n\t\t DASEX = 3 : First message , about "" reply : sample file "" . \n\t', ""\n\t\t Say ' read ' , if you want to hear the messages , or ' tell me more ' , if you want to hear a summary and the send date and length of the message . \n\t"", '\n\t\t These examples show the basic idea behind the DASEX effect on surface generation . \n\t', '\n\t\t In the first example , the novice user is given additional information about how to try and avoid ASR problems , while the expert user is only given the error message . \n\t', '\n\t\t In the second example , the expert user gets the basic information about the message only , whereas the novice user is also provided with some possible commands how to continue . \n\t', '\n\t\t A full interaction with AthosMail is given in Appendix 1 . \n\t', '\n\t\t 4 Evaluation of AthosMail Within the DUMAS project , we are in the process of conducting exhaustive user studies with the prototype AthosMail system that incorporates the user expertise model described above . \n\t', '\n\t\t We have already conducted a preliminary qualitative expert evaluation , the goal of which was to provide insights into the design of system utterances so as to appropriately reflect the three user expertise levels , and the first set of user evaluations where a set of four tasks was carried out during two consecutive days . \n\t', '\n\t\t 4.1 Adaptation and system utterances For the expert evaluation , we interviewed 5 interactive systems experts ( two women and three men ) . \n\t', '\n\t\t They all had earlier experience in interactive systems and interface design , but were unfamiliar with the current system and with interactive email systems in general . \n\t', '\n\t\t Each interview included three walkthroughs of the system , one for a novice , one for a competent , and one for an expert user . \n\t', '\n\t\t The experts were asked to comment on the naturalness and appropriateness of each system utterance , as well as provide any other comments that they may have on adaptation and adaptive systems . \n\t', '\n\t\t All interviewees agreed on one major theme , namely that the system should be as friendly and reassuring as possible towards novices . \n\t', '\n\t\t Dialogue systems can be intimidating to new users , and many people are so afraid of making mistakes that they give up after the first communication failure , regardless of what caused it . \n\t', '\n\t\t Graphical user interfaces differ from speech interfaces in this respect , because there is always something salient to observe as long as the system is running at all . \n\t', '\n\t\t Four of the five experts agreed that in an error situation the system should always signal the user that the machine is to blame , but there are things that the user can do in case she wants to help the system in the task . \n\t', '\n\t\t The system should acknowledge its shortcomings "" humbly "" and make sure that the user does n\'t get feelings of guilt \x96 all problems are due to imperfect design . \n\t', '\n\t\t E.g. , the responses in Example 1 were viewed as accusing the user of not being able to act in the correct way . \n\t', '\n\t\t We have since moved towards forms like "" I may have misheard "" , where the system appears responsible for the miscommunication . \n\t', '\n\t\t This can pave the way when the user is taking the first wary steps in getting acquainted with the system . \n\t', '\n\t\t Novice users also need error messages that do not bother the user with technical matters that concern only the designers . \n\t', ""\n\t\t For instance , a novice user does n't need information about error codes or characteristics of the speech recognizer ; when ASR errors occur , the system can simply talk about not hearing correctly ; a reference to a piece of equipment that does the job \x96 namely , the speech recognizer \x96 is unnecessary and the user should not be burdened with it . \n\t"", '\n\t\t Experienced users , on the other hand , wish to hear only the essentials . \n\t', '\n\t\t All our interviewees agreed that at the highest skill level , the system prompts should be as terse as possible , to the point of being blunt . \n\t', '\n\t\t Politeness words like "" I \'m sorry "" are not necessary at this level , because the expert \'s attitude towards the system is pragmatic : they see it as a tool , know its limitations , and "" rudeness "" on the part of the system does n\'t scare or annoy them anymore . \n\t', '\n\t\t However , it is not clear how the change in politeness when migrating from novice to expert levels actually affects the user\x92s perception of the system ; the transition should at least be gradual and not too fast . \n\t', '\n\t\t There may also be cultural differences regarding certain politeness rules . \n\t', '\n\t\t The virtues of adaptivity are still a matter of debate . \n\t', '\n\t\t One of the experts expressed serious doubt over the usability of any kind of automatic adaptivity and maintained that the user should decide whether she wants the system to adapt at a given moment or not . \n\t', '\n\t\t In the related field of tutoring systems , \n\t\t']",Positive
"['\n\t\t Whatever the case , it is clear that badly designed adaptivity is confusing to the user , and especially a novice user may feel disoriented if faced with prompts where nothing seems to stay the same . \n\t', '\n\t\t It is essential that the system is consistent in its use of concepts , and manner of speech . \n\t', '\n\t\t In AthosMail , the expert level ( DASEX=1 for all dialogue acts ) acts as the core around which the other two expertise levels are built . \n\t', '\n\t\t While the core remains essentially unchanged , further information elements are added after it . \n\t', '\n\t\t In practise , when the perceived user expertise rises , the system simply removes information elements that have become unnecessary from the end of the utterance , without touching the core . \n\t', '\n\t\t This should contribute to a feeling of consistency and dependability . \n\t', '\n\t\t On the other hand , \n\t\t']",Positive
"['\n\t\t It will prove interesting to reconcile these views in a more general kind of user expertise modeling . \n\t', '\n\t\t 4.2 Adaptation and user errors The user evaluation of AthosMail consisted of four tasks that were performed on two consecutive days . \n\t', '\n\t\t The 26 test users , aged 20-62 , thus produced four separate dialogues each and a total of 104 dialogues . \n\t', '\n\t\t They had no previous experience with speech-based dialogue systems , and to familiarize themselves to synthesized speech and speech recognizers , they had a short training session with another speech application in the beginning of the first test session . \n\t', '\n\t\t An outline of AthosMail functionality was presented to the users , and they were allowed to keep it when interacting with the system . \n\t', '\n\t\t At the end of each of the four tests , the users were asked to assess how familiar they were with the system functionality and how confident they felt about using it . \n\t', '\n\t\t Also , they were asked to assess whether the system gave too little information about its functionality , too much , or the right amount . \n\t', '\n\t\t The results are reported in \n\t\t']",Positive
"['\n\t\t We also identified four error types , as a point of comparison for the user expertise model . \n\t', '\n\t\t 5 Conclusions Previous studies concerning user modelling in various interactive applications have shown the importance of the user model in making the interaction with the system more enjoyable . \n\t', '\n\t\t We have introduced the three-level user expertise model , implemented in our speech-based e-mail system , AthosMail , and argued for its effect on the behaviour of the overall system . \n\t', '\n\t\t Future work will focus on analyzing the data collected through the evaluations of the complete AthosMail system with real users . \n\t', '\n\t\t Preliminary expert evaluation revealed that it is important to make sure the novice user is not intimidated and feels comfortable with the system , but also that the experienced users should not be forced to listen to the same advice every time they use the system . \n\t', '\n\t\t The hand-tagged error classification shows a slight downward tendency in user errors , suggesting accumulation of user experience . \n\t', '\n\t\t This will act as a point of comparison for the user expertise model assembled automatically by the system . \n\t', '\n\t\t Another future research topic is to apply machine-learning and statistical techniques in the implementation of the user expertise model . \n\t', '\n\t\t Through the user studies we will also collect data which we plan to use in re-implementing the DASEX decision mechanism as a Bayesian network . \n\t', '\n\t\t 6 Acknowledgements This research was carried out within the EU\x92s Information Society Technologies project DUMAS ( Dynamic Universal Mobility for Adaptive Speech Interfaces ) , IST-2000-29452 . \n\t', '\n\t\t We thank all project participants from KTH and SICS , Sweden ; UMIST , UK ; ETeX Sprachsynthese AG , Germany ; U. of Tampere , U. of Art and Design , Connexor Oy , and Timehouse Oy , Finland . \n\t', '\n\t\t References Jennifer Chu-Carroll . \n\t', '\n\t\t 2000. MIMIC : An Adaptive Mixed Initiative Spoken Dialogue System for Information Queries . \n\t', '\n\t\t In Procs of ANLP 6 , 2000 , pp. 97-104 . \n\t', '\n\t\t Morena Danieli and Elisabetta Gerbino . \n\t', '\n\t\t 1995. Metrics for Evaluating Dialogue Strategies in a Spoken Language System . \n\t', '\n\t\t Working Notes , AAAI Spring Symposium Series , Stanford University . \n\t', '\n\t\t Hubert L. Dreyfus and Stuart E. Dreyfus . \n\t', '\n\t\t 1986. Mind over Machine : The Power of Human Intuition and Expertise in the Era of the Computer . \n\t', '\n\t\t New York : The Free Press . \n\t', '\n\t\t Kristiina Jokinen and Björn Gambäck . \n\t', '\n\t\t 2004. DUMAS - Adaptation and Robust Information Processing for Mobile Speech Interfaces . \n\t', '\n\t\t Procs of The 1st Baltic Conference \x93Human Language Technologies \x96 The Baltic Perspective\x94 , Riga , Latvia , 115-120 . \n\t', '\n\t\t Kristiina Jokinen , Kari Kanto , Antti Kerminen and Jyrki Rissanen . \n\t', '\n\t\t 2004. Evaluation of Adaptivity and User Expertise in a Speech-based E-mail System . \n\t', '\n\t\t Procs of the COLING Satellite Workshop Robust and Adaptive Information Processing for Mobile Speech Interfaces , Geneva , Switzerland . \n\t', '\n\t\t Kristiina Jokinen and Graham Wilcock . \n\t', '\n\t\t 2001. Adaptivity and Response Generation in a Spoken Dialogue System . \n\t', '\n\t\t In van Kuppevelt , J. and R. W. Smith ( eds . \n\t', '\n\t\t ) Current and New Directions in Discourse and Dialogue . \n\t', '\n\t\t Kluwer Academic Publishers . \n\t', '\n\t\t pp. 213-234 . \n\t', '\n\t\t Candace Kamm , Diane Litman , and Marilyn Walker . \n\t', '\n\t\t 1998. From novice to expert : the effect of tutorials on user expertise with spoken dialogue systems . \n\t', '\n\t\t Procs of the International Conference on Spoken Language Processing ( ICSLP98 ) . \n\t', '\n\t\t Judy Kay . \n\t', '\n\t\t 2001. Learner control . \n\t', '\n\t\t User Modeling and User-Adapted Interaction 11 : 111-127 . \n\t', '\n\t\t Emiel Krahmer , Marc Swerts , Mariet Theune and Mieke Weegels . \n\t', '\n\t\t 1999. Problem Spotting in Human- Machine Interaction . \n\t', ""\n\t\t In Procs of Eurospeech '99 . \n\t"", '\n\t\t Vol. 3 , 1423-1426 . \n\t', '\n\t\t Budapest , Hungary . \n\t', '\n\t\t Diane J. Litman and Shimei Pan . \n\t', '\n\t\t 2002. Designing and Evaluating an Adaptive Spoken Dialogue System . \n\t', '\n\t\t User Modeling and User-Adapted Interaction . \n\t', '\n\t\t Vol 12(2/3):111-137 . \n\t', '\n\t\t Cécile Paris . \n\t', ""\n\t\t 1988. Tailoring Descriptions to a User 's Level of Expertise . \n\t"", '\n\t\t Journal of Computational Linguistics , 14 ( 3 ) : 64-78 . \n\t', '\n\t\t Ronnie W. Smith . \n\t', '\n\t\t 1993. Effective Spoken Natural Language Dialog Requires Variable Initiative Behavior : An Empirical Study . \n\t', '\n\t\t Procs of the AAAI Fall Symposium on Human-Computer Collaboration : Reconciling Theory , Synthesizing Practice . \n\t', '\n\t\t M. Turunen , E-P. Salonen , M. Hartikainen , J. Hakulinen , W.J. Black , A : Ramsay , A. Funk , A. Conroy , P. Thompson , M. Stairmand , K. Jokinen , J. Rissanen , K. Kanto , A. Kerminen , B. Gambäck , M. Cheadle , F. Olsson , M. Sahlgren . \n\t', '\n\t\t 2004. AthosMail \x96 a multilingual Adaptive Spoken Dialogue System for E-mail Domain . \n\t', '\n\t\t Procs of the COLING Workshop Robust and Adaptive Information Processing for Mobile Speech Interfaces , Geneva , Switzerland . \n\t', '\n\t\t Marilyn A. Walker , Irene Langkilde , Jerry Wright , Allen Gorin , Diane J. Litman . \n\t', '\n\t\t 2000. Learning to Predict Problematic Situations in a Spoken Dialogue System : Experiments with How May I Help You ? \n\t', '\n\t\t Procs of NAACL\x9200 , Seattle , US , 210-217 . \n\t', '\n\t\t Nicole Yankelovich . \n\t', '\n\t\t 1996. How do users know what to say ? \n\t', '\n\t\t Interactions 3(6) : 32-43 . \n\t', '\n\t\t 7 Appendix 1 . \n\t', '\n\t\t Sample dialogue with a novice user System : Welcome to AthosMail . \n\t', '\n\t\t Please dial your user code . \n\t', '\n\t\t User : [ dials the four-digit code ] S : Please dial your secret passcode . \n\t', ""\n\t\t U : [ dials the passcode ] System : Please wait , I 'll contact your mail server . \n\t"", '\n\t\t ( ... ) Hello Jukka , you have twelve unread messages . \n\t', '\n\t\t They have been divided into three groups . \n\t', '\n\t\t In the first group , there are the messages about Re : Comments about the draft . \n\t', '\n\t\t In the second group , the messages from Timothy . \n\t', '\n\t\t In the third group , the rest of the messages . \n\t', '\n\t\t You can select a group by saying for example "" first group "" or "" second group "" . \n\t', '\n\t\t Say help if you want further instructions . \n\t', '\n\t\t User : First group . \n\t', '\n\t\t S : You are in the first group , with three messages about Re : Comments about the draft . \n\t', '\n\t\t The first message is from Matti Hahtomaa , the second from Jaana Jaakkola , the third from Sakari Rantala. . \n\t', '\n\t\t If you want to select a message , say for example "" second message "" or "" Eddie Everyman \'s message "" . \n\t', '\n\t\t U : First message . \n\t', '\n\t\t S : First message , from Matti Hahtomaa , about Re : Comments about the draft . \n\t', '\n\t\t If you want to hear the message , say "" read "" . \n\t', '\n\t\t You can have additional information about the message by saying "" tell me more "" . \n\t', '\n\t\t U : Second message . \n\t', '\n\t\t S : Second message , from Jaana Jaakkola , about Re : Comments about the draft . \n\t', '\n\t\t You can have additional information about the message by saying "" tell me more "" . \n\t', '\n\t\t U : Tell me more . \n\t', '\n\t\t S : Message sent on February 3rd , 2004 . \n\t', '\n\t\t Message length 24 lines . \n\t', ""\n\t\t U : Read Sakari Rantala 's message . \n\t"", '\n\t\t S : Third message , from Sakari Rantala , about Re : Comments about the draft . \n\t', '\n\t\t [ reads the message ] Message read . \n\t', '\n\t\t You are in the first message group . \n\t', '\n\t\t U : Quit . \n\t', '\n\t\t S : Are you sure you want to quit ? \n\t', '\n\t\t Say yes or no . \n\t', '\n\t\t U : Yes . \n\t', '\n\t\t S : Bye bye . \n\t', '\n\t\t Discriminative Training of a Neural Network Statistical Parser James HENDERSON School of Informatics , University of Edinburgh 2 Buccleuch Place Edinburgh EH8 9LW United Kingdom james.henderson@ed.ac.uk Abstract Discriminative methods have shown significant improvements over traditional generative methods in many machine learning applications , but there has been difficulty in extending them to natural language parsing . \n\t', '\n\t\t One problem is that much of the work on discriminative methods conflates changes to the learning method with changes to the parameterization of the problem . \n\t', '\n\t\t We show how a parser can be trained with a discriminative learning method while still parameterizing the problem according to a generative probability model . \n\t', '\n\t\t We present three methods for training a neural network to estimate the probabilities for a statistical parser , one generative , one discriminative , and one where the probability model is generative but the training criteria is discriminative . \n\t', '\n\t\t The latter model outperforms the previous two , achieving state-of- the-art levels of performance ( 90.1 % F-measure on constituents ) . \n\t', '\n\t\t 1 Introduction Much recent work has investigated the application of discriminative methods to NLP tasks , with mixed results . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t We show how this approach can be applied to broad coverage natural language parsing . \n\t', '\n\t\t Our estimation and training methods successfully balance the conflicting requirements that the training method be both computationally tractable for large datasets and a good approximation to the theoretically optimal method . \n\t', '\n\t\t The parser which uses this approach outperforms both a generative model and a discriminative model , achieving state-of-the-art levels of performance ( 90.1 % F-measure on constituents ) . \n\t', '\n\t\t To compare these different approaches , we use a neural network architecture called Simple Synchrony Networks ( SSNs ) \n\t\t']",Positive
"['\n\t\t SSNs have the advantage that they avoid the need to impose hand-crafted independence assumptions on the learning process . \n\t', '\n\t\t Training an SSN simultaneously trains a finite representations of the unbounded parse history and a mapping from this history representation to the parameter estimates . \n\t', '\n\t\t The history representations are automatically tuned to optimize the parameter estimates . \n\t', '\n\t\t This avoids the problem that any choice of hand-crafted independence assumptions may bias our results towards one approach or another . \n\t', '\n\t\t The independence assumptions would have to be different for the generative and discriminative probability models , and even for the parsers which use the generative probability model , the same set of independence assumptions may be more appropriate for maximizing one training criteria over another . \n\t', '\n\t\t By inducing the history representations specifically to fit the chosen model and training criteria , we avoid having to choose independence assumptions which might bias our results . \n\t', '\n\t\t Each complete parsing system we propose consists of three components , a probability model for sequences of parser decisions , a Simple Synchrony Network which estimates the parameters of the probability model , and a procedure which searches for the most probable parse given these parameter estimates . \n\t', '\n\t\t This paper outlines each of these components , but more details can be found in \n\t\t']",Positive
"['\n\t\t We also present the training methods , and experiments on the proposed parsing models . \n\t', '\n\t\t 2 Two History-Based Probability Models As with many previous statistical parsers \n\t\t']",Positive
"['\n\t\t Designing a history-based model of parsing involves two steps , first choosing a mapping from the set of phrase structure trees to the set of parses , and then choosing a probability model in which the probability of each parser decision is conditioned on the history of previous decisions in the parse . \n\t', '\n\t\t We use the same mapping for both our probability models , but we use two different ways of conditioning the probabilities , one generative and one discriminative . \n\t', '\n\t\t As we will show in section 6 , these two different ways of parameterizing the probability model have a big impact on the ease with which the parameters can be estimated . \n\t', '\n\t\t To define the mapping from phrase structure trees to parses , we use a form of left-corner parsing strategy \n\t\t']",Positive
"['\n\t\t In a left-corner parse , each node is introduced after the subtree rooted at the node\x92s first child has been fully parsed . \n\t', '\n\t\t Then the subtrees for the node\x92s remaining children are parsed in their left-to-right order . \n\t', '\n\t\t Parsing a constituent starts by pushing the leftmost word w of the constituent onto the stack with a shift(w) action . \n\t', '\n\t\t Parsing a constituent ends by either introducing the constituent\x92s parent nonterminal ( labeled Y ) with a project(Y) action , or attaching to the parent with an attach action.1 A complete parse consists of a sequence of these actions , d1 , ... , dm , such that performing d1 , ... , dm results in a complete phrase structure tree . \n\t', '\n\t\t Because this mapping from phrase structure trees to sequences of decisions about parser actions is one-to-one , finding the most probable phrase structure tree is equivalent to finding the parse d1 , ... , dm which maximizes P(d1 , ... , dm I w1 , ... , wn ) . \n\t', '\n\t\t This probability is only nonzero if yield(d1 , ... , dm ) = w1 , ... , wn , so we can restrict attention to only those parses which actually yield the given sentence . \n\t', '\n\t\t With this restriction , it is equivalent to maximize P(d1 , ... , dm ) , as is done with our first probability model . \n\t', '\n\t\t The first probability model is generative , because it specifies the joint probability of the input sentence and the output tree . \n\t', ""\n\t\t This joint probability is simply P(d1 , ... , dm ) , since the ' More details on the mapping to parses can be found in \n\t\t""]",Positive
"['\n\t\t probability of the input sentence is included in the probabilities for the shift(wi) decisions included in d1 , ... , dm . \n\t', '\n\t\t The probability model is then defined by using the chain rule for conditional probabilities to derive the probability of a parse as the multiplication of the probabilities of each decision di conditioned on that decision\x92s prior parse history d1 , ... , di- 1 . \n\t', '\n\t\t P(d1 , ... , dm ) = HiP(diI d1 , ... , di-1 ) The parameters of this probability model are the P ( di Id1 , ... , di- 1 ) . \n\t', '\n\t\t Generative models are the standard way to transform a parsing strategy into a probability model , but note that we are not assuming any bound on the amount of information from the parse history which might be relevant to each parameter . \n\t', '\n\t\t The second probability model is discriminative , because it specifies the conditional probability of the output tree given the input sentence . \n\t', '\n\t\t More generally , discriminative models try to maximize this conditional probability , but often do not actually calculate the probability , as with Support Vector Machines \n\t\t']",Negative
"['\n\t\t We take the approach of actually calculating an estimate of the conditional probability because it differs minimally from the generative probability model . \n\t', '\n\t\t In this form , the distinction between our two models is sometimes referred to as \x93joint versus conditional\x94 \n\t\t']",Positive
"['\n\t\t As with the generative model , we use the chain rule to decompose the entire conditional probability into a sequence of probabilities for individual parser decisions , where yield(dj , ... , dk ) is the sequence of words wi from the shift(wi) actions in dj , ... , dk. P(d1 , ... , dmI yield(d1 , ... , dm ) ) = HiP(diI d1 , ... , di-1 , yield(di , ... , dm ) ) Note that d1 , ... , di-1 specifies yield(d1 , ... , di-1 ) , so it is sufficient to only add yield(di , ... , dm ) to the conditional in order for the entire input sentence to be included in the conditional . \n\t', '\n\t\t We will refer to the string yield(di , ... , dm ) as the lookahead string , because it represents all those words which have not yet been reached by the parse at the time when decision di is chosen . \n\t', '\n\t\t The parameters of this model differ from those of the generative model only in that they include the lookahead string in the conditional . \n\t', '\n\t\t Although maximizing the joint probability is the same as maximizing the conditional proba- bility , the fact that they have different parameters means that estimating one can be much harder than estimating the other . \n\t', '\n\t\t In general we would expect that estimating the joint probability would be harder than estimating the conditional probability , because the joint probability contains more information than the conditional probability . \n\t', '\n\t\t In particular , the probability distribution over sentences can be derived from the joint probability distribution , but not from the conditional one . \n\t', '\n\t\t However , the unbounded nature of the parsing problem means that the individual parameters of the discriminative model are much harder to estimate than those of the generative model . \n\t', '\n\t\t The parameters of the discriminative model include an unbounded lookahead string in the conditional . \n\t', '\n\t\t Because these words have not yet been reached by the parse , we cannot assign them any structure , and thus the estimation process has no way of knowing what words in this string will end up being relevant to the next decision it needs to make . \n\t', '\n\t\t The estimation process has to guess about the future role of an unbounded number of words , which makes the estimate quite difficult . \n\t', '\n\t\t In contrast , the parameters of the generative model only include words which are either already incorporated into the structure , or are the immediate next word to be incorporated . \n\t', '\n\t\t Thus it is relatively easy to determine the significance of each word . \n\t', '\n\t\t 3 Estimating the Parameters with a Neural Network The most challenging problem in estimating P(dzld1,...,dz-1 , yield(dz,...,dm)) and P(dz ld1 , ... , dz-1 ) is that the conditionals include an unbounded amount of information . \n\t', '\n\t\t Both the parse history d1 , ... , dz-1 and the lookahead string yield(dz , ... , dm ) grow with the length of the sentence . \n\t', '\n\t\t In order to apply standard probability estimation methods , we use neural networks to induce finite representations of both these sequences , which we will denote h(d1 , ... , dz-1 ) and l(yield(dz , ... , dm ) ) , respectively . \n\t', '\n\t\t The neural network training methods we use try to find representations which preserve all the information about the sequences which are relevant to estimating the desired probabilities . \n\t', '\n\t\t P(dz l d1 , ... , dz-1 ) Pz~ P(dz l h(d1 , ... , dz-1 ) ) P(dzl d1 , ... , dz-1 , yield(dz , ... , dm ) ) ~ P(dz l h(d1 , ... , dz-1 ) , l ( yield(dz , ... , dm ) ) ) Of the previous work on using neural net- works for parsing natural language , by far the most empirically successful has been the work using Simple Synchrony Networks . \n\t', '\n\t\t Like other recurrent network architectures , SSNs compute a representation of an unbounded sequence by incrementally computing a representation of each prefix of the sequence . \n\t', '\n\t\t At each position i , representations from earlier in the sequence are combined with features of the new position i to produce a vector of real valued features which represent the prefix ending at i . \n\t', '\n\t\t This repre- sentation is called a hidden representation . \n\t', '\n\t\t It is analogous to the hidden state of a Hidden Markov Model . \n\t', '\n\t\t As long as the hidden representation for position i ^ 1 is always used to compute the hidden representation for position i , any information about the entire sequence could be passed from hidden representation to hidden representation and be included in the hidden representation of that sequence . \n\t', '\n\t\t When these representations are then used to estimate probabilities , this property means that we are not making any a priori hard independence assumptions ( although some independence may be learned from the data ) . \n\t', '\n\t\t The difference between SSNs and most other recurrent neural network architectures is that SSNs are specifically designed for processing structures . \n\t', '\n\t\t When computing the history representation h(d1 , ... , dz-1 ) , the SSN uses not only the previous history representation h(d1 , ... , dz-2 ) , but also uses history representations for earlier positions which are particularly relevant to choosing the next parser decision dz . \n\t', '\n\t\t This relevance is determined by first assigning each position to a node in the parse tree , namely the node which is on the top of the parser\x92s stack when that decision is made . \n\t', '\n\t\t Then the relevant earlier positions are chosen based on the structural locality of the current decision\x92s node to the earlier decisions\x92 nodes . \n\t', '\n\t\t In this way , the number of representations which information needs to pass through in order to flow from history representation i to history representation j is determined by the structural distance between i\x92s node and j\x92s node , and not just the distance between i and j in the parse sequence . \n\t', '\n\t\t This provides the neural network with a linguistically appropriate inductive bias when it learns the history representations , as explained in more detail in \n\t\t']",Positive
"['\n\t\t When computing the lookahead representation l(yield(dz , ... , dm ) ) , there is no structural information available to tell us which positions are most relevant to choosing the decision di . \n\t', '\n\t\t Proximity in the string is our only indication of relevance . \n\t', '\n\t\t Therefore we compute l(yield(di,...,dm)) by running a recurrent neural network backward over the string , so that the most recent input is the first word in the lookahead string , as discussed in more detail in \n\t\t']",Positive
"['\n\t\t Once it has computed h(d1 , ... , di_1 ) and ( for the discriminative model ) l(yield(di , ... , dm ) ) , the SSN uses standard methods \n\t\t']",Positive
"['\n\t\t This involves further decomposing the distribution over all possible next parser actions into a small hierarchy of conditional probabilities , and then using log-linear models to estimate each of these conditional probability distributions . \n\t', '\n\t\t The input features for these log- linear models are the real-valued vectors computed by h(d1 , ... , di_ 1 ) and l(yield(di , ... , dm ) ) , as explained in more detail in \n\t\t']",Positive
"['\n\t\t Thus the full neural network consists of a recurrent hidden layer for h(d1,...,di_1) , ( for the discriminative model ) a recurrent hidden layer for l(yield(di , ... , dm ) ) , and an output layer for the log-linear model . \n\t', '\n\t\t Training is applied to this full neural network , as described in the next section . \n\t', '\n\t\t 4 Three Optimization Criteria and their Training Methods As with many other machine learning methods , training a Simple Synchrony Network involves first defining an appropriate learning criteria and then performing some form of gradient descent learning to search for the optimum values of the network\x92s parameters according to this criteria . \n\t', '\n\t\t In all the parsing models investigated here , we use the on-line version of Backpropagation to perform the gradient descent . \n\t', '\n\t\t This learning simultaneously tries to optimize the parameters of the output computation and the parameters of the mappings h(d1,...,di_1) and l(yield(di , ... , dm ) ) . \n\t', '\n\t\t With multi-layered networks such as SSNs , this training is not guaranteed to converge to a global optimum , but in practice a network whose criteria value is close to the optimum can be found . \n\t', '\n\t\t The three parsing models differ in the criteria the neural networks are trained to optimize . \n\t', '\n\t\t Two of the neural networks are trained using the standard maximum likelihood approach of optimizing the same probability which they are estimating , one generative and one discriminative . \n\t', '\n\t\t For the generative model , this means maximiz ing the total joint probability of the parses and the sentences in the training corpus . \n\t', '\n\t\t For the discriminative model , this means maximizing the conditional probability of the parses in the training corpus given the sentences in the training corpus . \n\t', '\n\t\t To make the computations easier , we actually minimize the negative log of these probabilities , which is called cross-entropy error . \n\t', '\n\t\t Minimizing this error ensures that training will converge to a neural network whose outputs are estimates of the desired probabilities.2 For each parse in the training corpus , Backpropagation training involves first computing the probability which the current network assigns to that parse , then computing the first derivative of ( the negative log of ) this probability with respect to each of the network\x92s parameters , and then updating the parameters proportionately to this derivative.3 The third neural network combines the advantages of the generative probability model with the advantages of the discriminative optimization criteria . \n\t', '\n\t\t The structure of the network and the set of outputs which it computes are exactly the same as the above network for the generative model . \n\t', '\n\t\t But the training procedure is designed to maximize the conditional probability of the parses in the training corpus given the sentences in the training corpus . \n\t', '\n\t\t The conditional probability for a sentence can be computed from the joint probability of the generative model by normalizing over the set of all parses d~1 , ... , d~m , for the sentence . \n\t', '\n\t\t P ( d1,...,dm| w1 , ... , wn ) _ /~d,,.pd,,,.P(di )..,dam , ) So , with this approach , we need to maximize this normalized probability , and not the probability computed by the network . \n\t', '\n\t\t The difficulty with this approach is that there are exponentially many parses for the sentence , so it is not computationally feasible to compute them all . \n\t', '\n\t\t We address this problem by only computing a small set of the most probable parses . \n\t', '\n\t\t The remainder of the sum is estimated using a combination of the probabilities from the best parses and the probabilities 2Cross-entropy error ensures that the minimum of the error function converges to the desired probabilities as the amount of training data increases \n\t\t']",Positive
"['\n\t\t 3A number of additional training techniques , such as regularization , are added to this basic procedure , as will be specified in section 6. from the partial parses which were pruned when searching for the best parses . \n\t', '\n\t\t The probabilities of pruned parses are estimated in such a way as to minimize their effect on the training process . \n\t', '\n\t\t For each decision which is part of some unpruned parses , we calculate the average probability of generating the remainder of the sentence by these un-pruned parses , and use this as the estimate for generating the remainder of the sentence by the pruned parses . \n\t', '\n\t\t With this estimate we can calculate the sum of the probabilities for all the pruned parses which originate from that decision . \n\t', '\n\t\t This approach gives us a slight overestimate of the total sum , but because this total sum acts simply as a weighting factor , it has little effect on learning . \n\t', '\n\t\t What is important is that this estimate minimizes the effect of the pruned parses\x92 probabilities on the part of the training process which occurs after the probabilities of the best parses have been calculated . \n\t', '\n\t\t After estimating P(d1 , ... , dm|w1 , ... , wn ) , training requires that we estimate the first derivative of ( the negative log of ) this probability with respect to each of the network\x92s parameters . \n\t', '\n\t\t The contribution to this derivative of the numerator in the above equation is the same as in the generative case , just scaled by the denominator . \n\t', '\n\t\t The difference between the two learning methods is that we also need to account for the contribution to this derivative of the denominator . \n\t', '\n\t\t Here again we are faced with the problem that there are an exponential number of derivations in the denominator , so here again we approximate this calculation using the most probable parses . \n\t', '\n\t\t To increase the conditional probability of the correct parse , we want to decrease the total joint probabilities of the incorrect parses . \n\t', '\n\t\t Probability mass is only lost from the sum over all parses because shift(wi) actions are only allowed for the correct wi . \n\t', '\n\t\t Thus we can decrease the total joint probability of the incorrect parses by making these parses be worse predictors of the words in the sentence.4 The combination of training the correct parses to be good predictors of the words and training the incorrect parses to be bad predictors of the words results in prediction prob- 4Non-prediction probability estimates for incorrect parses can make a small contribution to the derivative , but because pruning makes the calculation of this contribution inaccurate , we treat this contribution as zero when training . \n\t', '\n\t\t This means that non-prediction outputs are trained to maximize the same criteria as in the generative case . \n\t', '\n\t\t abilities which are not accurate estimates , but which are good at discriminating correct parses from incorrect parses . \n\t', '\n\t\t It is this feature which gives discriminative training an advantage over generative training . \n\t', '\n\t\t The network does not need to learn an accurate model of the distribution of words . \n\t', '\n\t\t The network only needs to learn an accurate model of how words disambiguate previous parsing decisions . \n\t', '\n\t\t When we apply discriminative training only to the most probable incorrect parses , we train the network to discriminate between the correct parse and those incorrect parses which are the most likely to be mistaken for the correct parse . \n\t', '\n\t\t In this sense our approximate training method results in optimizing the decision boundary between correct and incorrect parses , rather than optimizing the match to the conditional probability . \n\t', '\n\t\t Modifying the training method to systematically optimize the decision boundary ( as in large margin methods such as Support Vector Machines ) is an area of future research . \n\t', '\n\t\t 5 Searching for the most probable parse The complete parsing system uses the probability estimates computed by the SSN to search for the most probable parse . \n\t', '\n\t\t The search incrementally constructs partial parses d1 , ... , di by taking a parse it has already constructed d1 , ... , di\x971 and using the SSN to estimate a probability distribution P(di |d1 , ... , di\x97 1 , ... ) over possible next decisions di . \n\t', '\n\t\t These probabilities are then used to compute the probabilities for d1 , ... , di . \n\t', '\n\t\t In general , the partial parse with the highest probability is chosen as the next one to be extended , but to perform the search efficiently it is necessary to prune the search space . \n\t', '\n\t\t The main pruning is that only a fixed number of the most probable derivations are allowed to continue past the shifting of each word . \n\t', '\n\t\t Setting this post-word beam width to 5 achieves fast parsing with reasonable performance in all models . \n\t', '\n\t\t For the parsers with generative probability models , maximum accuracy is achieved with a post-word beam width of 100 . \n\t', '\n\t\t 6 The Experiments We used the Penn Treebank \n\t\t']",Positive
['\n\t\t In each case the input to the network is a sequence of tag-word pairs.5 5We used a publicly available tagger \n\t\t'],Positive
"['\n\t\t For each tag , there is an We report results for three different vocabulary sizes , varying in the frequency with which tag- word pairs must occur in the training set in order to be included explicitly in the vocabulary . \n\t', '\n\t\t A frequency threshold of 200 resulted in a vocabulary of 508 tag-word pairs , a threshold of 20 resulted in 4215 tag-word pairs , and a threshold of 5 resulted in 11,993 tag-word pairs For the generative model we trained networks for the 508 ( \x93GSSN-Freq>200\x94 ) and 4215 ( \x93GSSN-Freq>20\x94 ) word vocabularies . \n\t', '\n\t\t The need to calculate word predictions makes training times for the 11,993 word vocabulary very long , and as of this writing no such network training has been completed . \n\t', '\n\t\t The discriminative model does not need to calculate word predictions , so it was feasible to train networks for the 11,993 word vocabulary ( \x93DSSN-Freq>5\x94 ) . \n\t', '\n\t\t Previous results \n\t\t']",Positive
"['\n\t\t For the networks trained with the discriminative optimization criteria and the generative probability model , we trained networks for the 508 ( \x93DGSSN-Freq>200\x94 ) and 4215 ( \x93DGSSNFreq>20\x94 ) word vocabularies . \n\t', '\n\t\t For this training , we need to select a small set of the most probable incorrect parses . \n\t', '\n\t\t When we tried using only the network being trained to choose these top parses , training times were very long and the resulting networks did not outperform their generative counterparts . \n\t', '\n\t\t In the experiments reported here , we provided the training with a list of the top 20 parses found by a network of the same type which had been trained with the generative criteria . \n\t', '\n\t\t The network being trained was then used to choose its top 10 parses from this list , and training was performed on these 10 parses and the correct parse.6 This reduced the time necessary to choose the top parses during training , and helped focus the early stages of training on learning relevant discriminations . \n\t', '\n\t\t Once the training of these networks was complete , we tested both their ability to parse on their own and their ability to re-rank the top unknown-word vocabulary item which is used for all those words which are not sufficiently frequent with that tag to be included individually in the vocabulary ( as well as other words if the unknown-word case itself does not have at least 5 instances ) . \n\t', '\n\t\t We did no morphological analysis of unknown words . \n\t', '\n\t\t 6The 20 candidate parses and the 10 training parses were found with post-word beam widths of 20 and 10 , respectively , so these are only approximations to the top parses . \n\t', '\n\t\t 20 parses of their associated generative model ( \x93DGSSN- ... , rerank\x94 ) . \n\t', '\n\t\t We determined appropriate training parameters and network size based on intermediate validation results and our previous experience.7 We trained several networks for each of the GSSN models and chose the best ones based on their validation performance . \n\t', '\n\t\t We then trained one network for each of the DGSSN models and for the DSSN model . \n\t', '\n\t\t The best post-word beam width was determined on the validation set , which was 5 for the DSSN model and 100 for the other models . \n\t', '\n\t\t To avoid repeated testing on the standard testing set , we first compare the different models with their performance on the validation set . \n\t', '\n\t\t Standard measures of accuracy are shown in table 1.8 The largest accuracy difference is between the parser with the discriminative probability model ( DSSN-Freq>5 ) and those with the generative probability model , despite the larger vocabulary of the former . \n\t', '\n\t\t This demonstrates the difficulty of estimating the parameters of a discriminative probability model . \n\t', '\n\t\t There is also a clear effect of vocabulary size , but there is a slightly larger effect of training method . \n\t', '\n\t\t When tested in the same way as they were trained ( for reranking ) , the parsers which were trained with a discriminative criteria achieve a 7 % and 8 % reduction in error rate over their respective parsers with the same generative probability model . \n\t', '\n\t\t When tested alone , these DGSSN parsers perform only slightly better than their respective GSSN parsers . \n\t', '\n\t\t Initial experiments on giving these networks exposure to parses outside the top 20 parses of the GSSN parsers at the very end of training did not result in any improvement on this task . \n\t', '\n\t\t This suggests that at least some of the advantage of the DSSN models is due to the fact that re-ranking is a simpler task than parsing from scratch . \n\t', '\n\t\t But additional experimental work would be necessary to make any definite conclusions about this issue . \n\t', '\n\t\t 7All the best networks had 80 hidden units for the history representation ( and 80 hidden units in the lookahead representation ) . \n\t', '\n\t\t Weight decay regularization was applied at the beginning of training but reduced to near 0 by the end of training . \n\t', '\n\t\t Training was stopped when maximum performance was reached on the validation set , using a post-word beam width of 5 . \n\t', '\n\t\t 8All our results are computed with the evalb program following the standard criteria in \n\t\t']",Positive
"['\n\t\t LR LP F,a_1* DSSN-Freq>5 84.9 86.0 85.5 GSSN-Freq>200 87.6 88.9 88.2 DGSSN-Freq>200 87.8 88.8 88.3 GSSN-Freq>20 88.2 89.3 88.8 DGSSN-Freq>200 , rerank 88.5 89.6 89.0 DGSSN-Freq>20 88.5 89.7 89.1 DGSSN-Freq>20 , rerank 89.0 90.3 89.6 Table 1 : Percentage labeled constituent recall ( LR ) , precision ( LP ) , and a combination of both ( F,a_1 ) on validation set sentences of length at most 100 . \n\t', '\n\t\t LR LP F,a_1* Ratnaparkhi99 86.3 87.5 86.9 Collins99 88.1 88.3 88.2 Collins&Duffy02 88.6 88.9 88.7 Charniak00 89.6 89.5 89.5 Collins00 89.6 89.9 89.7 DGSSN-Freq>20 , rerank 89.8 90.4 90.1 Bod03 90.7 90.8 90.7 * Fp=1 for previous models may have rounding errors . \n\t', '\n\t\t Table 2 : Percentage labeled constituent recall ( LR ) , precision ( LP ) , and a combination of both ( F,a_1 ) on the entire testing set . \n\t', '\n\t\t For comparison to previous results , table 2 lists the results for our best model ( DGSSNFreq>20 , rerank)9 and several other statistical parsers \n\t\t']",Positive
['\n\t\t Our best performing model is more accurate than all these previous models except \n\t\t'],Positive
"['\n\t\t This DGSSN parser achieves this result using much less lexical knowledge than other approaches , which mostly use at least the words which occur at least 5 times , plus morphological features of the remaining words . \n\t', '\n\t\t However , the fact that the DGSSN uses a large-vocabulary tagger \n\t\t']",Positive
"['\n\t\t Also , the main reason for using a smaller vocabulary is the computational complexity of computing probabilities for the shift(wi) actions on-line , which other models do not require . \n\t', '\n\t\t 9On sentences of length at most 40 , the DGSSNFreq>20-rerank model gets 90.1 % recall and 90.7 % precision . \n\t', '\n\t\t 7 Related Work \n\t\t']",Positive
"['\n\t\t His maximal conditional likelihood estimate for a PCFG takes the same approach as our generative model trained with a discriminative criteria . \n\t', '\n\t\t While he shows a non-significant increase in performance over the standard maximal joint likelihood estimate on a small dataset , because he did not have a computationally efficient way to train this model , he was not able to test it on the standard datasets . \n\t', '\n\t\t The other models he investigates conflate changes in the probability models with changes in the training criteria , and the discriminative probability models do worse . \n\t', '\n\t\t In the context of part-of-speech tagging , \n\t\t']",Positive
"['\n\t\t However , their arguments are made in terms of independence assumptions . \n\t', '\n\t\t Our results show that these generalizations also apply to methods which do not rely on independence assumptions . \n\t', '\n\t\t While both \n\t\t']",Negative
"['\n\t\t Our proposed training method succeeds in being both tractable and effective , demonstrating both a significant improvement over the equivalent generative model and state-of-the-art accuracy . \n\t', '\n\t\t \n\t\t']",Negative
"['\n\t\t Both these methods are limited to reranking the output of another parser , while our trained parser can be used alone . \n\t', '\n\t\t Neither of these methods use the parameters of a generative probability model , which might explain our better performance ( see table 2 ) . \n\t', '\n\t\t 8 Conclusions This article has investigated the application of discriminative methods to broad coverage natural language parsing . \n\t', '\n\t\t We distinguish between two different ways to apply discriminative methods , one where the probability model is changed to a discriminative one , and one where the probability model remains generative but the training method optimizes a discriminative criteria . \n\t', '\n\t\t We find that the discriminative probability model is much worse than the generative one , but that training to optimize the discriminative criteria results in improved performance . \n\t', '\n\t\t Performance of the latter model on the standard test set achieves 90.1 % F-measure on constituents , which is the second best current accuracy level , and only 0.6 % below the current best \n\t\t']",Positive
"['\n\t\t This paper has also proposed a neural network training method which optimizes a discriminative criteria even when the parameters being estimated are those of a generative probability model . \n\t', '\n\t\t This training method successfully satisfies the conflicting constraints that it be computationally tractable and that it be a good approximation to the theoretically optimal method . \n\t', '\n\t\t This approach contrasts with previous approaches to scaling up discriminative methods to broad coverage natural language parsing , which have parameterizations which depart substantially from the successful previous generative models of parsing . \n\t', '\n\t\t References Christopher M. Bishop . \n\t', '\n\t\t 1995 . \n\t', '\n\t\t Neural Networks for Pattern Recognition . \n\t', '\n\t\t Oxford University Press , Oxford , UK . \n\t', '\n\t\t Rens Bod . \n\t', '\n\t\t 2003. An efficient implementation of a new DOP model . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 10th Conf . \n\t', '\n\t\t of European Chapter of the Association for Computational Linguistics , Budapest , Hungary . \n\t', '\n\t\t Eugene Charniak . \n\t', '\n\t\t 2000. A maximum-entropyinspired parser . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 1st Meeting of North American Chapter of Association for Computational Linguistics , pages 132\x96139 , Seattle , Washington . \n\t', '\n\t\t Michael Collins and Nigel Duffy . \n\t', '\n\t\t 2002. New ranking algorithms for parsing and tagging : Kernels over discrete structures and the voted perceptron . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 35th Meeting of Association for Computational Linguistics , pages 263\x96270 . \n\t', '\n\t\t Michael Collins . \n\t', '\n\t\t 1999. Head-Driven Statistical Models for Natural Language Parsing . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , University of Pennsylvania , Philadelphia , PA . \n\t', '\n\t\t Michael Collins . \n\t', '\n\t\t 2000. Discriminative rerank- ing for natural language parsing . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 17th Int. Conf . \n\t', '\n\t\t on Machine Learning , pages 175\x96182 , Stanford , CA . \n\t', '\n\t\t James Henderson . \n\t', '\n\t\t 2003a . \n\t', '\n\t\t Generative ver- sus discriminative models for statistical left- corner parsing . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 8th Int. Workshop on Parsing Technologies , pages 115\x96126 , Nancy , France . \n\t', '\n\t\t James Henderson . \n\t', '\n\t\t 2003b . \n\t', '\n\t\t Inducing history representations for broad coverage statistical parsing . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t joint meeting of North American Chapter of the Association for Computational Linguistics and the Human Language Technology Conf. , pages 103\x96110 , Edmonton , Canada . \n\t', '\n\t\t Mark Johnson . \n\t', '\n\t\t 2001. Joint and conditional estimation of tagging and parsing models . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 39th Meeting of Association for Computational Linguistics , pages 314\x96321 , Toulouse , France . \n\t', '\n\t\t Dan Klein and Christopher D. Manning . \n\t', '\n\t\t 2002. Conditional structure versus conditional estimation in NLP models . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t Conf . \n\t', '\n\t\t on Empirical Methods in Natural Language Processing , pages 9\x9616 , Univ . \n\t', '\n\t\t of Pennsylvania , PA . \n\t', '\n\t\t Peter Lane and James Henderson . \n\t', '\n\t\t 2001 . \n\t', '\n\t\t Incremental syntactic parsing of natural language corpora with Simple Synchrony Networks . \n\t', '\n\t\t IEEE Transactions on Knowledge and Data Engineering , 13(2):219\x96231 . \n\t', '\n\t\t Mitchell P. Marcus , Beatrice Santorini , and Mary Ann Marcinkiewicz . \n\t', '\n\t\t 1993. Building a large annotated corpus of English : The Penn Treebank . \n\t', '\n\t\t Computational Linguistics , 19(2):313\x96330 . \n\t', '\n\t\t A. Y. Ng and M. I. Jordan . \n\t', '\n\t\t 2002. On discriminative vs. generative classifiers : A comparison of logistic regression and naive bayes . \n\t', '\n\t\t In T. G. Dietterich , S. Becker , and Z. Ghahramani , editors , Advances in Neural Information Processing Systems 14 , Cambridge , MA . \n\t', '\n\t\t MIT Press . \n\t', '\n\t\t Adwait Ratnaparkhi . \n\t', '\n\t\t 1996. A maximum entropy model for part-of-speech tagging . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t Conf . \n\t', '\n\t\t on Empirical Methods in Natural Language Processing , pages 133\x96142 , Univ . \n\t', '\n\t\t of Pennsylvania , PA . \n\t', '\n\t\t Adwait Ratnaparkhi . \n\t', '\n\t\t 1999. Learning to parse natural language with maximum entropy models . \n\t', '\n\t\t Machine Learning , 34:151\x96175 . \n\t', '\n\t\t D.J. Rosenkrantz and P.M. Lewis . \n\t', '\n\t\t 1970. Deterministic left corner parsing . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 11th Symposium on Switching and Automata Theory , pages 139\x96152 . \n\t', '\n\t\t Vladimir N. Vapnik . \n\t', '\n\t\t 1995. The Nature of Statistical Learning Theory . \n\t', '\n\t\t Springer-Verlag , New York . \n\t', '\n\t\t Parsing the WSJ using CCG and Log-Linear Models Stephen Clark School of Informatics University of Edinburgh 2 Buccleuch Place , Edinburgh , UK stephen.clark@ed.ac.uk James R. Curran School of Information Technologies University of Sydney NSW 2006 , Australia james@it.usyd.edu.au Abstract This paper describes and evaluates log-linear parsing models for Combinatory Categorial Grammar ( CCG ) . \n\t', '\n\t\t A parallel implementation of the L-BFGS optimisation algorithm is described , which runs on a Beowulf cluster allowing the complete Penn Treebank to be used for estimation . \n\t', '\n\t\t We also develop a new efficient parsing algorithm for CCG which maximises expected recall of dependencies . \n\t', '\n\t\t We compare models which use all CCG derivations , including nonstandard derivations , with normal-form models . \n\t', '\n\t\t The performances of the two models are comparable and the results are competitive with existing wide-coverage CCG parsers . \n\t', '\n\t\t 1 Introduction A number of statistical parsing models have recently been developed for Combinatory Categorial Grammar ( CCG ; Steedman , 2000 ) and used in parsers applied to the WSJ Penn Treebank \n\t\t']",Positive
['\n\t\t In \n\t\t'],Positive
"['\n\t\t However , estimating a log-linear model for a wide- coverage CCG grammar is very computationally expensive . \n\t', '\n\t\t Following \n\t\t']",Positive
"['\n\t\t We also showed how the complete WSJ Penn Treebank can be used for training by developing a parallel version of Generalised Iterative Scaling ( GIS ) to perform the estimation . \n\t', '\n\t\t This paper significantly extends our earlier work in a number of ways . \n\t', '\n\t\t First , we evaluate a number of log-linear models , obtaining results which are competitive with the state-of-the-art for CCG parsing . \n\t', '\n\t\t We also compare log-linear models which use all CCG derivations , including non-standard derivations , with normal-form models . \n\t', '\n\t\t Second , we find that GIS is unsuitable for estimating a model of the size being considered , and develop a parallel version of the L-BFGS algorithm \n\t\t']",Positive
"['\n\t\t And finally , we show that the parsing algo rithm described in \n\t\t']",Negative
"['\n\t\t The development of parsing and estimation algorithms for models which use all derivations extends existing CCG parsing techniques , and allows us to test whether there is useful information in the additional derivations . \n\t', '\n\t\t However , we find that the performance of the normal-form model is at least as good as the all-derivations model , in our experiments to- date . \n\t', '\n\t\t The normal-form approach allows the use of additional constraints on rule applications , leading to a smaller model , reducing the computational resources required for estimation , and resulting in an extremely efficient parser . \n\t', '\n\t\t This paper assumes a basic understanding of CCG ; see \n\t\t']",Positive
"['\n\t\t 2 Parsing Models for CCG CCG is unusual among grammar formalisms in that , for each derived structure for a sentence , there can be many derivations leading to that structure . \n\t', '\n\t\t The presence of such ambiguity , sometimes referred to as spurious ambiguity , enables CCG to produce elegant analyses of coordination and extraction phenomena \n\t\t']",Positive
"['\n\t\t However , the introduction of extra derivations increases the complexity of the modelling and parsing problem . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t They use a conditional model , based on \n\t\t']",Negative
"['\n\t\t provide a useful baseline for the new models presented here . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t In this paper we compare the normal-form approach with a dependency model . \n\t', '\n\t\t For the dependency model , we define the probabil- ity of a dependency structure as follows : P(7rIS) = z P(d , 7rIS ) ( 1 ) dE4(7r) where 7r is a dependency structure , S is a sentence and A(7r) is the set of derivations which lead to 7r . \n\t', '\n\t\t This extends the approach of \n\t\t']",Positive
"['\n\t\t In contrast to the dependency model , the normal-form model simply defines a distribution over normal- form derivations . \n\t', '\n\t\t The dependency structures considered in this paper are described in detail in \n\t\t']",Positive
"['\n\t\t Each argument slot in a CCG lexical category represents a dependency relation , and a dependency is defined as a 5-tuple ( h f , f , s , ha , l ) , where h f is the head word of the lexical category , f is the lexical category , s is the argument slot , ha is the head word of the argument , and l indicates whether the dependency is long-range . \n\t', '\n\t\t For example , the long-range dependency encoding company as the extracted object of bought ( as in the company that IBM bought ) is represented as the following 5-tuple : ( bought , (S[dcl]\\NP,)/NP , , 2 , company , * ) where * is the category (NP\\NP)/(S[dcl]/NP) assigned to the relative pronoun . \n\t', '\n\t\t For local dependencies l is assigned a null value . \n\t', '\n\t\t A dependency structure is a multiset of these dependencies . \n\t', '\n\t\t 3 Log-Linear Parsing Models Log-linear models ( also known as Maximum Entropy models ) are popular in NLP because of the ease with which discriminating features can be included in the model . \n\t', '\n\t\t Log-linear models have been applied to the parsing problem across a range of grammar formalisms , e.g. \n\t\t']",Positive
"['\n\t\t One motivation for using a log-linear model is that long-range dependencies which CCG was designed to handle can easily be encoded as features . \n\t', '\n\t\t A conditional log-linear model of a parse W E 92 , given a sentence S , is defined as follows : P(WIS) = 1 eA.f(-) ( 2 ) ZS where A. f(W) = EZ AZfZ ( W ) . \n\t', '\n\t\t The function fZ is a feature of the parse which can be any real-valued function over the space of parses 92 . \n\t', '\n\t\t Each feature fZ has an associated weight AZ which is a parameter of the model to be estimated . \n\t', ""\n\t\t ZS is a normalising constant which ensures that P(WIS) is a probability distribution : ZS = z eA.f(-') ( 3 ) -'EP(S) where p(S) is the set of possible parses for S . \n\t"", '\n\t\t For the dependency model a parse , W , is a ( d , 7r ) pair ( as given in ( 1 ) ) . \n\t', '\n\t\t A feature is a count of the number of times some configuration occurs in d or the number of times some dependency occurs in 7r . \n\t', '\n\t\t Section 6 gives examples of features . \n\t', '\n\t\t 3.1 The Dependency Model We follow \n\t\t']",Positive
"['\n\t\t For the dependency model , the data consists of sentences S 1 , ... , SM , together with gold standard dependency structures , 7r1 , ... , 7rM . \n\t', '\n\t\t The gold standard structures are multisets of dependencies , as described earlier . \n\t', '\n\t\t Section 6 explains how the gold standard structures are obtained . \n\t', '\n\t\t The objective function of a model A is the conditional log-likelihood , L(A) , minus a Gaussian prior term , G(A) , used to reduce overfitting \n\t\t']",Positive
"[""\n\t\t Hence , given the definition of the probability of a dependency structure ( 1 ) , the objective function is as follows : L'(A) = L(A) \x97 G(A) ( 4 ) = log M P~(7rjISj) \x97 zn A. H Z=1 Z j=1 20 . \n\t"", '\n\t\t Z lo EdEA(7rj) eA.f(d,7rj) g eA.f(-) G -EP(Sj) log z eA.f(d,7rj) dEA(7rj) log z eA.f(-) \x97 -EP(Sj) where n is the number of features . \n\t', '\n\t\t Rather than have a different smoothing parameter 0Z for each feature , we use a single parameter 0 . \n\t', '\n\t\t We use a technique from the numerical optimisation literature , the L-BFGS algorithm \n\t\t']",Positive
"['\n\t\t L-BFGS is an iterative algorithm which requires the gradient of the objective function to be computed at each iteration . \n\t', '\n\t\t The components of the gradient vec- = M = z j=1 M z j=1 \x97 M z j=1 A. Z zn Z=1 20 . \n\t', '\n\t\t Z A. Z zn Z=1 20 . \n\t', '\n\t\t Z tor are as follows : eA.f(d,,rj) fi ( d , ~ j ) ( 5 ) EdE4(,rj) eA.f(d,,rj) eA.f(-) fi(to) Ai E\x97 -EP(Sj) eA.f(-) ~2i The first two terms in ( 5 ) are expectations of feature fi : the first expectation is over all derivations leading to each gold standard dependency structure ; the second is over all derivations for each sentence in the training data . \n\t', '\n\t\t Setting the gradient to zero yields the usual maximum entropy constraints \n\t\t']",Positive
"['\n\t\t The estimation process attempts to make the expectations equal , by putting as much mass as possible on the derivations leading to the gold standard structures.1 The Gaussian prior term penalises any model whose weights get too large in absolute value . \n\t', '\n\t\t Calculation of the feature expectations requires summing over all derivations for a sentence , and summing over all derivations leading to a gold standard dependency structure . \n\t', '\n\t\t In both cases there can be exponentially many derivations , and so enumerating all derivations is not possible ( at least for wide-coverage automatically extracted grammars ) . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t Section 5 shows how the same technique can also be applied to all derivations leading to a gold standard dependency structure . \n\t', ""\n\t\t 3.2 The Normal-Form Model The objective function and gradient vector for the normal-form model are as follows : L'(A) = L(A) \x97 G(A) ( 6 ) = log M PA(djlSj) \x97 Zn i=1 A2 H i j=1 2^2 i aL ' ( A ) M fi ( dj ) ( 7 ) Z j=1 aAi = eA.f(d) fi(d) EdEB(Sj) eA.f(d) 1See \n\t\t""]",Positive
"['\n\t\t where dj is the the gold standard derivation for sentence Sj and B(Sj) is the set of possible derivations for Sj . \n\t', '\n\t\t Note that the empirical expectation in ( 7 ) is simply a count of the number of times the feature appears in the gold-standard derivations . \n\t', '\n\t\t 4 Packed Charts The packed charts perform a number of roles : they are a compact representation of a very large number of CCG derivations ; they allow recovery of the highest scoring parse or dependency structure without enumerating all derivations ; and they represent an instance of what \n\t\t']",Positive
"['\n\t\t The idea behind a packed chart is simple : equivalent chart entries of the same type , in the same cell , are grouped together , and back pointers to the daughters indicate how an individual entry was created . \n\t', '\n\t\t Equivalent entries form the same structures in any subsequent parsing . \n\t', '\n\t\t Since the packed charts are used for model estimation and recovery of the highest scoring parse or dependency structure , the features in the model partly determine which entries can be grouped together . \n\t', '\n\t\t In this paper we use features from the dependency structure , and features defined on the local rule instantiations.2 Hence , any two entries with identical category type , identical head , and identical unfilled dependencies are equivalent . \n\t', '\n\t\t Note that not all features are local to a rule instantiation ; for example , features encoding long-range dependencies may involve words which are a long way apart in the sentence . \n\t', '\n\t\t For the purposes of estimation and finding the highest scoring parse or dependency structure , only entries which are part of a derivation spanning the whole sentence are relevant . \n\t', '\n\t\t These entries can be easily found by traversing the chart top-down , starting with the entries which span the sentence . \n\t', '\n\t\t The entries within spanning derivations form a feature forest \n\t\t']",Positive
"['\n\t\t A feature forest ( D is a tuple ( C , D , R , y , S ) where : C is a set of conjunctive nodes ; D is a set of disjunctive nodes ; R c D is a set of root disjunctive nodes ; y : D 2C is a conjunctive daughter function ; 6 : C 2D is a disjunctive daughter function . \n\t', '\n\t\t The individual entries in a cell are conjunctive nodes , and the equivalence classes of entries are dis- 2By rule instantiation we mean the local tree arising from the application of a CCG combinatory rule . \n\t', ""\n\t\t aL ' ( A ) M Z Z dE4(,rj) j=1 aAi = Z -EP(Sj) \x97 M Z j=1 \x97 M Z Z dEB(Sj) j=1 Ai ~2 i ( C , D , R , y , 8 ) is a packed chart / feature forest G is a set of gold standard dependencies Let c be a conjunctive node Let d be a disjunctive node deps(c) is the set of dependencies on node c ~cdeps(c) c \x971 if , for some T E deps(c),T V G O \x97 |deps(c)| otherwise \x971 if cdeps(c) = \x971 \x971 if dmax(d) = \x971 for some d E 8(c) Zd-~8(c) dmax(d) + cdeps(c) otherwise dmax(d) = max{dmax(c) |I c E y(d) } mark(d) : mark d as a correct node foreach c E y(d) if dmax(c) = dmax(d) mark c as a correct node foreach d~ E 8(c) mark(d~) foreach d , E R such that dmax.(d,) = |G| mark(d,) Figure 1 : Finding nodes in correct derivations junctive nodes . \n\t"", '\n\t\t The roots of the CCG derivations represent the root disjunctive nodes.3 5 Efficient Estimation The L-BFGS algorithm requires the following values at each iteration : the expected value , and the empirical expected value , of each feature ( to calculate the gradient ) ; and the value of the likelihood function . \n\t', '\n\t\t For the normal-form model , the empirical expected values and the likelihood can easily be obtained , since these only involve the single gold- standard derivation for each sentence . \n\t', '\n\t\t The expected values can be calculated using the method in \n\t\t']",Positive
"['\n\t\t For the dependency model , the computations of the empirical expected values ( 5 ) and the likelihood function ( 4 ) are more complex , since these require sums over just those derivations leading to the gold standard dependency structure . \n\t', '\n\t\t We will refer to such derivations as correct derivations . \n\t', '\n\t\t Figure 1 gives an algorithm for finding nodes in a packed chart which appear in correct derivations . \n\t', '\n\t\t cdeps(c) is the number of correct dependencies on conjunctive node c , and takes the value \x971 if there are any incorrect dependencies on c. dmax(c) is 3A more complete description of CCG feature forests is given in \n\t\t']",Positive
"['\n\t\t the maximum number of correct dependencies produced by any sub-derivation headed by c , and takes the value \x971 if there are no sub-derivations producing only correct dependencies . \n\t', '\n\t\t dmax(d) is the same value but for disjunctive node d. Recursive definitions for calculating these values are given in Figure 1 ; the base case occurs when conjunctive nodes have no disjunctive daughters . \n\t', '\n\t\t The algorithm identifies all those root nodes heading derivations which produce just the correct dependencies , and traverses the chart top-down marking the nodes in those derivations . \n\t', '\n\t\t The insight behind the algorithm is that , for two conjunctive nodes in the same equivalence class , if one node heads a sub-derivation producing more correct dependencies than the other node ( and each sub-derivation only produces correct dependencies ) , then the node with less correct dependencies cannot be part of a correct derivation . \n\t', '\n\t\t The conjunctive and disjunctive nodes appearing in correct derivations form a new correct feature forest . \n\t', '\n\t\t The correct forest , and the complete forest containing all derivations spanning the sentence , can be used to estimate the required likelihood value and feature expectations . \n\t', ""\n\t\t Let E ' fi be the expected value of fi over the forest ( D for model A ; then the values in ( 5 ) can be obtained by calculating E( ' fi for the complete forest ( Dj for each sentence Sj in the train- ing data ( the second sum in ( 5 ) ) , and also EIY ' ~fi for each forest Tj of correct derivations ( the first sum in ( 5 ) ) : ( E~'.f \x97 EA'.f ) ( 8 ) The likelihood in ( 4 ) can be calculated as follows : ( log ZIY ' \x97 log Z~ ' ) ( 9 ) where log Z(D is the normalisation constant for ( D. 6 Estimation in Practice The gold standard dependency structures are produced by running our CCG parser over the normal-form derivations in CCGbank \n\t\t""]",Positive
"['\n\t\t Not all rule instantiations in CCGbank are instances of combinatory rules , and not all can be produced by the parser , and so gold standard structures were created for 85.5 % of the sentences in sections 2-21 ( 33,777 sentences ) . \n\t', '\n\t\t The same parser is used to produce the packed charts . \n\t', '\n\t\t The parser uses a maximum entropy supertagger \n\t\t']",Positive
"['\n\t\t For parsing the training data , we ensure that the correct category is a member of the set assigned to each word . \n\t', '\n\t\t The average number of categories assigned to each word is determined by a parameter in the supertagger . \n\t', '\n\t\t For the first set of experiments , we used a setting which assigns 1.7 categories on average per word . \n\t', '\n\t\t The feature set for the dependency model consists of the following types of features : dependency features ( with and without distance measures ) , rule instantiation features ( with and without a lexical head ) , lexical category features , and root category features . \n\t', '\n\t\t Dependency features are the 5-tuples defined in Section 1 . \n\t', '\n\t\t There are also three additional dependency feature types which have an extra distance field ( and only include the head of the lexical category , and not the head of the argument ) ; these count the number of words ( 0 , 1 , 2 or more ) , punctuation marks ( 0 , 1 , 2 or more ) , and verbs ( 0 , 1 or more ) between head and dependent . \n\t', '\n\t\t Lexical category features are word\x96category pairs at the leaf nodes , and root features are headword\x96category pairs at the root nodes . \n\t', '\n\t\t Rule instantiation features simply encode the combining categories together with the result category . \n\t', '\n\t\t There is an additional rule feature type which also encodes the lexical head of the resulting category . \n\t', '\n\t\t Additional generalised features for each feature type are formed by replacing words with their POS tags . \n\t', '\n\t\t The feature set for the normal-form model is the same except that , following \n\t\t']",Positive
"['\n\t\t Again there are 3 additional distance feature types , as above , which only include the head of the resulting category . \n\t', '\n\t\t We had hoped that by modelling the predicate-argument dependencies produced by the parser , rather than local rule dependencies , we would improve performance . \n\t', '\n\t\t However , using the predicate-argument dependencies in the normal-form model instead of , or in addition to , the local rule dependencies , has not led to an improvement in parsing accuracy . \n\t', '\n\t\t Only features which occurred more than once in the training data were included , except that , for the dependency model , the cutoff for the rule features was 9 and the counting was performed across all derivations , not just the gold-standard derivation . \n\t', '\n\t\t The normal-form model has 482,007 features and the dependency model has 984,522 features . \n\t', '\n\t\t We used 45 machines of a 64-node Beowulf clus ter to estimate the dependency model , with an average memory usage of approximately 550 MB for each machine . \n\t', '\n\t\t For the normal-form model we were able to reduce the size of the charts considerably by applying two types of restriction to the parser : first , categories can only combine if they appear together in a rule instantiation in sections 2\x9621 of CCGbank ; and second , we apply the normal-form restrictions described in \n\t\t']",Positive
['\n\t\t ( See \n\t\t'],Positive
"['\n\t\t ) The normal-form model requires only 5 machines for estimation , with an average memory usage of 730 MB for each machine . \n\t', '\n\t\t Initially we tried the parallel version of GIS described in \n\t\t']",Negative
"['\n\t\t However , we found that GIS converged extremely slowly ; this is in line with other recent results in the literature applying GIS to globally optimised models such as conditional random fields , e.g. \n\t\t']",Positive
"['\n\t\t As an alternative to GIS , we have implemented a parallel version of our L-BFGS code using the Message Passing Interface ( MPI ) standard . \n\t', '\n\t\t L-BFGS over forests can be parallelised , using the method described in \n\t\t']",Positive
"['\n\t\t The L-BFGS algorithm , run to convergence on the cluster , takes 479 iterations and 2 hours for the normal-form model , and 1,550 iterations and roughly 17 hours for the dependency model . \n\t', '\n\t\t 7 Parsing Algorithm For the normal-form model , the Viterbi algorithm is used to find the most probable derivation . \n\t', '\n\t\t For the dependency model , the highest scoring dependency structure is required . \n\t', '\n\t\t \n\t\t']",Negative
"['\n\t\t For a set of equivalent entries in the chart ( a disjunctive node ) , this involves summing over all conjunctive node daughters which head sub- derivations leading to the same set of high scoring dependencies . \n\t', '\n\t\t In practice large numbers of such conjunctive nodes lead to very long parse times . \n\t', '\n\t\t As an alternative to finding the most probable dependency structure , we have developed an algorithm which maximises the expected labelled recall over dependencies . \n\t', '\n\t\t Our algorithm is based on Goodman\x92s ( 1996 ) labelled recall algorithm for the phrase-structure PARSEVAL measures . \n\t', '\n\t\t Let L , be the number of correct dependencies in 7r with respect to a gold standard dependency structure G ; then the dependency structure , 7rmax , which maximises the expected recall rate is : LP LR UP UR cat Dep model 86.7 85.6 92.6 91.5 93.5 N-form model 86.4 86.2 92.4 92.2 93.6 7rmax = arg max E(LK/IGI) ( 10 ) K = arg max P(7riIS)I7r n 7riI Table 1 : Results on development set ; labelled and unlabelled precision and recall , and lexical category accuracy Z where S is the sentence for gold standard dependency structure G and 7ri ranges over the dependency structures for S . \n\t', '\n\t\t This expression can be expanded further : z 7rmax = arg max K Ki z = arg max K TEK z= arg max KTEK The final score for a dependency structure 7r is a sum of the scores for each dependency T in 7r ; and the score for a dependency T is the sum of the probabilities of those derivations producing T . \n\t', '\n\t\t This latter sum can be calculated efficiently using inside and outside scores : z 7rmax = arg max K TEK ( 12 ) where Oc is the inside score and q1c is the outside score for node c ( see \n\t\t']",Positive
"['\n\t\t The intuition behind the expected recall score is that a dependency structure scores highly if it has dependencies produced by high scoring derivations.4 The algorithm which finds 7rmax is a simple variant on the Viterbi algorithm , efficiently finding a derivation which produces the highest scoring set of dependencies . \n\t', '\n\t\t 8 Experiments Gold standard dependency structures were derived from section 00 ( for development ) and section 23 ( for testing ) by running the parser over the derivations in CCGbank , some of which the parser could not process . \n\t', '\n\t\t In order to increase the number of test sentences , and to allow a fair comparison with other CCG parsers , extra rules were encoded in the parser ( but we emphasise these were only used to obtain 4Coordinate constructions can create multiple dependencies for a single argument slot ; in this case the score for the multiple dependencies is the average of the individual scores . \n\t', '\n\t\t Features LP LR UP UR cat RULES 82.6 82.0 89.7 89.1 92.4 +HEADS 83.6 83.3 90.2 90.0 92.8 +DEPS 85.5 85.3 91.6 91.3 93.5 +DISTANCE 86.4 86.2 92.4 92.2 93.6 FINAL 87.0 86.8 92.7 92.5 93.9 Table 2 : Results on development set for the normal- form models the section 23 test data ; they were not used to parse unseen data as part of the testing ) . \n\t', '\n\t\t This resulted in 2,365 dependency structures for section 23 ( 98.5 % of the full section ) , and 1,825 ( 95.5 % ) dependency structures for section 00 . \n\t', '\n\t\t The first stage in parsing the test data is to apply the supertagger . \n\t', '\n\t\t We use the novel strategy developed in \n\t\t']",Positive
"['\n\t\t We were able to parse 98.9 % of section 23 using this strategy . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t For the normal-form model we returned the dependency structure for the most probable derivation , applying the two types of normal-form constraints described in Section 6 . \n\t', '\n\t\t For the dependency model we returned the dependency structure with the highest expected labelled recall score . \n\t', '\n\t\t Following \n\t\t']",Positive
"['\n\t\t For a labelled dependency to be correct , the first 4 elements of the dependency tuple must match exactly . \n\t', '\n\t\t For an unlabelled dependency to be correct , the heads of the functor and argument must appear together in some relation in the gold standard ( in any order ) . \n\t', '\n\t\t The results on section 00 , using the feature sets described earlier , are given in Table 1 , with similar results overall for the normal-form model and the dependency model . \n\t', '\n\t\t Since experimentation is easier with the normal-form model than the dependency model , we present additional results for the normal- form model . \n\t', '\n\t\t Table 2 gives the results for the normal-form model for various feature sets . \n\t', ""\n\t\t The results show that each additional feature type increases perfor- P(7riIS) z 1 if T E 7ri TEK z P(7r ' IS ) K1 ITEK1 z P(dIS) ( 11 ) dE0(K1)ITEK1 z Ocq1c if T E deps(c) 1 ZS cEC LP LR UP UR cat Clark et al . 2002 81.9 81.8 90.1 89.9 90.3 Hockenmaier 2003 84.3 84.6 91.8 92.2 92.2 Log-linear 86.6 86.3 92.5 92.1 93.6 Hockenmaier(POS) 83.1 83.5 91.1 91.5 91.5 Log-linear ( POS ) 84.8 84.5 91.4 91.0 92.5 Table 3 : Results on the test set mance . \n\t"", '\n\t\t Hockenmaier also found the dependencies to be very beneficial \x97 in contrast to recent results from the lexicalised PCFG parsing literature \n\t\t']",Positive
"['\n\t\t One of the advantages of a log-linear model is that it is easy to include additional information , such as distance , as features . \n\t', '\n\t\t The FINAL result in Table 2 is obtained by using a larger derivation space for training , created using more categories per word from the supertagger , 2.9 , and hence using charts containing more derivations . \n\t', '\n\t\t ( 15 machines were used to estimate this model . \n\t', '\n\t\t ) More investigation is needed to find the optimal chart size for estimation , but the results show a gain in accuracy . \n\t', '\n\t\t Table 3 gives the results of the best performing normal-form model on the test set . \n\t', '\n\t\t The results of \n\t\t']",Negative
"['\n\t\t The dependency set used by Hockenmaier contains some minor differences to the set used here , but \x93evaluating\x94 our test set against Hockenmaier\x92s gives an F-score of over 97 % , showing the test sets to be very similar . \n\t', '\n\t\t The results show that our parser is performing significantly better than that of Clark et al. , demonstrating the benefit of derivation features and the use of a sound statistical model . \n\t', '\n\t\t The results given so far have all used gold standard POS tags from CCGbank . \n\t', '\n\t\t Table 3 also gives the results if automatically assigned POS tags are used in the training and testing phases , using the C&C POS tagger \n\t\t']",Positive
"['\n\t\t The performance reduction is expected given that the supertagger relies heavily on POS tags as features . \n\t', '\n\t\t More investigation is needed to properly compare our parser and Hockenmaier\x92s , since there are a number of differences in addition to the models used : Hockenmaier effectively reads a lexicalised PCFG off CCGbank , and is able to use all of the available training data ; Hockenmaier does not use a supertagger , but does use a beam search . \n\t', '\n\t\t Parsing the 2,401 sentences in section 23 takes 1.6 minutes using the normal-form model , and 10.5 minutes using the dependency model . \n\t', '\n\t\t The difference is due largely to the normal-form constraints used by the normal-form parser . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t As a final oracle experiment we parsed the sentences in section 00 using the correct lexical categories from CCGbank . \n\t', '\n\t\t Since the parser uses only a subset of the lexical categories in CCGbank , 7 % of the sentences could not be parsed ; however , the labelled F-score for the parsed sentences was almost 98 % . \n\t', '\n\t\t This very high score demonstrates the large amount of information in lexical categories . \n\t', '\n\t\t 9 Conclusion A major contribution of this paper has been the development of a parsing model for CCG which uses all derivations , including non-standard derivations . \n\t', '\n\t\t Non-standard derivations are an integral part of the CCG formalism , and it is an interesting question whether efficient estimation and parsing algorithms can be defined for models which use all derivations . \n\t', '\n\t\t We have answered this question , and in doing so developed a new parsing algorithm for CCG which maximises expected recall of dependencies . \n\t', '\n\t\t We would like to extend the dependency model , by including the local-rule dependencies which are used by the normal-form model , for example . \n\t', '\n\t\t However , one of the disadvantages of the dependency model is that the estimation process is already using a large proportion of our existing resources , and extending the feature set will further increase the execution time and memory requirement of the estimation algorithm . \n\t', '\n\t\t We have also shown that a normal-form model performs as well as the dependency model . \n\t', '\n\t\t There are a number of advantages to the normal-form model : it requires less space and time resources for estimation and it produces a faster parser . \n\t', '\n\t\t Our normal-form parser significantly outperforms the parser of \n\t\t']",Negative
"['\n\t\t The use of adaptive supertagging and the normal-form constraints result in a very efficient wide-coverage parser . \n\t', '\n\t\t Our system demonstrates that accurate and efficient wide-coverage CCG parsing is feasible . \n\t', '\n\t\t Future work will investigate extending the feature sets used by the log-linear models with the aim of further increasing parsing accuracy . \n\t', '\n\t\t Finally , the oracle results suggest that further experimentation with the supertagger will significantly improve parsing accuracy , efficiency and robustness . \n\t', '\n\t\t Acknowledgements We would like to thank Julia Hockenmaier for the use of CCGbank and helpful comments , and Mark Steedman for guidance and advice . \n\t', '\n\t\t Jason Baldridge , Frank Keller , Yuval Krymolowski and Miles Osborne provided useful feedback . \n\t', '\n\t\t This work was supported by EPSRC grant GR/M96889 , and a Commonwealth scholarship and a Sydney University Travelling scholarship to the second author . \n\t', '\n\t\t References Adam Berger , Stephen Della Pietra , and Vincent Della Pietra . \n\t', '\n\t\t 1996. A maximum entropy approach to natural language processing . \n\t', '\n\t\t Computational Linguistics , 22(1):39\x9671 . \n\t', '\n\t\t Stanley Chen and Ronald Rosenfeld . \n\t', '\n\t\t 1999. A Gaussian prior for smoothing maximum entropy models . \n\t', '\n\t\t Technical report , Carnegie Mellon University , Pittsburgh , PA . \n\t', '\n\t\t Stephen Clark and James R. Curran . \n\t', '\n\t\t 2003. Log-linear models for wide-coverage CCG parsing . \n\t', '\n\t\t In Proceedings of the EMNLP Conference , pages 97\x96104 , Sapporo , Japan . \n\t', '\n\t\t Stephen Clark and James R. Curran . \n\t', '\n\t\t 2004. The importance of supertagging for wide-coverage CCG parsing . \n\t', '\n\t\t In Proceedings of COLING-04 , Geneva , Switzerland . \n\t', '\n\t\t Stephen Clark , Julia Hockenmaier , and Mark Steedman . \n\t', '\n\t\t 2002. Building deep dependency structures with a wide-coverage CCG parser . \n\t', '\n\t\t In Proceedings of the 40th Meeting of the ACL , pages 327\x96334 , Philadelphia , PA . \n\t', '\n\t\t Michael Collins . \n\t', '\n\t\t 1996. A new statistical parser based on bigram lexical dependencies . \n\t', '\n\t\t In Proceedings of the 34th Meeting of the ACL , pages 184\x96191 , Santa Cruz , CA . \n\t', '\n\t\t James R. Curran and Stephen Clark . \n\t', '\n\t\t 2003. Investigating GIS and smoothing for maximum entropy taggers . \n\t', '\n\t\t In Proceedings of the 10th Meeting of the EACL , pages 91\x9698 , Budapest , Hungary . \n\t', '\n\t\t Jason Eisner . \n\t', '\n\t\t 1996. Efficient normal-form parsing for Combinatory Categorial Grammar . \n\t', '\n\t\t In Proceedings of the 34th Meeting of the ACL , pages 79\x9686 , Santa Cruz , CA . \n\t', '\n\t\t Daniel Gildea . \n\t', '\n\t\t 2001. Corpus variation and parser performance . \n\t', '\n\t\t In Proceedings of the EMNLP Conference , pages 167\x96202 , Pittsburgh , PA . \n\t', '\n\t\t Joshua Goodman . \n\t', '\n\t\t 1996. Parsing algorithms and metrics . \n\t', '\n\t\t In Proceedings of the 34th Meeting of the ACL , pages 177\x96183 , Santa Cruz , CA . \n\t', '\n\t\t Julia Hockenmaier and Mark Steedman . \n\t', '\n\t\t 2002. Generative models for statistical parsing with Combinatory Categorial Grammar . \n\t', '\n\t\t In Proceedings of the 40th Meeting ofthe ACL , pages 335\x96342 , Philadelphia , PA . \n\t', '\n\t\t Julia Hockenmaier . \n\t', '\n\t\t 2003a . \n\t', '\n\t\t Data and Models for Statistical Parsing with Combinatory Categorial Grammar . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , University of Edinburgh . \n\t', '\n\t\t Julia Hockenmaier . \n\t', '\n\t\t 2003b . \n\t', '\n\t\t Parsing with generative models of predicate-argument structure . \n\t', '\n\t\t In Proceedings of the 41st Meeting of the ACL , pages 359\x96366 , Sapporo , Japan . \n\t', '\n\t\t Yusuke Miyao and Jun\x92ichi Tsujii . \n\t', '\n\t\t 2002. Maximum entropy estimation for feature forests . \n\t', '\n\t\t In Proceedings of the Human Language Technology Conference , San Diego , CA . \n\t', '\n\t\t Jorge Nocedal and Stephen J. Wright . \n\t', '\n\t\t 1999. Numerical Optimization . \n\t', '\n\t\t Springer , New York , USA . \n\t', '\n\t\t Stefan Riezler , Tracy H. King , Ronald M. Kaplan , Richard Crouch , John T. Maxwell III , and Mark Johnson . \n\t', '\n\t\t 2002. Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques . \n\t', '\n\t\t In Proceedings of the 40th Meeting of the ACL , pages 271\x96278 , Philadelphia , PA . \n\t', '\n\t\t Fei Sha and Fernando Pereira . \n\t', '\n\t\t 2003. Shallow parsing with conditional random fields . \n\t', '\n\t\t In Proceedings of the HLT/NAACL Conference , pages 213\x96220 , Edmonton , Canada . \n\t', '\n\t\t Mark Steedman . \n\t', '\n\t\t 2000. The Syntactic Process . \n\t', '\n\t\t The MIT Press , Cambridge , MA . \n\t', '\n\t\t Kristina Toutanova , Christopher Manning , Stuart Shieber , Dan Flickinger , and Stephan Oepen . \n\t', '\n\t\t 2002. Parse disambiguation for a rich HPSG grammar . \n\t', '\n\t\t In Proceedings of the First Workshop on Treebanks and Linguistic Theories , pages 253\x96263 , Sozopol , Bulgaria . \n\t', '\n\t\t Incremental Parsing with the Perceptron Algorithm Michael Collins MIT CSAIL mcollins@csail.mit.edu Brian Roark AT&T Labs - Research roark@research.att.com Abstract This paper describes an incremental parsing approach where parameters are estimated using a variant of the perceptron algorithm . \n\t', '\n\t\t A beam-search algorithm is used during both training and decoding phases of the method . \n\t', '\n\t\t The perceptron approach was implemented with the same feature set as that of an existing generative model \n\t\t']",Positive
"['\n\t\t We demonstrate that training a perceptron model to combine with the generative model during search provides a 2.1 percent F-measure improvement over the generative model alone , to 88.8 percent . \n\t', '\n\t\t 1 Introduction In statistical approaches to NLP problems such as tagging or parsing , it seems clear that the representation used as input to a learning algorithm is central to the accuracy of an approach . \n\t', '\n\t\t In an ideal world , the designer of a parser or tagger would be free to choose any features which might be useful in discriminating good from bad structures , without concerns about how the features interact with the problems of training ( parameter estimation ) or decoding ( search for the most plausible candidate under the model ) . \n\t', '\n\t\t To this end , a number of recently proposed methods allow a model to incorporate \x93arbitrary\x94 global features of candidate analyses or parses . \n\t', '\n\t\t Examples of such techniques are Markov Random Fields \n\t\t']",Negative
"[""\n\t\t A drawback of these approaches is that in the general case , they can require exhaustive enumeration of the set of candidates for each input sentence in both the train- ing and decoding phases ' . \n\t"", '\n\t\t For example , \n\t\t']",Negative
['\n\t\t \n\t\t'],Negative
"['\n\t\t presupposes that there is an existing baseline model with reasonable performance . \n\t', '\n\t\t Many of these baseline models are themselves used with heuristic search techniques , so that the potential gain through the use of discriminative re-ranking techniques is further dependent on effective search . \n\t', '\n\t\t This paper explores an alternative approach to parsing , based on the perceptron training algorithm introduced in \n\t\t']",Positive
"['\n\t\t In this approach the training and decoding problems are very closely related \x96 the training method decodes training examples in sequence , and makes simple corrective updates to the parameters when errors are made . \n\t', '\n\t\t Thus the main complexity of the method is isolated to the decoding problem . \n\t', '\n\t\t We describe an approach that uses an incremental , left-to-right parser , with beam search , to find the highest scoring analysis under the model . \n\t', '\n\t\t The same search method is used in both training and decoding . \n\t', '\n\t\t We implemented the perceptron approach with the same feature set as that of an existing generative model ( Roark , 2001 a ) , and show that the perceptron model gives performance competitive to that of the generative model on parsing the Penn treebank , thus demonstrating that an unnormalized discriminative parsing model can be applied with heuristic search . \n\t', '\n\t\t We also describe several refinements to the training algorithm , and demonstrate their impact on convergence properties of the method . \n\t', '\n\t\t Finally , we describe training the perceptron model with the negative log probability given by the generative model as another feature . \n\t', '\n\t\t This provides the perceptron algorithm with a better starting point , leading to large improvements over using either the generative model or the perceptron algorithm in isolation ( the hybrid model achieves 88.8 % f-measure on the WSJ treebank , compared to figures of 86.7 % and 86.6 % for the separate generative and perceptron models ) . \n\t', '\n\t\t The approach is an extremely simple method for integrating new features into the generative model : essentially all that is needed is a definition of feature-vector representations of entire parse trees , and then the existing parsing algorithms can be used for both training and decoding with the models . \n\t', '\n\t\t 2 The General Framework In this section we describe a general framework \x96 linear models for NLP \x96 that could be applied to a diverse range of tasks , including parsing and tagging . \n\t', '\n\t\t We then describe a particular method for parameter estimation , which is a generalization of the perceptron algorithm . \n\t', '\n\t\t Finally , we give an abstract description of an incremental parser , and describe how it can be used with the perceptron algorithm . \n\t', '\n\t\t 2.1 Linear Models for NLP We follow the framework outlined in Collins ( 2002 ; 2004 ) . \n\t', '\n\t\t The task is to learn a mapping from inputs x E X to outputs y E Y. For example , X might be a set of sentences , with Y being a set of possible parse trees . \n\t', '\n\t\t We assume : \x95 Training examples ( xi , yi ) for i = 1 ... n. \x95 A function GEN which enumerates a set of candidates GEN(x) for an input x. \x95 A representation 4 ) mapping each ( x , y ) E X x Y to a feature vector 4)(x , y ) E Rd. \x95 A parameter vector a¯ E Rd. . \n\t', '\n\t\t The components GEN , 4 ) and a¯ define a mapping from an input x to an output F(x) through F(x) = arg max yEGEN(x) where 4)(x , y ) · a¯ is the inner product E3 a34)3(x , y ) . \n\t', '\n\t\t The learning task is to set the parameter values a¯ using the training examples as evidence . \n\t', '\n\t\t The decoding algorithm is a method for searching for the arg max in Eq . \n\t', '\n\t\t 1. This framework is general enough to encompass several tasks in NLP . \n\t', '\n\t\t In this paper we are interested in parsing , where ( xi , yi ) , GEN , and 4 ) can be defined as follows : \x95 Each training example ( xi , yi ) is a pair where xi is a sentence , and yi is the gold-standard parse for that sentence . \n\t', '\n\t\t \x95 Given an input sentence x , GEN(x) is a set of possible parses for that sentence . \n\t', '\n\t\t For example , GEN(x) could be defined as the set of possible parses for x under some context-free grammar , perhaps a context-free grammar induced from the training examples . \n\t', '\n\t\t \x95 The representation 4)(x , y ) could track arbitrary features of parse trees . \n\t', '\n\t\t As one example , suppose that there are m rules in a context-free grammar ( CFG ) that defines GEN(x) . \n\t', '\n\t\t Then we could define the i\x92th component of the representation , 4)i ( x , y ) , to be the number of times the i\x92th context-free rule appears in the parse tree ( x , y ) . \n\t', '\n\t\t This is implicitly the representation used in probabilistic or weighted CFGs . \n\t', '\n\t\t Note that the difficulty of finding the arg max in Eq . \n\t', '\n\t\t 1 is dependent on the interaction of GEN and 4 ) . \n\t', '\n\t\t In many cases GEN(x) could grow exponentially with the size of x , making brute force enumeration of the members of GEN(x) intractable . \n\t', '\n\t\t For example , a context-free grammar could easily produce an exponentially growing number of analyses with sentence length . \n\t', '\n\t\t For some representations , such as the \x93rule-based\x94 representation described above , the arg max in the set enumerated by the CFG can be found efficiently , using dynamic programming algorithms , without having to explicitly enumerate all members of GEN(x) . \n\t', '\n\t\t However in many cases we may be interested in representations which do not allow efficient dynamic programming solutions . \n\t', '\n\t\t One way around this problem is to adopt a two-pass approach , where GEN(x) is the top N analyses under some initial model , as in the reranking approach of \n\t\t']",Positive
"['\n\t\t In the current paper we explore alternatives to reranking approaches , namely heuristic methods for finding the arg max , specifically incremental beam-search strategies related to the parsers of Roark ( 2001 a ) and \n\t\t']",Positive
"['\n\t\t 2.2 The Perceptron Algorithm for Parameter Estimation We now consider the problem of setting the parameters , ¯a , given training examples ( xi , yi ) . \n\t', '\n\t\t We will briefly review the perceptron algorithm , and its convergence properties \x96 see \n\t\t']",Positive
['\n\t\t The algorithm and theorems are based on the approach to classification problems described in \n\t\t'],Positive
"['\n\t\t Figure 1 shows the algorithm . \n\t', '\n\t\t Note that the most complex step of the method is finding zi = arg maxzEGEN(x;) 4)(xi , z ) · a¯ \x96 and this is precisely the decoding problem . \n\t', '\n\t\t Thus the training algorithm is in principle a simple part of the parser : any system will need a decoding method , and once the decoding algorithm is implemented the training algorithm is relatively straightforward . \n\t', '\n\t\t We will now give a first theorem regarding the convergence of this algorithm . \n\t', '\n\t\t First , we need the following definition : Definition 1 Let GEN(xi) = GEN(xi) \x97 { yi } . \n\t', '\n\t\t In other words GEN(xi) is the set of incorrect candidates for an example xi . \n\t', '\n\t\t We will say that a training sequence ( xi , yi ) for i = 1 ... n is separable with margin S > 0 if there exists some vector U with I IUI I = 1 such that di , dz E GEN ( xi ) , U · 4 ) ( xi , yi ) \x97 U · 4 ) ( xi , z ) ^ S ( 2 ) ( I IUI I is the 2-norm of U , i.e. , I IUI I = pP3 U23 . \n\t', '\n\t\t ) Next , define Ne to be the number of times an error is made by the algorithm in figure 1\x96 that is , the number of times that zi =~ yi for some ( t , i ) pair . \n\t', '\n\t\t We can then state the following theorem ( see \n\t\t']",Positive
"['\n\t\t This theorem implies that if there is a parameter vector U which makes zero errors on the training set , then after a finite number of iterations the training algorithm will converge to parameter values with zero training error . \n\t', '\n\t\t A crucial point is that the number of mistakes is independent of the number of candidates for each example 4)(x , y ) · a¯ ( 1 ) S2 Inputs : Training examples ( xi , yi ) Algorithm : Initialization : Set a¯ = 0 Fort = 1 ... \n\t', '\n\t\t T , i = 1 ... n Output : Parameters a¯ Calculate zi = argmaxzEGEN(x , 4)(xi , z ) \x95 a¯ If(zi # yi ) then a¯ = a¯ + 4)(xi , yi~ ^ 4)(xi , zi ) Figure 1 : A variant of the perceptron algorithm . \n\t', '\n\t\t ( i.e. the size of GEN(xi) for each i ) , depending only on the separation of the training data , where separation is defined above . \n\t', '\n\t\t This is important because in many NLP problems GEN ( x ) can be exponential in the size of the inputs . \n\t', '\n\t\t All of the convergence and generalization results in \n\t\t']",Positive
"['\n\t\t Two questions come to mind . \n\t', '\n\t\t First , are there guarantees for the algorithm if the training data is not separable ? \n\t', '\n\t\t Second , performance on a training sample is all very well , but what does this guarantee about how well the algorithm generalizes to newly drawn test examples ? \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t As a final note , following \n\t\t']",Positive
"['\n\t\t Say ¯ai is the parameter vector after the i\x92th example is processed on the t\x92th pass through the data in the algorithm in figure 1 . \n\t', '\n\t\t Then the averaged parameters ¯aAVG are defined as ¯aAVG = Ei,t ¯ati/NT . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t 2.3 An Abstract Description of Incremental Parsing This section gives a description of the basic incremental parsing approach . \n\t', '\n\t\t The input to the parser is a sentence x with length n . \n\t', '\n\t\t A hypothesis is a triple ( x , t , i ) such that x is the sentence being parsed , t is a partial or full analysis of that sentence , and i is an integer specifying the number of words of the sentence which have been processed . \n\t', '\n\t\t Each full parse for a sentence will have the form ( x , t , n ) . \n\t', '\n\t\t The initial state is ( x , 0 , 0 ) where 0 is a \x93null\x94 or empty analysis . \n\t', '\n\t\t We assume an \x93advance\x94 function ADV which takes a hypothesis triple as input , and returns a set of new hypotheses as output . \n\t', ""\n\t\t The advance function will absorb another word in the sentence : this means that if the input to ADV is ( x , t , i ) , then each member of ADV((x , t , i ) ) will have the form ( x , t',i+1 ) . \n\t"", ""\n\t\t Each new analysis t ' will be formed by somehow incorporating the i+1\x92th word into the previous analysis t . \n\t"", ""\n\t\t With these definitions in place , we can iteratively define the full set of partial analyses Hi for the first i words of the sentence as H0 ( x ) = { ( x , 0 , 0 ) } , and Hi ( x ) = ^h'EH,_1(x)ADV(h') for i = 1 ... n . \n\t"", '\n\t\t The full set of parses for a sentence x is then GEN ( x ) = Hn ( x ) where n is the length of x . \n\t', '\n\t\t Under this definition GEN(x) can include a huge number of parses , and searching for the highest scoring parse , argmaxhEHn(x) 4)(h) \x95 ¯a , will be intractable . \n\t', '\n\t\t For this reason we introduce one additional function , FILTER(H) , which takes a set of hypotheses H , and returns a much smaller set of \x93filtered\x94 hypotheses . \n\t', '\n\t\t Typically , FILTER will calculate the score 4)(h) \x95 a¯ for each h E H , and then eliminate partial analyses which have low scores under this criterion . \n\t', '\n\t\t For example , a simple version of FILTER would take the top N highest scoring members of H for some constant N . \n\t', ""\n\t\t We can then redefine the set of partial analyses as follows ( we use Fi ( x ) to denote the set of filtered partial analyses for the first i words of the sentence ) : F0(x) = { ( x , 0 , 0 ) } Fi(x) =FILTER (^h'Ey,_1(x)ADV(h')) for i=1 ... n The parsing algorithm returns argmaxhEyn 4)(h) \x95 ¯a . \n\t"", '\n\t\t Note that this is a heuristic , in that there is no guarantee that this procedure will find the highest scoring parse , argmaxhEHn 4)(h) \x95 ¯a . \n\t', '\n\t\t Search errors , where argmaxhEyn 4)(h) \x95 a¯ =~ argmaxhEHn 4)(h) \x95 ¯a , will create errors in decoding test sentences , and also errors in implementing the perceptron training algorithm in Figure 1 . \n\t', '\n\t\t In this paper we give empirical results that suggest that FILTER can be chosen in such a way as to give efficient parsing performance together with high parsing accuracy . \n\t', '\n\t\t The exact implementation of the parser will depend on the definition of partial analyses , of ADV and FILTER , and of the representation 4 ) . \n\t', '\n\t\t The next section describes our instantiation of these choices . \n\t', '\n\t\t 3 A full description of the parsing approach The parser is an incremental beam-search parser very similar to the sort described in Roark ( 2001 a ; 2004 ) , with some changes in the search strategy to accommodate the perceptron feature weights . \n\t', '\n\t\t We first describe the parsing algorithm , and then move on to the baseline feature set for the perceptron model . \n\t', '\n\t\t 3.1 Parser control The input to the parser is a string wn0 , a grammar G , a mapping 4 ) from derivations to feature vectors , and a parameter vector ¯a . \n\t', '\n\t\t The grammar G = ( V , T , St , ¯S , C , B ) consists of a set of non-terminal symbols V , a set of terminal symbols T , a start symbol St E V , an end-ofconstituent symbol S¯ E V , a set of \x93allowable chains\x94 C , and a set of \x93allowable triples\x94 B. S¯ is a special empty non-terminal that marks the end of a constituent . \n\t', '\n\t\t Each chain is a sequence of non-terminals followed by a terminal symbol , for example ( St \x97> S \x97> NP \x97> NN \x97> Tree POS tags f24 Type Type f2-21 f2-21 , Type # > 1 OOV transform OOV None Gold 386 1680 0.1 % 1013 0.1 % None Tagged 401 1776 0.1 % 1043 0.2 % FSLC Gold 289 1214 0.1 % 746 0.1 % FSLC Tagged 300 1294 0.1 % 781 0.1 % . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t .. VP . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t ..NN MD Trash St .S ^^ NP . \n\t', '\n\t\t ...VP NN VP MD can can can Figure 2 : Left child chains and connection paths . \n\t', '\n\t\t Dotted lines represent potential attachments Trash ) . \n\t', '\n\t\t Each \x93allowable triple\x94 is a tuple ( X , Y , Z ) where X , Y , Z E V. The triples specify which non- terminals Z are allowed to follow a non-terminal Y under a parent X . \n\t', '\n\t\t For example , the triple ( S , NP , VP ) specifies that a VP can follow an NP under an S . \n\t', '\n\t\t The triple ( NP , NN,¯S ) would specify that the S¯ symbol can follow an NN under an NP \x96 i.e. , that the symbol NN is allowed to be the final child of a rule with parent NP The initial state of the parser is the input string alone , wn0 . \n\t', '\n\t\t In absorbing the first word , we add all chains of the form St. ... , w0 . \n\t', '\n\t\t For example , in figure 2 the chain ( St , S , NP , NN , Trash ) is used to construct an analysis for the first word alone . \n\t', '\n\t\t Other chains which start with St and end with Trash would give competing analyses for the first word of the string . \n\t', '\n\t\t Figure 2 shows an example of how the next word in a sentence can be incorporated into a partial analysis for the previous words . \n\t', '\n\t\t For any partial analysis there will be a set of potential attachment sites : in the example , the attachment sites are under the NP or the S . \n\t', '\n\t\t There will also be a set of possible chains terminating in the next word \x96 there are three in the example . \n\t', '\n\t\t Each chain could potentially be attached at each attachment site , giving 6 ways of incorporating the next word in the example . \n\t', '\n\t\t For illustration , assume that the set B is { ( S , NP , VP ) , ( NP , NN , NN ) , ( NP , NN,¯S ) , ( S , NP , VP ) } . \n\t', '\n\t\t Then some of the 6 possible attachments may be disallowed because they create triples that are not in the set B . \n\t', '\n\t\t For example , in figure 2 attaching either of the VP chains under the NP is disallowed because the triple ( NP , NN , VP ) is not in B . \n\t', '\n\t\t Similarly , attaching the NN chain under the S will be disallowed if the triple ( S , NP , NN ) is not in B . \n\t', '\n\t\t In contrast , adjoining ( NN , can ) under the NP creates a single triple , ( NP , NN , NN ) , which is allowed . \n\t', '\n\t\t Adjoining either of the VP chains under the S creates two triples , ( S , NP , VP ) and ( NP , NN,¯S ) , which are both in the set B . \n\t', '\n\t\t Note that the \x93allowable chains\x94 in our grammar are what \n\t\t']",Positive
"['\n\t\t It can be shown that the method is equivalent to parsing with a transformed context-free grammar ( a first-order \x93Markov\x94 grammar ) \x96 for brevity we omit the details here . \n\t', '\n\t\t In this way , given a set of candidates .77i ( x ) for the first i words of the string , we can generate a set of candidates Table 1 : Left-child chain type counts ( of length > 2 ) for sections of the Wall St. Journal Treebank , and out-ofvocabulary ( OOV ) rate on the held-out corpus . \n\t', ""\n\t\t for the first i + 1 words , Uh1EY-,(x)ADV(h') , where the ADV function uses the grammar as described above . \n\t"", '\n\t\t We then calculate 4 ) ( h ) \x95 ^¯ for all of these partial hypotheses , and rank the set from best to worst . \n\t', '\n\t\t A FILTER function is then applied to this ranked set to give .77i+1 . \n\t', '\n\t\t Let hk be the kth ranked hypothesis in Hi+1(x) . \n\t', '\n\t\t Then hk E .77i+1 if and only if 4)(hk) \x95 ^¯ > ^k . \n\t', '\n\t\t In our case , we parameterize the calculation of ^k with y as follows : ^k = 4)(h0) \x95 ^¯ \x97 k3y . \n\t', '\n\t\t ( 3 ) The problem with using left-child chains is limiting them in number . \n\t', '\n\t\t With a left-recursive grammar , of course , the set of all possible left-child chains is infinite . \n\t', '\n\t\t We use two techniques to reduce the number of left-child chains : first , we remove some ( but not all ) of the recursion from the grammar through a tree transform ; next , we limit the left-child chains consisting of more than two non-terminal categories to those actually observed in the training data more than once . \n\t', '\n\t\t Left-child chains of length less than or equal to two are all those observed in training data . \n\t', '\n\t\t As a practical matter , the set of left- child chains for a terminal x is taken to be the union of the sets of left-child chains for all pre-terminal part-ofspeech ( POS ) tags T for x . \n\t', '\n\t\t Before inducing the left-child chains and allowable triples from the treebank , the trees are transformed with a selective left-corner transformation \n\t\t']",Positive
"['\n\t\t This transform is only applied to left-recursive productions , i.e. productions of the form A , Ay . \n\t', '\n\t\t The transformed trees look as in figure 3 . \n\t', '\n\t\t The transform has the benefit of dramatically reducing the number of left-child chains , without unduly disrupting the immediate dominance relationships that provide features for the model . \n\t', '\n\t\t The parse trees that are returned by the parser are then de-transformed to the original form of the grammar for evaluation2 . \n\t', '\n\t\t Table 1 presents the number of left-child chains of length greater than 2 in sections 2-21 and 24 of the Penn Wall St. Journal Treebank , both with and without the flattened selective left-corner transformation ( FSLC ) , for gold-standard part-of-speech ( POS ) tags and automatically tagged POS tags . \n\t', '\n\t\t When the FSLC has been applied and the set is restricted to those occurring more than once ZSee \n\t\t']",Positive
"['\n\t\t ( a ) ( b ) ( c ) PPP PP ^^ NN IN NP ^^^^ P NP ^^^ ^^^ NP ^^^^ NNP POS \x92s Jim NN NP/NP PP ^^^ IN NP dog POS NNP \x92s Jim Jim \x92s NP ^^^^^ ~~~~~ NP/NP ^^^ ^^^ NP ^^^^^^ ^^^ ^ ~~~~~~ NP dog with ... \n\t', '\n\t\t NP/NP NN dog NP/NP PP ^ ^ IN NP with ... \n\t', '\n\t\t POS NNP with ... \n\t', '\n\t\t Figure 3 : Three representations of NP modifications : ( a ) the original treebank representation ; ( b ) Selective left-corner representation ; and ( c ) a flat structure that is unambiguously equivalent to ( b ) F0={L00,L10} F4=F3U{L03} F8=F7U{L21} F12=F11U{L11} F1=F0U{LKP} F5=F4U{L20} F9=F8U{CL} F13 = F12 U { L30 } F2=F1U{L01} F6=F5U{L11} F10 =F9U{LK} F14=F13U{CCP} F3 =F2U{L02} F7=F6U{L30} F11 =F0U{L20} F15 =F14U{CC} Table 2 : Baseline feature set . \n\t', '\n\t\t Features F0 \x97 F10 fire at non-terminal nodes . \n\t', '\n\t\t Features F0 , F11 \x97 F15 fire at terminal nodes . \n\t', '\n\t\t in the training corpus , we can reduce the total number of left-child chains of length greater than 2 by half , while leaving the number of words in the held-out corpus with an unobserved left-child chain ( out-of-vocabulary rate \x96 OOV ) to just one in every thousand words . \n\t', '\n\t\t 3.2 Features For this paper , we wanted to compare the results of a perceptron model with a generative model for a comparable feature set . \n\t', '\n\t\t Unlike in Roark ( 2001a ; 2004 ) , there is no look-ahead statistic , so we modified the feature set from those papers to explicitly include the lexical item and POS tag of the next word . \n\t', '\n\t\t Otherwise the features are basically the same as in those papers . \n\t', '\n\t\t We then built a generative model with this feature set and the same tree transform , for use with the beam-search parser from \n\t\t']",Positive
"['\n\t\t To concisely present the baseline feature set , let us establish a notation . \n\t', '\n\t\t Features will fire whenever a new node is built in the tree . \n\t', '\n\t\t The features are labels from the left-context , i.e. the already built part of the tree . \n\t', '\n\t\t All of the labels that we will include in our feature sets are i levels above the current node in the tree , and j nodes to the left , which we will denote Lij . \n\t', '\n\t\t Hence , L00 is the node label itself ; L10 is the label of parent of the current node ; L01 is the label of the sibling of the node , immediately to its left ; L11 is the label of the sibling of the parent node , etc. . \n\t', '\n\t\t We also include : the lexical head of the current constituent ( CL ) ; the c-commanding lexical head ( CC ) and its POS ( CCP ) ; and the look-ahead word ( LK ) and its POS ( LKP ) . \n\t', '\n\t\t All of these features are discussed at more length in the citations above . \n\t', '\n\t\t Table 2 presents the baseline feature set . \n\t', '\n\t\t In addition to the baseline feature set , we will also present results using features that would be more difficult to embed in a generative model . \n\t', '\n\t\t We included some punctuation-oriented features , which included ( i ) a Boolean feature indicating whether the final punctuation is a question mark or not ; ( ii ) the POS label of the word after the current look-ahead , if the current look- ahead is punctuation or a coordinating conjunction ; and ( iii ) a Boolean feature indicating whether the look-ahead is punctuation or not , that fires when the category immediately to the left of the current position is immediately preceded by punctuation . \n\t', '\n\t\t 4 Refinements to the Training Algorithm This section describes two modifications to the \x93basic\x94 training algorithm in figure 1. 4.1 Making Repeated Use of Hypotheses Figure 4 shows a modified algorithm for parameter estimation . \n\t', '\n\t\t The input to the function is a gold standard parse , together with a set of candidates F generated by the incremental parser . \n\t', '\n\t\t There are two steps . \n\t', '\n\t\t First , the model is updated as usual with the current example , which is then added to a cache of examples . \n\t', '\n\t\t Second , the method repeatedly iterates over the cache , updating the model at each cached example if the gold standard parse is not the best scoring parse from among the stored candidates for that example . \n\t', '\n\t\t In our experiments , the cache was restricted to contain the parses from up to N previously processed sentences , where N was set to be the size of the training set . \n\t', '\n\t\t The motivation for these changes is primarily efficiency . \n\t', '\n\t\t One way to think about the algorithms in this paper is as methods for finding parameter values that satisfy a set of linear constraints \x96 one constraint for each incorrect parse in training data . \n\t', '\n\t\t The incremental parser is Input : A gold-standard parse = g for sentence k of N . \n\t', '\n\t\t A set of candidate parses T . \n\t', '\n\t\t Current parameters ¯^ . \n\t', '\n\t\t A Cache of triples ( gj , Tj , cj ) for j = 1 ... \n\t', '\n\t\t N where each gj is a previously generated gold standard parse , Tj is a previously generated set of candidate parses , and cj is a counter of the number of times that ^¯ has been updated due to this particular triple . \n\t', '\n\t\t Parameters T1 and T2 controlling the number of iterations be- low . \n\t', '\n\t\t In our experiments , T1 = 5 and T2 = 50 . \n\t', '\n\t\t Initialize the Cache to include , for j = 1 ... \n\t', '\n\t\t N , ( gj , 0 , T2 ) . \n\t', '\n\t\t Step 1 : Step 2 : Calculate z = arg maxtEY 4)(t) ^¯ Fort = 1 ... \n\t', '\n\t\t T1 , j = 1 ... \n\t', '\n\t\t N If ( z =~ g ) then ^¯ = ^¯ + 4)(g) \x97 4)(z) If cj < T2 then Set the kth triple in the Cache to ( g , T , 0 ) Calculate z = arg maxtEY,f 4 ) ( t ) ^¯ If ( z =~ gj ) then ^¯ = ^¯ + 4)(gj) \x97 4)(z) cj = cj + 1 Figure 4 : The refined parameter update method makes repeated use of hypotheses a method for dynamically generating constraints ( i.e. incorrect parses ) which are violated , or close to being violated , under the current parameter settings . \n\t', '\n\t\t The basic algorithm in Figure 1 is extremely wasteful with the generated constraints , in that it only looks at one constraint on each sentence ( the arg max ) , and it ignores constraints implied by previously parsed sentences . \n\t', '\n\t\t This is inefficient because the generation of constraints ( i.e. , parsing an input sentence ) , is computationally quite demanding . \n\t', '\n\t\t More formally , it can be shown that the algorithm in figure 4 also has the upper bound in theorem 1 on the number of parameter updates performed . \n\t', '\n\t\t If the cost of steps 1 and 2 of the method are negligible compared to the cost of parsing a sentence , then the refined algorithm will certainly converge no more slowly than the basic algorithm , and may well converge more quickly . \n\t', '\n\t\t As a final note , we used the parameters T1 and T2 to limit the number of passes over examples , the aim being to prevent repeated updates based on outlier examples which are not separable . \n\t', '\n\t\t 4.2 Early Update During Training As before , define yi to be the gold standard parse for the i\x92th sentence , and also define yji to be the partial analysis under the gold-standard parse for the first j words of the i\x92th sentence . \n\t', '\n\t\t Then if yji ~/ Tj ( xi ) a search error has been made , and there is no possibility of the gold standard parse yi being in the final set of parses , Tn ( xi ) . \n\t', '\n\t\t We call the following modification to the parsing algorithm during training \x93early update\x94 : if yji ~/ Tj ( xi ) , exit the parsing process , pass yji , Tj ( xi ) to the parameter estimation method , and move on to the next string in the training set . \n\t', '\n\t\t Intuitively , the motivation behind this is clear . \n\t', '\n\t\t It makes sense to make a correction to the parameter values at the point that a search error has been made , rather than allowing the parser to continue to the end of the sentence . \n\t', '\n\t\t This is likely to lead to less noisy input to the parameter estimation algorithm ; and early update will also improve efficiency , as at the early stages of training the parser will frequently give up after a small proportion of each sentence is processed . \n\t', '\n\t\t It is more difficult to justify from a formal point of view , we leave this to future work . \n\t', '\n\t\t Figure 5 shows the convergence of the training algorithm with neither of the two refinements presented ; with just early update ; and with both . \n\t', '\n\t\t Early update makes Number of passes over training data Figure 5 : Performance on development data ( section f24 ) after each pass over the training data , with and without repeated use of examples and early update . \n\t', '\n\t\t an enormous difference in the quality of the resulting model ; repeated use of examples gives a small improvement , mainly in recall . \n\t', '\n\t\t 5 Empirical results The parsing models were trained and tested on treebanks from the Penn Wall St. Journal Treebank : sections 2-21 were kept training data ; section 24 was held-out development data ; and section 23 was for evaluation . \n\t', '\n\t\t After each pass over the training data , the averaged perceptron model was scored on the development data , and the best performing model was used for test evaluation . \n\t', '\n\t\t For this paper , we used POS tags that were provided either by the Treebank itself ( gold standard tags ) or by the perceptron POS tagger3 presented in \n\t\t']",Positive
"['\n\t\t The former gives us an upper bound on the improvement that we might expect if we integrated the POS tagging with the parsing . \n\t', '\n\t\t 3For trials when the generative or perceptron parser was given POS tagger output , the models were trained on POS tagged sections 2-21 , which in both cases helped performance slightly . \n\t', '\n\t\t 88 87 86 85 84 83 82 1 2 3 4 5 6 No early update , no repeated use of examples Early update , no repeated use of examples Early update , repeated use of examples Model Gold-standard tags POS-tagger tags LP LR F LP LR F Generative Perceptron ( baseline ) Perceptron ( w/ punctuation features ) 88.1 87.5 88.1 87.6 86.9 87.6 87.8 87.2 87.8 86.8 86.2 87.0 86.5 85.5 86.3 86.7 85.8 86.6 Table 3 : Parsing results , section 23 , all sentences , including labeled precision ( LP ) , labeled recall ( LR ) , and F-measure Table 3 shows results on section 23 , when either gold- standard or POS-tagger tags are provided to the parser4 . \n\t', '\n\t\t With the base features , the generative model outperforms the perceptron parser by between a half and one point , but with the additional punctuation features , the perceptron model matches the generative model performance . \n\t', '\n\t\t Of course , using the generative model and using the perceptron algorithm are not necessarily mutually exclusive . \n\t', '\n\t\t Another training scenario would be to include the generative model score as another feature , with some weight in the linear model learned by the perceptron algorithm . \n\t', '\n\t\t This sort of scenario was used in \n\t\t']",Positive
"['\n\t\t We follow that paper in fixing the weight of the generative model , rather than learning the weight along the the weights of the other perceptron features . \n\t', '\n\t\t The value of the weight was empirically optimized on the held-out set by performing trials with several values . \n\t', '\n\t\t Our optimal value was 10 . \n\t', '\n\t\t In order to train this model , we had to provide generative model scores for strings in the training set . \n\t', '\n\t\t Of course , to be similar to the testing conditions , we cannot use the standard generative model trained on every sentence , since then the generative score would be from a model that had already seen that string in the training data . \n\t', '\n\t\t To control for this , we built ten generative models , each trained on 90 percent of the training data , and used each of the ten to score the remaining 10 percent that was not seen in that training set . \n\t', '\n\t\t For the held-out and testing conditions , we used the generative model trained on all of sections 2-21 . \n\t', '\n\t\t In table 4 we present the results of including the generative model score along with the other perceptron features , just for the run with POS-tagger tags . \n\t', '\n\t\t The generative model score ( negative log probability ) effectively provides a much better initial starting point for the perceptron algorithm . \n\t', '\n\t\t The resulting F-measure on section 23 is 2.1 percent higher than either the generative model or perceptron-trained model used in isolation . \n\t', '\n\t\t 6 Conclusions In this paper we have presented a discriminative training approach , based on the perceptron algorithm with a couple of effective refinements , that provides a model capable of effective heuristic search over a very difficult search space . \n\t', '\n\t\t In such an approach , the unnormalized discriminative parsing model can be applied without either 4When POS tagging is integrated directly into the generative parsing process , the baseline performance is 87.0 . \n\t', '\n\t\t For comparison with the perceptron model , results are shown with pre-tagged input . \n\t', '\n\t\t Model POS-tagger tags LP LR F Generative baseline 86.8 86.5 86.7 Perceptron ( w/ punctuation features ) 87.0 86.3 86.6 Generative + Perceptron ( w/ punct ) 89.1 88.4 88.8 Table 4 : Parsing results , section 23 , all sentences , including labeled precision ( LP ) , labeled recall ( LR ) , and F-measure an external model to present it with candidates , or potentially expensive dynamic programming . \n\t', '\n\t\t When the training algorithm is provided the generative model scores as an additional feature , the resulting parser is quite competitive on this task . \n\t', '\n\t\t The improvement that was derived from the additional punctuation features demonstrates the flexibility of the approach in incorporating novel features in the model . \n\t', '\n\t\t Future research will look in two directions . \n\t', '\n\t\t First , we will look to include more useful features that are difficult for a generative model to include . \n\t', '\n\t\t This paper was intended to compare search with the generative model and the perceptron model with roughly similar feature sets . \n\t', '\n\t\t Much improvement could potentially be had by looking for other features that could improve the models . \n\t', '\n\t\t Secondly , combining with the generative model can be done in several ways . \n\t', '\n\t\t Some of the constraints on the search technique that were required in the absence of the generative model can be relaxed if the generative model score is included as another feature . \n\t', '\n\t\t In the current paper , the generative score was simply added as another feature . \n\t', '\n\t\t Another approach might be to use the generative model to produce candidates at a word , then assign perceptron features for those candidates . \n\t', '\n\t\t Such variants deserve investigation . \n\t', '\n\t\t Overall , these results show much promise in the use of discriminative learning techniques such as the perceptron algorithm to help perform heuristic search in difficult domains such as statistical parsing . \n\t', '\n\t\t Acknowledgements The work by Michael Collins was supported by the National Science Foundation under Grant No. 0347631 . \n\t', '\n\t\t References Steven Abney . \n\t', '\n\t\t 1997. Stochastic attribute-value gram- mars . \n\t', '\n\t\t Computational Linguistics , 23(4):597\x96617 . \n\t', '\n\t\t Michael Collins and Nigel Duffy . \n\t', '\n\t\t 2002. New ranking algorithms for parsing and tagging : Kernels over dis- crete structures and the voted perceptron . \n\t', '\n\t\t In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , pages 263\x96270 . \n\t', '\n\t\t Michael Collins . \n\t', '\n\t\t 2000. Discriminative reranking for natural language parsing . \n\t', '\n\t\t In The Proceedings of the 17th International Conference on Machine Learning . \n\t', '\n\t\t Michael Collins . \n\t', '\n\t\t 2002. Discriminative training methods for hidden markov models : Theory and experiments with perceptron algorithms . \n\t', '\n\t\t In Proceedings of the Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 1\x968 . \n\t', '\n\t\t Michael Collins . \n\t', '\n\t\t 2004. Parameter estimation for statistical parsing models : Theory and practice of distribution-free methods . \n\t', '\n\t\t In Harry Bunt , John Carroll , and Giorgio Satta , editors , New Developments in Parsing Technology . \n\t', '\n\t\t Kluwer . \n\t', '\n\t\t Fabrizio Costa , Vincenzo Lombardo , Paolo Frasconi , and Giovanni Soda . \n\t', '\n\t\t 2001. Wide coverage incremental parsing by learning attachment preferences . \n\t', '\n\t\t In Conference of the Italian Association for Artificial Intelligence ( AIIA ) , pages 297\x96307 . \n\t', '\n\t\t Stephen Della Pietra , Vincent Della Pietra , and John Lafferty . \n\t', '\n\t\t 1997. Inducing features of random fields . \n\t', '\n\t\t IEEE Transactions on Pattern Analysis and Machine Intelligence , 19:380\x96393 . \n\t', '\n\t\t Yoav Freund and Robert Schapire . \n\t', '\n\t\t 1999. Large margin classification using the perceptron algorithm . \n\t', '\n\t\t Machine Learning , 3(37):277\x96296 . \n\t', '\n\t\t Yoav Freund , Raj Iyer , Robert Schapire , and Yoram Singer . \n\t', '\n\t\t 1998. An efficient boosting algorithm for combining preferences . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 15th Intl . \n\t', '\n\t\t Conference on Machine Learning . \n\t', '\n\t\t Stuart Geman and Mark Johnson . \n\t', '\n\t\t 2002. Dynamic programming for parsing and estimation of stochastic unification-based grammars . \n\t', '\n\t\t In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , pages 279\x96286 . \n\t', '\n\t\t Mark Johnson and Brian Roark . \n\t', '\n\t\t 2000. Compact nonleft-recursive grammars using the selective left-corner transform and factoring . \n\t', '\n\t\t In Proceedings of the 18th International Conference on Computational Linguistics ( COLING ) , pages 355\x96361 . \n\t', '\n\t\t Mark Johnson , Stuart Geman , Steven Canon , Zhiyi Chi , and Stefan Riezler . \n\t', '\n\t\t 1999. Estimators for stochastic \x93unification-based\x94 grammars . \n\t', '\n\t\t In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics , pages 535\x96541 . \n\t', '\n\t\t Mark Johnson . \n\t', '\n\t\t 1998. PCFG models of linguistic tree representations . \n\t', '\n\t\t Computational Linguistics , 24(4):617\x96636 . \n\t', '\n\t\t John Lafferty , Andrew McCallum , and Fernando Pereira . \n\t', '\n\t\t 2001. Conditional random fields : Probabilistic models for segmenting and labeling sequence data . \n\t', '\n\t\t In Proceedings of the 18th International Conference on Machine Learning , pages 282\x96289 . \n\t', '\n\t\t Adwait Ratnaparkhi , Salim Roukos , and R. Todd Ward . \n\t', '\n\t\t 1994. A maximum entropy model for parsing . \n\t', '\n\t\t In Proceedings of the International Conference on Spoken Language Processing ( ICSLP ) , pages 803\x96806 . \n\t', '\n\t\t Adwait Ratnaparkhi . \n\t', '\n\t\t 1999. Learning to parse natural language with maximum entropy models . \n\t', '\n\t\t Machine Learning , 34:151\x96175 . \n\t', '\n\t\t Stefan Riezler , Tracy King , Ronald M. Kaplan , Richard Crouch , John T. Maxwell III , and Mark Johnson . \n\t', '\n\t\t 2002. Parsing the wall street journal using a lexical- functional grammar and discriminative estimation techniques . \n\t', '\n\t\t In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , pages 271\x96278 . \n\t', '\n\t\t Brian Roark , Murat Saraclar , and Michael Collins . \n\t', '\n\t\t 2004. Corrective language modeling for large vocabulary ASR with the perceptron algorithm . \n\t', '\n\t\t In Proceedings of the International Conference on Acoustics , Speech , and Signal Processing ( ICASSP ) , pages 749\x96752 . \n\t', '\n\t\t Brian Roark . \n\t', '\n\t\t 2001a . \n\t', '\n\t\t Probabilistic top-down parsing and language modeling . \n\t', '\n\t\t Computational Linguistics , 27(2):249\x96276 . \n\t', '\n\t\t Brian Roark . \n\t', '\n\t\t 2001b . \n\t', '\n\t\t Robust Probabilistic Predictive Syntactic Processing . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , Brown University . \n\t', '\n\t\t http://arXiv.org/abs/cs/0105019 . \n\t', '\n\t\t Brian Roark . \n\t', '\n\t\t 2004 . \n\t', '\n\t\t Robust garden path parsing . \n\t', '\n\t\t Natural Language Engineering , 10(1):1\x9624 . \n\t', '\n\t\t Convolution Kernels with Feature Selection for Natural Language Processing Tasks Jun Suzuki , Hideki Isozaki and Eisaku Maeda NTT Communication Science Laboratories , NTT Corp. 2-4 Hikaridai , Seika-cho , Soraku-gun , Kyoto,619-0237 Japan { jun , isozaki , maeda}@cslab.kecl.ntt.co.jp Abstract Convolution kernels , such as sequence and tree kernels , are advantageous for both the concept and accuracy of many natural language processing ( NLP ) tasks . \n\t', '\n\t\t Experiments have , however , shown that the over-fitting problem often arises when these kernels are used in NLP tasks . \n\t', '\n\t\t This paper discusses this issue of convolution kernels , and then proposes a new approach based on statistical feature selection that avoids this issue . \n\t', '\n\t\t To enable the proposed method to be executed efficiently , it is embedded into an original kernel calculation process by using sub-structure mining algorithms . \n\t', '\n\t\t Experiments are undertaken on real NLP tasks to confirm the problem with a conventional method and to compare its performance with that of the proposed method . \n\t', '\n\t\t 1 Introduction Over the past few years , many machine learning methods have been successfully applied to tasks in natural language processing ( NLP ) . \n\t', '\n\t\t Especially , state-of-the-art performance can be achieved with kernel methods , such as Support Vector Machine \n\t\t']",Positive
['\n\t\t Examples include text categorization \n\t\t'],Positive
"['\n\t\t Another feature of this kernel methodology is that it not only provides high accuracy but also allows us to design a kernel function suited to modeling the task at hand . \n\t', '\n\t\t Since natural language data take the form of sequences of words , and are generally analyzed using discrete structures , such as trees ( parsed trees ) and graphs ( relational graphs ) , discrete kernels , such as sequence kernels \n\t\t']",Positive
['\n\t\t These discrete kernels are related to convolution kernels \n\t\t'],Positive
"['\n\t\t Convolution kernels allow us to treat structural features without explicitly representing the feature vectors from the input object . \n\t', '\n\t\t That is , convolution kernels are well suited to NLP tasks in terms of both accuracy and concept . \n\t', '\n\t\t Unfortunately , experiments have shown that in some cases there is a critical issue with convolution kernels , especially in NLP tasks \n\t\t']",Positive
"['\n\t\t That is , the over-fitting problem arises if large \x93substructures\x94 are used in the kernel calculations . \n\t', '\n\t\t As a result , the machine learning approach can never be trained efficiently . \n\t', '\n\t\t To solve this issue , we generally eliminate large sub-structures from the set of features used . \n\t', '\n\t\t However , the main reason for using convolution kernels is that we aim to use structural features easily and efficiently . \n\t', '\n\t\t If use is limited to only very small structures , it negates the advantages of using convolution kernels . \n\t', '\n\t\t This paper discusses this issue of convolution kernels , and proposes a new method based on statistical feature selection . \n\t', '\n\t\t The proposed method deals only with those features that are statistically significant for kernel calculation , large significant substructures can be used without over-fitting . \n\t', '\n\t\t Moreover , the proposed method can be executed efficiently by embedding it in an original kernel calculation process by using sub-structure mining algorithms . \n\t', '\n\t\t In the next section , we provide a brief overview of convolution kernels . \n\t', '\n\t\t Section 3 discusses one issue of convolution kernels , the main topic of this paper , and introduces some conventional methods for solving this issue . \n\t', '\n\t\t In Section 4 , we propose a new approach based on statistical feature selection to offset the issue of convolution kernels using an example consisting of sequence kernels . \n\t', '\n\t\t In Section 5 , we briefly discuss the application of the proposed method to other convolution kernels . \n\t', '\n\t\t In Section 6 , we compare the performance of conventional methods with that of the proposed method by using real NLP tasks : question classification and sentence modality identification . \n\t', '\n\t\t The experimental results described in Section 7 clarify the advantages of the proposed method . \n\t', '\n\t\t 2 Convolution Kernels Convolution kernels have been proposed as a concept of kernels for discrete structures , such as sequences , trees and graphs . \n\t', '\n\t\t This framework defines the kernel function between input objects as the convolution of \x93sub-kernels\x94 , i.e. the kernels for the decompositions ( parts ) of the objects . \n\t', '\n\t\t Let X and Y be discrete objects . \n\t', '\n\t\t Conceptually , convolution kernels K(X , Y ) enumerate all substructures occurring in X and Y and then calculate their inner product , which is simply written as : K(X,Y) _ ( O(X) , O(Y)) _ E Oi(X) - Oi(Y)\x95 ( 1 ) i O represents the feature mapping from the discrete object to the feature space ; that is , O(X) = ( O1(X) , ... , Oi(X) , ... ) . \n\t', '\n\t\t With sequence kernels \n\t\t']",Positive
['\n\t\t With tree kernels \n\t\t'],Positive
"['\n\t\t When implemented , these kernels can be efficiently calculated in quadratic time by using dynamic programming ( DP ) . \n\t', '\n\t\t Finally , since the size of the input objects is not constant , the kernel value is normalized using the following equation . \n\t', ""\n\t\t ( X , K(X,Y) \x88K 1 ' ) _ ~K(X , X ) K(Y , Y ) ( 2 ) The value of \x88K(X , Y ) is from 0 to 1 , \x88K(X , Y ) = 1 if and only if X = Y. 2.1 Sequence Kernels To simplify the discussion , we restrict ourselves hereafter to sequence kernels . \n\t"", '\n\t\t Other convolution kernels are briefly addressed in Section 5 . \n\t', '\n\t\t Many kinds of sequence kernels have been proposed for a variety of different tasks . \n\t', '\n\t\t This paper basically follows the framework of word sequence kernels \n\t\t']",Positive
"['\n\t\t Let E be a set of finite symbols , and En be a set of possible ( symbol ) sequences whose sizes are n or less that are constructed by symbols in E . \n\t', '\n\t\t The meaning of \x93size\x94 in this paper is the number of symbols in the sub-structure . \n\t', '\n\t\t Namely , in the case of sequence , size n means length n. S and T can represent any sequence . \n\t', '\n\t\t si and tj represent the ith and jth symbols in S and T , respectively . \n\t', '\n\t\t Therefore , a sequences sub-sequences S=abc ( a , b , c , ab , ac , bc , abc ) T = abac ( a , b , c , aa , ab , ac , ba , bc , aba , aac , abc , bac , abac ) Figure 1 : Example of sequence kernel output sequence S can be written as S = s1 ... si ... slSl , where ISI represents the length of S . \n\t', '\n\t\t If sequence u is contained in sub-sequence S[i : j ] def= si . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t sj of S ( allowing the existence of gaps ) , the position of u in S is written as i = ( i1 : ilul ) . \n\t', '\n\t\t The length of S[i] is l(i) = ilul \x97 i1 + 1 . \n\t', '\n\t\t For example , if u = ab and S = cacbd , then i = ( 2 : 4 ) and l(i)=4\x972+1=3 . \n\t', '\n\t\t By using the above notations , sequence kernels can be defined as : KS- ( S,T ) _ E E A-Y(i) E ~~(j) , ( 3 ) uEEn ilu=S[i] jlu=T[j] where A is the decay factor that handles the gap present in a common sub-sequence u , and -y(i) = l ( i ) \x97 I u I . \n\t', '\n\t\t In this paper , I means \x93such that\x94 . \n\t', '\n\t\t Figure 1 shows a simple example of the output of this kernel . \n\t', '\n\t\t However , in general , the number of features I En I , which is the dimension of the feature space , becomes very high , and it is computationally infeasible to calculate Equation ( 3 ) explicitly . \n\t', '\n\t\t The efficient recursive calculation has been introduced in \n\t\t']",Positive
"['\n\t\t To clarify the discussion , we redefine the sequence kernels with our notation . \n\t', '\n\t\t The sequence kernel can be written as follows : n KS- ( S,T ) _ E E E J.(Si,Tj)\x95 ( 4 ) .=1 1<i<S 1<j<lTl where Si and Tj represent the sub-sequences Si = s1 , s2 , ... , si and Tj = t1 , t2 , ... , tj , respectively . \n\t', '\n\t\t Let Jm ( Si , Tj ) be a function that returns the value of common sub-sequences if si = tj. J.(Si,Tj) _ Jll.-1 ( Si,Tj ) -I(si,tj) ( 5 ) I(si , tj ) is a function that returns a matching value between si and tj . \n\t', '\n\t\t This paper defines I ( si , tj ) as an indicator function that returns 1 if si = tj , otherwise 0. 0 prod . \n\t', '\n\t\t 2 1 1 0 1 A+A3 0 A 0 0 A 0 kernel value 5+3A+A 3 u a , b , c , aa , ab , ac , ba , bc , aba , aac , abc , bac , abac 0 1 1 1+A2 s T 1 2 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 Then , J~m ( Si , Tj ) and J~~m(Si , Tj ) are introduced to calculate the common gapped sub-sequences between Si and Tj . \n\t', '\n\t\t Table 1 : Contingency table and notation for the chi- squared value I ( 6 ) J ; . \n\t', '\n\t\t ( Si , Tj ) = 1 if m = 0 , 0 if j = 0 and m > 0 , AJ;.(Si , Tj-1 ) + J ; ; . \n\t', '\n\t\t ( Si , Tj-1 ) otherwise c c¯ E row u Ouc = y Ou¯c Ou = x u¯ O¯uc O¯u¯c O¯u E column Oc = M O¯c N 0 if i=0 , AJ ; ; . \n\t', '\n\t\t ( Si -1 , Tj ) + J. ( Si-1 , Tj ) ( 7 ) otherwise If we calculate Equations ( 5 ) to ( 7 ) recursively , Equation ( 4 ) provides exactly the same value as Equation ( 3 ) . \n\t', '\n\t\t 3 Problem of Applying Convolution Kernels to NLP tasks This section discusses an issue that arises when applying convolution kernels to NLP tasks . \n\t', '\n\t\t According to the original definition of convolution kernels , all the sub-structures are enumerated and calculated for the kernels . \n\t', '\n\t\t The number of substructures in the input object usually becomes exponential against input object size . \n\t', '\n\t\t As a result , all kernel values \x88K(X , Y ) are nearly 0 except the kernel value of the object itself , \x88K(X , X ) , which is 1 . \n\t', '\n\t\t In this situation , the machine learning process becomes almost the same as memory-based learning . \n\t', '\n\t\t This means that we obtain a result that is very precise but with very low recall . \n\t', '\n\t\t To avoid this , most conventional methods use an approach that involves smoothing the kernel values or eliminating features based on the sub-structure size . \n\t', '\n\t\t For sequence kernels , \n\t\t']",Positive
"['\n\t\t This means that the kernel calculation deals only with those sub-sequences whose size is n or less . \n\t', '\n\t\t For tree kernels , \n\t\t']",Negative
"['\n\t\t These methods seem to work well on the surface , however , good results are achieved only when n is very small , i.e. n = 2 . \n\t', '\n\t\t The main reason for using convolution kernels is that they allow us to employ structural features simply and efficiently . \n\t', '\n\t\t When only small sized substructures are used ( i.e. n = 2 ) , the full benefits of convolution kernels are missed . \n\t', '\n\t\t Moreover , these results do not mean that larger sized sub-structures are not useful . \n\t', '\n\t\t In some cases we already know that larger sub-structures are significant features as regards solving the target problem . \n\t', '\n\t\t That is , these significant larger sub-structures , which the conventional methods cannot deal with efficiently , should have a possibility of improving the performance furthermore . \n\t', '\n\t\t The aim of the work described in this paper is to be able to use any significant sub-structure efficiently , regardless of its size , to solve NLP tasks . \n\t', '\n\t\t 4 Proposed Feature Selection Method Our approach is based on statistical feature selection in contrast to the conventional methods , which use sub-structure size . \n\t', '\n\t\t For a better understanding , consider the two- class ( positive and negative ) supervised classification problem . \n\t', '\n\t\t In our approach we test the statistical deviation of all the sub-structures in the training samples between the appearance of positive samples and negative samples . \n\t', '\n\t\t This allows us to select only the statistically significant sub-structures when calculating the kernel value . \n\t', '\n\t\t Our approach , which uses a statistical metric to select features , is quite natural . \n\t', '\n\t\t We note , however , that kernels are calculated using the DP algorithm . \n\t', '\n\t\t Therefore , it is not clear how to calculate kernels efficiently with a statistical feature selection method . \n\t', '\n\t\t First , we briefly explain a statistical metric , the chi- squared ( x2 ) value , and provide an idea of how to select significant features . \n\t', '\n\t\t We then describe a method for embedding statistical feature selection into kernel calculation . \n\t', '\n\t\t 4.1 Statistical Metric : Chi-squared Value There are many kinds of statistical metrics , such as chi-squared value , correlation coefficient and mutual information . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t Following this information , we use x2 values as statistical feature selection criteria . \n\t', '\n\t\t Although we selected x2 values , any other statistical metric can be used as long as it is based on the contingency table shown in Table 1 . \n\t', '\n\t\t We briefly explain how to calculate the x2 value by referring to Table 1 . \n\t', '\n\t\t In the table , c and c¯ represent the names of classes , c for the positive class J ; ; . \n\t', ""\n\t\t ( Si,Tj ) = I 2 1 1 0 1 A+A3 0 A 0 0 A 0 0 t ' ( u ) 0.1 0.5 1.2 1.5 0.9 0.8 2.5 0 0 1 1 0 0 A kernel value under the feature selection 2+A Figure 2 : Example of statistical feature selection and c¯ for the negative class . \n\t"", '\n\t\t Ou , , Ou¯ , , O¯u , and O¯u¯ , represent the number of u that appeared in the positive sample c , the number of u that appeared in the negative sample ¯c , the number of u that did not appear in c , and the number of u that did not appear in ¯c , respectively . \n\t', '\n\t\t Let y be the number of samples of positive class c that contain sub-sequence u , and x be the number of samples that contain u . \n\t', '\n\t\t Let N be the total number of ( training ) samples , and M be the number of positive samples . \n\t', '\n\t\t Since N and M are constant for ( fixed ) data , X2 can be written as a function of x and y , X2(x , y ) = N(Ouc O¯u¯c _ O¯uc Ou¯c)2 .(8) Ou O¯u Oc O¯c X2 expresses the normalized deviation of the observation from the expectation . \n\t', '\n\t\t We simply represent X2 ( x , y ) as X2 ( u ) . \n\t', '\n\t\t 4.2 Feature Selection Criterion The basic idea of feature selection is quite natural . \n\t', '\n\t\t First , we decide the threshold T of the X2 value . \n\t', '\n\t\t If X2 ( u ) < T holds , that is , u is not statistically significant , then u is eliminated from the features and the value of u is presumed to be 0 for the kernel value . \n\t', '\n\t\t The sequence kernel with feature selection ( FSSK ) can be defined as follows : K FSSK ( S T ) = E E A-Y(i) E A7(j) . \n\t', '\n\t\t ( 9 ) ~<~2 ( u ) juE~n iju=S[i] ju=T[j] The difference between Equations ( 3 ) and ( 9 ) is simply the condition of the first summation . \n\t', '\n\t\t FSSK selects significant sub-sequence u by using the condition of the statistical metric T < X2 ( u ) . \n\t', '\n\t\t Figure 2 shows a simple example of what FSSK calculates for the kernel value . \n\t', '\n\t\t 4.3 Efficient X2(u) Calculation Method It is computationally infeasible to calculate X2 ( u ) for all possible u with a naive exhaustive method . \n\t', '\n\t\t In our approach , we use a sub-structure mining algorithm to calculate X2 ( u ) . \n\t', '\n\t\t The basic idea comes from a sequential pattern mining technique , PrefixSpan \n\t\t']",Positive
"['\n\t\t By using these techniques , all the significant sub-sequences u that satisfy T < X2(u) can be found efficiently by depth-first search and pruning . \n\t', '\n\t\t Below , we briefly explain the concept involved in finding the significant features . \n\t', '\n\t\t First , we denote uv , which is the concatenation of sequences u and v. . \n\t', '\n\t\t Then , u is a specific sequence and uv is any sequence that is constructed by u with any suffix v. . \n\t', '\n\t\t The upper bound of the X2 value of uv can be defined by the value of u \n\t\t']",Positive
"['\n\t\t X2(uv)<max ~X2 ( yu , yu ) , X2 ( xu _ yu , 0 ) ) ~X2(u) where xu and yu represent the value of x and y of u . \n\t', '\n\t\t This inequation indicates that if ~X2(u) is less than a certain threshold T , all sub-sequences uv can be eliminated from the features , because no subsequence uv can be a feature . \n\t', '\n\t\t The PrefixSpan algorithm enumerates all the significant sub-sequences by using a depth-first search and constructing a TRIE structure to store the significant sequences of internal results efficiently . \n\t', '\n\t\t Specifically , PrefixSpan algorithm evaluates uw , where uw represents a concatenation of a sequence u and a symbol w , using the following three conditions . \n\t', '\n\t\t 1. T < X2 ( uw ) 2 . \n\t', '\n\t\t T > X2(uw) , T > ~X2(uw) 3 . \n\t', '\n\t\t T > X2(uw) , T ~X2(uw) With 1 , sub-sequence uw is selected as a significant feature . \n\t', '\n\t\t With 2 , sub-sequence uw and arbitrary subsequences uwv , are less than the threshold T . \n\t', '\n\t\t Then w is pruned from the TRIE , that is , all uwv where v represents any suffix pruned from the search space . \n\t', '\n\t\t With 3 , uw is not selected as a significant feature because the X2 value of uw is less than T , however , uwv can be a significant feature because the upper- bound X2 value of uwv is greater than T , thus the search is continued to uwv . \n\t', '\n\t\t Figure 3 shows a simple example of PrefixSpan with SMP that searches for the significant features u a , b , c , aa , ab , ac , ba , bc , aba , aac , abc , bac , abac 0 1 1 1+A2 prod . \n\t', '\n\t\t 2 1 1 0 1 A+A3 0 A 0 0 A 0 0 s T 1 2 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 5+3A+A \' kernel value feature selection r =1.0 threshold S=abc T = abac ( a , b , c , ab , ac , bc , abc ) ( a , b , c , aa , ab , ac , ba , bc , aba , aac , abc , bac , abac ) sequences sub-sequences = n E m=1 E 1<j<T E 1<i<ISI KFSSK(S , T ) = Km(Si,Tj) ( 10 ) /u(Si,Tj) , ( 11 ) Let Km ( Si , Tj ) be a function that returns the sum value of all statistically significant common subsequences u if si = tj. Km(Si,Tj) = E uE~m(Si,Tj) ( 12 ) /1u(Si , Tj ) ·Z(w) if uw E ~rIuw I ( Si , Tj ) , 0 otherwise ~ ~~ ~~ /1u(Si,Tj) = { /.""(Si,Tj) = ( 13 ) 1 if u = A , 0 if j=0 and u=~A , A/1u ( Si , Tj-1 ) + /u1 ( Si , Tj-1 ) otherwise ( 14 ) 0 if i = 0 , A/.""(Si-1,Tj) + /u(Si-1,Tj) otherwise { ( 16 ) ~rm(Si,Tj) = IF(~r1m-1(Si,Tj),si) if si = tj 0 otherwise { /uw(Si,Tj) = 2 2 0 5 4 4 2 a : b : c : d : w = c : d : 1 1 1 0 x Y u=A w= search order 2 50 50 5.0 22 a0:0 b0:8 c0.8 d2.2 pruned suffix a b c c d b c b a c a c d a b d +1 -1 +1 -1 -1 a b c c d b c a b a c a c d a b d +1 -1 +1 -1 -1 2 X Y M= 1 1 2 suffix u=a a b c c d b c a b a c a dabd +1 -1 +1 -1 -1 b : c : d : 2 3 1 1 2 1 w= class training data N=5 +1 -1 +1 -1 +1 suffix a b c c d b c b a c a c d a b d 3 c11 . \n\t', ""\n\t\t 40.8 .8 ... z =1.0 X Y b1.9 5.0 0.1 c2.2 u=ab 4 4.8 .8 pruned 19 c1.9 w x ' ( ,, ) TRIE representation 5 Figure 3 : Efficient search for statistically significant sub-sequences using the PrefixSpan algorithm with SMP by using a depth-first search with a TRIE representation of the significant sequences . \n\t"", '\n\t\t The values of each symbol represent X2 ( u ) and ~X2 ( u ) that can be calculated from the number of xu and yu . \n\t', '\n\t\t The TRIE structure in the figure represents the statistically significant sub-sequences that can be shown in a path from L to the symbol . \n\t', '\n\t\t We exploit this TRIE structure and PrefixSpan pruning method in our kernel calculation . \n\t', '\n\t\t 4.4 Embedding Feature Selection in Kernel Calculation This section shows how to integrate statistical feature selection in the kernel calculation . \n\t', '\n\t\t Our proposed method is defined in the following equations . \n\t', '\n\t\t where Fm ( Si , Tj ) represents a set of sub-sequences whose size I u I is m and that satisfy the above condition 1 . \n\t', '\n\t\t The Fm ( Si,Tj ) is defined in detail in Equation ( 15 ) . \n\t', '\n\t\t Then , let Ju(Si,Tj) , J~u(Si,Tj) and Ju ( Si , Tj ) be functions that calculate the value of the common sub-sequences between Si and Tj recursively , as well as equations ( 5 ) to ( 7 ) for sequence kernels . \n\t', '\n\t\t We introduce a special symbol A to represent an \x93empty sequence\x94 , and define Aw = w and IAwI =1. where Z(w) is a function that returns a matching value of w . \n\t', '\n\t\t In this paper , we define Z(w) is 1 . \n\t', '\n\t\t ~Fm ( Si , Tj ) has realized conditions 2 and 3 ; the details are defined in Equation ( 16 ) . \n\t', '\n\t\t The following five equations are introduced to select a set of significant sub-sequences . \n\t', '\n\t\t Fm ( Si , Tj ) and ~Fm(Si,Tj) are sets of sub-sequences ( features ) that satisfy condition 1 and 3 , respectively , when calculating the value between Si and Tj in Equations ( 11 ) and ( 12 ) . \n\t', '\n\t\t rm(Si,Tj) = { u I u E ~rm(Si,Tj),~ < ~2(u) } ( 15 ) IF(F , w ) = { uw I u E F,T < ~~2 ( uw ) } , ( 17 ) where F represents a set of sub-sequences . \n\t', '\n\t\t Notice that Fm ( Si , Tj ) and ~F m ( Si , Tj ) have only subsequences u that satisfy T < X2 ( uw ) or T < ~X2 ( uw ) , respectively , if si = tj ( = w ) ; otherwise they become empty sets . \n\t', '\n\t\t The following two equations are introduced for recursive set operations to calculate Fm(Si,Tj) and ~Fm(Si,Tj) . \n\t', '\n\t\t ( 18 ) ( 19 ) ~r1m(Si,Tj) = ~r11m(Si , Tj ) = otherwise { { { A } if m = 0 , 0 if j = 0 and m > 0 , ~r1m(Si,Tj-1) U otherwise 0 if i=0 , ~r11m(Si-1,Tj) U ~r11m(Si,Tj-1) ~rm(Si-1 , Tj ) In the implementation , Equations ( 11 ) to ( 14 ) can be performed in the same way as those used to calculate the original sequence kernels , if the feature selection condition of Equations ( 15 ) to ( 19 ) has been removed . \n\t', '\n\t\t Then , Equations ( 15 ) to ( 19 ) , which select significant features , are performed by the PrefixSpan algorithm described above and the TRIE representation of statistically significant features . \n\t', '\n\t\t The recursive calculation of Equations ( 12 ) to ( 14 ) and Equations ( 16 ) to ( 19 ) can be executed in the same way and at the same time in parallel . \n\t', '\n\t\t As a result , statistical feature selection can be embedded in oroginal sequence kernel calculation based on a dynamic programming technique . \n\t', '\n\t\t 4.5 Properties The proposed method has several important advantages over the conventional methods . \n\t', '\n\t\t First , the feature selection criterion is based on a statistical measure , so statistically significant features are automatically selected . \n\t', '\n\t\t Second , according to Equations ( 10 ) to ( 18 ) , the proposed method can be embedded in an original kernel calculation process , which allows us to use the same calculation procedure as the conventional methods . \n\t', '\n\t\t The only difference between the original sequence kernels and the proposed method is that the latter calculates a statistical metric x2 ( u ) by using a sub-structure mining algorithm in the kernel calculation . \n\t', '\n\t\t Third , although the kernel calculation , which unifies our proposed method , requires a longer training time because of the feature selection , the selected sub-sequences have a TRIE data structure . \n\t', '\n\t\t This means a fast calculation technique proposed in \n\t\t']",Positive
"['\n\t\t In the classification part , the features ( subsequences ) selected in the learning part must be known . \n\t', '\n\t\t Therefore , we store the TRIE of selected sub-sequences and use them during classification . \n\t', '\n\t\t 5 Proposed Method Applied to Other Convolution Kernels We have insufficient space to discuss this subject in detail in relation to other convolution kernels . \n\t', '\n\t\t However , our proposals can be easily applied to tree kernels \n\t\t']",Positive
"['\n\t\t We enumerate nodes ( labels ) of tree in postorder traversal . \n\t', '\n\t\t After that , we can employ a sequential pattern mining technique to select statistically significant sub-trees . \n\t', '\n\t\t This is because we can convert to the original sub-tree form from the string encoding representation . \n\t', '\n\t\t Table 2 : Parameter values of proposed kernels and Support Vector Machines parameter value soft margin for SVM ( C ) 1000 decay factor of gap ( A ) 0.5 2.7055 threshold of x2 ( T ) 3.8415 As a result , we can calculate tree kernels with statistical feature selection by using the original tree kernel calculation with the sequential pattern mining technique introduced in this paper . \n\t', '\n\t\t Moreover , we can expand our proposals to hierarchically structured graph kernels \n\t\t']",Positive
"['\n\t\t 6 Experiments We evaluated the performance of the proposed method in actual NLP tasks , namely English question classification ( EQC ) , Japanese question classification ( JQC ) and sentence modality identification ( MI ) tasks . \n\t', '\n\t\t We compared the proposed method ( FSSK ) with a conventional method ( SK ) , as discussed in Section 3 , and with bag-of-words ( BOW ) Kernel (BOW-K)\n\t\t']",Positive
"['\n\t\t Support Vector Machine ( SVM ) was selected as the kernel-based classifier for training and classification . \n\t', '\n\t\t Table 2 shows some of the parameter values that we used in the comparison . \n\t', '\n\t\t We set thresholds of T = 2.7055 ( FSSK1 ) and T = 3.8415 ( FSSK2 ) for the proposed methods ; these values represent the 10 % and 5 % level of significance in the x2 distribution with one degree of freedom , which used the x2 significant test . \n\t', '\n\t\t 6.1 Question Classification Question classification is defined as a task similar to text categorization ; it maps a given question into a question type . \n\t', '\n\t\t We evaluated the performance by using data provided by \n\t\t']",Positive
"['\n\t\t We used the one-vs-rest classifier of SVM as the multi-class classification method for EQC . \n\t', '\n\t\t Figure 4 shows examples of the question classification data used here . \n\t', '\n\t\t question types input object : word sequences ( [ ] : information of chunk and ( ) : named entity ) ABBREVIATION what,[B-NP] be,[B-VP] the,[B-NP] abbreviation,[I-NP] for,[B-PP] Texas,[B-NP],(B-GPE) ?,[O] DESCRIPTION what,[B-NP] be,[B-VP] Aborigines,[B-NP] ?,[O] HUMAN who,[B-NP] discover,[B-VP] America,[B-NP],(B-GPE) ?,[O] Figure 4 : Examples of English question classification data Table 3 : Results of the Japanese question classification ( F-measure ) ( a ) TIME TOP ( b ) LOCATION ( c ) ORGANIZATION ( d ) NUMEX n 1 2 3 4 oo 1 2 3 4 oo 1 2 3 4 oo 1 2 3 4 oo FSSK1 - .961 .958 .957 .956 - .795 .793 .798 .792 - .709 .720 .720 .723 - .912 .915 .908 .908 FSSK2 -.961 .956 .957 .956 -.788 .799 .804 .800 -.703 .710 .716 .720 -.913 .916 .911 .913 SK -.946 .910 .866 .223 -.791 .775 .732 .169 -.705 .668 .594 .035 -.912 .885 .817 .036 BOW-K .902 .909 .886 .855 - .744 .768 .756 .747 - .641 690 .636 .572 - .842 .852 .807 .726 - 6.2 Sentence Modality Identification For example , sentence modality identification techniques are used in automatic text analysis systems that identify the modality of a sentence , such as \x93opinion\x94 or \x93description\x94 . \n\t', '\n\t\t The data set was created from Mainichi news articles and one of three modality tags , \x93opinion\x94 , \x93decision\x94 and \x93description\x94 was applied to each sentence . \n\t', '\n\t\t The data size was 1135 sentences consisting of 123 sentences of \x93opinion\x94 , 326 of \x93decision\x94 and 686 of \x93description\x94 . \n\t', '\n\t\t We evaluated the results by using 5-fold cross validation . \n\t', '\n\t\t 7 Results and Discussion Tables 3 and 4 show the results of Japanese and English question classification , respectively . \n\t', '\n\t\t Table 5 shows the results of sentence modality identification . \n\t', '\n\t\t n in each table indicates the threshold of the sub-sequence size . \n\t', '\n\t\t n = oc means all possible subsequences are used . \n\t', '\n\t\t First , SK was consistently superior to BOW-K . \n\t', '\n\t\t This indicates that the structural features were quite efficient in performing these tasks . \n\t', '\n\t\t In general we can say that the use of structural features can improve the performance of NLP tasks that require the details of the contents to perform the task . \n\t', '\n\t\t Most of the results showed that SK achieves its maximum performance when n = 2 . \n\t', '\n\t\t The performance deteriorates considerably once n exceeds 4 . \n\t', '\n\t\t This implies that SK with larger sub-structures degrade classification performance . \n\t', '\n\t\t These results show the same tendency as the previous studies discussed in Section 3 . \n\t', '\n\t\t Table 6 shows the precision and recall of SK when n = oc . \n\t', '\n\t\t As shown in Table 6 , the classifier offered high precision but low recall . \n\t', '\n\t\t This is evidence of over-fitting in learning . \n\t', '\n\t\t As shown by the above experiments , FSSK pro- Table 6 : Precision and recall of SK : n = oc Precision Recall F MI:Opinion .917 .209 .339 JQA:LOCATION .896 .093 .168 vided consistently better performance than the conventional methods . \n\t', '\n\t\t Moreover , the experiments confirmed one important fact . \n\t', '\n\t\t That is , in some cases maximum performance was achieved with n = oc . \n\t', '\n\t\t This indicates that sub-sequences created using very large structures can be extremely effective . \n\t', '\n\t\t Of course , a larger feature space also includes the smaller feature spaces , En C En+1 . \n\t', '\n\t\t If the performance is improved by using a larger n , this means that significant features do exist . \n\t', '\n\t\t Thus , we can improve the performance of some classification problems by dealing with larger substructures . \n\t', '\n\t\t Even if optimum performance was not achieved with n = oc , difference between the performance of smaller n are quite small compared to that of SK . \n\t', '\n\t\t This indicates that our method is very robust as regards substructure size ; It therefore becomes unnecessary for us to decide sub-structure size carefully . \n\t', '\n\t\t This indicates our approach , using large sub-structures , is better than the conventional approach of eliminating sub-sequences based on size . \n\t', '\n\t\t 8 Conclusion This paper proposed a statistical feature selection method for convolution kernels . \n\t', '\n\t\t Our approach can select significant features automatically based on a statistical significance test . \n\t', '\n\t\t Our proposed method can be embedded in the DP based kernel calculation process for convolution kernels by using substructure mining algorithms . \n\t', '\n\t\t Table 4 : Results of English question classification ( Accuracy ) ( a ) coarse ( b ) fine n 1 2 3 4 oo 1 2 3 4 oo FSSK1 - .908 .914 .916 .912 - .852 .854 .852 .850 FSSK2 - .902 .896 .902 .906 - .858 .856 .854 .854 SK - .912 .914 .912 .892 - .850 .840 .830 .796 BOW-K .728 .836 .864 .858 - .754 .792 .790 .778 - Table 5 : Results of sentence modality identification ( F-measure ) ( a ) opinion ( b ) decision ( c ) description n 1 2 3 4 oo 1 2 3 4 oo 1 2 3 4 oo FSSK1 - .734 .743 .746 .751 - .828 .858 .854 .857 - .896 .906 .910 .910 FSSK2 - .740 .748 .750 .750 - .824 .855 .859 .860 - .894 .903 .909 .909 SK - .706 .672 .577 .058 - .816 .834 .830 .339 - .902 .913 .910 .808 BOW-K .507 .531 .438 .368 - .652 .708 .686 .665 - .819 .839 .826 .793 - Experiments show that our method is superior to conventional methods . \n\t', '\n\t\t Moreover , the results indicate that complex features exist and can be effective . \n\t', '\n\t\t Our method can employ them without over-fitting problems , which yields benefits in terms of concept and performance . \n\t', '\n\t\t References N. Cancedda , E. Gaussier , C. Goutte , and J.-M. Renders . \n\t', '\n\t\t 2003. Word-Sequence Kernels . \n\t', '\n\t\t Journal ofMachine Learning Research , 3:1059\x961082 . \n\t', '\n\t\t M. Collins and N. Duffy . \n\t', '\n\t\t 2001. Convolution Kernels for Natural Language . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ofNeural Information Processing Systems ( NIPS\x922001 ) . \n\t', '\n\t\t C. Cortes and V. N. Vapnik . \n\t', '\n\t\t 1995. Support Vector Networks . \n\t', '\n\t\t Machine Learning , 20:273\x96297 . \n\t', '\n\t\t D. Haussler . \n\t', '\n\t\t 1999. Convolution Kernels on Discrete Structures . \n\t', '\n\t\t In Technical Report UCS-CRL99-10 . \n\t', '\n\t\t UC Santa Cruz . \n\t', '\n\t\t T. Joachims . \n\t', '\n\t\t 1998. Text Categorization with Support Vector Machines : Learning with Many Relevant Features . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ofEuropean Conference on Machine Learning ( ECML \x9298 ) , pages 137\x96 142 . \n\t', '\n\t\t T. Kudo and Y. Matsumoto . \n\t', '\n\t\t 2002. Japanese Dependency Analysis Using Cascaded Chunking . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 6th Conference on Natural Language Learning \n\t\t']",Positive
"['\n\t\t T. Kudo and Y. Matsumoto . \n\t', '\n\t\t 2003. Fast Methods for Kernel-based Text Analysis . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 41st Annual Meeting of the Association for Computa- tional Linguistics ( ACL-2003 ) , pages 24\x9631 . \n\t', '\n\t\t X. Li and D. Roth . \n\t', '\n\t\t 2002. Learning Question Clas- sifiers . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 19th International Con- ference on Computational Linguistics ( COLING 2002 ) , pages 556\x96562 . \n\t', '\n\t\t H. Lodhi , C. Saunders , J. Shawe-Taylor , N. Cristianini , and C. Watkins . \n\t', '\n\t\t 2002. Text Classification Using String Kernel . \n\t', '\n\t\t Journal ofMachine Learning Research , 2:419\x96444 . \n\t', '\n\t\t S. Morishita and J. Sese . \n\t', '\n\t\t 2000. Traversing Item- set Lattices with Statistical Metric Pruning . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ofACM SIGACT-SIGMOD-SIGART Symp . \n\t', '\n\t\t on Database Systems ( PODS\x9200 ) , pages 226\x96 236. J. Pei , J. Han , B. Mortazavi-Asl , and H. Pinto . \n\t', '\n\t\t 2001. PrefixSpan : Mining Sequential Patterns Efficiently by Prefix-Projected Pattern Growth . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 17th International Conference on Data Engineering ( ICDE 2001 ) , pages 215\x96224 . \n\t', '\n\t\t M. Rogati and Y. Yang . \n\t', '\n\t\t 2002. High-performing Feature Selection for Text Classification . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 2002 ACM CIKMInternational Conference on Information and Knowledge Management , pages 659\x96661 . \n\t', '\n\t\t J. Suzuki , T. Hirao , Y. Sasaki , and E. Maeda . \n\t', '\n\t\t 2003a . \n\t', '\n\t\t Hierarchical Directed Acyclic Graph Kernel : Methods for Natural Language Data . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 41st Annual Meeting of the Association for Computational Linguistics ( ACL-2003 ) , pages 32\x9639 . \n\t', '\n\t\t J. Suzuki , Y. Sasaki , and E. Maeda . \n\t', '\n\t\t 2003b . \n\t', '\n\t\t Kernels for Structured Natural Language Data . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 17th Annual Conference on Neural Information Processing Systems ( NIPS2003 ) . \n\t', '\n\t\t Improving Pronoun Resolution by Incorporating Coreferential Information of Candidates Xiaofeng Yangt $ Jian Sut Guodong Zhout Chew Lim Tan $ tInstitute for Infocomm Research $ Department of Computer Science 21 Heng Mui Keng Terrace , National University of Singapore , Singapore , 119613 Singapore , 117543 { xiaofengy,suj ian,zhougd } {yangxiao,tancl}@comp.nus.edu.sg @i2r.a-star.edu.sg Abstract Coreferential information of a candidate , such as the properties of its antecedents , is important for pronoun resolution because it reflects the salience of the candidate in the local discourse . \n\t', '\n\t\t Such information , however , is usually ignored in previous learning-based systems . \n\t', '\n\t\t In this paper we present a trainable model which incorporates coreferential information of candidates into pronoun resolution . \n\t', '\n\t\t Preliminary experiments show that our model will boost the resolution performance given the right antecedents of the candidates . \n\t', '\n\t\t We further discuss how to apply our model in real resolution where the antecedents of the candidate are found by a separate noun phrase resolution module . \n\t', '\n\t\t The experimental results show that our model still achieves better performance than the baseline . \n\t', '\n\t\t 1 Introduction In recent years , supervised machine learning approaches have been widely explored in reference resolution and achieved considerable success \n\t\t']",Positive
"['\n\t\t Most learning-based pronoun resolution systems determine the reference relationship between an anaphor and its antecedent candidate only from the properties of the pair . \n\t', '\n\t\t The knowledge about the context of anaphor and antecedent is nevertheless ignored . \n\t', '\n\t\t However , research in centering theory \n\t\t']",Positive
['\n\t\t The choices of the antecedents of pronouns usually depend on the center of attention throughout the local discourse segment \n\t\t'],Positive
"['\n\t\t To determine the salience of a candidate in the local context , we may need to check the coreferential information of the candidate , such as the existence and properties of its antecedents . \n\t', '\n\t\t In fact , such information has been used for pronoun resolution in many heuristic- based systems . \n\t', '\n\t\t The S-List model \n\t\t']",Positive
['\n\t\t In the algorithms based on the centering theory \n\t\t'],Positive
"['\n\t\t In this paper , we present a supervised learning-based pronoun resolution system which incorporates coreferential information of candidates in a trainable model . \n\t', '\n\t\t For each candidate , we take into consideration the properties of its antecedents in terms of features ( henceforth backward features ) , and use the supervised learning method to explore their influences on pronoun resolution . \n\t', '\n\t\t In the study , we start our exploration on the capability of the model by applying it in an ideal environment where the antecedents of the candidates are correctly identified and the backward features are optimally set . \n\t', '\n\t\t The experiments on MUC-6 ( 1995 ) and MUC-7 ( 1998 ) corpora show that incorporating coreferential information of candidates boosts the system performance significantly . \n\t', '\n\t\t Further , we apply our model in the real resolution where the antecedents of the candidates are provided by separate noun phrase resolution modules . \n\t', '\n\t\t The experimental results show that our model still outperforms the baseline , even with the low recall of the non-pronoun resolution module . \n\t', '\n\t\t The remaining of this paper is organized as follows . \n\t', '\n\t\t Section 2 discusses the importance of the coreferential information for candidate evaluation . \n\t', '\n\t\t Section 3 introduces the baseline learning framework . \n\t', '\n\t\t Section 4 presents and evaluates the learning model which uses backward fea- tures to capture coreferential information , while Section 5 proposes how to apply the model in real resolution . \n\t', '\n\t\t Section 6 describes related research work . \n\t', '\n\t\t Finally , conclusion is given in Section 7 . \n\t', '\n\t\t 2 The Impact of Coreferential Information on Pronoun Resolution In pronoun resolution , the center of attention throughout the discourse segment is a very important factor for antecedent selection \n\t\t']",Positive
"['\n\t\t If a candidate is the focus ( or center ) of the local discourse , it would be selected as the antecedent with a high possibility . \n\t', ""\n\t\t See the following example , <s> Gitano ' has pulled off a clever illusion2 with its3 advertising4 . \n\t"", '\n\t\t <s> <s> The campaigns gives its6 clothes a youthful and trendy image to lure consumers into the store . \n\t', ""\n\t\t <s> Table 1 : A text segment from MUC-6 data set In the above text , the pronoun \x93its6\x94 has several antecedent candidates , i.e. , \x93Gitano'\x94 , \x93a clever illusion2\x94 , \x93its3\x94 , \x93its advertising4\x94 and \x93The campaigns\x94 . \n\t"", '\n\t\t Without looking back , \x93The campaigns\x94 would be probably selected because of its syntactic role ( Subject ) and its distance to the anaphor . \n\t', ""\n\t\t However , given the knowledge that the company Gitano is the focus of the local context and \x93its3\x94 refers to \x93Gitano'\x94 , it would be clear that the pronoun \x93its6\x94 should be resolved to \x93its3\x94 and thus \x93Gitano'\x94 , rather than other competitors . \n\t"", '\n\t\t To determine whether a candidate is the \x93focus\x94 entity , we should check how the status ( e.g. grammatical functions ) of the entity alternates in the local context . \n\t', '\n\t\t Therefore , it is necessary to track the NPs in the coreferential chain of the candidate . \n\t', '\n\t\t For example , the syntactic roles ( i.e. , subject ) of the antecedents of \x93its3\x94 would indicate that \x93its3\x94 refers to the most salient entity in the discourse segment . \n\t', '\n\t\t In our study , we keep the properties of the antecedents as features of the candidates , and use the supervised learning method to explore their influence on pronoun resolution . \n\t', '\n\t\t Actually , to determine the local focus , we only need to check the entities in a short discourse segment . \n\t', '\n\t\t That is , for a candidate , the number of its adjacent antecedents to be checked is limited . \n\t', '\n\t\t Therefore , we could evaluate the salience of a candidate by looking back only its closest antecedent instead of each element in its coreferential chain , with the assumption that the closest antecedent is able to provide sufficient information for the evaluation . \n\t', '\n\t\t 3 The Baseline Learning Framework Our baseline system adopts the common learning-based framework employed in the system by \n\t\t']",Positive
"[""\n\t\t In the learning framework , each training or testing instance takes the form of i{ana , candi } , where ana is the possible anaphor and candi is its antecedent candidate ' . \n\t"", '\n\t\t An instance is associated with a feature vector to describe their relationships . \n\t', '\n\t\t As listed in Table 2 , we only consider those knowledge-poor and domain-independent features which , although superficial , have been proved efficient for pronoun resolution in many previous systems . \n\t', '\n\t\t During training , for each anaphor in a given text , a positive instance is created by paring the anaphor and its closest antecedent . \n\t', '\n\t\t Also a set of negative instances is formed by paring the anaphor and each of the intervening candidates . \n\t', '\n\t\t Based on the training instances , a binary classifier is generated using C5.0 learning algorithm \n\t\t']",Positive
"['\n\t\t During resolution , each possible anaphor ana , is paired in turn with each preceding antecedent candidate , candi , from right to left to form a testing instance . \n\t', '\n\t\t This instance is presented to the classifier , which will then return a positive or negative result indicating whether or not they are co-referent . \n\t', '\n\t\t The process terminates once an instance i{ana , candi } is labelled as positive , and ana will be resolved to candi in that case . \n\t', '\n\t\t 4 The Learning Model Incorporating Corefere nt ial Information The learning procedure in our model is similar to the above baseline method , except that for each candidate , we take into consideration its closest antecedent , if possible . \n\t', '\n\t\t 4.1 Instance Structure During both training and testing , we adopt the same instance selection strategy as in the baseline model . \n\t', '\n\t\t The only difference , however , is the structure of the training or testing instances . \n\t', ""\n\t\t Specifically , each instance in our model is composed of three elements like below : ' In our study candidates are filtered by checking the gender , number and animacy agreements in advance . \n\t"", '\n\t\t Features describing the candidate ( candi ) 1. candi DefNp 1 if candi is a definite NP ; else 0 2 . \n\t', '\n\t\t candi DemoNP 1 if candi is an indefinite NP ; else 0 3 . \n\t', '\n\t\t candi Pron 1 if candi is a pronoun ; else 0 4 . \n\t', '\n\t\t candi ProperNP 1 if candi is a proper name ; else 0 5 . \n\t', '\n\t\t candi NE Type 1 if candi is an \x93organization\x94 named-entity ; 2 if \x93person\x94 , 3 if other types , 0 if not a NE 6. candi Human the likelihood ( 0-100 ) that candi is a human entity ( obtained from WordNet ) 7. candi FirstNPInSent 1 if candi is the first NP in the sentence where it occurs 8. candi Nearest 1 if candi is the candidate nearest to the anaphor ; else 0 9 . \n\t', '\n\t\t candi SubjNP 1 if candi is the subject of the sentence it occurs ; else 0 Features describing the anaphor ( ana ) : 10. ana Reflexive 1 if ana is a reflexive pronoun ; else 0 11 . \n\t', '\n\t\t ana Type 1 if ana is a third-person pronoun ( he , she , ... ) ; 2 if a single neuter pronoun ( it , ... ) ; 3 if a plural neuter pronoun ( they , ... ) ; 4 if other types Features describing the relationships between candi and ana : 12 . \n\t', '\n\t\t SentDist Distance between candi and ana in sentences 13 . \n\t', '\n\t\t ParaDist Distance between candi and ana in paragraphs 14 . \n\t', '\n\t\t CollPattern 1 if candi has an identical collocation pattern with ana ; else 0 Table 2 : Feature set for the baseline pronoun resolution system i{ana , candi , ante-of-candi } where ana and candi , similar to the definition in the baseline model , are the anaphor and one of its candidates , respectively . \n\t', '\n\t\t The new added element in the instance definition , anteof-candi , is the possible closest antecedent of candi in its coreferential chain . \n\t', '\n\t\t The ante-ofcandi is set to NIL in the case when candi has no antecedent . \n\t', '\n\t\t Consider the example in Table 1 again . \n\t', '\n\t\t For the pronoun \x93it6\x94 , three training instances will be generated , namely , i{its6 , The compaign5 , NIL } , i{its6 , its advertising4 , NIL } , and i{its6 , its3 , Gitano1 } . \n\t', '\n\t\t 4.2 Backward Features In addition to the features adopted in the baseline system , we introduce a set of backward features to describe the element ante-of-candi . \n\t', '\n\t\t The ten features ( 15-24 ) are listed in Table 3 with their respective possible values . \n\t', '\n\t\t Like feature 1-9 , features 15-22 describe the lexical , grammatical and semantic properties of ante-of-candi . \n\t', '\n\t\t The inclusion of the two features Apposition ( 23 ) and candi NoAntecedent ( 24 ) is inspired by the work of \n\t\t']",Positive
"['\n\t\t The feature Apposition marks whether or not candi and ante-of-candi occur in the same appositive structure . \n\t', '\n\t\t The underlying purpose of this feature is to capture the pattern that proper names are accompanied by an appositive . \n\t', '\n\t\t The entity with such a pattern may often be related to the hearers\x92 knowledge and has low preference . \n\t', '\n\t\t The feature candi NoAntecedent marks whether or not a candidate has a valid antecedent in the preceding text . \n\t', '\n\t\t As stipulated in Strube\x92s work , co-referring expressions belong to hearer-old entities and therefore have higher preference than other candidates . \n\t', '\n\t\t When the feature is assigned value 1 , all the other backward features ( 15-23 ) are set to 0 . \n\t', '\n\t\t 4.3 Results and Discussions In our study we used the standard MUC6 and MUC-7 coreference corpora . \n\t', '\n\t\t In each data set , 30 \x93dry-run\x94 documents were annotated for training as well as 20-30 documents for testing . \n\t', '\n\t\t The raw documents were preprocessed by a pipeline of automatic NLP components ( e.g. NP chunker , part-of-speech tagger , named-entity recognizer ) to determine the boundary of the NPs , and to provide necessary information for feature calculation . \n\t', '\n\t\t In an attempt to investigate the capability of our model , we evaluated the model in an optimal environment where the closest antecedent of each candidate is correctly identified . \n\t', '\n\t\t MUC6 and MUC-7 can serve this purpose quite well ; the annotated coreference information in the data sets enables us to obtain the correct closest Features describing the antecedent of the candidate ( ante-of-candi ) : 15. ante-candi DefNp 1 if ante-of-candi is a definite NP ; else 0 16 . \n\t', '\n\t\t ante-candi IndefNp 1 if ante-of-candi is an indefinite NP ; else 0 17 . \n\t', '\n\t\t ante-candi Pron 1 if ante-of-candi is a pronoun ; else 0 18 . \n\t', '\n\t\t ante-candi Proper 1 if ante-of-candi is a proper name ; else 0 19 . \n\t', '\n\t\t ante-candi NE Type 1 if ante-of-candi is an \x93organization\x94 named-entity ; 2 if \x93per- son\x94 , 3 if other types , 0 if not a NE 20. ante-candi Human the likelihood ( 0-100 ) that ante-of-candi is a human entity 21. ante-candi FirstNPInSent 1 if ante-of-candi is the first NP in the sentence where it occurs 22. ante-candi SubjNP 1 if ante-of-candi is the subject of the sentence where it occurs Features describing the relationships between the candidate ( candi ) and ante-of-candi : 23 . \n\t', '\n\t\t Apposition 1 if ante-of-candi and candi are in an appositive structure Features describing the candidate ( candi ) : 24. candi NoAntecedent 1 if candi has no antecedent available ; else 0 Table 3 : Backward features used to capture the coreferential information of a candidate antecedent for each candidate and accordingly generate the training and testing instances . \n\t', '\n\t\t In the next section we will further discuss how to apply our model into the real resolution . \n\t', '\n\t\t Table 4 shows the performance of different systems for resolving the pronominal anaphors 2 in MUC-6 and MUC-7 . \n\t', '\n\t\t Default learning parameters for C5.0 were used throughout the experiments . \n\t', '\n\t\t In this table we evaluated the performance based on two kinds of measurements : \x95 \x93Recall-and-Precision\x94 : Recall = #positive instances classified correctly #positive instances Precision= #positive instances classified correctly #instances classified as positive The above metrics evaluate the capability of the learned classifier in identifying positive instances3 . \n\t', '\n\t\t F-measure is the harmonic mean of the two measurements . \n\t', '\n\t\t \x95 \x93Success\x94 : #anaphors resolved correctly Success = #total anaphors The metric4 directly reflects the pronoun resolution capability . \n\t', '\n\t\t The first and second lines of Table 4 compare the performance of the baseline system ( Base- 2 Thefirst and second person pronouns are discarded in our study . \n\t', '\n\t\t 3The testing instances are collected in the same ways as the training instances . \n\t', '\n\t\t 4In the experiments , an anaphor is considered correctly resolved only if the found antecedent is in the same coreferential chain of the anaphor . \n\t', '\n\t\t ante-candi_SubjNP = 1 : 1 ( 49/5 ) ante-candi_SubjNP = 0 : :..candi_SubjNP = 1 : :..SentDist = 2 : 0 ( 3 ) :SentDist = 0 : ::..candi_Human > 0 : 1 ( 39/2 ) : :candi_Human <= 0 : : : :..candi_NoAntecedent = 0 : 1 ( 8/3 ) : : candi_NoAntecedent = 1 : 0 ( 3 ) :SentDist = 1 : ::..ante-candi_Human <= 50 : 0 ( 4 ) :ante-candi_Human > 50 : 1 ( 10/2 ) : candi_SubjNP = 0 : :..candi_Pron = 1 : 1 ( 32/7 ) candi_Pron = 0 : :..candi_NoAntecedent = 1 : :..candi_FirstNPInSent = 1 : 1 ( 6/2 ) : candi_FirstNPInSent = 0 : ... candi_NoAntecedent = 0 : ... \n\t', '\n\t\t Figure 1 : Top portion of the decision tree learned on MUC-6 with the backward features line ) and our system ( Optimal ) , where DTpron and DT pron\x97opt are the classifiers learned in the two systems , respectively . \n\t', '\n\t\t The results indicate that our system outperforms the baseline system significantly . \n\t', '\n\t\t Compared with Baseline , Optimal achieves gains in both recall ( 6.4 % for MUC-6 and 4.1 % for MUC-7 ) and precision ( 1.3 % for MUC-6 and 9.0 % for MUC-7 ) . \n\t', '\n\t\t For Success , we also observe an apparent improvement by 4.7 % ( MUC-6 ) and 3.5 % ( MUC-7 ) . \n\t', '\n\t\t Figure 1 shows the portion of the pruned decision tree learned for MUC-6 data set . \n\t', '\n\t\t It visualizes the importance of the backward features for the pronoun resolution on the data set . \n\t', '\n\t\t From Experiments Testing classifier Backward feature MUC-6 MUC-7 assigner* R P F S R P F S Baseline DTpron NIL 77.2 83.4 80.2 70.0 71.9 68.6 70.2 59.0 Optimal DTpronopt ( Annotated ) 83.6 84.7 84.1 74.7 76.0 77.6 76.8 62.5 RealResolve-1 DTpron-opt DTpron-opt 75.8 83.8 79.5 73.1 62.3 77.7 69.1 53.8 RealResolve-2 DTpron-opt DTpron 75.8 83.8 79.5 73.1 63.0 77.9 69.7 54.9 RealResolve-3 DTpron DTpron 79.3 86.3 82.7 74.7 74.7 67.3 70.8 60.8 RealResolve-4 DTpron DT~pron 79.3 86.3 82.7 74.7 74.7 67.3 70.8 60.8 Table 4 : Results of different systems for pronoun resolution on MUC-6 and MUC-7 ( *Here we only list backward feature assigner for pronominal candidates . \n\t', '\n\t\t In RealResolve-1 to RealResolve-4 , the backward features for non-pronominal candidates are all found by DTnon-pron . \n\t', '\n\t\t ) the tree we could find that : 1 . \n\t', '\n\t\t ) Feature ante-candi SubjNP is of the most importance as the root feature of the tree . \n\t', '\n\t\t The decision tree would first examine the syntactic role of a candidate\x92s antecedent , followed by that of the candidate . \n\t', '\n\t\t This nicely proves our assumption that the properties of the antecedents of the candidates provide very important information for the candidate evaluation . \n\t', '\n\t\t 2. ) Both features ante-candi SubjNP and candi SubjNP rank top in the decision tree . \n\t', '\n\t\t That is , for the reference determination , the subject roles of the candidate\x92s referent within a discourse segment will be checked in the first place . \n\t', '\n\t\t This finding supports well the suggestion in centering theory that the grammatical relations should be used as the key criteria to rank forward-looking centers in the process of focus tracking \n\t\t']",Positive
"['\n\t\t 3. ) candi Pron and candi NoAntecedent are to be examined in the cases when the subject-role checking fails , which confirms the hypothesis in the S-List model by \n\t\t']",Positive
"['\n\t\t 5 Applying the Model in Real Resolution In Section 4 we explored the effectiveness of the backward feature for pronoun resolution . \n\t', '\n\t\t In those experiments our model was tested in an ideal environment where the closest antecedent of a candidate can be identified correctly when generating the feature vector . \n\t', '\n\t\t However , during real resolution such coreferential information is not available , and thus a separate module has algorithm PRON-RESOLVE input : DTnonpron : classifier for resolving non-pronouns DTpron : classifier for resolving pronouns begin : M1..n:= the valid markables in the given document Ante[1..n] := 0 for i=1toN for j = i - 1 downto 0 if ( Mi is a non-pron and DTnon-pron(i{Mi , Mj } ) == + ) or ( Mi is a pron and DTpron ( i{Mi,Mj,Ante[j]} ) ==+ ) then Ante[i] := Mj break return Ante Figure 2 : The pronoun resolution algorithm by incorporating coreferential information of candidates to be employed to obtain the closest antecedent for a candidate . \n\t', '\n\t\t We describe the algorithm in Figure 2 . \n\t', '\n\t\t The algorithm takes as input two classifiers , one for the non-pronoun resolution and the other for pronoun resolution . \n\t', '\n\t\t Given a testing document , the antecedent of each NP is identified using one of these two classifiers , depending on the type of NP . \n\t', '\n\t\t Although a separate non- pronoun resolution module is required for the pronoun resolution task , this is usually not a big problem as these two modules are often integrated in coreference resolution systems . \n\t', '\n\t\t We just use the results of the one module to improve the performance of the other . \n\t', '\n\t\t 5.1 New Training and Testing Procedures For a pronominal candidate , its antecedent can be obtained by simply using DTpron-opt . \n\t', '\n\t\t For Training Procedure : T1 . \n\t', '\n\t\t Train a non-pronoun resolution classifier DTnon\x97pron and a pronoun resolution classifier DTpron , using the baseline learning framework ( without backward features ) . \n\t', '\n\t\t T2 . \n\t', '\n\t\t Apply DTnon\x97pron and DTpron to identify the antecedent of each non-pronominal and pronominal markable , respectively , in a given document . \n\t', '\n\t\t T3 . \n\t', '\n\t\t Go through the document again . \n\t', '\n\t\t Generate instances with backward features assigned using the antecedent information obtained in T2 . \n\t', '\n\t\t T4 . \n\t', ""\n\t\t Train a new pronoun resolution classifier DT ' on the instances generated in T3 . \n\t"", '\n\t\t pron Testing Procedure : R1 . \n\t', '\n\t\t For each given document , do T2\x97T3 . \n\t', '\n\t\t R2 . \n\t', ""\n\t\t Resolve pronouns by applying DT ' pron . \n\t"", '\n\t\t Table 5 : New training and testing procedures a non-pronominal candidate , we built a non- pronoun resolution module to identify its antecedent . \n\t', '\n\t\t The module is a duplicate of the NP coreference resolution system by Soon et al . \n\t', '\n\t\t (2001)5 , which uses the similar learning framework as described in Section 3 . \n\t', '\n\t\t In this way , we could do pronoun resolution just by running PRON-RESOLVE(DTnon\x97pron , DTpron\x97opt ) , where DTnon\x97pron is the classifier of the non-pronoun resolution module . \n\t', '\n\t\t One problem , however , is that DTpron\x97opt is trained on the instances whose backward features are correctly assigned . \n\t', '\n\t\t During real resolution , the antecedent of a candidate is found by DTnon\x97pron or DTpron\x97opt , and the backward feature values are not always correct . \n\t', '\n\t\t Indeed , for most noun phrase resolution systems , the recall is not very high . \n\t', '\n\t\t The antecedent sometimes can not be found , or is not the closest one in the preceding coreferential chain . \n\t', '\n\t\t Consequently , the classifier trained on the \x93perfect\x94 feature vectors would probably fail to output anticipated results on the noisy data during real resolution . \n\t', '\n\t\t Thus we modify the training and testing procedures of the system . \n\t', '\n\t\t For both training and testing instances , we assign the backward feature values based on the results from separate NP resolution modules . \n\t', '\n\t\t The detailed procedures are described in Table 5 . \n\t', '\n\t\t SDetails of the features can be found in \n\t\t']",Positive
"['\n\t\t Here the purpose of DTpron and DTnon\x97pron is to provide backward feature values for training and testing instances . \n\t', '\n\t\t From this point of view , the two modules could be thought of as a preprocessing component of our pronoun resolution system . \n\t', ""\n\t\t 5.2 Classifier Refining If the classifier DT ' pron outperforms DTpron as expected , we can employ DT'pron in place of DTpron to generate backward features for pronominal candidates , and then train a classifier DT''pron based on the updated training in- stances . \n\t"", ""\n\t\t Since DT'pron produces more correct feature values than DTpron , we could expect that DT''pron will not be worse , if not better , than DT'pron . \n\t"", '\n\t\t Such a process could be repeated to refine the pronoun resolution classifier . \n\t', '\n\t\t The algorithm is described in Figure 3 . \n\t', '\n\t\t In algorithm REFINE-CLASSIFIER , the iteration terminates when the new trained classifier DTP n provides no further improvement than DTipron . \n\t', '\n\t\t In this case , we can replace DTPon by DTipron during the i+1(th) testing procedure . \n\t', ""\n\t\t That means , by simply running PRON-RESOLVE(DTnon\x97pron,DTipron) , we can use for both backward feature computation and instance classification tasks , rather than apply- ing DTpron and DT ' pron subsequently . \n\t"", '\n\t\t 5.3 Results and Discussions In the experiments we evaluated the perfor- mance of our model in real pronoun resolution . \n\t', '\n\t\t The performance of our model depends on the performance of the non-pronoun resolution clas- sifier , DTnon\x97 pron . \n\t', '\n\t\t Hence we first examined the coreference resolution capability of DTnon\x97pron based on the standard scoring scheme by Vi- lain et al . ( 1995 ) . \n\t', '\n\t\t For MUC-6 , the module obtains 62.2 % recall and 78.8 % precision , while for MUC-7 , it obtains 50.1 % recall and 75.4 % precision . \n\t', '\n\t\t The poor recall and comparatively high precision reflect the capability of the state-of- the-art learning-based NP resolution systems . \n\t', '\n\t\t The third block of Table 4 summarizes the performance of the classifier DTpron\x97opt in real resolution . \n\t', '\n\t\t In the systems RealResolve-1 and RealResolve-2 , the antecedents of pronominal candidates are found by DTpron\x97opt and DTpron respectively , while in both systems the antecedents of non-pronominal candidates are by DTnon\x97pron . \n\t', '\n\t\t As shown in the table , compared with the Optimal where the backward features of testing instances are optimally assigned , the recall rates of two systems drop largely by 7.8 % for MUC-6 and by about 14 % for MUC-7 . \n\t', '\n\t\t The scores of recall are even lower than those of Baseline . \n\t', '\n\t\t As a result , in comparison with Optimal , we see the degrade of the F-measure and the success rate , which confirms our hypothesis that the classifier learned on perfect training instances would probably not perform well on the noisy testing instances . \n\t', '\n\t\t The system RealResolve-3 listed in the fifth line of the table uses the classifier trained and tested on instances whose backward features are assigned according to the results from DTnon\x97pron and DTpron . \n\t', '\n\t\t From the table we can find that : ( 1 ) Compared with Baseline , the system produces gains in recall ( 2.1 % for MUC-6 and 2.8 % for MUC-7 ) with no significant loss in precision . \n\t', '\n\t\t Overall , we observe the increase in F-measure for both data sets . \n\t', '\n\t\t If measured by Success , the improvement is more apparent by 4.7 % ( MUC-6 ) and 1.8 % ( MUC-7 ) . \n\t', '\n\t\t ( 2 ) Compared with RealResolve-1(2) , the performance decrease of RealResolve-3 against Optimal is not so large . \n\t', '\n\t\t Especially for MUC-6 , the system obtains a success rate as high as Optimal . \n\t', '\n\t\t The above results show that our model can be successfully applied in the real pronoun resolution task , even given the low recall of the current non-pronoun resolution module . \n\t', '\n\t\t This should be owed to the fact that for a candidate , its adjacent antecedents , even not the closest one , could give clues to reflect its salience in the local discourse . \n\t', '\n\t\t That is , the model prefers a high precision to a high recall , which copes well with the capability of the existing non-pronoun resolution module . \n\t', '\n\t\t In our experiments we also tested the classifier refining algorithm described in Figure 3 . \n\t', '\n\t\t We found that for both MUC-6 and MUC-7 data set , the algorithm terminated in the second round . \n\t', ""\n\t\t The comparison of DT2pron and DT'pron ( i.e. DT~pron ) showed that these two trees were exactly the same . \n\t"", '\n\t\t The algorithm converges fast probably because in the data set , most of the antecedent candidates are non-pronouns ( 89.1 % for MUC-6 and 83.7 % for MUC-7 ) . \n\t', '\n\t\t Consequently , the ratio of the training instances with backward features changed may be not substantial enough to affect the classifier generation . \n\t', '\n\t\t Although the alprithm provided no further refinement for DTpron , we can use DT~pron , as suggested in Section 5.2 , to calculate backward features and classify instances by running PRON-RESOLVE(DTnon\x97pron , DT~pron ) . \n\t', '\n\t\t The results of such a system , RealResolve-4 , are listed in the last line of Table 4 . \n\t', '\n\t\t For both MUC6 and MUC-7 , RealResolve-4 obtains exactly the same performance as RealResolve-3 . \n\t', '\n\t\t 6 Related Work To our knowledge , our work is the first effort that systematically explores the influence of coreferential information of candidates on pronoun resolution in learning-based ways . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t However , similar to common centering models , in their system the ranking of entities in SRL is also heuristic-based . \n\t', '\n\t\t The coreferential chain length of a candidate , or its variants such as occurrence frequency and TFIDF , has been used as a salience factor in some learning-based reference resolution systems \n\t\t']",Positive
"['\n\t\t However , for an entity , the coreferential length only reflects its global salience in the whole text(s) , instead of the local salience in a discourse segment which is nevertheless more informative for pronoun resolution . \n\t', '\n\t\t Moreover , during resolution , the found coreferential length of an entity is often incomplete , and thus the obtained length value is usually inaccurate for the salience evaluation . \n\t', '\n\t\t 7 Conclusion and Future Work In this paper we have proposed a model which incorporates coreferential information of candi- dates to improve pronoun resolution . \n\t', '\n\t\t When evaluating a candidate , the model considers its adjacent antecedent by describing its properties in terms of backward features . \n\t', '\n\t\t We first examined the effectiveness of the model by applying it in an optimal environment where the closest antecedent of a candidate is obtained correctly . \n\t', '\n\t\t The experiments show that it boosts the success rate of the baseline system for both MUC-6 ( 4.7 % ) and MUC-7 ( 3.5 % ) . \n\t', '\n\t\t Then we proposed how to apply our model in the real resolution where the antecedent of a non-pronoun is found by an additional non-pronoun resolution module . \n\t', '\n\t\t Our model can still produce Success improvement ( 4.7 % for MUC-6 and 1.8 % for MUC-7 ) against the baseline system , despite the low recall of the non-pronoun resolution module . \n\t', '\n\t\t In the current work we restrict our study only to pronoun resolution . \n\t', '\n\t\t In fact , the coreferential information of candidates is expected to be also helpful for non-pronoun resolution . \n\t', '\n\t\t We would like to investigate the influence of the coreferential factors on general NP reference resolution in our future work . \n\t', '\n\t\t References S. Brennan , M. Friedman , and C. Pollard . \n\t', '\n\t\t 1987. A centering approach to pronouns . \n\t', '\n\t\t In Proceedings of the 25th Annual Meeting of the Association for Compuational Linguistics , pages 155\x96162 . \n\t', '\n\t\t N. Ge , J. Hale , and E. Charniak . \n\t', '\n\t\t 1998. A statistical approach to anaphora resolution . \n\t', '\n\t\t In Proceedings of the 6th Workshop on Very Large Corpora . \n\t', '\n\t\t B. Grosz , A. Joshi , and S. Weinstein . \n\t', '\n\t\t 1983. Providing a unified account of definite noun phrases in discourse . \n\t', '\n\t\t In Proceedings of the 21st Annual meeting of the Association for Computational Linguistics , pages 44\x9650 . \n\t', '\n\t\t B. Grosz , A. Joshi , and S. Weinstein . \n\t', '\n\t\t 1995 . \n\t', '\n\t\t Centering : a framework for modeling the local coherence of discourse . \n\t', '\n\t\t Computational Linguistics , 21(2):203\x96225 . \n\t', '\n\t\t R. Iida , K. Inui , H. Takamura , and Y. Matsumoto . \n\t', '\n\t\t 2003 . \n\t', '\n\t\t Incorporating contextual cues in trainable models for coreference resolution . \n\t', '\n\t\t In Proceedings of the 10th Conference of EACL , Workshop \x94The Computational Treatment of Anaphora\x94 . \n\t', '\n\t\t R. Mitkov . \n\t', '\n\t\t 1998 . \n\t', '\n\t\t Robust pronoun resolution with limited knowledge . \n\t', '\n\t\t In Proceedings of the 17th Int. Conference on Computational Linguistics , pages 869\x96875 . \n\t', '\n\t\t R. Mitkov . \n\t', '\n\t\t 1999. Anaphora resolution : The state of the art . \n\t', '\n\t\t Technical report , University of Wolverhampton . \n\t', '\n\t\t MUC-6 . \n\t', '\n\t\t 1995. Proceedings of the Sixth Message Understanding Conference . \n\t', '\n\t\t Morgan Kaufmann Publishers , San Francisco , CA . \n\t', '\n\t\t MUC-7 . \n\t', '\n\t\t 1998. Proceedings of the Seventh Message Understanding Conference . \n\t', '\n\t\t Morgan Kaufmann Publishers , San Francisco , CA . \n\t', '\n\t\t V. Ng and C. Cardie . \n\t', '\n\t\t 2002. Improving machine learning approaches to coreference resolution . \n\t', '\n\t\t In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , pages 104\x96111 , Philadelphia . \n\t', '\n\t\t M. Paul , K. Yamamoto , and E. Sumita . \n\t', '\n\t\t 1999. Corpus-based anaphora resolution towards antecedent preference . \n\t', '\n\t\t In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics , Workshop \x94Coreference and It\x92s Applications\x94 , pages 47\x9652 . \n\t', '\n\t\t J. R. Quinlan . \n\t', '\n\t\t 1993. C4.5 : Programs for machine learning . \n\t', '\n\t\t Morgan Kaufmann Publishers , San Francisco , CA . \n\t', '\n\t\t C. Sidner . \n\t', '\n\t\t 1981. Focusing for interpretation of pronouns . \n\t', '\n\t\t American Journal of Computational Linguistics , 7(4):217\x96231 . \n\t', '\n\t\t W. . \n\t', '\n\t\t Soon , H. Ng , and D. Lim . \n\t', '\n\t\t 2001. A machine learning approach to coreference resolution of noun phrases . \n\t', '\n\t\t Computational Linguistics , 27(4):521\x96544 . \n\t', '\n\t\t M. Strube and C. Muller . \n\t', '\n\t\t 2003. A machine learning approach to pronoun resolution in spoken dialogue . \n\t', '\n\t\t In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics , pages 168\x96175 , Japan . \n\t', '\n\t\t M. Strube . \n\t', '\n\t\t 1998. Never look back : An alternative to centering . \n\t', '\n\t\t In Proceedings of the 17th Int. Conference on Computational Linguistics and 36th Annual Meeting of ACL , pages 1251\x961257 . \n\t', '\n\t\t J. R. Tetreault . \n\t', '\n\t\t 2001. A corpus-based evaluation of centering and pronoun resolution . \n\t', '\n\t\t Computational Linguistics , 27(4):507\x96520 . \n\t', '\n\t\t M. Vilain , J. Burger , J. Aberdeen , D. Connolly , and L. Hirschman . \n\t', '\n\t\t 1995. A model-theoretic coreference scoring scheme . \n\t', '\n\t\t In Proceedings of the Sixth Message understanding Conference ( MUC-6 ) , pages 45\x9652 , San Francisco , CA . \n\t', '\n\t\t Morgan Kaufmann Publishers . \n\t', '\n\t\t X. Yang , G. Zhou , J. Su , and C. Tan . \n\t', '\n\t\t 2003. Coreference resolution using competition learning approach . \n\t', '\n\t\t In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics , Japan . \n\t', '\n\t\t A Mention-Synchronous Coreference Resolution Algorithm Based on the Bell Tree Xiaoqiang Luo and Abe Ittycheriah Hongyan Jing and Nanda Kambhatla and Salim Roukos 1101 Kitchawan Road Yorktown Heights , NY 10598 , U.S.A. {xiaoluo,abei,hjing,nanda,roukos}@us.ibm.com Abstract This paper proposes a new approach for coreference resolution which uses the Bell tree to represent the search space and casts the coreference resolution problem as finding the best path from the root of the Bell tree to the leaf nodes . \n\t', '\n\t\t A Maximum Entropy model is used to rank these paths . \n\t', '\n\t\t The coreference performance on the 2002 and 2003 Automatic Content Extraction ( ACE ) data will be reported . \n\t', '\n\t\t We also train a coreference system using the MUC6 data and competitive results are obtained . \n\t', '\n\t\t 1 Introduction In this paper , we will adopt the terminologies used in the Automatic Content Extraction ( ACE ) task ( NIST , 2003 ) . \n\t', '\n\t\t Coreference resolution in this context is defined as partitioning mentions into entities . \n\t', '\n\t\t A mention is an instance of reference to an object , and the collection of mentions referring to the same object in a document form an entity . \n\t', '\n\t\t For example , in the following sentence , mentions are underlined : \x93The American Medical Association voted yesterday to install the heir apparent as its president-elect , rejecting a strong , upstart challenge by a District doctor who argued that the nation\x92s largest physicians\x92 group needs stronger ethics and new leadership.\x94 \x93American Medical Association\x94 , \x93its\x94 and \x93group\x94 belong to the same entity as they refer to the same object . \n\t', '\n\t\t Early work of anaphora resolution focuses on finding antecedents of pronouns \n\t\t']",Positive
['\n\t\t One common strategy shared by \n\t\t'],Positive
"['\n\t\t While this approach has yielded encouraging results , the way mentions are linked is arguably suboptimal in that an instant decision is made when considering whether two mentions are linked or not . \n\t', '\n\t\t In this paper , we propose to use the Bell tree to represent the process of forming entities from mentions . \n\t', '\n\t\t The Bell tree represents the search space of the coreference resolution problem\x96 each leaf node corresponds to a possible coreference outcome . \n\t', '\n\t\t We choose to model the process from mentions to entities represented in the Bell tree , and the problem of coreference resolution is cast as finding the \x93best\x94 path from the root node to leaves . \n\t', '\n\t\t A binary maximum entropy model is trained to compute the linking probability between a partial entity and a mention . \n\t', '\n\t\t The rest of the paper is organized as follows . \n\t', '\n\t\t In Section 2 , we present how the Bell tree can be used to represent the process of creating entities from mentions and the search space . \n\t', '\n\t\t We use a maximum entropy model to rank paths in the Bell tree , which is discussed in Section 3 . \n\t', '\n\t\t After presenting the search strategy in Section 4 , we show the experimental results on the ACE 2002 and 2003 data , and the Message Understanding Conference ( MUC ) ( MUC , 1995 ) data in Section 5 . \n\t', '\n\t\t We compare our approach with some recent work in Section 6 . \n\t', '\n\t\t 2 Bell Tree : From Mention to Entity Let us consider traversing mentions in a document from beginning ( left ) to end ( right ) . \n\t', '\n\t\t The process of forming entities from mentions can be represented by a tree structure . \n\t', '\n\t\t The root node is the initial state of the process , which consists of a partial entity containing the first mention of a document . \n\t', '\n\t\t The second mention is Figure 1 : Bell tree representation for three mentions : numbers in [ ] denote a partial entity . \n\t', '\n\t\t In-focus entities are marked on the solid arrows , and active mentions are marked by * . \n\t', '\n\t\t Solid arrows signify that a mention is linked with an in-focus partial entity while dashed arrows indicate starting of a new entity . \n\t', '\n\t\t added in the next step by either linking to the existing entity , or starting a new entity . \n\t', '\n\t\t A second layer of nodes are created to represent the two possible outcomes . \n\t', '\n\t\t Subsequent mentions are added to the tree in the same manner . \n\t', '\n\t\t The process is mention-synchronous in that each layer of tree nodes are created by adding one mention at a time . \n\t', '\n\t\t Since the number of tree leaves is the number of possible coreference outcomes and it equals the Bell Number \n\t\t']",Positive
"['\n\t\t The Bell Number is the number of ways of partitioning distinguishable objects ( i.e. , mentions ) into non-empty disjoint subsets ( i.e. , entities ) . \n\t', '\n\t\t The Bell Number has a \x93closed\x94 formula and it increases rapidly as increases : ! \n\t', '\n\t\t Clearly , an efficient search strategy is necessary , and it will be addressed in Section 4 . \n\t', '\n\t\t Figure 1 illustrates how the Bell tree is created for a document with three mentions . \n\t', '\n\t\t The initial node consists of the first partial entity [ 1 ] ( i.e. , node ( a ) in Figure 1 ) . \n\t', '\n\t\t Next , mention 2 becomes active ( marked by \x93*\x94 in node ( a ) ) and can either link with the partial entity [ 1 ] and result in a new node ( b 1 ) , or start a new entity and create another node ( b2 ) . \n\t', '\n\t\t The partial entity which the active mention considers linking with is said to be in -focus . \n\t', '\n\t\t In-focus entities are highlighted on the solid arrows in Figure 1 . \n\t', '\n\t\t Similarly , mention 3 will be active in the next stage and can take five possible actions , which create five possible coreference results shown in node ( c1 ) through ( c5 ) . \n\t', '\n\t\t Under the derivation illustrated in Figure 1 , each leaf node in the Bell tree corresponds to a possible coreference outcome , and there is no other way to form entities . \n\t', '\n\t\t The Bell tree clearly represents the search space of the coreference resolution problem . \n\t', '\n\t\t The coreference resolution can therefore be cast equivalently as finding the \x93best\x94 leaf node . \n\t', '\n\t\t Since the search space is large ( even for a document with a moderate number of mentions ) , it is difficult to estimate a distribution over leaves directly . \n\t', '\n\t\t Instead , we choose to model the process from mentions to entities , or in other words , score paths from the root to leaves in the Bell tree . \n\t', '\n\t\t A nice property of the Bell tree representation is that the number of linking or starting steps is the same for all the hypotheses . \n\t', '\n\t\t This makes it easy to rank them using the \x93local\x94 linking and starting probabilities as the number of factors is the same . \n\t', '\n\t\t The Bell tree representation is also incremental in that mentions are added sequentially . \n\t', '\n\t\t This makes it easy to design a decoder and search algorithm . \n\t', '\n\t\t 3 Coreference Model 3.1 Linking and Starting Model We use a binary conditional model to compute the probability that an active mention links with an in- focus partial entity . \n\t', '\n\t\t The conditions include all the partially-formed entities before , the focus entity index , and the active mention . \n\t', '\n\t\t Formally , let be mentions in a document . \n\t', '\n\t\t Mention index represents the order it appears in the document . \n\t', '\n\t\t Let be an entity , and be the ( many-to-one ) map from mention index to entity index . \n\t', '\n\t\t For an active mention index , define for some the set of indices of the partially-established entities to the left of ( note that ) , and the set of the partially-established entities . \n\t', '\n\t\t The link model is then ( 1 ) the probability linking the active mention with the in-focus entity . \n\t', '\n\t\t The random variable takes value from the set and signifies which entity is in focus ; takes binary value and is if links with . \n\t', '\n\t\t As an example , for the branch from ( b2 ) to ( c4 ) in Figure 1 , the active mention is \x933\x94 , the set of partial entities to the left of \x933\x94 is , the active entity is the second partial entity \x93[2]\x94 . \n\t', '\n\t\t Probability measures how likely mention \x933\x94 links with the entity \x93[2].\x94 The model only computes how likely links with ; It does not say anything about the possibility that starts a new entity . \n\t', '\n\t\t Fortu- nately , the starting probability can be computed using link probabilities ( 1 ) , as shown now . \n\t', '\n\t\t Since starting a new entity means that does not link with any entities in , the probability of starting ( b1 ) [ 12 ] 3* ( b2 ) [1][2] 3* [ 123 ] [ 1 ] 2* 3 ( a ) [ 1 ] [ 1 ] [ 2 ] [ 1 ] [ 23 ] [ 12 ] ( c1 ) ( c2 ) ( c3 ) ( c4 ) ( c5 ) [12][3] [13][2] [1][2][3] a new entity , , can be computed as ( 2 ) ( 3 ) ( 3 ) indicates that the probability of starting an entity can be computed using the linking probabilities , provided that the marginal is known . \n\t', '\n\t\t In this paper , is approximated as : ( 4 ) With the approximation ( 4 ) , the starting probability ( 3 ) is ( 5 ) The linking model ( 1 ) and approximated starting model ( 5 ) can be used to score paths in the Bell tree . \n\t', '\n\t\t For example , the score for the path (a)-(b2)-(c4) in Figure 1 is the product of the start probability from ( a ) to ( b2 ) and the linking probability from ( b2 ) to ( c4 ) . \n\t', '\n\t\t Since ( 5 ) is an approximation , not true probability , a constant is introduced to balance the linking probability and starting probability and the starting probability becomes : ( 6 ) If , it penalizes creating new entities ; Therefore , is called start penalty . \n\t', '\n\t\t The start penalty can be used to balance entity miss and false alarm . \n\t', '\n\t\t 3.2 Model Training and Features The model depends on all par- tial entities , which can be very expensive . \n\t', '\n\t\t After making some modeling assumptions , we can approximate it as : ( 7 ) ( 8 ) ( 9 ) From ( 7 ) to ( 8 ) , entities other than the one in focus , , are assumed to have no influence on the decision of linking with . \n\t', '\n\t\t ( 9 ) further assumes that the entity-mention score can be obtained by the maximum mention pair score . \n\t', '\n\t\t The model ( 9 ) is very similar to the model in \n\t\t']",Positive
['\n\t\t We use maximum entropy model \n\t\t'],Positive
['\n\t\t Effective training algorithm exists \n\t\t'],Positive
"['\n\t\t The basic features used in the models are tabulated in Table 1 . \n\t', '\n\t\t Features in the lexical category are applicable to non-pronominal mentions only . \n\t', '\n\t\t Distance features characterize how far the two mentions are , either by the number of tokens , by the number of sentences , or by the number of mentions in-between . \n\t', '\n\t\t Syntactic features are derived from parse trees output from a maximum entropy parser ( Ratnaparkhi,1997 ) . \n\t', '\n\t\t The \x93Count\x94 feature calculates how many times a mention string is seen . \n\t', '\n\t\t For pronominal mentions , attributes such as gender , number , possessiveness and reflexiveness are also used . \n\t', '\n\t\t Apart from basic features in Table 1 , composite features can be generated by taking conjunction of basic features . \n\t', '\n\t\t For example , a distance feature together with reflexiveness of a pronoun mention can help to capture that the antecedent of a reflexive pronoun is often closer than that of a non-reflexive pronoun . \n\t', '\n\t\t The same set of basic features in Table 1 is used in the entity-mention model , but feature definitions are slightly different . \n\t', '\n\t\t Lexical features , including the acronym features , and the apposition feature are computed by testing any mention in the entity against the active mention . \n\t', '\n\t\t Editing distance for is defined as the minimum distance over any non-pronoun mentions and the active mention . \n\t', '\n\t\t Distance features are computed by taking minimum between mentions in the entity and the active mention . \n\t', '\n\t\t In the ACE data , mentions are annotated with three levels : NAME , NOMINAL and PRONOUN . \n\t', '\n\t\t For each ACE entity , a canonical mention is defined as the longest NAME mention if available ; or if the entity does not have a NAME mention , the most recent NOMINAL mention ; if there is no NAME and NOMINAL mention , the most recent pronoun mention . \n\t', '\n\t\t In the entity-mention model , \x93ncd\x94,\x93spell\x94 and \x93count\x94 features are computed over the canonical mention of the in-focus entity and the active mention . \n\t', '\n\t\t Conjunction features are used in the entity-mention model too . \n\t', '\n\t\t The mention-pair model is appealing for its simplicity : features are easy to compute over a pair of men- if otherwise Category Features Remark Lexical exact_strm left_subsm right_subsm acronym edit_dist 1 if two mentions have the same spelling ; 0 otherwise spell 1 if one mention is a left substring of the other ; 0 otherwise ncd 1 if one mention is a right substring of the other ; 0 otherwise 1 if one mention is an acronym of the other ; 0 otherwise quantized editing distance between two mention strings pair of actual mention strings number of different capitalized words in two mentions Distance token dist sent_1st gap_dist how many tokens two mentions are apart ( quantized ) how many sentences two mentions are apart ( quantized ) how many mentions in between the two mentions in question ( quantized ) Syntax POS_pair apposition POS-pair of two mention heads 1 if two mentions are appositive ; 0 otherwise Count count pair of ( quantized ) numbers , each counting how many times a mention string is seen Pronoun gender number possessive reflexive pair of attributes of { female , male , neutral , unknown } pair of attributes of { singular , plural , unknown } 1 if a pronoun is possessive ; 0 otherwise 1 if a pronoun is reflexive ; 0 otherwise Table 1 : Basic features used in the maximum entropy model . \n\t', '\n\t\t tions ; its drawback is that information outside the mention pair is ignored . \n\t', '\n\t\t Suppose a document has three mentions \x93Mr . \n\t', '\n\t\t Clinton\x94 , \x93Clinton\x94 and \x93she\x94 , appearing in that order . \n\t', '\n\t\t When considering the mention pair \x93Clinton\x94 and \x93she\x94 , the model may tend to link them because of their proximity ; But this mistake can be easily avoided if \x93Mr . \n\t', '\n\t\t Clinton\x94 and \x93Clinton\x94 have been put into the same entity and the model knows \x93Mr . \n\t', '\n\t\t Clinton\x94 referring to a male while \x93she\x94 is female . \n\t', '\n\t\t Since gender and number information is propagated at the entity level , the entity-mention model is able to check the gender consistency when considering the active mention \x93she\x94 . \n\t', '\n\t\t 3.3 Discussion There is an in-focus entity in the condition of the linking model ( 1 ) while the starting model ( 2 ) conditions on all left entities . \n\t', '\n\t\t The disparity is intentional as the starting action is influenced by all established entities on the left . \n\t', '\n\t\t ( 4 ) is not the only way can be approximated . \n\t', '\n\t\t For example , one could use a uniform distribution over . \n\t', '\n\t\t We experimented several schemes of approximation , including a uniform distribution , and ( 4 ) worked the best and is adopted here . \n\t', '\n\t\t One may consider training directly and use it to score paths in the Bell tree . \n\t', '\n\t\t The problem is that 1 ) the size of from which takes value is variable ; 2 ) the start action depends on all entities in , which makes it difficult to train directly . \n\t', '\n\t\t 4 Search Issues As shown in Section 2 , the search space of the coreference problem can be represented by the Bell tree . \n\t', '\n\t\t Thus , the search problem reduces to creating the Bell tree while keeping track of path scores and picking the top-N best paths . \n\t', '\n\t\t This is exactly what is described in Algorithm 1 . \n\t', '\n\t\t In Algorithm 1 , contains all the hypotheses , or paths from the root to the current layer of nodes . \n\t', '\n\t\t Vari- able stores the cumulative score for a corefer- ence result . \n\t', '\n\t\t At line 1 , is initialized with a single entity consisting of mention , which corresponds to the root node of the Bell tree in Figure 1 . \n\t', '\n\t\t Line 2 to 15 loops over the remaining mentions ( to ) , and for each mention , the algorithm extends each result in ( or a path in the Bell tree ) by either linking with an existing entity ( line 5 to 10 ) , or starting an entity ( line 11 to 14 ) . \n\t', '\n\t\t The loop from line 2 to 12 corresponds to creating a new layer of nodes for the active mention in the Bell tree . \n\t', '\n\t\t in line 4 and in line 6 and 11 have to do with pruning , which will be discussed shortly . \n\t', '\n\t\t The last line returns top results , where denotes the result ranked by : Algorithm 1 Search Algorithm if ( ) { 8 : Extend to by linking with 9 : 10 : } 11 : if( ) { 12 : Extend to by starting . \n\t', '\n\t\t 13 : 14 : } 15 : 16:return Input : mentions ; Output : top entity results 1:Initialize : 2:for to 3 : foreach node 4 : compute . \n\t', '\n\t\t 5 : foreach 6 : . \n\t', '\n\t\t The complexity of the search Algorithm 1 is the total number of nodes in the Bell tree , which is , where is the Bell Number . \n\t', '\n\t\t Since the Bell number increases rapidly as a function of the number of mentions , pruning is necessary . \n\t', '\n\t\t We prune the search space in the following places : Local pruning : any children with a score below a fixed factor of the maximum score are pruned . \n\t', '\n\t\t This is done at line 6 and 11 in Algorithm 1 . \n\t', '\n\t\t The operation in line 4 is : first aligns the system entities with the reference entities so that the number of common mentions is maximized . \n\t', '\n\t\t Each system entity is constrained to align with at most one reference entity , and vice versa . \n\t', '\n\t\t For example , suppose that a reference document contains three entities : while a system out- puts four entities : , then the best alignment ( from reference to system ) would be , and other entities are not aligned . \n\t', '\n\t\t The number of common mentions of the best alignment is ( i.e. , and ) , which leads to a mention recall and precision . \n\t', '\n\t\t The ECM-F measures the percentage of mentions that are in the \x93right\x94 entities . \n\t', '\n\t\t For tests on the MUC data , we report both F-measure using the official MUC score \n\t\t']",Positive
"['\n\t\t The MUC score counts the common links between the reference and the system output . \n\t', '\n\t\t 5.2 Results on the ACE data The system is first developed and tested using the ACE data . \n\t', '\n\t\t The ACE coreference system is trained with documents ( about words ) of ACE 2002 training data . \n\t', '\n\t\t A separate documents ( words ) is used as the development-test ( Devtest ) set . \n\t', '\n\t\t In 2002 , NIST released two test sets in February ( Feb02 ) and September ( Sep02 ) , respectively . \n\t', '\n\t\t Statistics of the three test sets is summarized in Table 2 . \n\t', '\n\t\t We will report coreference results on the true mentions of the three test sets . \n\t', '\n\t\t TestSet #-docs #-words #-mentions #-entities Devtest 90 50426 7470 2891 Feb02 97 52677 7665 3104 Sep02 186 69649 10577 4355 Table 2 : Statistics of three test sets . \n\t', '\n\t\t Block 8-9 is carried out only if and block 12-13 is car- ried out only if. . \n\t', '\n\t\t Global pruning : similar to local pruning except that this is done using the cumulative score . \n\t', '\n\t\t Pruning based on the global scores is carried out at line 15 of Algorithm 1 . \n\t', '\n\t\t Limit hypotheses : we set a limit on the maximum number of live paths . \n\t', '\n\t\t This is useful when a document contains many mentions , in which case excessive number of paths may survive local and global pruning . \n\t', '\n\t\t Whenever available , we check the compatibility of entity types between the in-focus entity and the active mention . \n\t', '\n\t\t A hypothesis with incompatible entity types is discarded . \n\t', '\n\t\t In the ACE annotation , every mention has an entity type . \n\t', '\n\t\t Therefore we can eliminate hypotheses with two mentions of different types . \n\t', '\n\t\t 5 Experiments 5.1 Performance Metrics The official performance metric for the ACE task is ACE-value . \n\t', '\n\t\t ACE-value is computed by first calculating the weighted cost of entity insertions , deletions and substitutions ; The cost is then normalized against the cost of a nominal coreference system which outputs no entities ; The ACE-value is obtained by subtracting the normalized cost from . \n\t', '\n\t\t Weights are designed to emphasize NAME entities , while PRONOUN entities ( i.e. , an entity consisting of only pronominal mentions ) carry very low weights . \n\t', '\n\t\t A perfect coreference system will get a ACE-value while a system outputs no entities will get a ACE-value . \n\t', '\n\t\t Thus , the ACE-value can be interpreted as percentage of value a system has , relative to the perfect system . \n\t', '\n\t\t Since the ACE-value is an entity-level metric and is weighted heavily toward NAME entities , we also measure our system\x92s performance by an entity-constrained mention F-measure ( henceforth \x93ECM-F\x94 ) . \n\t', '\n\t\t The metric For the mention-pair model , training events are generated for all compatible mention-pairs , which results in about events , about of which are positive examples . \n\t', '\n\t\t The full mention-pair model uses about features ; Most are conjunction features . \n\t', '\n\t\t For the entity-mention model , events are generated by walking through the Bell tree . \n\t', '\n\t\t Only events on the true path ( i.e. , positive examples ) and branches emitting from a node on the true path to a node not on the true path ( i.e. , negative examples ) are generated . \n\t', '\n\t\t For example , in Figure 1 , suppose that the path (a)-(b2)-(c4) is the truth , then positive training examples are starting event from ( a ) to ( b2 ) and linking event from ( b2 ) to ( c4 ) ; While the negative examples are linking events from ( a ) to ( b1 ) , ( b2 ) to ( c3 ) , and the starting event from ( b2 ) to ( c5 ) . \n\t', '\n\t\t This scheme generates about events , out of which about are positive training examples . \n\t', '\n\t\t The full entity-mention model has about features , due to less number of conjunction features and training examples . \n\t', '\n\t\t Coreference results on the true mentions of the De- vtest , Feb02 , and Sep02 test sets are tabulated in Table 3 . \n\t', '\n\t\t These numbers are obtained with a fixed search beam and pruning threshold ( widening the search beam or using a smaller pruning threshold did not change results significantly ) . \n\t', '\n\t\t The mention-pair model in most cases performs better than the mention-entity model by both ACE-value and ECM-F measure although none of the differences is statistically significant ( pair-wise t-test ) at p-value . \n\t', '\n\t\t Note that , however , the mention-pair model uses times more features than the entity-pair model . \n\t', '\n\t\t We also observed that , because the score between the in- focus entity and the active mention is computed by ( 9 ) in the mention-pair model , the mention-pair sometimes mistakenly places a male pronoun and female pronoun into the same entity , while the same mistake is avoided in the entity-mention model . \n\t', '\n\t\t Using the canonical mentions when computing some features ( e.g. , \x93spell\x94 ) in the entity-mention model is probably not optimal and it is an area that needs further research . \n\t', '\n\t\t When the same mention-pair model is used to score the ACE 2003 evaluation data , an ACE-value is obtained on the systems mentions . \n\t', '\n\t\t After retrained with Chinese and Arabic data ( much less training data than English ) , the system got and ACE-value on the system mentions of ACE 2003 evaluation data for Chinese and Arabic , respectively . \n\t', '\n\t\t The results for all three languages are among the top-tier submission systems . \n\t', '\n\t\t Details of the mention detection and coreference system can be found in \n\t\t']",Positive
"['\n\t\t Since the mention-pair model is better , subsequent analyses are done with the mention pair model only . \n\t', '\n\t\t 5.2.1 Feature Impact To see how each category of features affects the performance , we start with the aforementioned mention- pair model , incrementally remove each feature category , retrain the system and test it on the Devtest set . \n\t', '\n\t\t The result is summarized in Table 4 . \n\t', '\n\t\t The last column lists the number of features . \n\t', '\n\t\t The second row is the full mention-pair model , the third through seventh row correspond to models by removing the syntactic features ( i.e. , POS tags and apposition features ) , count features , distance features , mention type and level information , and pair of mention-spelling features . \n\t', '\n\t\t If a basic feature is removed , conjunction features using that basic feature are also removed . \n\t', '\n\t\t It is striking that the smallest system consisting of only features ( string and substring match , acronym , edit distance and number of different capitalized words ) can get as much as ACE-value . \n\t', '\n\t\t Table 4 shows clearly that these lexical features and the distance features are the most important . \n\t', '\n\t\t Sometimes the ACE-value increases after removing a set of features , but the ECM-F measure tracks nicely the trend that the more features there are , the better the performance is . \n\t', '\n\t\t This is because the ACE-value 1 System mentions are output from a mention detection system . \n\t', '\n\t\t log a Figure 2 : Performance vs. log start penalty is a weighted metric . \n\t', '\n\t\t A small fluctuation of NAME entities will impact the ACE-value more than many NOMINAL or PRONOUN entities . \n\t', '\n\t\t Model ACE-val(%) ECM-F(%) #-features Full 89.8 73.20( 2.9 ) 171K -syntax 89.0 72.6( 2.5 ) 71K -count 89.4 72.0( 3.3 ) 70K -dist 86.7 *66.2 ( 3.9 ) 24K -type/level 86.8 65.7( 2.2 ) 5.4K -spell 86.0 64.4( 1.9 ) 39 Table 4 : Impact of feature categories . \n\t', '\n\t\t Numbers after are the standard deviations . \n\t', '\n\t\t * indicates that the result is significantly ( pair-wise t-test ) different from the line above at . \n\t', '\n\t\t 5.2.2 Effect of Start Penalty As discussed in Section 3.1 , the start penalty can be used to balance the entity miss and false alarm . \n\t', '\n\t\t To see this effect , we decode the Devtest set by varying the start penalty and the result is depicted in Figure 2 . \n\t', '\n\t\t The ACE-value and ECM-F track each other fairly well . \n\t', '\n\t\t Both achieve the optimal when . \n\t', '\n\t\t 5.3 Experiments on the MUC data To see how the proposed algorithm works on the MUC data , we test our algorithm on the MUC6 data . \n\t', '\n\t\t To minimize the change to the coreference system , we first map the MUC data into the ACE style . \n\t', '\n\t\t The original MUC coreference data does not have entity types ( i.e. , \x93ORGANIZATION\x94 , \x93LOCATION\x94 etc ) , required in the ACE style . \n\t', '\n\t\t Part of entity types can be recovered from the corresponding named-entity annotations . \n\t', '\n\t\t The recovered named-entity label is propagated to all mentions belonging to the same entity . \n\t', '\n\t\t There are 504 out of 2072 mentions of the MUC6 formal test set and 695 out of 2141 mentions of the MUC6 dry-run test set that cannot be assigned labels by this procedure . \n\t', '\n\t\t A 0.9 085 ECM^F ACE^value 0.8 075 0.7 065 ^2.5 ^2 ^1.5 ^1 ^0.5 0 Devtest Feb02 Sep02 Model ACE-val(%) ECM-F(%) ACE-val(%) ECM-F(%) ACE-val(%) ECM-F(%) MP EM 89.8 73.2( 2.9 ) 90.0 73.1( 4.0 ) 88.0 73.1( 6.8 ) 89.9 71.7( 2.4 ) 88.2 70.8( 3.9 ) 87.6 72.4( 6.2 ) Table 3 : Coreference results on true mentions : MP \x96 mention-pair model ; EM \x96 entity-mention model ; ACE-val : ACE-value ; ECM-F : Entity-constrained Mention F-measure . \n\t', '\n\t\t MP uses features while EM uses only features . \n\t', '\n\t\t None of the ECM-F differences between MP and EM is statistically significant at . \n\t', '\n\t\t generic type \x93UNKNOWN\x94 is assigned to these mentions . \n\t', '\n\t\t Mentions that can be found in the named-entity annotation are assumed to have the ACE mention level \x93NAME\x94 ; All other mentions other than English pronouns are assigned the level \x93NOMINAL.\x94 After the MUC data is mapped into the ACE-style , the same set of feature templates is used to train a coreference system . \n\t', '\n\t\t Two coreference systems are trained on the MUC6 data : one trained with 30 dry-run test documents ( henceforth \x93MUC6-small\x94 ) ; the other trained with 191 \x93dryrun-train\x94 documents that have both coreference and named-entity annotations ( henceforth \x93MUC6-big\x94 ) in the latest LDC release . \n\t', '\n\t\t To use the official MUC scorer , we convert the output of the ACE-style coreference system back into the MUC format . \n\t', '\n\t\t Since MUC does not require entity label and level , the conversion from ACE to MUC is \x93loss- less.\x94 Table 5 tabulates the test results on the true mentions of the MUC6 formal test set . \n\t', '\n\t\t The numbers in the table represent the optimal operating point determined by ECM-F . \n\t', '\n\t\t The MUC scorer cannot be used since it inherently favors systems that output fewer number of entities ( e.g. , putting all mentions of the MUC6 formal test set into one entity will yield a recall and precision of links , which gives an F-measure ) . \n\t', '\n\t\t The MUC6-small system compares favorably with the similar experiment in \n\t\t']",Negative
"['\n\t\t When measured by the ECM-F measure , the MUC6-small system has the same level of performance as the ACE system , while the MUC6-big system performs better than the ACE system . \n\t', '\n\t\t The results show that the algorithm works well on the MUC6 data despite some information is lost in the conversion from the MUC format to the ACE format . \n\t', '\n\t\t System MUC F-measure ECM-F MUC6-small MUC6-big 83.9 % 72.1 % 85.7 % 76.8 % Table 5 : Results on the MUC6 formal test set . \n\t', '\n\t\t 6 Related Work There exists a large body of literature on the topic of coreference resolution . \n\t', '\n\t\t We will compare this study with some relevant work using machine learning or statistical methods only . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t Leaves of the decision tree are labeled with \x93link\x94 or \x93not-link\x94 in training . \n\t', '\n\t\t At test time , the system checks a mention against all its preceding mentions , and the first one labeled with \x93link\x94 is picked as the antecedent . \n\t', '\n\t\t Their work is later enhanced by \n\t\t']",Positive
['\n\t\t The model in \n\t\t'],Positive
['\n\t\t Neither \n\t\t'],Positive
"['\n\t\t In contrast , our decoder always searches for the best result ranked by the cumulative score ( subject to pruning ) , and subsequent decisions depend on earlier ones . \n\t', '\n\t\t Recently , \n\t\t']",Positive
"['\n\t\t The model is appealing in that it can potentially overcome the limitation of mention-pair model in which dependency among mentions other than the two in question is ignored . \n\t', '\n\t\t However , models in \n\t\t']",Positive
"['\n\t\t The Bell tree representation proposed in this paper , however , provides us with a naturally incremental framework for coreference resolution . \n\t', '\n\t\t Maximum entropy method has been used in coreference resolution before . \n\t', '\n\t\t For example , \n\t\t']",Positive
"['\n\t\t In contrast , in our mention pair model , an entity-mention pair is scored by taking the maximum score among possible mention pairs . \n\t', '\n\t\t Our entity-mention model eliminates the need to synthesize an entity-mention score from mention-pair scores . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t Features involving the dummy mention are essentially computed with the single ( normal ) mention , and therefore the starting model is weak . \n\t', '\n\t\t In our model , the starting model is obtained by \x93complementing\x94 the linking scores . \n\t', '\n\t\t The advantage is that we do not need to train a starting model . \n\t', '\n\t\t To compensate the model inaccuracy , we introduce a \x93starting penalty\x94 to balance the linking and starting scores . \n\t', '\n\t\t To our knowledge , the paper is the first time the Bell tree is used to represent the search space of the coreference resolution problem . \n\t', '\n\t\t 7 Conclusion We propose to use the Bell tree to represent the process of forming entities from mentions . \n\t', '\n\t\t The Bell tree represents the search space of the coreference resolution problem . \n\t', '\n\t\t We studied two maximum entropy models , namely the mention-pair model and the entity- mention model , both of which can be used to score entity hypotheses . \n\t', '\n\t\t A beam search algorithm is used to search the best entity result . \n\t', '\n\t\t State-of-the-art performance has been achieved on the ACE coreference data across three languages . \n\t', '\n\t\t Acknowledgments This work was partially supported by the Defense Advanced Research Projects Agency and monitored by SPAWAR under contract No . \n\t', '\n\t\t N66001-99-2-8916 . \n\t', '\n\t\t The views and findings contained in this material are those of the authors and do not necessarily reflect the position of policy of the Government and no official endorsement should be inferred . \n\t', '\n\t\t We also would like to thank the anonymous reviewers for suggestions of improving the paper . \n\t', '\n\t\t References E.T. Bell . \n\t', '\n\t\t 1934. Exponential numbers . \n\t', '\n\t\t Amer . \n\t', '\n\t\t Math . \n\t', '\n\t\t Monthly , pages 411\x96419 . \n\t', '\n\t\t Adam L. Berger , Stephen A. Della Pietra , and Vincent J. Della Pietra . \n\t', '\n\t\t 1996. A maximum entropy approach to natural language processing . \n\t', '\n\t\t Computational Linguistics , 22(1):39\x9671 , March . \n\t', '\n\t\t R Florian , H Hassan , A Ittycheriah , H Jing , N Kambhatla , X Luo , N Nicolov , and S Roukos . \n\t', '\n\t\t 2004. A statistical model for multilingual entity detection and tracking . \n\t', '\n\t\t In Daniel Marcu Susan Dumais and Salim Roukos , editors , HLT-NAACL 2004 : Main Proceedings , pages 1\x968 , Boston , Massachusetts , USA , May 2 - May 7 . \n\t', '\n\t\t Association for Computational Linguistics . \n\t', '\n\t\t Niyu Ge , John Hale , and Eugene Charniak . \n\t', '\n\t\t 1998. A statistical approach to anaphora resolution . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the sixth Workshop on Very Large Corpora . \n\t', '\n\t\t Sanda M. Harabagiu , Razvan C. Bunescu , and Steven J. Maiorano . \n\t', '\n\t\t 2001. Text and knowledge mining for coreference resolution . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ofNAACL . \n\t', '\n\t\t J. Hobbs . \n\t', '\n\t\t 1976. Pronoun resolution . \n\t', '\n\t\t Technical report , Dept. of Computer Science , CUNY , Technical Report TR76-1 . \n\t', '\n\t\t A. Ittycheriah , L. Lita , N. Kambhatla , N. Nicolov , S. Roukos , and M. Stys . \n\t', '\n\t\t 2003. Identifying and tracking entity mentions in a maximum entropy framework . \n\t', '\n\t\t In HLT-NAACL 2003 : Short Papers , May 27 - June 1 . \n\t', '\n\t\t Andrew Kehler . \n\t', '\n\t\t 1997. Probabilistic coreference in information extraction . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ofEMNLP . \n\t', '\n\t\t Andrew McCallum and Ben Wellner . \n\t', '\n\t\t 2003. Toward conditional models of identity uncertainty with application to proper noun coreference . \n\t', '\n\t\t In IJCAI Workshop on Information Integration on the Web. R. Mitkov . \n\t', '\n\t\t 1998 . \n\t', '\n\t\t Robust pronoun resolution with limited knowledge . \n\t', '\n\t\t In Procs . \n\t', '\n\t\t of the 17th Internaltional Conference on Computational Linguistics , pages 869\x96875 . \n\t', '\n\t\t Thomas S. Morton . \n\t', '\n\t\t 2000. Coreference for NLP applications . \n\t', '\n\t\t In In Proceedings of the 38th Annual Meeting ofthe Associationfor Computational Linguistics . \n\t', '\n\t\t MUC-6 . \n\t', '\n\t\t 1995. Proceedings of the Sixth Message Understanding Conference(MUC-6) , San Francisco , CA . \n\t', '\n\t\t Morgan Kaufmann. Vincent Ng and Claire Cardie . \n\t', '\n\t\t 2002. Improving machine learning approaches to coreference resolution . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ofACL , pages 104\x96111 . \n\t', '\n\t\t NIST . \n\t', '\n\t\t 2003. The ACE evaluation plan . \n\t', '\n\t\t www.nist.gov/speech/tests/ace/index.htm . \n\t', '\n\t\t Adwait Ratnaparkhi . \n\t', '\n\t\t 1997. A Linear Observed Time Statistical Parser Based on Maximum Entropy Models . \n\t', '\n\t\t In Second Conference on Empirical Methods in Natural Language Processing , pages 1 \x9610 . \n\t', '\n\t\t Wee Meng Soon , Hwee Tou Ng , and Chung Yong Lim . \n\t', '\n\t\t 2001. A machine learning approach to coreference resolution of noun phrases . \n\t', '\n\t\t Computational Linguistics , 27(4):521\x96544 . \n\t', '\n\t\t M. Vilain , J. Burger , J. Aberdeen , D. Connolly , , and L. Hirschman . \n\t', '\n\t\t 1995. A model-theoretic coreference scoring scheme . \n\t', '\n\t\t In In Proc . \n\t', '\n\t\t ofMUC6 , pages 45\x9652 . \n\t', '\n\t\t Xiaofeng Yang , Guodong Zhou , Jian Su , and Chew Lim Tan . \n\t', '\n\t\t 2003. Coreference resolution using competition learning approach . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the ACL . \n\t', '\n\t\t LEARNING TO RESOLVE BRIDGING REFERENCES Massimo Poesio,4 Rahul Mehta,4 Axel Maroudas,4 and Janet Hitzeman* 4Dept . \n\t', '\n\t\t of Comp . \n\t', '\n\t\t Science , University of Essex , UK p o e s i o at e s s e x dot ac dot u k *MITRE Corporation , USA hitz at mitre dot org Abstract We use machine learning techniques to find the best combination of local focus and lexical distance features for identifying the anchor of mereological bridging references . \n\t', '\n\t\t We find that using first mention , utterance distance , and lexical distance computed using either Google or WordNet results in an accuracy significantly higher than obtained in previous experiments . \n\t', '\n\t\t 1 Introduction BRIDGING REFERENCES ( BR ) \n\t\t']",Positive
['\n\t\t Work such as \n\t\t'],Positive
"['\n\t\t This previuous work also suggested that simply choosing the entity whose description is lexically closest to that of the bridging description among those in the current focus space gives poor results ; in fact , better results are obtained by always choosing as ANCHOR of the bridging reference2 the first-mentioned entity of the previous sentence \n\t\t']",Positive
"['\n\t\t But neither source of information in isolation resulted in an accuracy over 40 % . \n\t', '\n\t\t In short , this earlier work suggested that a combination of salience and lexical / 1We will use the term bridging descriptions to indicate bridging references realized by definite descriptions , equated here with noun phrases with determiner the , like the top . \n\t', '\n\t\t 2Following \n\t\t']",Positive
"['\n\t\t commonsense information is needed to choose the most likely anchor ; the problem remained of how to combine this information . \n\t', '\n\t\t In the work described in this paper , we used machine learning techniques to find the best combination of local focus features and lexical distance features , focusing on MEREOLOGICAL bridging refer- ences:3 references referring to parts of an object al- ready introduced ( the cabinet ) , such as the panels or the top ( underlined ) in the following example from the GNOME corpus \n\t\t']",Positive
"['\n\t\t ( 1 ) The combination of rare and expensive ma- terials used on [ this cabinet]i indicates that it was a particularly expensive commission . \n\t', '\n\t\t The four Japanese lacquer panels date from the mid- to late 1600s and were created with a technique known as kijimaki-e . \n\t', '\n\t\t For this type of lacquer , artisans sanded plain wood to heighten its strong grain and used it as the background of each panel . \n\t', '\n\t\t They then added the scenic elements of landscape , plants , and animals in raised lacquer . \n\t', '\n\t\t Although this technique was common in Japan , such large panels were rarely incorporated into French eighteenth-century furniture . \n\t', '\n\t\t Heavy Ionic pilasters , whose copper-filled flutes give an added rich color and contrast to the gilt- bronze mounts , flank the panels . \n\t', '\n\t\t Yellow jasper , a semiprecious stone , rather than the usual marble , forms the top . \n\t', '\n\t\t 2 Two sources of information for bridging reference resolution 2.1 Lexical information The use of different sources of lexical knowledge for resolving bridging references has been investigated in a series of papers by Poesio et al . \n\t', '\n\t\t all using as dataset the Bridging Descriptions ( BDs ) contained in the corpus used by Vieira and Poesio 3 We make use of the classification of bridging references proposed by \n\t\t']",Positive
"['\n\t\t \x91Mereological\x92 bridging references are one of the the \x91WordNet\x92 bridging classes , which cover cases where the information required to bridge the gap may be found in a resource such as WordNet \n\t\t']",Positive
"['\n\t\t ( 2000 ) . \n\t', '\n\t\t In these studies , the lexical distance between a BD and its antecedent was used to choose the anchor for the BD among the antecedents in the previous five sentences . \n\t', '\n\t\t In \n\t\t']",Positive
"['\n\t\t These results were due in part to missing entries and / or relations ; in part to the fact that because of the monotonic organization of information in Word- Net , complex searches are required even to find apparently close associations ( like that between wheel and car ) . \n\t', '\n\t\t Similar results using WordNet 1.6 were reported at around the same time by other groups - e.g. , \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
['\n\t\t \n\t\t'],Positive
['\n\t\t \n\t\t'],Positive
['\n\t\t The HAL model discussed in \n\t\t'],Positive
"['\n\t\t However , using vectorial representations did not improve the results for the \x91Word- Net\x92 BDs : for the synonymy cases the results were comparable to those obtained with WordNet ( 4/12 , 33 % ) , but for the hyponymy BDs ( 2/14 , as opposed to 8/14 with WordNet ) and especially for mereological references ( 2/12 ) they were clearly worse . \n\t', '\n\t\t On the other hand , the post-hoc analysis of results suggested that the poor results were in part due to the lack of mechanisms for choosing the most salient ( or most recent ) BDs . \n\t', '\n\t\t The poor results for mereological BDs with both WordNet and vectorial representations indicated that a different approach was needed to acquire information about part-of relations . \n\t', '\n\t\t Grefenstette\x92s work on semantic similarity \n\t\t']",Positive
['\n\t\t In \n\t\t'],Positive
"['\n\t\t These representations were then used to choose the anchor of BDs , using again the same dataset and the same methods as in the previous two attempts , and using mutual information to determine the strength of association . \n\t', '\n\t\t The results on mereological BDs\x96recall .67 , precision=.73\x96were drastically better than those obtained with WordNet or with simple vectorial representations . \n\t', '\n\t\t The results with the three types of lexical resources and the different types of BDs in the Vieira / Poesio dataset are summarized in Table 1 . \n\t', '\n\t\t Finally , a number of researchers recently argued for using the Web as a way of addressing data sparseness \n\t\t']",Positive
"['\n\t\t The Web has proven a useful resource for work in anaphora resolution as well . \n\t', '\n\t\t \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t Markert et al . \n\t', '\n\t\t also found a sharp difference between using the Web as a a corpus and using the BNC , the results in the latter case being significantly worse than when using WordNet . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t 2.2 Salience One of the motivations behind Grosz and Sidner\x92s ( 1986 ) distinction between two aspects of the attentional state - the LOCAL FOCUS and the GLOBAL FOCUS\x96is the difference between the interpretive preferences of pronouns and definite descriptions . \n\t', '\n\t\t According to Grosz and Sidner , the interpretation for pronouns is preferentially found in the local focus , whereas that of definite descriptions is preferentially found in the global focus . \n\t', '\n\t\t 4A similar approach was pursued in parallel by \n\t\t']",Positive
"['\n\t\t Synonymy Hyponymy Meronymy Total WN Total BDs BDs in Vieira / Poesio corpus 12 14 12 38 204 Using WordNet 4(33.3%) 8(57.1%) 3(33.3%) 15 ( 39 % ) 34 ( 16.7 % ) Using HAL Lexicon 4(33.3%) 2(14.3%) 2(16.7%) 8 ( 22.2 % ) 46(22.7%) Using Construction Lexicon 1(8.3%) 0 8(66.7%) 9 ( 23.7 % ) 34(16.7%) Table 1 : BD resolution results using only lexical distance with WordNet , HAL-style vectorial lexicon , and construction-based lexicon . \n\t', '\n\t\t However , already \n\t\t']",Positive
"['\n\t\t As already mentioned , the error analysis of \n\t\t']",Positive
"['\n\t\t The following example illustrates how the local focus affects the interpretation of a mereological BD , the sides , in the third sentence . \n\t', '\n\t\t ( 2 ) [ Cartonnier ( Filing Cabinet)]i with Clock [ This piece of mid-eighteenth-century furniture]i was meant to be used like a modern filing cabinet ; papers were placed in [ leather- fronted cardboard boxes]j ( now missing ) that were fitted into the open shelves . \n\t', '\n\t\t [ A large table]k decorated in the same manner would have been placed in front for working with those papers . \n\t', '\n\t\t Access to [ the cartonnier]i\x92s lower half can only be gained by the doors at the sides , because the table would have blocked the front . \n\t', '\n\t\t The three main candidate anchors in this example\x96 the cabinet , the boxes , and the table\x96all have sides . \n\t', '\n\t\t However , the actual anchor , the cabinet , is clearly the Backward-Looking Center ( CB ) \n\t\t']",Positive
['\n\t\t In \n\t\t'],Positive
['\n\t\t the parameters of Centering by \n\t\t'],Positive
['\n\t\t \n\t\t'],Positive
['\n\t\t These percentages are very similar to those found with pronouns \n\t\t'],Positive
"['\n\t\t Next , Poesio analyzed the order of mention of the anchors of the 72 associative BD whose anchor was in the previous sentence , finding that 49/72 , 68 % , were realized in first position . \n\t', '\n\t\t This finding is consistent with the preference for first-mentioned entities ( as opposed to the most recent ones ) repeatedly observed in the psychological literature on anaphora \n\t\t']",Positive
"['\n\t\t Finally , Poesio examined the hypothesis that finding the anchor of a BD involves knowing which entities are the CB and the CP in the sense of Centering \n\t\t']",Positive
"['\n\t\t He found that CB(U-1) is the anchor of 37/72 of the BDs whose anchor is in the previous utterance ( 51.3 % ) , and only 33.6 % overall . \n\t', '\n\t\t ( CP(U-1) was the anchor for 38.2 % associative BDs . \n\t', '\n\t\t ) Clearly , simply choosing the CB ( or the CP ) of the previous sentence as the anchor doesn\x92t work very well . \n\t', '\n\t\t However , Poesio also found that 89 % of the anchors of associative BDs had been CBs or CPs . \n\t', '\n\t\t This suggested that while knowing the local focus isn\x92t sufficient to determine the anchor of a BD , restricting the search for anchors to CBs and CPs only might increase the precision of the BD resolution process . \n\t', '\n\t\t This hypothesis was supported by a preliminary test with 20 associative BDs . \n\t', '\n\t\t The anchor for a BD with head noun NBD was chosen among the subset of all potential antecedents ( PA ) in the previous five sentences that had been CBs or CPs by calling Google ( by hand ) with the query \x93the NBD of the NPA\x94 , where NPA is the head noun of the potential antecedent , and choosing the PA with the highest hit count . \n\t', '\n\t\t 14 mereological BDs ( 70 % ) were resolved correctly this way . \n\t', '\n\t\t 3 Methods The results just discussed suggest that lexical information and salience information combine to deter- mine the anchor of associative BRs . \n\t', '\n\t\t The goal of the experiments discussed in this paper was to test more thoroughly this hypothesis using machine learning techniques to combine the two types of information , using a larger dataset than used in this previous work , and using completely automatic techniques . \n\t', '\n\t\t We concentrated on mereological BDs , but our methods could be used to study other types of bridging references , using , e.g. , the constructions used by Markert et al . \n\t', '\n\t\t (2003).6 3.1 The corpus We used for these experiments the GNOME corpus , already used in \n\t\t']",Positive
"['\n\t\t An important property of this corpus for the purpose of studying BR resolution is that fewer types of BDs are annotated than in the original Vieira / Poesio dataset , but the annotation is reliable \n\t\t']",Positive
"['\n\t\t The GNOME corpus contains about 500 sentences and 3000 NPs . \n\t', '\n\t\t A variety of semantic and discourse information has been annotated ( the manual is available from the GNOME project\x92s home page at http://www.hcrc.ed.ac.uk/ \x98gnome ) . \n\t', '\n\t\t Four types of anaphoric relations were annotated : identity ( IDENT ) , set membership ( ELEMENT ) , subset ( SUBSET ) , and \x91generalized possession\x92 ( POSS ) , which also includes part-of relations . \n\t', '\n\t\t A total of 2073 anaphoric relations were annotated ; these include 1164 identity relations ( including those realized with synonyms and hyponyms ) and 153 POSS relations . \n\t', '\n\t\t Bridging references are realized by noun phrases of different types , including indefinites ( as in I bought a book and a page fell out \n\t\t']",Positive
"['\n\t\t Of the 153 mereological references , 58 mereological references are realized by definite descriptions . \n\t', '\n\t\t 6In \n\t\t']",Negative
"['\n\t\t 7A serious problem when working with bridging references is the fact that subjects , when asked for judgments about bridging references in general , have a great deal of difficulty in agreeing on which expressions in the corpus are bridging references , and what their anchors are \n\t\t']",Positive
"['\n\t\t This finding raises a number of interesting theoretical questions concerning the extent of agreement on semantic judgments , but also the practical question of whether it is possible to evaluate the performance of a system on this task . \n\t', '\n\t\t Subsequent work found , however , that restricting the type of bridging inferences required does make it possible for annotators to agree among themselves \n\t\t']",Positive
"['\n\t\t In the GNOME corpus only a few types of associative relations are marked , but these can be marked reliably , and do include part-of relations like that between the top and the cabinet that we are concerned with . \n\t', '\n\t\t 3.2 Features Our classifiers use two types of input features . \n\t', '\n\t\t Lexical features Only one lexical feature was used : lexical distance , but extracted from two different lexical sources . \n\t', '\n\t\t Google distance was computed as in \n\t\t']",Positive
"['\n\t\t Then ~ 1 if NHits = 0 Google distance = 1 NHits otherwise The query \x93the NBD of NPA\x94 ( e.g. , the amount of cream ) is used when NPA is used as a mass noun ( information about mass vs count is annotated in the GNOME corpus ) . \n\t', '\n\t\t If the potential antecedent is a pronoun , the head of the closest realization of the same discourse entity is used . \n\t', '\n\t\t We also reconsidered WordNet ( 1.7.1 ) as an alternative way of establishing lexical distance , but made a crucial change from the studies reported above . \n\t', '\n\t\t Both earlier studies such as \n\t\t']",Positive
"['\n\t\t However , these studies also showed that information about hypernyms is much more extensive . \n\t', '\n\t\t This suggested trading precision for recall with an alternative way of using WordNet to compute lexical distance : instead of requiring the path between the head predicate of the associative BD and the head predicate of the potential antecedent to contain at least one mereological link ( various strategies for performing a search of this type were considered in \n\t\t']",Positive
"['\n\t\t To compute our second measure of lexical distance between NBD and NPA defined as above , WordNet distance , the following algorithm was used . \n\t', ""\n\t\t Let distance(s , s ' ) be the number of hyper- nim links between concepts s and s ' . \n\t"", '\n\t\t Then 1 . \n\t', '\n\t\t Get from WordNet all the senses of both NBD and NPA ; 2 . \n\t', '\n\t\t Get the hypernym tree of each of these senses ; 3 . \n\t', '\n\t\t For each pair of senses sNBDi and sNPAj , find the Most Specific Common Subsumer Z?mm ( this is the closest concept which is an hypernym of both senses ) . \n\t', '\n\t\t 4. The ShortestWNDistance between NBD and NPA is then computed as the shortest distance between any of the senses of NBD and any of the senses of NPA : ShtstWNDist(NBD , NPA ) = minz,j ( distance(sNBD , , 3z~ m ) + distance(ssom zj , sNPAj ) ) 5 . \n\t', '\n\t\t Finally , a normalized WordNet distance in the range 0..1 is then obtained by dividing ShtstWNDist by a Max WNDist factor ( 30 in our experiments ) . \n\t', '\n\t\t WordNet distance = 1 if no path between the concepts was found . \n\t', '\n\t\t 1 if no path WN distance = S ShtstWNDist MaxWNDistotherwise therwise Salience features In choosing the salience features we took into account the results in \n\t\t']",Positive
"['\n\t\t The first of these features was utterance distance , the distance between the utterance in which the BR occurs and the utterance containing the potential antecedent . \n\t', '\n\t\t ( Sentences are used as utterances , as suggested by the results of \n\t\t']",Positive
"['\n\t\t ) As discussed above , studies such as \n\t\t']",Positive
"['\n\t\t This finding was confirmed in our study ; all anchors of the 58 mereological BDs occurred within the previous five sentences , and 47/58 ( 81 % ) in the previous two . \n\t', '\n\t\t ( It is interesting to note that no anchor occurred in the same sentence as the BD . \n\t', '\n\t\t ) The second salience feature was boolean : whether the potential antecedent had been realized in first mention position in a sentence \n\t\t']",Positive
"['\n\t\t Two forms of this feature were tried : local first mention ( whether the entity had been realized in first position within the previous five sentences ) and global first mention ( whether it had been realized in first position anywhere ) . \n\t', '\n\t\t 269 entities are realized in first position in the five sentences preceding one of the 58 BDs ; 298 entities are realized in first position anywhere in the preceding text . \n\t', '\n\t\t For 31/58 of the anchors of mereological BDs , 53.5 % , local first mention = 1 ; global first mention = 1 for 33/58 of anchors , 56.9 % . \n\t', '\n\t\t 3.3 Training Methods Constructing the data set The data set used to train and test BR resolution consisted of a set of positive instances ( the actual anchors of the mereological BRs ) and a set of negative instances ( other entities mentioned in the previous five sentences of the text ) . \n\t', '\n\t\t However , preliminary tests showed that simply including all potential antecedents as negative instances would make the data set too unbalanced , particularly when only bridging descriptions were considered : in this case we would have had 58 positive instances vs. 1672 negative ones . \n\t', '\n\t\t We therefore developed a parametric script that could create datasets with different positive / negative ratios - 1:1 , 1:2 , 1:3 - by including , with each positive instance , a varying number of negative instances ( 1 , 2 , 3 , ... ) randomly chosen among the other potential antecedents , the number of negative instances to be included for each positive one being a parameter chosen by the experimenter . \n\t', '\n\t\t We report the results obtained with 1:1 and 1:3 ratios . \n\t', '\n\t\t The dataset thus constructed was used for both training and testing , by means of a 10-fold cross- validation . \n\t', '\n\t\t Types of Classifiers Used Multi-layer perceptrons ( MLPs ) have been claimed to work well with small datasets ; we tested both our own implementation of an MLP with back-propagation in Mat- Lab 6.5 , experimenting with different configurations , and an off-the-shelf MLP included in the Weka Machine Learning Library8 , Weka-NN. . \n\t', '\n\t\t The best configuration for our own MLP proved to be one with a sigle hidden layer and 10 hidden nodes . \n\t', '\n\t\t We also used the implementation of a Naive Bayes classifier included in the Weka MLL , as \n\t\t']",Positive
"['\n\t\t 4 Experimental Results In the first series of experiments only mereological Bridging Descriptions were considered ( i.e. , only bridging references realized by the-NPs ) . \n\t', '\n\t\t In a second series of experiments we considered all 153 mereological BRs , including ones realized with indefinites . \n\t', '\n\t\t Finally , we tested a classifier trained on balanced data ( 1:1 and 1:3 ) to find the anchors of BDs among all possible anchors . \n\t', '\n\t\t 4.1 Experiment 1 : Mereological descriptions The GNOME corpus contains 58 mereological BDs . \n\t', '\n\t\t The five sentences preceding these 58 BDs contain a total of 1511 distinct entities for which a head could be recovered , possibly by examining their antecedents . \n\t', '\n\t\t This means an average of 26 distinct potential antecedents per BD , and 5.2 entities per sentence . \n\t', '\n\t\t The simplest baselines for the task of finding 8The library is available from http://www.cs.waikato.ac.nz/ml/weka/ . \n\t', '\n\t\t the anchor are therefore 4 % ( by randomly choosing one antecedent among those in the previous five sentences ) and 19.2 % ( by randomly choosing one antecedent among those in the previous sentence only ) . \n\t', '\n\t\t As 4.6 entities on average were realized in first mention position in the five sentences preceding a BD ( 269/58 ) , choosing randomly among the first-mentioned entities gives a slighly higher accuracy of 21.3 % . \n\t', '\n\t\t A few further baselines can be established by examining each feature separately . \n\t', '\n\t\t Google didn\x92t return any hits for 1089 out of 1511 distinct PAs , and no hit for 24/58 anchors ; in 8/58 of cases ( 13.8 % ) the entity with the minimum Google distance is the correct anchor . \n\t', '\n\t\t We saw before that the method for computing WordNet distance used in \n\t\t']",Negative
"['\n\t\t Pairwise combinations of these features were also considered . \n\t', '\n\t\t The best such combination , choosing the first mentioned entity in the previous sentence , achieves an accuracy of 18/58 , 31 % . \n\t', '\n\t\t These baseline results are summarized in the following table . \n\t', '\n\t\t Notice how even the best baselines achieve pretty low accuracy , and how even simple \x92salience\x92 measures work better than lexical distance measures . \n\t', '\n\t\t Baseline Accuracy Random choice between entities in previous 5 4 % Random choice between entities in previous 1 19 % Random choice between First Ment . \n\t', '\n\t\t 21.3 % entities in previous 5 Entity with min Google distance 13.8 % Entity with min WordNet distance 13.8 % FM entity in previous sentence 31 % Min Google distance in previous sentence 17.2 % Min WN distance in previous sentence 25.9 % FM and Min Google distance 12 % FM and Min WN distance 24.1 % Table 2 : Baselines for the BD task The features utterance distance , local first mention , and global f.m. were used in all machine learning experiments . \n\t', '\n\t\t But since one of our goals was to compare different lexical resources , only one lexical distance feature was used in the first two experiment . \n\t', '\n\t\t The three classifiers were trained to classify a potential antecedent as either \x91anchor\x92 or \x91not anchor\x92 . \n\t', '\n\t\t The classification results with Google distance and WN distance for all three classifiers and the 1:1 data set ( 116 instances in total , 58 real anchor , 58 negative instances ) , for all elements of the data set , and averaging across the 10 cross-validations , are shown in Table 3 . \n\t', '\n\t\t WN Distance Google Distance ( Correct ) ( Correct ) Our own MLP 92(79.3%) 89(76.7%) Weka NN 91(78.4%) 86(74.1%) Weka Naive Bayes 88(75.9%) 85(73.3%) Table 3 : Classification results for BDs These results are clearly better than those obtained with any of the baseline methods discussed above . \n\t', '\n\t\t The differences between WN distance and Google distance , and that between our own MLP and the Weka implementation of Naive Bayes , are also significant ( by a sign test , p < .05 ) , whereas the pairwise differences between our own MLP and Weka\x92s NN , and between this and the Naive Bayes classifier , aren\x92t . \n\t', '\n\t\t In other words , although we find little difference between using WordNet and Google to compute lexical distance , using WordNet leads to slightly better results for BDs . \n\t', '\n\t\t The next table shows precision , recall and f-values for the positive data points , for the feature sets using WN distance and Google distance , respectively : Precision Recall F-value WN features 75.4 % 84.5 % 79.6 % Google features 70.6 % 86.2 % 77.6 % Table 4 : Precision and recall for positive instances Using a 1:3 dataset ( 3 negative data points for each anchor ) , overall accuracy increases ( to 82 % using Google distance ) and accuracy with Google distance is better than with Wordnet distance ( 80.6 % ) ; however , the precision and recall figures for the positive data points get much worse : 56.7 % with Google , 55.7 % with Wordnet . \n\t', '\n\t\t 4.2 All mereological references Clearly , 58 positive instances is a fairly small dataset . \n\t', '\n\t\t In order to have a larger dataset , we included every bridging reference in the corpus , including those realized with indefinite NPs , thus bringing the total to 153 positive instances . \n\t', '\n\t\t We then ran a second series of experiments using the same methods as before . \n\t', '\n\t\t The results were slightly lower than those for BDs only , but in this case there was no difference between using Google and using WN . \n\t', '\n\t\t F- measure on positive instances was 76.3 % with WN , 4.375.8%AwithharderGoogle . \n\t', '\n\t\t test In a last experiment , we used classifiers trained on balanced and moderately unbalanced data to determine the anchor of 6 randomly chosen BDs among WN Distance Google Distance ( Correct ) ( Correct ) Weka NN 227(74.2%) 230(75.2%) Table 5 : Classification results for all BDs all of their 346 possible antecedents in context . \n\t', '\n\t\t For these experiments , we also tried to use both Google and WordNet simultaneously . \n\t', '\n\t\t The results for BDs are shown in Table 6 . \n\t', '\n\t\t The first column of the table specifies the lexical resource used ; the second the degree of balance ; the next two columns percentage correct and F value on a testing set with the same balance as the training set ; the final two columns perc . \n\t', '\n\t\t correct and F value on the harder test set . \n\t', '\n\t\t The best results,F=.5 , are obtained using both Google and WN distance , and using a larger ( if unbalanced ) training corpus . \n\t', '\n\t\t These results are not as good as those obtained ( by hand ) by Poesio ( which , however , used a complete focus tracking mechanism ) , but the F measure is still 66 % higher than that obtained with the highest baseline ( FM only ) , and not far off from the results obtained with direct anaphoric definite descriptions ( e.g. , by \n\t\t']",Positive
"['\n\t\t It\x92s also conforting to note that results with the harder test improve the more data are used , which suggests that better results could be obtained with a larger corpus . \n\t', '\n\t\t 5 Related work In recent years there has been a lot of work to develop anaphora resolution algorithms using both symbolic and statistical methods that could be quantitatively evaluated \n\t\t']",Positive
"['\n\t\t Thus , most work on bridging has been theoretical , like the work by \n\t\t']",Positive
"['\n\t\t Apart from the work by Poesio et al. , the main other studies attempting quantitative evaluations of bridging reference resolution are \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t 6 Discussion and Conclusions The two main results of this study are , first of all , that combining \x92salience\x92 features with \x92lexical\x92 features leads to much better results than using either method in isolation ; and that these results are an improvement over those previously reported in the literature . \n\t', '\n\t\t A secondary , but still interesting , result is that using WordNet in a different way \x96taking advantage of its extensive information about hypernyms to obviate its lack of information about meronymy\x96obviates the problems previously reported in the literature on using WordNet for resolving mereological bridging references , leading to results comparable to those obtained using Google . \n\t', '\n\t\t ( Of course , from a practical perspective Google may still be preferrable , particularly for languages for which no WordNet exists . \n\t', '\n\t\t ) The main limitation of the present work is that the number of BDs and BRs considered , while larger than in our previous studies , is still fairly small . \n\t', '\n\t\t Unfortunately , creating a reasonably accurate gold standard for this type of semantic interpretation process is slow work . \n\t', '\n\t\t Our first priority will be therefore to extend the data set , including also the original cases studied by Poesio and Vieira . \n\t', '\n\t\t Current and future work will also include incorporating the methods tested here in an actual anaphora resolution system , the GUITAR system \n\t\t']",Positive
"['\n\t\t We are also working on methods for automatically recognizing bridging descriptions , and dealing with other types of ( non-associative ) bridging references based on synonymy and hyponymy . \n\t', '\n\t\t Acknowledgments The creation of the GNOME corpus was supported by the EPSRC project GNOME , GR/L51126/01 . \n\t', '\n\t\t References N. Asher and A. Lascarides . \n\t', '\n\t\t 1998. Bridging . \n\t', '\n\t\t Journal of Semantics , 15(1):83\x9613 . \n\t', '\n\t\t M. Berland and E. Charniak . \n\t', '\n\t\t 1999. Finding parts in very large corpora . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 37th ACL . \n\t', '\n\t\t H. H. Clark and C. J. Sengul . \n\t', '\n\t\t 1979. In search of referents for nouns and pronouns . \n\t', '\n\t\t Memory and Cognition , 7(1):35\x9641 . \n\t', '\n\t\t H. H. Clark . \n\t', '\n\t\t 1977. Bridging . \n\t', '\n\t\t In P. N. Johnson- Laird and P.C. Wason , editors , Thinking : Readings in Cognitive Science . \n\t', '\n\t\t Cambridge . \n\t', '\n\t\t C. Fellbaum , editor . \n\t', '\n\t\t 1998. WordNet : An electronic lexical database . \n\t', '\n\t\t The MIT Press . \n\t', '\n\t\t A. Garcia-Almanza . \n\t', '\n\t\t 2003. Using WordNet for mereological anaphora resolution . \n\t', '\n\t\t Master\x92s thesis , University of Essex . \n\t', '\n\t\t Lex Res Balance Perc on bal F on bal Perc on Hard F on Hard WN 1:1 70.2 % .7 80.2 % .2 1:3 75.9 % .4 91.7 % 0 Google 1:1 64.4 % .7 63.6 % .1 1.3 79.8 % .5 88.4 % .3 WN + 1:1 66.3 % .6 65.3 % .2 Google 1.3 77.9 % .4 92.5 % .5 Table 6 : Results using a classifier trained on balanced data on unbalanced ones . \n\t', '\n\t\t M. A. Gernsbacher and D. Hargreaves . \n\t', '\n\t\t 1988. Accessing sentence participants . \n\t', '\n\t\t Journal of Memory and Language , 27:699\x96717 . \n\t', '\n\t\t P. C. Gordon , B. J. Grosz , and L. A. Gillion . \n\t', '\n\t\t 1993 . \n\t', '\n\t\t Pronouns , names , and the centering of attention in discourse . \n\t', '\n\t\t Cognitive Science , 17:311\x96348 . \n\t', '\n\t\t G. Grefenstette . \n\t', '\n\t\t 1993. SEXTANT : extracting semantics from raw text . \n\t', '\n\t\t Heuristics . \n\t', '\n\t\t B. J. Grosz and C. L. Sidner . \n\t', '\n\t\t 1986. Attention , intention , and the structure of discourse . \n\t', '\n\t\t Computational Linguistics , 12(3):175\x96204 . \n\t', '\n\t\t B. J. Grosz , A. K. Joshi , and S. Weinstein . \n\t', '\n\t\t 1995 . \n\t', '\n\t\t Centering . \n\t', '\n\t\t Computational Linguistics , 21(2):202\x96225 . \n\t', '\n\t\t S. Harabagiu and D. Moldovan . \n\t', '\n\t\t 1998. Knowledge processing on extended WordNet . \n\t', '\n\t\t In \n\t\t', '\n\t\t M. A. Hearst . \n\t', '\n\t\t 1998. Automated discovery of Word- net relations . \n\t', '\n\t\t In \n\t\t', '\n\t\t J. R. Hobbs . \n\t', '\n\t\t 1978. Resolving pronoun references . \n\t', '\n\t\t Lingua , 44:311\x96338 . \n\t', '\n\t\t K. Humphreys , R. Gaizauskas , S. Azzam , C. Huyck , B. Mitchell , and H. Cunningham Y. Wilks . \n\t', '\n\t\t 1997. Description of the LaSIE-II System as used for MUC-7 . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 7th Message Understanding Conference ( MUC-7 ) . \n\t', '\n\t\t T. Ishikawa . \n\t', '\n\t\t 1998. Acquisition of associative information and resolution of bridging descriptions . \n\t', '\n\t\t Master\x92s thesis , University of Edinburgh . \n\t', '\n\t\t F. Keller and M. Lapata . \n\t', '\n\t\t 2003. Using the Web to obtain frequencies for unseen bigrams . \n\t', '\n\t\t Computational Linguistics , 29(3) . \n\t', '\n\t\t K. Lund , C. Burgess , and R. A. Atchley . \n\t', '\n\t\t 1995. Semantic and associative priming in high- dimensional semantic space . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 17th Conf . \n\t', '\n\t\t of the Cogn . \n\t', '\n\t\t Science Soc. , pages 660\x96665 . \n\t', '\n\t\t K. Markert , M. Strube , and U. Hahn . \n\t', '\n\t\t 1996. Inferential realization constraints on functional anaphora in the centering model . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of 18th Conf . \n\t', '\n\t\t of the Cog . \n\t', '\n\t\t Science Soc. , pages 609\x96614 . \n\t', '\n\t\t K. Markert , M. Nissim , and N .. Modjeska . \n\t', '\n\t\t 2003. Using the Web for nominal anaphora resolution . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the EACL Workshop on the Computational Treatment ofAnaphora , pages 39\x9646 . \n\t', '\n\t\t N. Modjeska , K. Markert , and M. Nissim . \n\t', '\n\t\t 2003. Using the Web in ML for anaphora resolution . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ofEMNLP-03 , pages 176\x96183 . \n\t', '\n\t\t V. Ng and C. Cardie . \n\t', '\n\t\t 2002. Improving machine learning approaches to coreference resolution . \n\t', '\n\t\t In Proceedings of the 40th Meeting of the ACL . \n\t', '\n\t\t M. Poesio and R. Vieira . \n\t', '\n\t\t 1998. A corpus-based investigation of definite description use . \n\t', '\n\t\t Computational Linguistics , 24(2):183\x96216 , June . \n\t', '\n\t\t M. Poesio , R. Vieira , and S. Teufel . \n\t', '\n\t\t 1997. Resolving bridging references in unrestricted text . \n\t', '\n\t\t In R. Mitkov , editor , Proc . \n\t', '\n\t\t of the ACL Workshop on Robust Anaphora Resolution , pages 1\x966 , Madrid . \n\t', '\n\t\t M. Poesio , S. Schulte im Walde , and C. Brew . \n\t', '\n\t\t 1998. Lexical clustering and definite description interpretation . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the AAAI Spring Symposium on Learning for Discourse , pages 82\x9689 . \n\t', '\n\t\t M. Poesio , T. Ishikawa , S. Schulte im Walde , and R. Vieira . \n\t', '\n\t\t 2002 . \n\t', '\n\t\t Acquiring lexical knowledge for anaphora resolution . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 3rd LREC . \n\t', '\n\t\t M. Poesio and M. Alexandrov-Kabadjov . \n\t', '\n\t\t 2004. A general-purpose , off the shelf anaphoric resolver . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 4th LREC , Lisbon . \n\t', '\n\t\t M. Poesio , R. Stevenson , B. Di Eugenio , and J. M. Hitzeman . \n\t', '\n\t\t 2004 . \n\t', '\n\t\t Centering : A parametric theory and its instantiations . \n\t', '\n\t\t Comp . \n\t', '\n\t\t Linguistics . \n\t', '\n\t\t 30(3). M. Poesio . \n\t', '\n\t\t 2003. Associative descriptions and salience . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the EACL Workshop on Computational Treatments ofAnaphora . \n\t', '\n\t\t E. F. Prince . \n\t', '\n\t\t 1981. Toward a taxonomy of given- new information . \n\t', '\n\t\t In P. Cole , editor , Radical Pragmatics , pages 223\x96256 . \n\t', '\n\t\t Academic Press . \n\t', '\n\t\t C. L. Sidner . \n\t', '\n\t\t 1979. Towards a computational theory of definite anaphora comprehension in English discourse . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , MIT . \n\t', '\n\t\t O. Uryupina . \n\t', '\n\t\t 2003. High-precision identification of discourse-new and unique noun phrases . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ofACL 2003 Stud . \n\t', '\n\t\t Workshop , pages 80\x9686 . \n\t', '\n\t\t R. Vieira and M. Poesio . \n\t', '\n\t\t 2000. An empirically- based system for processing definite descriptions . \n\t', '\n\t\t Computational Linguistics , 26(4) , December . \n\t', '\n\t\t Learning Noun Phrase Anaphoricity to Improve Coreference Resolution : Issues in Representation and Optimization Vincent Ng Department of Computer Science Cornell University Ithaca , NY 14853-7501 yung@cs.cornell.edu Abstract Knowledge of the anaphoricity of a noun phrase might be profitably exploited by a coreference system to bypass the resolution of non-anaphoric noun phrases . \n\t', '\n\t\t Perhaps surprisingly , recent attempts to incorporate automatically acquired anaphoricity information into coreference systems , however , have led to the degradation in resolution performance . \n\t', '\n\t\t This paper examines several key issues in computing and using anaphoricity information to improve learning-based coreference systems . \n\t', '\n\t\t In particular , we present a new corpus-based approach to anaphoricity determination . \n\t', '\n\t\t Experiments on three standard coreference data sets demonstrate the effectiveness of our approach . \n\t', '\n\t\t 1 Introduction Noun phrase coreference resolution , the task of determining which noun phrases ( NPs ) in a text refer to the same real-world entity , has long been considered an important and difficult problem in natural language processing . \n\t', '\n\t\t Identifying the linguistic constraints on when two NPs can co-refer remains an active area of research in the community . \n\t', '\n\t\t One significant constraint on coreference , the non-anaphoricity constraint , specifies that a non- anaphoric NP cannot be coreferent with any of its preceding NPs in a given text . \n\t', '\n\t\t Given the potential usefulness of knowledge of (non-)anaphoricity for coreference resolution , anaphoricity determination has been studied fairly extensively . \n\t', '\n\t\t One common approach involves the design of heuristic rules to identify specific types of (non-)anaphoric NPs such as pleonastic pronouns ( e.g. , \n\t\t']",Positive
"['\n\t\t More recently , the problem has been tackled using unsupervised ( e.g. , \n\t\t']",Positive
"['\n\t\t Interestingly , existing machine learning ap proaches to coreference resolution have performed reasonably well without anaphoricity determination ( e.g. , \n\t\t']",Positive
"['\n\t\t Nevertheless , there is empirical evidence that resolution systems might further be improved with anaphoricity information . \n\t', '\n\t\t For instance , our coreference system mistakenly identifies an antecedent for many non-anaphoric common nouns in the absence of anaphoricity information \n\t\t']",Negative
"['\n\t\t Our goal in this paper is to improve learning- based coreference systems using automatically computed anaphoricity information . \n\t', '\n\t\t In particular , we examine two important , yet largely unexplored , issues in anaphoricity determination for coreference resolution : representation and optimization . \n\t', '\n\t\t Constraint-based vs. feature-based representation . \n\t', '\n\t\t How should the computed anaphoricity information be used by a coreference system ? \n\t', '\n\t\t From a linguistic perspective , knowledge of nonanaphoricity is most naturally represented as \x93bypassing\x94 constraints , with which the coreference system bypasses the resolution of NPs that are determined to be non-anaphoric . \n\t', '\n\t\t But for learning-based coreference systems , anaphoricity information can be simply and naturally accommodated into the machine learning framework by including it as a feature in the instance representation . \n\t', '\n\t\t Local vs. global optimization . \n\t', '\n\t\t Should the anaphoricity determination procedure be developed independently of the coreference system that uses the computed anaphoricity information ( local optimization ) , or should it be optimized with respect to coreference performance ( global optimization ) ? \n\t', '\n\t\t The principle of software modularity calls for local optimization . \n\t', '\n\t\t However , if the primary goal is to improve coreference performance , global optimization appears to be the preferred choice . \n\t', '\n\t\t Existing work on anaphoricity determination for anaphora/coreference resolution can be characterized along these two dimensions . \n\t', '\n\t\t Interestingly , most existing work employs constraint- based , locally-optimized methods ( e.g. , \n\t\t']",Positive
"['\n\t\t In particular , to our knowledge , there have been no attempts to ( 1 ) globally optimize an anaphoricity determination procedure for coreference performance and ( 2 ) incorporate anaphoricity into coreference systems as a feature . \n\t', '\n\t\t Consequently , as part of our investigation , we propose a new corpus-based method for achieving global optimization and experiment with representing anaphoricity as a feature in the coreference system . \n\t', '\n\t\t In particular , we systematically evaluate all four combinations of local vs. global optimization and constraint-based vs. feature-based representation of anaphoricity information in terms of their effectiveness in improving a learning-based coreference system . \n\t', '\n\t\t Results on three standard coreference data sets are somewhat surprising : our proposed globally-optimized method , when used in conjunction with the constraint-based representation , outperforms not only the commonly-adopted locally- optimized approach but also its seemingly more natural feature-based counterparts . \n\t', '\n\t\t The rest of the paper is structured as follows . \n\t', '\n\t\t Section 2 focuses on optimization issues , discussing locally- and globally-optimized approaches to anaphoricity determination . \n\t', '\n\t\t In Section 3 , we give an overview of the standard machine learning framework for coreference resolution . \n\t', '\n\t\t Sections 4 and 5 present the experimental setup and evaluation results , respectively . \n\t', '\n\t\t We examine the features that are important to anaphoricity determination in Section 6 and conclude in Section 7 . \n\t', '\n\t\t 2 The Anaphoricity Determination System : Local vs. . \n\t', '\n\t\t Global Optimization In this section , we will show how to build a model of anaphoricity determination . \n\t', '\n\t\t We will first present the standard , locally-optimized approach and then introduce our globally-optimized approach . \n\t', '\n\t\t 2.1 The Locally-Optimized Approach In this approach , the anaphoricity model is simply a classifier that is trained and optimized independently of the coreference system ( e.g. , \n\t\t']",Positive
"['\n\t\t Building a classifier for anaphoricity determination . \n\t', '\n\t\t A learning algorithm is used to train a classifier that , given a description of an NP in a document , decides whether or not the NP is anaphoric . \n\t', '\n\t\t Each training instance represents a single NP and consists of a set of features that are potentially useful for distinguishing anaphoric and non-anaphoric NPs . \n\t', '\n\t\t The classification associated with a training instance \x97 one of ANAPHORIC or NOT ANAPHORIC \x97 is derived from coreference chains in the training documents . \n\t', '\n\t\t Specifically , a positive instance is created for each NP that is involved in a coreference chain but is not the head of the chain . \n\t', '\n\t\t A negative instance is created for each of the remaining NPs . \n\t', '\n\t\t Applying the classifier . \n\t', '\n\t\t To determine the anaphoricity of an NP in a test document , an instance is created for it as during training and presented to the anaphoricity classifier , which returns a value of ANAPHORIC or NOT ANAPHORIC . \n\t', '\n\t\t 2.2 The Globally-Optimized Approach To achieve global optimization , we construct a parametric anaphoricity model with which we optimize the parameter1 for coreference accuracy on held- out development data . \n\t', '\n\t\t In other words , we tighten the connection between anaphoricity determination and coreference resolution by using the parameter to generate a set of anaphoricity models from which we select the one that yields the best coreference performance on held-out data . \n\t', '\n\t\t Global optimization for a constraint-based representation . \n\t', '\n\t\t We view anaphoricity determination as a problem of determining how conservative an anaphoricity model should be in classifying an NP as (non-)anaphoric . \n\t', '\n\t\t Given a constraint-based representation of anaphoricity information for the coreference system , if the model is too liberal in classifying an NP as non-anaphoric , then many anaphoric NPs will be misclassified , ultimately leading to a deterioration of recall and of the overall performance of the coreference system . \n\t', '\n\t\t On the other hand , if the model is too conservative , then only a small fraction of the truly non-anaphoric NPs will be identified , and so the resulting anaphoricity information may not be effective in improving the coreference system . \n\t', '\n\t\t The challenge then is to determine a \x93good\x94 degree of conservativeness . \n\t', '\n\t\t As a result , we can design a parametric anaphoricity model whose conservativeness can be adjusted via a conservativeness parameter . \n\t', '\n\t\t To achieve global optimization , we can simply tune this parameter to optimize for coreference performance on held-out development data . \n\t', '\n\t\t Now , to implement this conservativeness-based anaphoricity determination model , we propose two methods , each of which is built upon a different definition of conservativeness . \n\t', '\n\t\t Method 1 : Varying the Cost Ratio Our first method exploits a parameter present in many off-the-shelf machine learning algorithms for 1We can introduce multiple parameters for this purpose , but to simply the optimization process , we will only consider single-parameter models in this paper . \n\t', '\n\t\t training a classifier \x97 the cost ratio ( cr ) , which is defined as follows . \n\t', '\n\t\t cost of misclassifying a positive instance cost of misclassifying a negative instance Inspection of this definition shows that cr provides a means of adjusting the relative misclassification penalties placed on training instances of different classes . \n\t', '\n\t\t In particular , the larger cr is , the more conservative the classifier is in classifying an instance as negative ( i.e. , non-anaphoric ) . \n\t', '\n\t\t Given this observation , we can naturally define the conservativeness of an anaphoricity classifier as follows . \n\t', '\n\t\t We say that classifier A is more conservative than classifier B in determining an NP as non-anaphoric if A is trained with a higher cost ratio than B . \n\t', '\n\t\t Based on this definition of conservativeness , we can construct an anaphoricity model parameterized by cr . \n\t', '\n\t\t Specifically , the parametric model maps a given value of cr to the anaphoricity classifier trained with this cost ratio . \n\t', '\n\t\t ( For the purpose of training anaphoricity classifiers with different values of cr , we use RIPPER \n\t\t']",Positive
"['\n\t\t ) It should be easy to see that increasing cr makes the model more conservative in classifying an NP as non-anaphoric . \n\t', '\n\t\t With this parametric model , we can tune cr to optimize for coreference performance on held-out data . \n\t', '\n\t\t Method 2 : Varying the Classification Threshold We can also define conservativeness in terms of the number of NPs classified as non-anaphoric for a given set of NPs . \n\t', '\n\t\t Specifically , given two anaphoricity models A and B and a set of instances I to be classified , we say that A is more conservative than B in determining an NP as non-anaphoric if A classifies fewer instances in I as non-anaphoric than B . \n\t', '\n\t\t Again , this definition is consistent with our intuition regarding conservativeness . \n\t', '\n\t\t We can now design a parametric anaphoricity model based on this definition . \n\t', '\n\t\t First , we train in a supervised fashion a probablistic model of anaphoricity PA ( c I i ) , where i is an instance representing an NP and c is one of the two possible anaphoricity values . \n\t', '\n\t\t ( In our experiments , we use maximum entropy classification ( MaxEnt ) \n\t\t']",Positive
"['\n\t\t ) Then , we can construct a parametric model making binary anaphoricity decisions from PA by introducing a threshold parameter t as follows . \n\t', '\n\t\t Given a specific t ( 0 < t < 1 ) and a new instance i , we define an anaphoricity model MtA in which MtA(i) = NOT ANAPHORIC if and only if PA(c = NOT ANAPHORIC I i ) > t . \n\t', '\n\t\t It should be easy to see that increasing t yields progressively more conservative anaphoricity models . \n\t', '\n\t\t Again , t can be tuned using held-out development data . \n\t', '\n\t\t Global optimization for a feature-based representation . \n\t', '\n\t\t We can similarly optimize our proposed conservativeness-based anaphoricity model for coreference performance when anaphoricity information is represented as a feature for the coreference system . \n\t', '\n\t\t Unlike in a constraint-based representation , however , we cannot expect that the recall of the coreference system would increase with the conservativeness parameter . \n\t', '\n\t\t The reason is that we have no control over whether or how the anaphoricity feature is used by the coreference learner . \n\t', '\n\t\t In other words , the behavior of the coreference system is less predictable in comparison to a constraint-based representation . \n\t', '\n\t\t Other than that , the conservativeness- based anaphoricity model is as good to use for global optimization with a feature-based representation as with a constraint-based representation . \n\t', '\n\t\t We conclude this section by pointing out that the locally-optimized approach to anaphoricity determination is indeed a special case of the global one . \n\t', '\n\t\t Unlike the global approach in which the conservativeness parameter values are tuned based on labeled data , the local approach uses \x93default\x94 parameter values . \n\t', '\n\t\t For instance , when RIPPER is used to train an anaphoricity classifier in the local approach , cr is set to the default value of one . \n\t', '\n\t\t Similarly , when probabilistic anaphoricity decisions generated via a MaxEnt model are converted to binary anaphoricity decisions for subsequent use by a coreference system , t is set to the default value of 0.5 . \n\t', '\n\t\t 3 The Machine Learning Framework for Coreference Resolution The coreference system to which our automatically computed anaphoricity information will be applied implements the standard machine learning approach to coreference resolution combining classification and clustering . \n\t', '\n\t\t Below we will give a brief overview of this standard approach . \n\t', '\n\t\t Details can be found in \n\t\t']",Positive
"['\n\t\t Training an NP coreference classifier . \n\t', '\n\t\t After a pre-processing step in which the NPs in a document are automatically identified , a learning algorithm is used to train a classifier that , given a description of two NPs in the document , decides whether they are COREFERENT or NOT COREFERENT . \n\t', '\n\t\t Applying the classifier to create coreference chains . \n\t', '\n\t\t Test texts are processed from left to right . \n\t', '\n\t\t Each NP encountered , NPj , is compared in turn to each preceding NP , NPi . \n\t', '\n\t\t For each pair , a test instance is created as during training and is presented cr := to the learned coreference classifier , which returns a number between 0 and 1 that indicates the likelihood that the two NPs are coreferent . \n\t', '\n\t\t The NP with the highest coreference likelihood value among the preceding NPs with coreference class values above 0.5 is selected as the antecedent of NPj ; otherwise , no antecedent is selected for NPj . \n\t', '\n\t\t 4 Experimental Setup In Section 2 , we examined how to construct locally- and globally-optimized anaphoricity models . \n\t', '\n\t\t Recall that , for each of these two types of models , the resulting (non-)anaphoricity information can be used by a learning-based coreference system either as hard bypassing constraints or as a feature . \n\t', '\n\t\t Hence , given a coreference system that implements the two- step learning approach shown above , we will be able to evaluate the four different combinations of computing and using anaphoricity information for improving the coreference system described in the introduction . \n\t', '\n\t\t Before presenting evaluation details , we will describe the experimental setup . \n\t', '\n\t\t Coreference system . \n\t', '\n\t\t In all of our experiments , we use our learning-based coreference system \n\t\t']",Positive
"['\n\t\t Features for anaphoricity determination . \n\t', '\n\t\t In both the locally-optimized and the globally- optimized approaches to anaphoricity determination described in Section 2 , an instance is represented by 37 features that are specifically designed for distinguishing anaphoric and non-anaphoric NPs . \n\t', '\n\t\t Space limitations preclude a description of these features ; see \n\t\t']",Positive
"['\n\t\t Learning algorithms . \n\t', '\n\t\t For training coreference classifiers and locally-optimized anaphoricity models , we use both RIPPER and MaxEnt as the underlying learning algorithms . \n\t', '\n\t\t However , for training globally-optimized anaphoricity models , RIPPER is always used in conjunction with Method 1 and Max- Ent with Method 2 , as described in Section 2.2 . \n\t', '\n\t\t In terms of setting learner-specific parameters , we use default values for all RIPPER parameters unless otherwise stated . \n\t', '\n\t\t For MaxEnt , we always train the feature-weight parameters with 100 iterations of the improved iterative scaling algorithm \n\t\t']",Positive
"['\n\t\t Data sets . \n\t', '\n\t\t We use the Automatic Content Extraction ( ACE ) Phase II data sets.2 We choose ACE rather than the more widely-used MUC corpus ( MUC-6 , 1995 ; MUC-7 , 1998 ) simply because 2See http://www.itl.nist.gov/iad/894.01/ tests /ace for details on the ACE research program . \n\t', '\n\t\t BNEWS NPAPER NWIRE Number of training texts 216 76 130 Number of test texts 51 17 29 Number of training insts ( for anaphoricity ) 20567 21970 27338 Number of training insts ( for coreference ) 97036 148850 122168 Table 1 : Statistics of the three ACE data sets ACE provides much more labeled data for both training and testing . \n\t', '\n\t\t However , our system was set up to perform coreference resolution according to the MUC rules , which are fairly different from the ACE guidelines in terms of the identification of markables as well as evaluation schemes . \n\t', '\n\t\t Since our goal is to evaluate the effect of anaphoricity information on coreference resolution , we make no attempt to modify our system to adhere to the rules specifically designed for ACE . \n\t', '\n\t\t The coreference corpus is composed of three data sets made up of three different news sources : Broadcast News ( BNEWS ) , Newspaper ( NPAPER ) , and Newswire ( NWIRE ) . \n\t', '\n\t\t Statistics collected from these data sets are shown in Table 1 . \n\t', '\n\t\t For each data set , we train an anaphoricity classifier and a coreference classifier on the ( same ) set of training texts and evaluate the coreference system on the test texts . \n\t', '\n\t\t 5 Evaluation In this section , we will compare the effectiveness of four approaches to anaphoricity determination ( see the introduction ) in improving our baseline coreference system . \n\t', '\n\t\t 5.1 Coreference Without Anaphoricity As mentioned above , we use our coreference system as the baseline system where no explicit anaphoricity determination system is employed . \n\t', '\n\t\t Results using RIPPER and MaxEnt as the underlying learners are shown in rows 1 and 2 of Table 2 where performance is reported in terms of recall , precision , and F-measure using the model-theoretic MUC scoring program \n\t\t']",Positive
"['\n\t\t With RIPPER , the system achieves an F-measure of 56.3 for BNEWS , 61.8 for NPAPER , and 51.7 for NWIRE . \n\t', '\n\t\t The performance of MaxEnt is comparable to that of RIPPER for the BNEWS and NPAPER data sets but slightly worse for the NWIRE data set . \n\t', '\n\t\t 5.2 Coreference With Anaphoricity The Constraint-Based , Locally-Optimized ( CBLO ) Approach . \n\t', '\n\t\t As mentioned before , in constraint-based approaches , the automatically computed non-anaphoricity information is used as System Variation Experiments L R BNEWS P F C R NPAPER P F C R NWIRE P F C No RIP 57.4 55.3 56.3 - 60.0 63.6 61.8 - 53.2 50.3 51.7 - Anaphoricity ME 60.9 52.1 56.2 - 65.4 58.6 61.8 - 54.9 46.7 50.4 - Constraint- RIP 42.5 77.2 54.8 cr=1 46.7 79.3 58.81 cr=1 42.1 64.2 50.9 cr=1 Based , RIP 45.4 72.8 55.9 t=0.5 52.2 75.9 61.9 t=0.5 36.9 61.5 46.11 t=0.5 Locally- ME 44.4 76.9 56.3 cr=1 50.1 75.7 60.3 cr=1 43.9 63.0 51.7 cr=1 Optimized ME 47.3 70.8 56.7 t=0.5 57.1 70.6 63.1* t=0.5 38.1 60.0 46.61 t=0.5 Feature- RIP 53.5 61.3 57.2 cr=1 58.7 69.7 63.7* cr=1 54.2 46.8 50.21 cr=1 Based , RIP 58.3 58.3 58.3* t=0.5 63.5 57.0 60.11 t=0.5 63.4 35.3 45.31 t=0.5 Locally- ME 59.6 51.6 55.31 cr=1 65.6 57.9 61.5 cr=1 55.1 46.2 50.3 cr=1 Optimized ME 59.6 51.6 55.31 t=0.5 66.0 57.7 61.6 t=0.5 54.9 46.7 50.4 t=0.5 Constraint- RIP 54.5 68.6 60.8* cr=5 58.4 68.8 63.2* cr=4 50.5 56.7 53.4* cr=3 Based , RIP 54.1 67.1 59.9* t=0.7 56.5 68.1 61.7 t=0.65 50.3 53.8 52.0 t=0.7 Globally- ME 54.8 62.9 58.5* cr=5 62.4 65.6 64.0* cr=3 52.2 57.0 54.5* cr=3 Optimized ME 54.1 60.6 57.2 t=0.7 61.7 64.0 62.8* t=0.7 52.0 52.8 52.4* t=0.7 Feature- RIP 60.8 56.1 58.4* cr=8 62.2 61.3 61.7 cr=6 54.6 49.4 51.9 cr=8 Based , RIP 59.7 57.0 58.3* t=0.6 63.6 59.1 61.3 t=0.8 56.7 48.4 52.3 t=0.7 Globally- ME 59.9 51.0 55.11 cr=9 66.5 57.1 61.4 cr=1 56.3 46.9 51.2* cr=10 Optimized ME 59.6 51.6 55.31 t=0.95 65.9 57.5 61.4 t=0.95 56.5 46.7 51.1* t=0.5 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Table 2 : Results of the coreference systems using different approaches to anaphoricity determination on the three ACE test data sets . \n\t', '\n\t\t Information on which Learner ( RIPPER or MaxEnt ) is used to train the coreference classifier , as well as performance results in terms of Recall , Precision , F-measure and the corresponding Conservativeness parameter are provided whenever appropriate . \n\t', '\n\t\t The strongest result obtained for each data set is boldfaced . \n\t', '\n\t\t In addition , results that represent statistically significant gains and drops with respect to the baseline are marked with an asterisk ( * ) and a dagger ( t ) , respectively . \n\t', '\n\t\t hard bypassing constraints , with which the coreference system attempts to resolve only NPs that the anaphoricity classifier determines to be anaphoric . \n\t', '\n\t\t As a result , we hypothesized that precision would increase in comparison to the baseline system . \n\t', '\n\t\t In addition , we expect that recall will drop owing to the anaphoricity classifier\x92s misclassifications of truly anaphoric NPs . \n\t', '\n\t\t Consequently , overall performance is not easily predictable : F-measure will improve only if gains in precision can compensate for the loss in recall . \n\t', '\n\t\t Results are shown in rows 3-6 of Table 2 . \n\t', '\n\t\t Each row corresponds to a different combination of learners employed in training the coreference and anaphoricity classifiers.3 As mentioned in Section 2.2 , locally-optimized approaches are a special case of their globally-optimized counterparts , with the conservativeness parameter set to the default value of one for RIPPER and 0.5 for MaxEnt . \n\t', '\n\t\t In comparison to the baseline , we see large gains in precision at the expense of recall . \n\t', '\n\t\t Moreover , CBLO does not seem to be very effective in improving the baseline , in part due to the dramatic loss in recall . \n\t', '\n\t\t In particular , although we see improvements in F-measure in five of the 12 experiments in this group , only one of them is statistically significant.4 3Bear in mind that different learners employed in training anaphoricity classifiers correspond to different parametric methods . \n\t', '\n\t\t For ease of exposition , however , we will refer to the method simply by the learner it employs . \n\t', '\n\t\t 4The Approximate Randomization test described in Noreen Worse still , F-measure drops significantly in three cases . \n\t', '\n\t\t The Feature-Based , Locally-Optimized ( FBLO ) Approach . \n\t', '\n\t\t The experimental setting employed here is essentially the same as that in CBLO , except that anaphoricity information is incorporated into the coreference system as a feature rather than as constraints . \n\t', ""\n\t\t Specifically , each training/test coreference instance i(NPi,NP') ( created from NPj and a preceding NP NPi ) is augmented with a feature whose value is the anaphoricity of NPj as computed by the anaphoricity classifier . \n\t"", '\n\t\t In general , we hypothesized that FBLO would perform better than the baseline : the addition of an anaphoricity feature to the coreference instance representation might give the learner additional flexibility in creating coreference rules . \n\t', '\n\t\t Similarly , we expect FBLO to outperform its constraint-based counterpart : since anaphoricity information is represented as a feature in FBLO , the coreference learner can incorporate the information selectively rather than as universal hard constraints . \n\t', '\n\t\t Results using the FBLO approach are shown in rows 7-10 of Table 2 . \n\t', '\n\t\t Somewhat unexpectedly , this approach is not effective in improving the baseline : F-measure increases significantly in only two of the 12 cases . \n\t', '\n\t\t Perhaps more surprisingly , we see significant drops in F-measure in five cases . \n\t', '\n\t\t To get a bet- ( 1989 ) is applied to determine if the differences in the F- measure scores between two coreference systems are statistically significant at the 0.05 level or higher . \n\t', '\n\t\t System Variation BNEWS ( dev ) NPAPER ( dev ) NWIRE ( dev ) Experiments L R P F C R P F C R P F C Constraint- RIP 62.6 76.3 68.8 cr=5 65.5 73.0 69.1 cr=4 56.1 58.9 57.4 cr=3 Based , RIP 62.5 75.5 68.4 t=0.7 63.0 71.7 67.1 t=0.65 56.7 54.8 55.7 t=0.7 Globally- ME 63.1 71.3 66.9 cr=5 66.2 71.8 68.9 cr=3 57.9 59.7 58.8 cr=3 Optimized ME 62.9 70.8 66.6 t=0.7 61.4 74.3 67.3 t=0.65 58.4 55.3 56.8 t=0.7 1 2 3 4 Table 3 : Results of the coreference systems using a constraint-based , globally-optimized approach to anaphoricity determination on the three ACE held-out development data sets . \n\t', '\n\t\t Information on which Learner ( RIPPER or MaxEnt ) is used to train the coreference classifier as well as performance results in terms of Recall , Precision , F-measure and the corresponding Conservativeness parameter are provided whenever appropriate . \n\t', '\n\t\t The strongest result obtained for each data set is boldfaced . \n\t', '\n\t\t ter idea of why F-measure decreases , we examine the relevant coreference classifiers induced by RIPPER . \n\t', '\n\t\t We find that the anaphoricity feature is used in a somewhat counter-intuitive manner : some of the induced rules posit a coreference relationship between NPR and a preceding NP NPi even though NPR is classified as non-anaphoric . \n\t', '\n\t\t These results seem to suggest that the anaphoricity feature is an irrelevant feature from a machine learning point of view . \n\t', '\n\t\t In comparison to CBLO , the results are mixed : there does not appear to be a clear winner in any of the three data sets . \n\t', '\n\t\t Nevertheless , it is worth noticing that the CBLO systems can be characterized as having high precision/low recall , whereas the reverse is true for FBLO systems in general . \n\t', '\n\t\t As a result , even though CBLO and FBLO systems achieve similar performance , the former is the preferred choice in applications where precision is critical . \n\t', '\n\t\t Finally , we note that there are other ways to encode anaphoricity information in a coreference system . \n\t', '\n\t\t For instance , it is possible to represent anaphoricity as a real-valued feature indicating the probability of an NP being anaphoric rather than as a binary-valued feature . \n\t', '\n\t\t Future work will examine alternative encodings of anaphoricity . \n\t', '\n\t\t The Constraint-Based , Globally-Optimized ( CBGO ) Approach . \n\t', '\n\t\t As discussed above , we optimize the anaphoricity model for coreference performance via the conservativeness parameter . \n\t', '\n\t\t In particular , we will use this parameter to maximize the F-measure score for a particular data set and learner combination using held-out development data . \n\t', '\n\t\t To ensure a fair comparison between global and local approaches , we do not rely on additional development data in the former ; instead we use 23 of the original training texts for acquiring the anaphoricity and coreference classifiers and the remaining 3s for development for each of the data sets . \n\t', '\n\t\t As far as parameter tuning is concerned , we tested values of 1 , 2 , ... , 10 as well as their reciprocals for cr and 0.05 , 0 . \n\t', '\n\t\t 1 , ... , 1.0 for t . \n\t', '\n\t\t In general , we hypothesized that CBGO would outperform both the baseline and the locally- optimized approaches , since coreference performance is being explicitly maximized . \n\t', '\n\t\t Results using CBGO , which are shown in rows 11-14 of Table 2 , are largely consistent with our hypothesis . \n\t', '\n\t\t The best results on all of the three data sets are achieved using this approach . \n\t', '\n\t\t In comparison to the baseline , we see statistically significant gains in F-measure in nine of the 12 experiments in this group . \n\t', '\n\t\t Improvements stem primarily from large gains in precision accompanied by smaller drops in recall . \n\t', '\n\t\t Perhaps more importantly , CBGO never produces results that are significantly worse than those of the baseline systems on these data sets , unlike CBLO and FBLO . \n\t', '\n\t\t Overall , these results suggest that CBGO is more robust than the locally-optimized approaches in improving the baseline system . \n\t', '\n\t\t As can be seen , CBGO fails to produce statistically significant improvements over the baseline in three cases . \n\t', '\n\t\t The relatively poorer performance in these cases can potentially be attributed to the underlying learner combination . \n\t', '\n\t\t Fortunately , we can use the development data not only for parameter tuning but also in predicting the best learner combination . \n\t', '\n\t\t Table 3 shows the performance of the coreference system using CBGO on the development data , along with the value of the conservativeness parameter used to achieve the results in each case . \n\t', '\n\t\t Using the notation Learners/Learner2 to denote the fact that Learners and Learner2 are used to train the underlying coreference classifier and anaphoricity classifier respectively , we can see that the RIPPER/RIPPER combination achieves the best performance on the BNEWS development set , whereas MaxEnt/RIPPER works best for the other two . \n\t', '\n\t\t Hence , if we rely on the development data to pick the best learner combination for use in testing , the resulting coreference system will outperform the baseline in all three data sets and yield the best- performing system on all but the NPAPER data sets , achieving an F-measure of 60.8 ( row 11 ) , 63.2 ( row 11 ) , and 54.5 ( row 13 ) for the BNEWS , NPAPER , cr Figure 1 : Effect of cr on the performance of the coreference system for the NPAPER development data using RIPPER/RIPPER and NWIRE data sets , respectively . \n\t', '\n\t\t Moreover , the high correlation between the relative coreference performance achieved by different learner combinations on the development data and that on the test data also reflects the stability of CBGO . \n\t', '\n\t\t In comparison to the locally-optimized approaches , CBGO achieves better F-measure scores in almost all cases . \n\t', '\n\t\t Moreover , the learned conservativeness parameter in CBGO always has a larger value than the default value employed by CBLO . \n\t', '\n\t\t This provides empirical evidence that the CBLO anaphoricity classifiers are too liberal in classifying NPs as non-anaphoric . \n\t', '\n\t\t To examine the effect of the conservativeness parameter on the performance of the coreference system , we plot in Figure 1 the recall , precision , F- measure curves against cr for the NPAPER development data using the RIPPER/RIPPER learner combination . \n\t', '\n\t\t As cr increases , recall rises and precision drops . \n\t', '\n\t\t This should not be surprising , since ( 1 ) increasing cr causes fewer anaphoric NPs to be misclassified and allows the coreference system to find a correct antecedent for some of them , and ( 2 ) decreasing cr causes more truly non-anaphoric NPs to be correctly classified and prevents the coreference system from attempting to resolve them . \n\t', '\n\t\t The best F-measure in this case is achieved when cr=4 . \n\t', '\n\t\t The Feature-Based , Globally-Optimized ( FBGO ) Approach . \n\t', '\n\t\t The experimental setting employed here is essentially the same as that in the CBGO setting , except that anaphoricity information is incorporated into the coreference system as a feature rather than as constraints . \n\t', '\n\t\t Specifically , each training/test instance i(NPi,NPj) is augmented with a feature whose value is the computed anaphoricity of NPj . \n\t', '\n\t\t The development data is used to select the anaphoricity model ( and hence the parameter value ) that yields the best-performing coreference system . \n\t', '\n\t\t This model is then used to compute the anaphoricity value for the test instances . \n\t', '\n\t\t As mentioned before , we use the same parametric anaphoricity model as in CBGO for achieving global optimization . \n\t', '\n\t\t Since the parametric model is designed with a constraint-based representation in mind , we hypothesized that global optimization in this case would not be as effective as in CBGO . \n\t', '\n\t\t Nevertheless , we expect that this approach is still more effective in improving the baseline than the locally-optimized approaches . \n\t', '\n\t\t Results using FBGO are shown in rows 15-18 of Table 2 . \n\t', '\n\t\t As expected , FBGO is less effective than CBGO in improving the baseline , underperforming its constraint-based counterpart in 11 of the 12 cases . \n\t', '\n\t\t In fact , FBGO is able to significantly improve the corresponding baseline in only four cases . \n\t', '\n\t\t Somewhat surprisingly , FBGO is by no means superior to the locally-optimized approaches with respect to improving the baseline . \n\t', '\n\t\t These results seem to suggest that global optimization is effective only if we have a \x93good\x94 parameterization that is able to take into account how anaphoricity information will be exploited by the coreference system . \n\t', '\n\t\t Nevertheless , as discussed before , effective global optimization with a feature-based representation is not easy to accomplish . \n\t', '\n\t\t 6 Analyzing Anaphoricity Features So far we have focused on computing and using anaphoricity information to improve the performance of a coreference system . \n\t', '\n\t\t In this section , we examine which anaphoricity features are important in order to gain linguistic insights into the problem . \n\t', '\n\t\t Specifically , we measure the informativeness of a feature by computing its information gain ( see p.22 of \n\t\t']",Positive
"['\n\t\t Overall , the most informative features are HEAD MATCH ( whether the NP under consideration has the same head as one of its preceding NPs ) , STR MATCH ( whether the NP under consideration is the same string as one of its preceding NPs ) , and PRONOUN ( whether the NP under consideration is a pronoun ) . \n\t', '\n\t\t The high discriminating power of HEAD MATCH and STR MATCH is a probable consequence of the fact that an NP is likely to be anaphoric if there is a lexically similar noun phrase preceding it in the text . \n\t', '\n\t\t The informativeness of PRONOUN can also be 85 80 75 70 65 60 55 50 1 2 3 4 5 6 7 8 9 10 Recall Precision F^measure expected : most pronominal NPs are anaphoric . \n\t', '\n\t\t Features that determine whether the NP under consideration is a PROPER NOUN , whether it is a BARE SINGULAR or a BARE PLURAL , and whether it begins with an \x93a\x94 or a \x93the\x94 ( ARTICLE ) are also highly informative . \n\t', '\n\t\t This is consistent with our intuition that the (in)definiteness of an NP plays an important role in determining its anaphoricity . \n\t', '\n\t\t 7 Conclusions We have examined two largely unexplored issues in computing and using anaphoricity information for improving learning-based coreference systems : representation and optimization . \n\t', '\n\t\t In particular , we have systematically evaluated all four combinations of local vs. global optimization and constraint-based vs. feature-based representation of anaphoricity information in terms of their effectiveness in improving a learning-based coreference system . \n\t', '\n\t\t Extensive experiments on the three ACE coreference data sets using a symbolic learner ( RIPPER ) and a statistical learner ( MaxEnt ) for training coreference classifiers demonstrate the effectiveness of the constraint-based , globally-optimized approach to anaphoricity determination , which employs our conservativeness-based anaphoricity model . \n\t', '\n\t\t Not only does this approach improve a \x93no anaphoricity\x94 baseline coreference system , it is more effective than the commonly-adopted locally-optimized approach without relying on additional labeled data . \n\t', '\n\t\t Acknowledgments We thank Regina Barzilay , Claire Cardie , Bo Pang , and the anonymous reviewers for their invaluable comments on earlier drafts of the paper . \n\t', '\n\t\t This work was supported in part by NSF Grant IIS\x960208028 . \n\t', '\n\t\t References David Bean and Ellen Riloff . \n\t', '\n\t\t 1999. Corpus-based identification of non-anaphoric noun phrases . \n\t', '\n\t\t In Proceedings of the ACL , pages 373\x96380 . \n\t', '\n\t\t Adam L. Berger , Stephen A. Della Pietra , and Vincent J. Della Pietra . \n\t', '\n\t\t 1996. A maximum entropy approach to natural language processing . \n\t', '\n\t\t Computational Linguistics , 22(1):39\x9671 . \n\t', '\n\t\t Stanley Chen and Ronald Rosenfeld . \n\t', '\n\t\t 2000. A survey of smoothing techniques for ME models . \n\t', '\n\t\t IEEE Transac- tions on Speech on Audio Processing , 8(1):37\x9650 . \n\t', '\n\t\t William Cohen . \n\t', '\n\t\t 1995. Fast effective rule induction . \n\t', '\n\t\t In Proceedings ofICML . \n\t', '\n\t\t Stephen Della Pietra , Vincent Della Pietra , and John Lafferty . \n\t', '\n\t\t 1997. Inducing features of random fields . \n\t', '\n\t\t IEEE Transactions on Pattern Analysis and Machine Intelligence , 19(4):380\x96393 . \n\t', '\n\t\t Michel Denber . \n\t', '\n\t\t 1998. Automatic resolution of anaphora in English . \n\t', '\n\t\t Technical report , Eastman Kodak Co. . \n\t', '\n\t\t Richard Evans . \n\t', '\n\t\t 2001. Applying machine learning toward an automatic classification of it . \n\t', '\n\t\t Literary and Linguistic Computing , 16(1):45\x9657 . \n\t', '\n\t\t Christopher Kennedy and Branimir Boguraev . \n\t', '\n\t\t 1996. Anaphor for everyone : Pronominal anaphora resolution without a parser . \n\t', '\n\t\t In Proceedings of COLING , pages 113\x96118 . \n\t', '\n\t\t Shalom Lappin and Herbert Leass . \n\t', '\n\t\t 1994. An algorithm for pronominal anaphora resolution . \n\t', '\n\t\t Computational Linguistics , 20(4):535\x96562 . \n\t', '\n\t\t Ruslan Mitkov , Richard Evans , and Constantin Orasan . \n\t', '\n\t\t 2002. A new , fully automatic version of Mitkov\x92s knowledge-poor pronoun resolution method . \n\t', '\n\t\t In Al . \n\t', '\n\t\t Gelbukh , editor , Computational Linguistics and Intelligent Text Processing , pages 169\x96187 . \n\t', '\n\t\t MUC-6 . \n\t', '\n\t\t 1995. Proceedings of the Sixth Message Understanding Conference ( MUC-6 ) . \n\t', '\n\t\t MUC-7 . \n\t', '\n\t\t 1998. Proceedings of the Seventh Message Understanding Conference ( MUC-7 ) . \n\t', '\n\t\t Vincent Ng and Claire Cardie . \n\t', '\n\t\t 2002a . \n\t', '\n\t\t Identifying anaphoric and non-anaphoric noun phrases to improve coreference resolution . \n\t', '\n\t\t In Proceedings of COLING , pages 730\x96736 . \n\t', '\n\t\t Vincent Ng and Claire Cardie . \n\t', '\n\t\t 2002b . \n\t', '\n\t\t Improving machine learning approaches to coreference resolution . \n\t', '\n\t\t In Proceedings of the ACL , pages 104\x96111 . \n\t', '\n\t\t Eric W. Noreen . \n\t', '\n\t\t 1989. Computer Intensive Methods for Testing Hypothesis : An Introduction . \n\t', '\n\t\t John Wiley & Sons . \n\t', '\n\t\t Chris Paice and Gareth Husk . \n\t', '\n\t\t 1987. Towards the automatic recognition of anaphoric features in English text : the impersonal pronoun \x92it\x92 . \n\t', '\n\t\t Computer Speech and Language , 2 . \n\t', '\n\t\t J. Ross Quinlan . \n\t', '\n\t\t 1993. C4.5 : Programs for Machine Learning . \n\t', '\n\t\t San Mateo , CA : Morgan Kaufmann. Wee Meng Soon , Hwee Tou Ng , and Daniel Chung Yong Lim . \n\t', '\n\t\t 2001. A machine learning approach to coreference resolution of noun phrases . \n\t', '\n\t\t Computational Linguistics , 27(4):521\x96544 . \n\t', '\n\t\t Michael Strube and Christoph M¨uller . \n\t', '\n\t\t 2003. A machine learning approach to pronoun resolution in spoken dialogue . \n\t', '\n\t\t In Proceedings of the ACL , pages 168\x96175 . \n\t', '\n\t\t Renata Vieira and Massimo Poesio . \n\t', '\n\t\t 2000. An empirically-based system for processing definite descriptions . \n\t', '\n\t\t Computational Linguistics , 26(4):539\x96 593 . \n\t', '\n\t\t Marc Vilain , John Burger , John Aberdeen , Dennis Connolly , and Lynette Hirschman . \n\t', '\n\t\t 1995. A model- theoretic coreference scoring scheme . \n\t', '\n\t\t In Proceedings of the Sixth Message Understanding Conference ( MUC-6 ) , pages 45\x9652 . \n\t', '\n\t\t Xiaofeng Yang , Guodong Zhou , Jian Su , and Chew Lim Tan . \n\t', '\n\t\t 2003. Coreference resolution using competitive learning approach . \n\t', '\n\t\t In Proceedings of the ACL , pages 176\x96183 . \n\t', '\n\t\t A Joint Source-Channel Model for Machine Transliteration Li Haizhou , Zhang Min , Su Jian Institute for Infocomm Research 21 Heng Mui Keng Terrace , Singapore 119613 {hli,sujian,mzhang}@i2r.a-star.edu.sg Abstract Most foreign names are transliterated into Chinese , Japanese or Korean with approximate phonetic equivalents . \n\t', '\n\t\t The transliteration is usually achieved through intermediate phonemic mapping . \n\t', '\n\t\t This paper presents a new framework that allows direct orthographical mapping ( DOM ) between two different languages , through a joint source-channel model , also called n-gram transliteration model ( TM ) . \n\t', '\n\t\t With the n-gram TM model , we automate the orthographic alignment process to derive the aligned transliteration units from a bilingual dictionary . \n\t', '\n\t\t The n-gram TM under the DOM framework greatly reduces system development effort and provides a quantum leap in improvement in transliteration accuracy over that of other state-of-the-art machine learning algorithms . \n\t', '\n\t\t The modeling framework is validated through several experiments for English-Chinese language pair . \n\t', '\n\t\t 1 Introduction In applications such as cross-lingual information retrieval ( CLIR ) and machine translation , there is an increasing need to translate out-of-vocabulary words from one language to another , especially from alphabet language to Chinese , Japanese or Korean . \n\t', '\n\t\t Proper names of English , French , German , Russian , Spanish and Arabic origins constitute a good portion of out-of-vocabulary words . \n\t', '\n\t\t They are translated through transliteration , the method of translating into another language by preserving how words sound in their original languages . \n\t', '\n\t\t For writing foreign names in Chinese , transliteration always follows the original romanization . \n\t', '\n\t\t Therefore , any foreign name will have only one Pinyin ( romanization of Chinese ) and thus in Chinese characters . \n\t', '\n\t\t In this paper , we focus on automatic Chinese transliteration of foreign alphabet names . \n\t', '\n\t\t Because some alphabet writing systems use various diacritical marks , we find it more practical to write names containing such diacriticals as they are rendered in English . \n\t', '\n\t\t Therefore , we refer all foreign-Chinese transliteration to English-Chinese transliteration , or E2C . \n\t', '\n\t\t Transliterating English names into Chinese is not straightforward . \n\t', '\n\t\t However , recalling the original from Chinese transliteration is even more challenging as the E2C transliteration may have lost some original phonemic evidences . \n\t', '\n\t\t The Chinese-English backward transliteration process is also called back-transliteration , or C2E ( Knight & Graehl , 1998 ) . \n\t', '\n\t\t In machine transliteration , the noisy channel model ( NCM ) , based on a phoneme-based approach , has recently received considerable attention ( Meng et al . 2001 ; Jung et al , 2000 ; Virga & Khudanpur , 2003 ; Knight & Graehl , 1998 ) . \n\t', '\n\t\t In this paper we discuss the limitations of such an approach and address its problems by firstly proposing a paradigm that allows direct orthographic mapping ( DOM ) , secondly further proposing a joint source-channel model as a realization of DOM . \n\t', '\n\t\t Two other machine learning techniques , NCM and ID3 \n\t\t']",Positive
"['\n\t\t This paper is organized as follows : In section 2 , we present the transliteration problems . \n\t', '\n\t\t In section 3 , a joint source-channel model is formulated . \n\t', '\n\t\t In section 4 , several experiments are carried out to study different aspects of proposed algorithm . \n\t', '\n\t\t In section 5 , we relate our algorithms to other reported work . \n\t', '\n\t\t Finally , we conclude the study with some discussions . \n\t', '\n\t\t 2 Problems in transliteration Transliteration is a process that takes a character string in source language as input and generates a character string in the target language as output . \n\t', '\n\t\t The process can be seen conceptually as two levels of decoding : segmentation of the source string into transliteration units ; and relating the source language transliteration units with units in the target language , by resolving different combinations of alignments and unit mappings . \n\t', '\n\t\t A unit could be a Chinese character or a monograph , a digraph or a trigraph and so on for English . \n\t', '\n\t\t 2.1 Phoneme-based approach The problems of English-Chinese transliteration have been studied extensively in the paradigm of noisy channel model ( NCM ) . \n\t', '\n\t\t For a given English name E as the observed channel output , one seeks a posteriori the most likely Chinese transliteration C that maximizes P(CI E ) . \n\t', '\n\t\t Applying Bayes rule , it means to find C to maximize P(E , C ) = P(E | C)*P(C) ( 1 ) with equivalent effect . \n\t', '\n\t\t To do so , we are left with modeling two probability distributions : P(EI C ) , the probability of transliterating C to E through a noisy channel , which is also called transformation rules , and P(C) , the probability distribution of source , which reflects what is considered good Chinese transliteration in general . \n\t', '\n\t\t Likewise , in C2E back- transliteration , we would find E that maximizes P(E , C ) = P(C | E)*P(E) ( 2 ) for a given Chinese name . \n\t', '\n\t\t In eqn ( 1 ) and ( 2 ) , P(C) and P(E) are usually estimated using n-gram language models \n\t\t']",Positive
"['\n\t\t Inspired by research results of grapheme-tophoneme research in speech synthesis literature , many have suggested phoneme-based approaches to resolving P(EIC) and P(CIE) , which approximates the probability distribution by introducing a phonemic representation . \n\t', '\n\t\t In this way , we convert the names in the source language , say E , into an intermediate phonemic representation P , and then convert the phonemic representation into the target language , say Chinese C. In E2C transliteration , the phoneme-based approach can be formulated as P(CIE) = P(CIP)P(PIE) and conversely we have P(EI C ) = P(EIP)P(PI C ) for C2E back-transliteration . \n\t', '\n\t\t Several phoneme-based techniques have been proposed in the recent past for machine transliteration using transformation-based learning algorithm ( Meng et al . 2001 ; Jung et al , 2000 ; Virga & Khudanpur , 2003 ) and using finite state transducer that implements transformation rules ( Knight & Graehl , 1998 ) , where both handcrafted and data-driven transformation rules have been studied . \n\t', '\n\t\t However , the phoneme-based approaches are limited by two major constraints , which could compromise transliterating precision , especially in English-Chinese transliteration : 1 ) Latin-alphabet foreign names are of different origins . \n\t', '\n\t\t For instance , French has different phonic rules from those of English . \n\t', '\n\t\t The phoneme-based approach requires derivation of proper phonemic representation for names of different origins . \n\t', '\n\t\t One may need to prepare multiple language-dependent grapheme-to-phoneme ( G2P ) conversion systems accordingly , and that is not easy to achieve \n\t\t']",Positive
"['\n\t\t For example , /Lafontant/ is transliterated into t,t4-)ft(La-FengTang) while /Constant/ becomes *W ( KangSi-Tan-Te ) , where syllable /-tant/ in the two names are transliterated differently depending on the names\x92 language of origin . \n\t', '\n\t\t 2 ) Suppose that language dependent graphemeto-phoneme systems are attainable , obtaining Chinese orthography will need two further steps : a ) conversion from generic phonemic representation to Chinese Pinyin ; b ) conversion from Pinyin to Chinese characters . \n\t', '\n\t\t Each step introduces a level of imprecision . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t Unlike Japanese katakana or Korean alphabet , Chinese characters are more ideographic than phonetic . \n\t', '\n\t\t To arrive at an appropriate Chinese transliteration , one cannot rely solely on the intermediate phonemic representation . \n\t', '\n\t\t 2.2 Useful orthographic context To illustrate the importance of contextual information in transliteration , let\x92s take name /Minahan/ as an example , the correct segmentation should be /Mi-na-han/ , to be transliterated as )K- 44-a ( Pinyin : Mi-Na-Han ) . \n\t', '\n\t\t English /mi- -na- -han/ Chinese )K 44 a Pinyin Mi Nan Han However , a possible segmentation /Min-ah-an/ could lead to an undesirable syllabication of HA- RaJ-_c ( Pinyin : Min-A-An ) . \n\t', '\n\t\t English /min- -ah- -an/ Chinese HA RaJ _c Pinyin Min A An According to the transliteration guidelines , a wise segmentation can be reached only after exploring the combination of the left and right context of transliteration units . \n\t', '\n\t\t From the computational point of view , this strongly suggests using a contextual n-gram as the knowledge base for the alignment decision . \n\t', '\n\t\t Another example will show us how one-to-many mappings could be resolved by context . \n\t', '\n\t\t Let\x92s take another name /Smith/ as an example . \n\t', '\n\t\t Although we can arrive at an obvious segmentation /s-mi-th/ , there are three Chinese characters for each of /s-/ , /-mi-/ and /-th/ . \n\t', '\n\t\t Furthermore , /s-/ and /-th/ correspond to overlapping characters as well , as shown next . \n\t', '\n\t\t English /s- -mi- -th/ Chinese 1 ^ ^ ^ Chinese 2 ^ ^ ^ Chinese 3 ^ ^ ^ A human translator will use transliteration rules between English syllable sequence and Chinese character sequence to obtain the best mapping ^^-^ , as indicated in italic in the table above . \n\t', '\n\t\t To address the issues in transliteration , we propose a direct orthographic mapping ( DOM ) framework through a joint source-channel model by fully exploring orthographic contextual information , aiming at alleviating the imprecision introduced by the multiple-step phoneme-based approach . \n\t', '\n\t\t 3 Joint source-channel model In view of the close coupling of the source and target transliteration units , we propose to estimate P(E,C) by a joint source-channel model , or n-gram transliteration model ( TM ) . \n\t', '\n\t\t For K aligned transliteration units , we have ) = P(< e , c >1 , < e , c > 2 ...<e,c>K)(3) K ^P(<e,c>k|<e,c>i^1) k=1 which provides an alternative to the phoneme- based approach for resolving eqn . \n\t', '\n\t\t ( 1 ) and ( 2 ) by eliminating the intermediate phonemic representation . \n\t', '\n\t\t Unlike the noisy-channel model , the joint source-channel model does not try to capture how source names can be mapped to target names , but rather how source and target names can be generated simultaneously . \n\t', '\n\t\t In other words , we estimate a joint probability model that can be easily marginalized in order to yield conditional probability models for both transliteration and back-transliteration . \n\t', '\n\t\t Suppose that we have an English name ^ = x1x2...x,n and a Chinese transliteration ^ = y1y2 . \n\t', '\n\t\t . \n\t', '\n\t\t .yn where xi are letters and yj are Chinese characters . \n\t', '\n\t\t Oftentimes , the number of letters is different from the number of Chinesecharacters . \n\t', '\n\t\t A Chinese character may correspond to a letter substring in English or vice versa . \n\t', '\n\t\t x1 x2x3 ... xix+1xi+2 ... x,n n where there exists an alignment ^ with < e , c >1=< x1 , y1 > <e,c>2=<x2x3,y2 > ... and < e , c >K=< x,n , yn > . \n\t', '\n\t\t A transliteration unit correspondence < e , c > is called a transliteration pair . \n\t', '\n\t\t Then , the E2C transliteration can be formulated as ^ = arg max P(^ , ^ , ^ ) ( 4 ) ^,^ and similarly the C2E back-transliteration as ^ = argmax P(^ , ^,^ ) ( 5 ) ^,^ An n-gram transliteration model is defined as the conditional probability , or transliteration probability , of a transliteration pair < e , c >k depending on its immediate n predecessor pairs : P(E , C ) = P(^ , ^ , ^ K k 1 ) ( 6 ) k=1 P(E , C ) = P(e1 , e2 ... eK , c1 , c2 ...cK 3.1 Transliteration alignment A bilingual dictionary contains entries mapping English names to their respective Chinese transliterations . \n\t', '\n\t\t Like many other solutions in computational linguistics , it is possible to automatically analyze the bilingual dictionary to acquire knowledge in order to map new English names to Chinese and vice versa . \n\t', '\n\t\t Based on the transliteration formulation above , a transliteration model can be built with transliteration unit\x92s n- gram statistics . \n\t', '\n\t\t To obtain the statistics , the bilingual dictionary needs to be aligned . \n\t', '\n\t\t The maximum likelihood approach , through EM algorithm \n\t\t']",Positive
"['\n\t\t . \n\t', '\n\t\t .y j ...y ) such an alignment easily as described in the table below . \n\t', '\n\t\t K P(^,^,^) ^ ^ P(ek | ck )P(ck | ck^1 ) ( 8 ) k=1 The Expectation-Maximization algorithm 1 . \n\t', '\n\t\t Bootstrap initial random alignment 2 . \n\t', '\n\t\t Expectation : Update n-gram statistics to estimate probability distribution 3. Maximization : Apply the n-gram TM to obtain new alignment 4 . \n\t', '\n\t\t Go to step 2 until the alignment converges 5 . \n\t', '\n\t\t Derive a list transliteration units from final alignment as transliteration table The aligning process is different from that of transliteration given in eqn . \n\t', '\n\t\t ( 4 ) or ( 5 ) in that , here we have fixed bilingual entries , ^ and ^ . \n\t', '\n\t\t The aligning process is just to find the alignment segmentation ^ between the two strings that maximizes the joint probability : ^ = arg max P(^ , ^ , ^ ^ A set of transliteration pairs that is derived from the aligning process forms a transliteration table , which is in turn used in the transliteration decoding . \n\t', '\n\t\t As the decoder is bounded by this table , it is important to make sure that the training database covers as much as possible the potential transliteration patterns . \n\t', '\n\t\t Here are some examples of resulting alignment pairs . \n\t', '\n\t\t ^|s ^|l ^|t ^|d ^|k ^|b ^|g ^ |r ^ |ll ^|c ^|ro ^|ri ^|man ^|m ^ |p ^|de ^|ra ^ |le ^|a ^|ber ^|la ^|son ^|ton ^|tt ^ |re ^|co ^|o ^|e ^|ma ^|ley ^|li ^|mer Knowing that the training data set will never be sufficient for every n-gram unit , different smoothing approaches are applied , for example , by using backoff or class-based models , which can be found in statistical language modeling literatures \n\t\t']",Positive
"['\n\t\t 3.2 DOM : n-gram TM vs. NCM Although in the literature , most noisy channel models ( NCM ) are studied under phoneme-based paradigm for machine transliteration , NCM can also be realized under direct orthographic mapping ( DOM ) . \n\t', '\n\t\t Next , let\x92s look into a bigram case to see what n-gram TM and NCM present to us . \n\t', '\n\t\t For E2C conversion , re-writing eqn ( 1 ) and eqn ( 6 ) , we have K P(^ , ^ , ^ ) ^ ^ P( < e , c >k |< e , c >k^1 ) ( 9 ) k 1 The formulation of eqn . \n\t', '\n\t\t ( 8 ) could be interpreted as a hidden Markov model with Chinese characters as its hidden states and English transliteration units as the observations \n\t\t']",Positive
"['\n\t\t The number of parameters in the bigram TM is potentially T2 , while in the noisy channel model ( NCM ) it\x92s T+ C2 , where T is the number of transliteration pairs and C is the number of Chinese transliteration units . \n\t', '\n\t\t In eqn . \n\t', '\n\t\t ( 9 ) , the current transliteration depends on both Chinese and English transliteration history while in eqn . \n\t', '\n\t\t ( 8 ) , it depends only on the previous Chinese unit . \n\t', '\n\t\t As T2 >>T+ C2 , an n-gram TM gives a finer description than that of NCM . \n\t', '\n\t\t The actual size of models largely depends on the availability of training data . \n\t', '\n\t\t In Table 1 , one can get an idea of how they unfold in a real scenario . \n\t', '\n\t\t With adequately sufficient training data , n-gram TM is expected to outperform NCM in the decoding . \n\t', '\n\t\t A perplexity study in section 4.1 will look at the model from another perspective . \n\t', '\n\t\t 4 The experiments1 We use a database from the bilingual dictionary \x93Chinese Transliteration of Foreign Personal Names\x94 which was edited by Xinhua News Agency and was considered the de facto standard of personal name transliteration in today\x92s Chinese press . \n\t', '\n\t\t The database includes a collection of 37,694 unique English entries and their official Chinese transliteration . \n\t', '\n\t\t The listing includes personal names of English , French , Spanish , German , Arabic , Russian and many other origins . \n\t', '\n\t\t The database is initially randomly distributed into 13 subsets . \n\t', '\n\t\t In the open test , one subset is withheld for testing while the remaining 12 subsets are used as the training materials . \n\t', '\n\t\t This process is repeated 13 times to yield an average result , which is called the 13-fold open test . \n\t', '\n\t\t After experiments , we found that each of the 13-fold open tests gave consistent error rates with less than 1 % deviation . \n\t', '\n\t\t Therefore , for simplicity , we randomly select one of the 13 subsets , which consists of 2896 entries , as the standard open test set to report results . \n\t', '\n\t\t In the close test , all data entries are used for training and testing . \n\t', '\n\t\t 1 demo at http://nlp.i2r.a-star.edu.sg/demo.htm ) ( 7 ) 4.1 Modeling The alignment of transliteration units is done fully automatically along with the n-gram TM training process . \n\t', '\n\t\t To model the boundary effects , we introduce two extra units <s> and </s> for start and end of each name in both languages . \n\t', '\n\t\t The EM iteration converges at 8th round when no further alignment changes are reported . \n\t', '\n\t\t Next are some statistics as a result of the model training : # close set bilingual entries ( full data ) 37,694 # unique Chinese transliteration ( close ) 28,632 # training entries for open test 34,777 # test entries for open test 2,896 # unique transliteration pairs T 5,640 # total transliteration pairs WT 119,364 # unique English units E 3,683 # unique Chinese units C 374 # bigram TM P(< e , c >k|< e , c >k^1 ) 38,655 # NCM Chinese bigram P(ck | ck^1 ) 12,742 Table 1 . \n\t', '\n\t\t Modeling statistics The most common metric for evaluating an n- gram model is the probability that the model assigns to test data , or perplexity \n\t\t']",Positive
"['\n\t\t For a test set W composed of V names , where each name has been aligned into a sequence of transliteration pair tokens , we can calculate the probability of test set V p(W) = ^P(^v , ^v,^v ) by applying the n-gram v=1 models to the token sequence . \n\t', '\n\t\t The cross-entropy Hp ( W ) of a model on data W is defined as Hp ( W ) = ^ 1 log2 p(W WT number of aligned transliteration pair tokens in the data W . \n\t', '\n\t\t The perplexity PPp ( W ) of a model is the reciprocal of the average probability assigned by the model to each aligned pair in the test set W as PPp ( W ) = 2 H ( W ) . \n\t', '\n\t\t Clearly , lower perplexity means that the model describes better the data . \n\t', '\n\t\t It is easy to understand that closed test always gives lower perplexity than open test . \n\t', '\n\t\t TM open NCM open TM closed NCM closed 1-gram 670 729 655 716 2-gram 324 512 151 210 3-gram 306 487 68 127 Table 2 . \n\t', '\n\t\t Perplexity study of bilingual database We have the perplexity reported in Table 2 on the aligned bilingual dictionary , a database of 119,364 aligned tokens . \n\t', '\n\t\t The NCM perplexity is computed using n-gram equivalents of eqn . \n\t', '\n\t\t ( 8 ) for E2C transliteration , while TM perplexity is based on those of eqn ( 9 ) which applies to both E2C and C2E . \n\t', '\n\t\t It is shown that TM consistently gives lower perplexity than NCM in open and closed tests . \n\t', '\n\t\t We have good reason to expect TM to provide better transliteration results which we expect to be confirmed later in the experiments . \n\t', '\n\t\t The Viterbi algorithm produces the best sequence by maximizing the overall probability , P(^ , ^ , ^ ) . \n\t', '\n\t\t In CLIR or multilingual corpus alignment \n\t\t']",Positive
"['\n\t\t In this paper , we adopted an N-best stack decoder \n\t\t']",Positive
"['\n\t\t The algorithm also allows us to apply higher order n-gram such as trigram in the search . \n\t', '\n\t\t 4.2 E2C transliteration In this experiment , we conduct both open and closed tests for TM and NCM models under DOM paradigm . \n\t', '\n\t\t Results are reported in Table 3 and Table 4. open ( word ) open ( char ) closed ( word ) closed ( char ) 1-gram 45.6 % 21.1 % 44.8 % 20.4 % 2-gram 31.6 % 13.6 % 10.8 % 4.7 % 3-gram 29.9 % 10.8 % 1.6 % 0.8 % Table 3 . \n\t', '\n\t\t E2C error rates for n-gram TM tests . \n\t', '\n\t\t open ( word ) open ( char ) closed ( word ) closed ( char ) 1-gram 47.3 % 23.9 % 46.9 % 22.1 % 2-gram 39.6 % 20.0 % 16.4 % 10.9 % 3-gram 39.0 % 18.8 % 7.8 % 1.9 % Table 4 . \n\t', '\n\t\t E2C error rates for n-gram NCM tests In word error report , a word is considered correct only if an exact match happens between transliteration and the reference . \n\t', '\n\t\t The character error rate is the sum of deletion , insertion and where WT is the total ) substitution errors . \n\t', '\n\t\t Only the top choice in N-best results is used for error rate reporting . \n\t', '\n\t\t Not surprisingly , one can see that n-gram TM , which benefits from the joint source-channel model coupling both source and target contextual information into the model , is superior to NCM in all the test cases . \n\t', '\n\t\t 4.3 C2E back-transliteration The C2E back-transliteration is more challenging than E2C transliteration . \n\t', '\n\t\t Not many studies have been reported in this area . \n\t', '\n\t\t It is common that multiple English names are mapped into the same Chinese transliteration . \n\t', '\n\t\t In Table 1 , we see only 28,632 unique Chinese transliterations exist for 37,694 English entries , meaning that some phonemic evidence is lost in the process of transliteration . \n\t', '\n\t\t To better understand the task , let\x92s compare the complexity of the two languages presented in the bilingual dictionary . \n\t', '\n\t\t Table 1 also shows that the 5,640 transliteration pairs are cross mappings between 3,683 English and 374 Chinese units . \n\t', '\n\t\t In order words , on average , for each English unit , we have 1.53 = 5,640/3,683 Chinese correspondences . \n\t', '\n\t\t In contrast , for each Chinese unit , we have 15.1 = 5,640/374 English back-transliteration units ! \n\t', '\n\t\t Confusion is increased tenfold going backward . \n\t', '\n\t\t The difficulty of back-transliteration is also reflected by the perplexity of the languages as in Table 5 . \n\t', '\n\t\t Based on the same alignment tokenization , we estimate the monolingual language perplexity for Chinese and English independently using the n-gram language models I Ck^n+1 ) and P(ek I ek^n+1 ) P(ck Without surprise , Chinese names have much lower perplexity than English names thanks to fewer Chinese units . \n\t', '\n\t\t This contributes to the success of E2C but presents a great challenge to C2E back- transliteration . \n\t', '\n\t\t 1-gram 2-gram 3-gram Chinese 207/206 97/86 79/45 English 710/706 265/152 234/67 Table 5 language perplexity comparison ( open/closed test ) open ( word ) open ( letter ) closed ( word ) closed ( letter ) 1 gram 82.3 % 28.2 % 81 % 27.7 % 2 gram 63.8 % 20.1 % 40.4 % 12.3 % 3 gram 62.1 % 19.6 % 14.7 % 5.0 % Table 6 . \n\t', '\n\t\t C2E error rate for n-gram TM tests E2C open E2C closed C2E open C2E closed 1-best 29.9 % 1.6 % 62.1 % 14.7 % 5-best 8.2 % 0.94 % 43.3 % 5.2 % 10-best 5.4 % 0.90 % 24.6 % 4.8 % Table 7 . \n\t', '\n\t\t N-best word error rates for 3-gram TM tests A back-transliteration is considered correct if it falls within the multiple valid orthographically correct options . \n\t', '\n\t\t Experiment results are reported in Table 6 . \n\t', '\n\t\t As expected , C2E error rate is much higher than that of E2C . \n\t', '\n\t\t In this paper , the n-gram TM model serves as the sole knowledge source for transliteration . \n\t', '\n\t\t However , if secondary knowledge , such as a lookup table of valid target transliterations , is available , it can help reduce error rate by discarding invalid transliterations top-down the N choices . \n\t', '\n\t\t In Table 7 , the word error rates for both E2C and C2E are reported which imply potential error reduction by secondary knowledge source . \n\t', '\n\t\t The N-best error rates are reduced significantly at 10-best level as reported in Table 7 . \n\t', '\n\t\t 5 Discussions It would be interesting to relate n-gram TM to other related framework . \n\t', '\n\t\t 5.1 DOM : n-gram TM vs. ID3 In section 4 , one observes that contextual information in both source and target languages is essential . \n\t', '\n\t\t To capture them in the modeling , one could think of decision tree , another popular machine learning approach . \n\t', '\n\t\t Under the DOM framework , here is the first attempt to apply decision tree in E2C and C2E transliteration . \n\t', '\n\t\t With the decision tree , given a fixed size learning vector , we used top-down induction trees to predict the corresponding output . \n\t', '\n\t\t Here we implement ID3 \n\t\t']",Positive
"['\n\t\t Similar to n-gram TM , for unseen names in open test , ID3 has backoff smoothing , which lies on the default case which returns the most probable value as its best guess for a partial tree path according to the learning set . \n\t', '\n\t\t In the case of E2C transliteration , we form a learning vector of 6 attributes by combining 2 left and 2 right letters around the letter of focus ek and 1 previous Chinese unit ck^1 . \n\t', '\n\t\t The process is illustrated in Table 8 , where both English and Chinese contexts are used to infer a Chinese character . \n\t', '\n\t\t Similarly , 4 attributes combining 1 left , 1 centre and 1 right Chinese character and 1 previous English unit are used for the learning vector in C2E test . \n\t', '\n\t\t An aligned bilingual dictionary is needed to build the decision tree . \n\t', '\n\t\t To minimize the effects from alignment variation , we use the same alignment results from section 4 . \n\t', '\n\t\t Two trees are built for two directions , E2C and C2E . \n\t', '\n\t\t The results are compared with those 3-gram TM in Table 9. ek^2 ek^1 ek ek+1 ek+2 ck^1 ck _ _ N I C _ > ^ _ N I C E ^ > _ N I C E _ __ > ^ I C E _ _ ^ > _ Table 8 . \n\t', '\n\t\t E2C transliteration using ID3 decision tree for transliterating Nice to ^ ^ ( ^ | NI ^ | CE ) open closed ID3 E2C 39.1 % 9.7 % 3-gram TM E2C 29.9 % 1.6 % ID3 C2E 63.3 % 38.4 % 3-gram TM C2E 62.1 % 14.7 % Table 9 . \n\t', '\n\t\t Word error rate ID3 vs. 3-gram TM One observes that n-gram TM consistently outperforms ID3 decision tree in all tests . \n\t', '\n\t\t Three factors could have contributed : 1 ) English transliteration unit size ranges from 1 letter to 7 letters . \n\t', '\n\t\t The fixed size windows in ID3 obviously find difficult to capture the dynamics of various ranges . \n\t', '\n\t\t n-gram TM seems to have better captured the dynamics of transliteration units ; 2 ) The backoff smoothing of n-gram TM is more effective than that of ID3 ; 3 ) Unlike n-gram TM , ID3 requires a separate aligning process for bilingual dictionary . \n\t', '\n\t\t The resulting alignment may not be optimal for tree construction . \n\t', '\n\t\t Nevertheless , ID3 presents another successful implementation of DOM framework . \n\t', '\n\t\t 5.2 DOM vs. phoneme-based approach Due to lack of standard data sets , it is difficult to compare the performance of the n-gram TM to that of other approaches . \n\t', '\n\t\t For reference purpose , we list some reported studies on other databases of E2C transliteration tasks in Table 10 . \n\t', '\n\t\t As in the references , only character and Pinyin error rates are reported , we only include our character and Pinyin error rates for easy reference . \n\t', '\n\t\t The reference data are extracted from Table 1 and 3 of \n\t\t']",Positive
"['\n\t\t As we have not found any C2E result in the literature , only E2C results are compared here . \n\t', '\n\t\t The first 4 setups by Virga et al all adopted the phoneme-based approach in the following steps : 1 ) English name to English phonemes ; 2 ) English phonemes to Chinese Pinyin ; 3 ) Chinese Pinyin to Chinese characters . \n\t', '\n\t\t It is obvious that the n-gram TM compares favorably to other techniques . \n\t', '\n\t\t n-gram TM presents an error reduction of 74.6%=(42.5-10.8)/42.5 % for Pinyin over the best reported result , Huge MT ( Big MT ) test case , which is noteworthy . \n\t', '\n\t\t The DOM framework shows a quantum leap in performance with n-gram TM being the most successful implementation . \n\t', '\n\t\t The n-gram TM and ID3 under direct orthographic mapping ( DOM ) paradigm simplify the process and reduce the chances of conversion errors . \n\t', '\n\t\t As a result , n-gram TM and ID3 do not generate Chinese Pinyin as intermediate results . \n\t', '\n\t\t It is noted that in the 374 legitimate Chinese characters for transliteration , character to Pinyin mapping is unique while Pinyin to character mapping could be one to many . \n\t', '\n\t\t Since we have obtained results in character already , we expect less Pinyin error than character error should a character-to-Pinyin mapping be needed . \n\t', '\n\t\t System Trainin g size Test size Pinyin errors Char errors Meng et al 2,233 1,541 52.5 % N/A Small MT 2,233 1,541 50.8 % 57.4 % Big MT 3,625 250 49.1 % 57.4 % Huge MT 309,01 3,122 42.5 % N/A ( Big MT ) 9 3-gram 34,777 2,896 < 10.8 % 10.8 % TM/DOM ID3/DOM 34,777 2,896 < 15.6 % 15.6 % Table 10 . \n\t', '\n\t\t Performance reference in recent studies 6 Conclusions In this paper , we propose a new framework ( DOM ) for transliteration . \n\t', '\n\t\t n-gram TM is a successful realization of DOM paradigm . \n\t', '\n\t\t It generates probabilistic orthographic transformation rules using a data driven approach . \n\t', '\n\t\t By skipping the intermediate phonemic interpretation , the transliteration error rate is reduced significantly . \n\t', '\n\t\t Furthermore , the bilingual aligning process is integrated into the decoding process in n-gram TM , which allows us to achieve a joint optimization of alignment and transliteration automatically . \n\t', '\n\t\t Unlike other related work where pre-alignment is needed , the new framework greatly reduces the development efforts of machine transliteration systems . \n\t', '\n\t\t Although the framework is implemented on an English-Chinese personal name data set , without loss of generality , it well applies to transliteration of other language pairs such as English/Korean and English/Japanese . \n\t', '\n\t\t It is noted that place and company names are sometimes translated in combination of transliteration and meanings , for example , /Victoria-Fall/ becomes ff ~ f1 R 4 ~ti ( Pinyin:Wei Duo Li Ya Pu Bu ) . \n\t', '\n\t\t As the proposed framework allows direct orthographical mapping , it can also be easily extended to handle such name translation . \n\t', '\n\t\t We expect to see the proposed model to be further explored in other related areas . \n\t', '\n\t\t References Dempster , A.P. , N.M. Laird and D.B.Rubin , 1977 . \n\t', '\n\t\t Maximum likelihood from incomplete data via the EM algorithm , J. Roy . \n\t', '\n\t\t Stat . \n\t', '\n\t\t Soc. , Ser . \n\t', '\n\t\t B. Vol. 39 , pp138 Helen M. Meng , Wai-Kit Lo , Berlin Chen and Karen Tang . \n\t', '\n\t\t 2001. Generate Phonetic Cognates to Handle Name Entities in English-Chinese cross-language spoken document retrieval , ASRU 2001 Jelinek , F. 1991 , Self-organized language modeling for speech recognition , In Waibel , A. and Lee K.F. ( eds ) , Readings in Speech Recognition , Morgan Kaufmann. , San Mateo , CA K. Knight and J. Graehl . \n\t', '\n\t\t 1998. Machine Transliteration , Computational Linguistics 24(4) Paola Virga , Sanjeev Khudanpur , 2003 . \n\t', '\n\t\t Transliteration of Proper Names in Cross- lingual Information Retrieval . \n\t', '\n\t\t ACL 2003 workshop MLNER Quinlan J. R. 1993 , C4.5 Programs for machine learning , Morgan Kaufmann , San Mateo , CA Rabiner , Lawrence R. 1989 , A tutorial on hidden Markov models and selected applications in speech recognition , Proceedings of the IEEE 77(2) Schwartz , R. and Chow Y. L. , 1990 , The N-best algorithm : An efficient and Exact procedure for finding the N most likely sentence hypothesis , Proceedings of ICASSP 1990 , Albuquerque , pp 81-84 Sung Young Jung , Sung Lim Hong and Eunok Paek , 2000 , An English to Korean Transliteration Model of Extended Markov Window , Proceedings of COLING The Onomastica Consortium , 1995 . \n\t', '\n\t\t The Onomastica interlanguage pronunciation lexicon , Proceedings of EuroSpeech , Madrid , Spain , Vol. 1 , pp829-832 Xinhua News Agency , 1992 , Chinese transliteration of foreign personal names , The Commercial Press \n\t', '\n\t\t Collocation Translation Acquisition Using Monolingual Corpora Yajuan LÜ Microsoft Research Asia 5F Sigma Center , No. 49 Zhichun Road , Haidian District , Beijing , China , 100080 t-yjlv@microsoft.com Ming ZHOU Microsoft Research Asia 5F Sigma Center , No. 49 Zhichun Road , Haidian District , Beijing , China , 100080 mingzhou@microsoft.com Abstract Collocation translation is important for machine translation and many other NLP tasks . \n\t', '\n\t\t Unlike previous methods using bilingual parallel corpora , this paper presents a new method for acquiring collocation translations by making use of monolingual corpora and linguistic knowledge . \n\t', '\n\t\t First , dependency triples are extracted from Chinese and English corpora with dependency parsers . \n\t', '\n\t\t Then , a dependency triple translation model is estimated using the EM algorithm based on a dependency correspondence assumption . \n\t', '\n\t\t The generated triple translation model is used to extract collocation translations from two monolingual corpora . \n\t', '\n\t\t Experiments show that our approach outperforms the existing monolingual corpus based methods in dependency triple translation and achieves promising results in collocation translation extraction . \n\t', '\n\t\t 1 Introduction A collocation is an arbitrary and recurrent word combination \n\t\t']",Positive
"['\n\t\t Previous work in collocation acquisition varies in the kinds of collocations they detect . \n\t', '\n\t\t These range from two- word to multi-word , with or without syntactic structure \n\t\t']",Positive
"['\n\t\t In this paper , a collocation refers to a recurrent word pair linked with a certain syntactic relation . \n\t', '\n\t\t For instance , <solve , verb-object , problem> is a collocation with a syntactic relation verb-object . \n\t', '\n\t\t Translation of collocations is difficult for nonnative speakers . \n\t', '\n\t\t Many collocation translations are idiosyncratic in the sense that they are unpredictable by syntactic or semantic features . \n\t', '\n\t\t Consider Chinese to English translation . \n\t', '\n\t\t The translations of \x93 &\x94 can be \x93solve\x94 or \x93resolve\x94 . \n\t', '\n\t\t The translations of \x93f p7)""\x94 can be \x93problem\x94 or \x93issue\x94 . \n\t', '\n\t\t However , translations of the collocation \x93 & \x97 f p7 ) "" \x94 as \x93solve \x97problem\x94 or \x93resolve\x97 issue\x94 is preferred over \x93solve\x97issue\x94 or \x93resolve \x97problem\x94 . \n\t', '\n\t\t Automatically acquiring these collocation translations will be very useful for machine translation , cross language information retrieval , second language learning and many other NLP applications . \n\t', '\n\t\t \n\t\t']",Positive
['\n\t\t Some studies have been done for acquiring collocation translations using parallel corpora \n\t\t'],Positive
"['\n\t\t These works implicitly assume that a bilingual corpus on a large scale can be obtained easily . \n\t', '\n\t\t However , despite efforts in compiling parallel corpora , sufficient amounts of such corpora are still unavailable . \n\t', '\n\t\t Instead of heavily relying on bilingual corpora , this paper aims to solve the bottleneck in a different way : to mine bilingual knowledge from structured monolingual corpora , which can be more easily obtained in a large volume . \n\t', '\n\t\t Our method is based on the observation that despite the great differences between Chinese and English , the main dependency relations tend to have a strong direct correspondence \n\t\t']",Positive
"['\n\t\t Based on this assumption , a new translation model based on dependency triples is proposed . \n\t', '\n\t\t The translation probabilities are estimated from two monolingual corpora using the EM algorithm with the help of a bilingual translation dictionary . \n\t', '\n\t\t Experimental results show that the proposed triple translation model outperforms the other three models in comparison . \n\t', '\n\t\t The obtained triple translation model is also used for collocation translation extraction . \n\t', '\n\t\t Evaluation results demonstrate the effectiveness of our method . \n\t', '\n\t\t The remainder of this paper is organized as follows . \n\t', '\n\t\t Section 2 provides a brief description on the related work . \n\t', '\n\t\t Section 3 describes our triple translation model and training algorithm . \n\t', '\n\t\t Section 4 extracts collocation translations from two independent monolingual corpora . \n\t', '\n\t\t Section 5 evaluates the proposed method , and the last section draws conclusions and presents the future work . \n\t', '\n\t\t 2 Related work There has been much previous work done on monolingual collocation extraction . \n\t', '\n\t\t They can in general be classified into two types : window-based and syntax-based methods . \n\t', '\n\t\t The former extracts collocations within a fixed window \n\t\t']",Positive
['\n\t\t The latter extracts collocations which have a syntactic relationship \n\t\t'],Positive
"['\n\t\t The syntax-based method becomes more favorable with recent significant increases in parsing efficiency and accuracy . \n\t', '\n\t\t Several metrics have been adopted to measure the association strength in collocation extraction . \n\t', '\n\t\t \n\t\t']",Positive
['\n\t\t Most previous research in translation knowledge acquisition is based on parallel corpora \n\t\t'],Positive
"['\n\t\t As for collocation translation , \n\t\t']",Positive
"['\n\t\t English collocations are first extracted using the Xtract system , then corresponding French translations are sought based on the Dice coefficient . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t In addition to collocation translation , there is also some related work in acquiring phrase or term translations from parallel corpus \n\t\t']",Positive
"['\n\t\t Since large aligned bilingual corpora are hard to obtain , some research has been conducted to exploit translation knowledge from non-parallel corpora . \n\t', '\n\t\t Their work is mainly on word level . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t The method exhibits promising results in selecting the right translation among several options provided by bilingual dictionary . \n\t', '\n\t\t Zhou et al.(2001) proposes a method to simulate translation probability with a cross language similarity score , which is estimated from monolingual corpora based on mutual information . \n\t', '\n\t\t The method achieves good results in word translation selection . \n\t', '\n\t\t In addition , \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
['\n\t\t \n\t\t'],Positive
['\n\t\t \n\t\t'],Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t 3 Training a triple translation model from monolingual corpora In this section , we first describe the dependency correspondence assumption underlying our approach . \n\t', '\n\t\t Then a dependency triple translation model and the monolingual corpus based training algorithm are proposed . \n\t', '\n\t\t The obtained triple translation model will be used for collocation translation extraction in next section . \n\t', '\n\t\t 3.1 Dependency correspondence between Chinese and English A dependency triple consists of a head , a dependant , and a dependency relation . \n\t', '\n\t\t Using a dependency parser , a sentence can be analyzed into dependency triples . \n\t', '\n\t\t We represent a triple as ( w1,r,w2 ) , where w1 and w2 are words and r is the dependency relation . \n\t', '\n\t\t It means that w2 has a dependency relation r with w1 . \n\t', '\n\t\t For example , a triple ( overcome , verb-object , d;ff;culty ) means that \x93d;ff;culty\x94 is the object of the verb \x93overcome\x94 . \n\t', '\n\t\t Among all the dependency relations , we only consider the following three key types that we think , are the most important in text analysis and machine translation : verb-object ( VO ) , nounadj(AN) , and verb- adv(AV) . \n\t', '\n\t\t It is our observation that there is a strong correspondence in major dependency relations in the translation between English and Chinese . \n\t', '\n\t\t For example , an object-verb relation in Chinese ( e.g.(AER , VO , ^^ ) ) is usually translated into the same verb-object relation in English(e.g . \n\t', '\n\t\t ( overcome , VO , d;ff;culty ) ) . \n\t', '\n\t\t This assumption has been experimentally justified based on a large and balanced bilingual corpus in our previous work \n\t\t']",Positive
"['\n\t\t We come to the conclusion that more than 80 % of the above dependency relations have a one-one mapping between Chinese and English . \n\t', '\n\t\t We can conclude that there is indeed a very strong correspondence between Chinese and English in the three considered dependency relations . \n\t', '\n\t\t This fact will be used to estimate triple translation model using two monolingual corpora . \n\t', '\n\t\t 3.2 Triple translation model According to Bayes\x92s theorem , given a Chinese triple ctr ; = ( c1 , rc , c2 ) , and the set of its candidate English triple translations etr ; = ( e1 , re , e2 ) , the best English triple e\x88tr ; = ( e\x881 , re , e\x882 ) is the one that maximizes the Equation ( 1 ) : = arg max etr ; = arg max etr ; = arg max p(etr ; | ctr ; ) p(etr ; p( )p( | etr ; ) Cm em )p( | ) / p(ctr ; ) em Cm ( 1 ) \x88 etr ; etr ; where p(etr;) is usually called the language model and p(ctr ; | etr ; ) is usually called the translation model . \n\t', '\n\t\t Language Model The language model p(etr;) is calculated with English triples database . \n\t', '\n\t\t In order to tackle with the data sparseness problem , we smooth the language model with an interpolation method , as described below . \n\t', '\n\t\t When the given English triple occurs in the corpus , we can calculate it as in Equation ( 2 ) . \n\t', '\n\t\t p(etr;) = freq(el,re , e2 ) ( 2 ) where freq ( e1 , re , e2 ) represents the frequency of triple etr ; . \n\t', '\n\t\t N represents the total counts of all the English triples in the training corpus . \n\t', '\n\t\t For an English triple et. ; = ( e1 , re , e2 ) , if we assume that two words e1 and e2 are conditionally independent given the relation re , Equation ( 2 ) can be rewritten as in (3)\n\t\t']",Positive
"['\n\t\t p(etr;) = p(re)p(e1 | re)p(e2 | re ) ( 3 ) p(ctr ; | etr;)=p(q,rc,c2 | etr ; ) , etr;)p(c2|r , etr;)p(rc Assumption 2 : Foran Englishtriple assume thatc ; only depends on e ; only depends on Equation(6) is , etr;)p(c2| i~ , etr;)p( e =p(c1 | e1)p(c2 | e2)p(rc | re ) p(e1 | re ) = freq(e1,re,*) * ) , p(e2 | r2 ) = fPeq ( *,re,e2 ) * ) , The wildcard symbol * means it can be any word or relation . \n\t', '\n\t\t With Equations ( 2 ) and ( 3 ) , we get the interpolated language model as shown in ( 4 ) . \n\t', '\n\t\t p(etr;) = ^freNetr ; ) +(1^A)p(re)p(e1 | re)p(e2 | re ) ( 4 ) where 0 < ^ < 1 . \n\t', '\n\t\t ^ is calculated as below : 1 ^ = ^ ( 5 ) 1 1 ( )+ freq etr ; freq ( * , , r e freq ( * , re fr eq p ( re ) = ( N re , where * ) , = p(c1 | rc | etr ; ) Translation Model We simplify the translationmodel accordingthe followingtwo assumptions . \n\t', '\n\t\t Assumption 1 : GivenanEnglish triple etr ; , and the corresponding Chinese dependency relationrc , c1 and c2 are conditionally independent . \n\t', '\n\t\t We have : ( 6 ) etr ; , ( i ^{1,2} ) , and rc re . \n\t', '\n\t\t rewritt en as : p(c tr ; |e tr ; =p(c1 c ) ( 7 ) Notice that p(c1|e1) and p(c2|e2) are translationprobabilities withintriples , they are differentfromthe unrestrictedprobabilities suchas the ones in IBM models \n\t\t']",Positive
"['\n\t\t We distinguishtranslationprobability betweenhead ( p(c1|e1)) and dependant ( p(c2|e2) ) . \n\t', '\n\t\t In the restofthe paper , we use phead ( c | e ) and pdep ( c | e ) to denote the headtranslation probability and dependanttranslationprobability respectively . \n\t', '\n\t\t As the correspondence betweenthe same dependency relationacross Englishand Chinese is strong , we simply assume p(rc|re) =1 forthe corresponding re and rc , and p(rc | re ) = 0 forthe othercases . \n\t', '\n\t\t phead(c1|e1 ) and pdep(c2|e2) cannotbe estimated directlybecause there is no triple-aligned corpus available . \n\t', '\n\t\t Here , we presentanapproachto estimating these probabilities fromtwo monolingual corporabased onthe EM algorithm . \n\t', '\n\t\t 3.3 Estimation ofword translation probability using the EM algorithm Chinese andEnglishcorporaare firstparsed usingadependencyparser , andtwo dependency triple databases are generated . \n\t', '\n\t\t The candidate English translationset ofChinese triples is generatedthrough abilingual dictionary andthe assumption ofstrongcorrespondence of dependency relations . \n\t', '\n\t\t There is ariskthatunrelated triples inChinese andEnglish can be connected withthis method . \n\t', '\n\t\t However , as the conditions that are used to make the connectionare quite strong ( i.e. possible wordtranslations inthe same triple structure ) , we believe thatthis risk , is notvery severe . \n\t', '\n\t\t Then , the expectationmaximization(EM) algorithmis introducedto iteratively strengthenthe correctconnections an d weaken the incorrect connections . \n\t', '\n\t\t EM Algorithm Accordingto section3.2 , the translation probabilities fromaChinese triple ctr ; to an Englishtriple etr ; can be computedusing the Englishtriple language model p(etr;) anda translationmodel from English to Chinese p(ctr;|etr;) . \n\t', '\n\t\t The Englishlanguage model can be |etr ; estimated using Equation ( 4 ) and the translation model can be calculated using Equation ( 7 ) . \n\t', '\n\t\t The translation probabilities phead ( c | e ) and pdep ( c | e ) are initially set to a uniform distribution as follows : Where ^e represents the translation set of the English word e . \n\t', '\n\t\t Then , the word translation probabilities are estimated iteratively using the EM algorithm . \n\t', '\n\t\t Figure 1 gives a formal description of the EM algorithm . \n\t', '\n\t\t Figure 1 : EM algorithm The basic idea is that under the restriction of the English triple language model p(etri) and translation dictionary , we wish to estimate the translation probabilities phead ( c | e ) and pdep ( c | e ) that best explain the Chinese triple database as a translation from the English triple database . \n\t', '\n\t\t In each iteration , the normalized triple translation probabilities are used to update the word translation probabilities . \n\t', '\n\t\t Intuitively , after finding the most probable translation of the Chinese triple , we can collect counts for the word translation it contains . \n\t', '\n\t\t Since the English triple language model provides context information for the disambiguation of the Chinese words , only the appropriate occurrences are counted . \n\t', '\n\t\t Now , with the language model estimated using Equation ( 4 ) and the translation probabilities estimated using EM algorithm , we can compute the best triple translation for a given Chinese triple using Equations ( 1 ) and ( 7 ) . \n\t', '\n\t\t 4 Collocation translation extraction from two monolingual corpora This section describes how to extract collocation translation from independent monolingual corpora . \n\t', '\n\t\t First , collocations are extracted from a monolingual triples database . \n\t', '\n\t\t Then , collocation translations are acquired using the triple translation model obtained in section 3 . \n\t', '\n\t\t 4.1 Monolingual collocation extraction As introduced in section 2 , much work has been done to extract collocations . \n\t', '\n\t\t Among all the measure metrics , log likelihood ratio ( LLR ) has proved to give better results \n\t\t']",Positive
"['\n\t\t In this paper , we take LLR as the metric to extract collocations from a dependency triple database . \n\t', '\n\t\t For a given Chinese triple ctri = ( c1 , rc , c2 ) , the LLR score is calculated as follows : Logl = a log a + blogb + clog c + d log d )^( a+c)log(a+c) ( b+d)log(b+d +NlogN where , a= freq ( c\x84 r , c2 c = freq ( * , rc , d N^a ^b ^ c. N is the total counts of all Chinese triples . \n\t', '\n\t\t Those triples whose LLR values are larger than a given threshold are taken as a collocation . \n\t', '\n\t\t This syntax-based collocation has the advantage that it can represent both adjacent and long distance word association . \n\t', '\n\t\t Here , we only extract the three main types of collocation that have been mentioned in section 3.1 . \n\t', '\n\t\t 4.2 Collocation translation extraction For the acquired collocations , we try to extract their translations from the other monolingual ^1 ( c | e ) = pdep ( c | e ) = ^ ^^ 0 , otherwise phead Train language model for English triple p(etri ) ; Initialize word translation probabilities phead ( c | e ) and pdep ( c | e ) uniformly as in Equation ( 8 ) ; Iterate Set scorehead ( c | e ) and scoredep ( c | e ) to 0 for all dictionary entries ( c,e ) ; for all Chinese triples ctri = ( c1 , rc , c2 ) for all candidate English triple translations etri = ( e1 , re , e2 compute triple translation probability p(etri | ctri ) by p(etri)phead(c1 | e1)pdep(c2 | e2)p(rc | re ) end for normalize p(etri | ctri ) , so that their sum is 1 ; for all triple translation etri = ( e1 , re , e2 ) add p(etri | ctri ) to scorehead ( c1 | e1 ) add p ( etri | ctri ) to scoredep ( c2 | e2 ) endfor endfor for all translation pairs ( c , e ) set phead ( c | e ) to normalized scorehead ( c | e ) ; set pdep ( c | e ) to normalized scoredep ( c | e ) ; endfor enditerate ) ( a+b)log( a+b ( 9 ) + d )log( c + c d ) , b = freq r* ) ^ freq ( c1 , r , c2 ) freq ( c1,rc , c2 c2 ) , ) , corpus using the triple translation model trained with the method proposed in section 3 . \n\t', '\n\t\t Our objective is to acquire collocation translations as translation knowledge for a machine translation system , so only highly reliable collocation translations are extracted . \n\t', '\n\t\t Figure 2 describes the algorithm for Chinese-English collocation translation extraction . \n\t', '\n\t\t It can be seen that the best English triple candidate is extracted as the translation of the given Chinese collocation only if the Chinese collocation is also the best translation candidate of the English triple . \n\t', '\n\t\t But the English triple is not necessarily a collocation . \n\t', '\n\t\t English collocation translations can be extracted in a similar way . \n\t', '\n\t\t Figure 2 : Collocation translation extraction 4.3 Implementation of our approach Our English corpus is from Wall Street Journal ( 1987-1992 ) and Associated Press ( 1988-1990 ) , and the Chinese corpus is from People\x92s Daily ( 1980-1998 ) . \n\t', '\n\t\t The two corpora are parsed using the NLPWin parser1 \n\t\t']",Positive
"['\n\t\t The statistics for three main types of dependency triples are shown in tables 1 and 2 . \n\t', '\n\t\t Token refers to the total number of triple occurrences and Type refers to the number of unique triples in the corpus . \n\t', '\n\t\t Statistic for the extracted Chinese collocations and the collocation translations is shown in Table 3 . \n\t', '\n\t\t Class #Type #Token VO 1,579,783 19,168,229 AN 311,560 5,383,200 AV 546,054 9,467,103 Table 1 : Chinese dependency triples 1 The NLPWin parser is a rule-based parser developed at Microsoft research , which parses several languages including Chinese and English . \n\t', '\n\t\t Its output can be a phrase structure parse tree or a logical form which is represented with dependency triples . \n\t', '\n\t\t Class #Type #Token VO 1,526,747 8,943,903 AN 1,163,440 6,386,097 AV 215,110 1,034,410 Table 2 : English dependency triples Class #Type #Translated VO 99,609 28,841 AN 35,951 12,615 AV 46,515 6,176 Table 3 : Extracted Chinese collocations and E-C translation pairs The translation dictionaries we used in training and translation are combined from two dictionaries : HITDic and NLPWinDic 2 . \n\t', '\n\t\t The final E-C dictionary contains 126,135 entries , and C-E dictionary contains 91,275 entries . \n\t', '\n\t\t 5 Experiments and evaluation To evaluate the effectiveness of our methods , two experiments have been conducted . \n\t', '\n\t\t The first one compares our method with three other monolingual corpus based methods in triple translation . \n\t', '\n\t\t The second one evaluates the accuracy of the acquired collocation translation . \n\t', '\n\t\t 5.1 Dependency triple translation Triple translation experiments are conducted from Chinese to English . \n\t', '\n\t\t We randomly selected 2000 Chinese triples ( whose frequency is larger than 2 ) from the dependency triple database . \n\t', '\n\t\t The standard translation answer sets were built manually by three linguistic experts . \n\t', '\n\t\t For each Chinese triple , its English translation set contain English triples provided by anyone of the three linguists . \n\t', '\n\t\t Among 2000 candidate triples , there are 101 triples that can\x92t be translated into English triples with same relation . \n\t', '\n\t\t For example , the Chinese triple ( 14 , VO , *A ) should be translated into \x93bargain\x94 . \n\t', '\n\t\t The two words in triple cannot be translated separately . \n\t', '\n\t\t We call this kind of collocation translation no-compositional translations . \n\t', '\n\t\t Our current model cannot deal with this kind of translation . \n\t', '\n\t\t In addition , there are also 157 error dependency triples , which result from parsing mistakes . \n\t', '\n\t\t We filtered out these two kinds of triples and got a standard test set with 1,742 Chinese triples and 4,645 translations in total . \n\t', '\n\t\t We compare our triple translation model with three other models on the same standard test set with the same translation dictionary . \n\t', '\n\t\t As the 2 These two dictionaries are built by Harbin Institute of Technology and Microsoft Research respectively . \n\t', '\n\t\t For each Chinese collocation ccol : a. Acquire the best English triple translation e\x88tri using C-E triple translation model : eri =argmaxp(etri)p(ctri | etri ) etri b . \n\t', '\n\t\t For the acquired e\x88tri , calculate the best Chinese triple translation c\x88tri using E-C triple translation model : ctri = arg max p(ctri) p(e ri | c ri ) c tri c . \n\t', '\n\t\t If ccol = c\x88tri , add ccol q e\x88tri to collocation translation database . \n\t', '\n\t\t baseline experiment , Model A selects the highest- frequency translation for each word in triple ; Model B selects translation with the maximal target triple probability , as proposed in \n\t\t']",Positive
"['\n\t\t And our triple translation model is model D . \n\t', '\n\t\t Suppose cori = ( c1 , rc , c2 ) is the Chinese triple to be translated . \n\t', '\n\t\t The four compared models can be formally expressed as follows : Model A : emax = ( arg max ( freq ( e1 ) ) , re , arg max ( freq ( e2 e1^ Trans ( q ) e2^ Trans ( c2 ) Model B : emax = arg max p ( eori ) = arg max p ( e1 , e , e2 eori e1^Trans ( c1 ) e2^ Trans ( c2 ) Model C : emax = arg max(p(eori )×likelyhood(cori | eori ) ) eori = arg max (p(eori)×Sim(e\x84 c1)×Sim(e2 , c2 e1^Trans(c1) e2^Trans(c2 ) where , Sim(e , c ) is similarity score between e and c \n\t\t']",Positive
"['\n\t\t Model D ( our model ) : emax = argmax(p(eori ) ( | ) ) p c e ori ori eori = argmax(p(eori )phead(c1 | e1)pdep ( c2 | e2)p(rc | re ) ) e1^Trans(c1) e2 ^ Trans( c2 ) Table 4 : Translationresults comparison The evaluationresults on the standard testsetare showninTable 4 , where coverage is the percentages oftriples whichcan be translated . \n\t', '\n\t\t Some triples can\x92t be translatedby Model B , C and D because ofthe lackofdictionarytranslations or datasparseness intriples . \n\t', '\n\t\t In fact , the coverage of Model A is 100 % . \n\t', '\n\t\t Itwas setto the same as others inorderto compare accuracy usingthe same test set . \n\t', '\n\t\t The oracle score is the upperboundaccuracy underthe conditions ofcurrent translation dictionary andstandard testset . \n\t', '\n\t\t Top Naccuracy is definedas the percentage oftriples whose selected top Ntran slations include correct translations . \n\t', '\n\t\t We cansee thatboth Model C andModel D achieve betterresults thanModel B . \n\t', '\n\t\t This shows thatthe translationmodel trained from monolingual corporareally helps to improve the performance oftranslation . \n\t', '\n\t\t Ourmodel also outperforms Model C , whichdemonstrates the probabilities trainedby ourEM algorithmachieve betterperformance thanheuristic similarity scores . \n\t', '\n\t\t Infact , ourevaluationmethodis very rigorous . \n\t', '\n\t\t To avoid bias in evaluation , we take human translationresults as standard . \n\t', '\n\t\t The real translation accuracy is reasonablybetterthanthe evaluation results . \n\t', '\n\t\t Butas we can see , compared to the oracle score , the current models still have much room for improvement . \n\t', '\n\t\t And coverage is also not high due to the limitations of the translation dictionary and the sparse data problem . \n\t', '\n\t\t 5.2 Coll ocation translation extraction 47,632 Chinese collocationtranslations are extracted with the methodproposedinsection4 . \n\t', '\n\t\t We randomlyselected1000 translations for evaluation . \n\t', '\n\t\t Three linguistic experts tagthe acceptability ofthe translation . \n\t', '\n\t\t Those tran slations that are tagged as acceptable by at least two experts are evaluated as correct . \n\t', '\n\t\t The evaluation results are shown in Table 5 . \n\t', '\n\t\t Table 5 : Extracted collocationtran slation results We cansee thatthe extractedcollocation translations achieve amuch betterresult thantriple translation . \n\t', '\n\t\t The average accuracy is 63.20 % and the collocations withrelation ANachieve the highestaccuracy of68.15 % . \n\t', '\n\t\t Ifwe only consider those Chinese collocations whose translations are also Englishcollocations , we obtain an even better accuracy of72.16 % as shownin the lastrowof Table 5 . \n\t', '\n\t\t The results justify ourideathatwe can acquire reliable translationforcollocationby makinguse oftriple translation model in two directions . \n\t', '\n\t\t These acquiredcollocationtranslations are very valuable fortranslation knowledge building . \n\t', '\n\t\t Manually crafting collocationtranslations canbe time-consuming an d cannot ensure high quality in a consistent way . \n\t', '\n\t\t Our work will certainly improve the quality and efficiency of collocation translation acquisition . \n\t', '\n\t\t Total Acceptance Accuracy ( % ) VO 590 373 63.22 AN 292 199 68.15 AV 118 60 50.85 All 1000 632 63.20 ColTrans 334 241 72.16 ) ) ) ) ) Cove- Accuracy(%) Oracle Rage(%) Top 1 Top 3 ( % ) Model A 17.21 ---- B Model C 35.88 57.74 Model D 36.91 58.58 Model 83.98 33.56 53.79 66.30 5.3 Discussion Although our approach achieves promising results , it still has some limitations to be remedied in future work . \n\t', '\n\t\t ( 1 ) Translation dictionary extension Due to the limited coverage of the dictionary , a correct translation may not be stored in the dictionary . \n\t', '\n\t\t This naturally limits the coverage of triple translations . \n\t', '\n\t\t Some research has been done to expand translation dictionary using a non-parallel corpus \n\t\t']",Positive
"['\n\t\t It can be used to improve our work . \n\t', '\n\t\t ( 2 ) Noise filtering of parsers Since we use parsers to generate dependency triple databases , this inevitably introduces some parsing mistakes . \n\t', '\n\t\t From our triple translation test data , we can see that 7.85 % ( 157/2000 ) types of triples are error triples . \n\t', '\n\t\t These errors will certainly influence the translation probability estimation in the training process . \n\t', '\n\t\t We need to find an effective way to filter out mistakes and perform necessary automatic correction . \n\t', '\n\t\t ( 3 ) Non-compositional collocation translation . \n\t', '\n\t\t Our model is based on the dependency correspondence assumption , which assumes that a triple\x92s translation is also a triple . \n\t', '\n\t\t But there are still some collocations that can\x92t be translated word by word . \n\t', ""\n\t\t For example , the Chinese triple ( &4 , VO , )AxA ) usually be translated into \x93be effective\x94 ; the English triple ( take , VO , place ) usually be translated into \x93R't\x94 . \n\t"", '\n\t\t The two words in triple cannot be translated separately . \n\t', '\n\t\t Our current model cannot deal with this kind of non-compositional collocation translation . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t We will consider taking their work as a complement to our model . \n\t', '\n\t\t 6 Conclusion and future work This paper proposes a novel method to train a triple translation model and extract collocation translations from two independent monolingual corpora . \n\t', '\n\t\t Evaluation results show that it outperforms the existing monolingual corpus based methods in triple translation , mainly due to the employment of EM algorithm in cross language translation probability estimation . \n\t', '\n\t\t By making use of the acquired triple translation model in two directions , promising results are achieved in collocation translation extraction . \n\t', '\n\t\t Our work also demonstrates the possibility of making full use of monolingual resources , such as corpora and parsers for bilingual tasks . \n\t', '\n\t\t This can help overcome the bottleneck of the lack of a large-scale bilingual corpus . \n\t', '\n\t\t This approach is also applicable to comparable corpora , which are also easier to access than bilingual corpora . \n\t', '\n\t\t In future work , we are interested in extending our method to solving the problem of non- compositional collocation translation . \n\t', '\n\t\t We are also interested in incorporating our triple translation model for sentence level translation . \n\t', '\n\t\t 7 Acknowledgements The authors would like to thank John Chen , Jianfeng Gao and Yunbo Cao for their valuable suggestions and comments on a preliminary draft of this paper . \n\t', '\n\t\t References Morton Benson . \n\t', '\n\t\t 1990. Collocations and general- purpose dictionaries . \n\t', '\n\t\t International Journal of Lexicography . \n\t', '\n\t\t 3(1):23\x9635 Yunbo Cao , Hang Li . \n\t', '\n\t\t 2002. Base noun phrase translation using Web data and the EM algorithm . \n\t', '\n\t\t The 19th International Conference on Computational Linguistics . \n\t', '\n\t\t pp. 127-133 Kenneth W. Church and Patrick Hanks . \n\t', '\n\t\t 1990. Word association norms , mutural information , and lexicography . \n\t', '\n\t\t Computational Linguistics , 16(1):22-29 Ido Dagan and Alon Itai . \n\t', '\n\t\t 1994. Word sense disambiguation using a second language monolingual corpus . \n\t', '\n\t\t Computational Linguistics , 20(4):563-596 Ted Dunning . \n\t', '\n\t\t 1993 . \n\t', '\n\t\t Accurate methods for the statistics of surprise and coincidence . \n\t', '\n\t\t Computational Linguistics . \n\t', '\n\t\t 19(1):61-74 Hiroshi Echizen-ya , Kenji Araki , Yoshi Momouchi , Koji Tochinai . \n\t', '\n\t\t 2003. Effectiveness of automatic extraction of bilingual collocations using recursive chain-link-type learning . \n\t', '\n\t\t The 9th Machine Translation Summit . \n\t', '\n\t\t pp. 102-109 Pascale Fung , and Yee Lo Yuen . \n\t', '\n\t\t 1998. An IR approach for translating new words from nonparallel , comparable Texts . \n\t', '\n\t\t The 36th annual conference of the Association for Computational Linguistics . \n\t', '\n\t\t pp. 414-420 Jianfeng Gao , Jianyun Nie , Hongzhao He , Weijun Chen , Ming Zhou . \n\t', '\n\t\t 2002. Resolving query translation ambiguity using a decaying co- occurrence model and syntactic dependence relations . \n\t', '\n\t\t The 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval . \n\t', '\n\t\t pp. 183 - 190 G. Heidorn . \n\t', '\n\t\t 2000. Intelligent writing assistant . \n\t', '\n\t\t In R. Dale , H. Moisl , and H. Somers , editors , A Handbook of Natural Language Processing : Techniques and Applications for the Processing of Language as Text . \n\t', '\n\t\t Marcel Dekker . \n\t', '\n\t\t Philipp Koehn and Kevin Knight . \n\t', '\n\t\t 2000 . \n\t', '\n\t\t Estimating word translation probabilities from unrelated monolingual corpora using the EM algorithm . \n\t', '\n\t\t National Conference on Artificial Intelligence . \n\t', '\n\t\t pp.711-715 Philipp Koehn and Kevin Knight . \n\t', '\n\t\t 2002. Learning a translation lexicon from monolingual corpora . \n\t', '\n\t\t Unsupervised Lexical Acquisition : Workshop of the ACL Special Interest Group on the Lexicon . \n\t', '\n\t\t pp. 9-16 Julian Kupiec . \n\t', '\n\t\t 1993. An algorithm for finding noun phrase correspondences in bilingual corpora . \n\t', '\n\t\t The 31st Annual Meeting of the Association for Computational Linguistics , pp. 23-30 Cong Li , Hang Li . \n\t', '\n\t\t 2002. Word translation disambiguation using bilingual bootstrapping . \n\t', '\n\t\t The 40th annual conference of the Association for Computational Linguistics . \n\t', '\n\t\t pp : 343-351 Dekang Lin . \n\t', '\n\t\t 1998. Extracting collocation from Text corpora . \n\t', '\n\t\t First Workshop on Computational Terminology . \n\t', '\n\t\t pp. 57-63 Dekang Lin 1999 . \n\t', '\n\t\t Automatic identification of non- compositional phrases . \n\t', '\n\t\t The 37th Annual Meeting of the Association for Computational Linguistics . \n\t', '\n\t\t pp.317--324 Ilya Dan Melamed . \n\t', '\n\t\t 1997. Automatic discovery of non-compositional compounds in parallel data . \n\t', '\n\t\t The 2nd Conference on Empirical Methods in Natural Language Processing . \n\t', '\n\t\t pp. 97~108 Brown P.F. , Pietra , S.A.D. , Pietra , V. J. D. , and Mercer R. L. 1993 . \n\t', '\n\t\t The mathematics of machine translation : parameter estimation . \n\t', '\n\t\t Computational Linguistics , 19(2):263-313 Reinhard Rapp . \n\t', '\n\t\t 1999. Automatic identification of word translations from unrelated English and German corpora . \n\t', '\n\t\t The 37th annual conference of the Association for Computational Linguistics . \n\t', '\n\t\t pp. 519-526 Violeta Seretan , Luka Nerima , Eric Wehrli . \n\t', '\n\t\t 2003. Extraction of Multi-Word collocations using syntactic bigram composition . \n\t', '\n\t\t International Conference on Recent Advances in NLP . \n\t', '\n\t\t pp. 424-431 Frank Smadja . \n\t', '\n\t\t 1993 . \n\t', '\n\t\t Retrieving collocations from text : Xtract . \n\t', '\n\t\t Computational Linguistics , 19(1):143-177 Frank Smadja , Kathleen R. Mckeown , Vasileios Hatzivassiloglou . \n\t', '\n\t\t 1996. Translation collocations for bilingual lexicons : a statistical approach . \n\t', '\n\t\t Computational Linguistics , 22:1-38 Aristomenis Thanopoulos , Nikos Fakotakis , George Kokkinakis . \n\t', '\n\t\t 2002. Comparative evaluation of collocation extraction metrics . \n\t', '\n\t\t The 3rd International Conference on Language Resource and Evaluation . \n\t', '\n\t\t pp.620-625 Hua Wu , Ming Zhou . \n\t', '\n\t\t 2003. Synonymous collocation extraction using translation Information . \n\t', '\n\t\t The 41 th annual conference of the Association for Computational Linguistics . \n\t', '\n\t\t pp. 120-127 Kaoru Yamamoto , Yuji Matsumoto . \n\t', '\n\t\t 2000. Acquisition of phrase-level bilingual correspondence using dependency structure . \n\t', '\n\t\t The 18th International Conference on Computational Linguistics . \n\t', '\n\t\t pp. 933-939 Ming Zhou , Ding Yuan and Changning Huang . \n\t', '\n\t\t 2001. Improving translation selection with a new translation model trained by independent monolingual corpora . \n\t', '\n\t\t Computaional Linguistics & Chinese Language Processing . \n\t', '\n\t\t 6(1) : 1-26 \n\t', '\n\t\t Statistical Machine Translation with Word- and Sentence-Aligned Parallel Corpora Chris Callison-Burch David Talbot Miles Osborne School on Informatics University of Edinburgh 2 Buccleuch Place Edinburgh , EH8 9LW callison-burch@ed.ac.uk Abstract The parameters of statistical translation models are typically estimated from sentence-aligned parallel corpora . \n\t', '\n\t\t We show that significant improvements in the alignment and translation quality of such models can be achieved by additionally including word- aligned data during training . \n\t', '\n\t\t Incorporating word- level alignments into the parameter estimation of the IBM models reduces alignment error rate and increases the Bleu score when compared to training the same models only on sentence-aligned data . \n\t', '\n\t\t On the Verbmobil data set , we attain a 38 % reduction in the alignment error rate and a higher Bleu score with half as many training examples . \n\t', '\n\t\t We discuss how varying the ratio of word-aligned to sentence- aligned data affects the expected performance gain . \n\t', '\n\t\t 1 Introduction Machine translation systems based on probabilistic translation models \n\t\t']",Positive
"['\n\t\t For many language pairs these exist in abundant quantities . \n\t', '\n\t\t However for new domains or uncommon language pairs extensive parallel corpora are often hard to come by . \n\t', '\n\t\t Two factors could increase the performance of statistical machine translation for new language pairs and domains : a reduction in the cost of creating new training data , and the development of more efficient methods for exploiting existing training data . \n\t', '\n\t\t Approaches such as harvesting parallel corpora from the web \n\t\t']",Positive
"['\n\t\t We take the second , complementary approach . \n\t', '\n\t\t We address the problem of efficiently exploiting existing parallel corpora by adding explicit word-level alignments between a number of the sentence pairs in the training corpus . \n\t', '\n\t\t We modify the standard parameter estimation procedure for IBM Models and HMM variants so that they can exploit these additional word- level alignments . \n\t', '\n\t\t Our approach uses both word- and sentence-level alignments for training material . \n\t', '\n\t\t In this paper we : 1 . \n\t', '\n\t\t Describe how the parameter estimation framework of \n\t\t']",Positive
"['\n\t\t Report significant improvements in alignment error rate and translation quality when training on data with word-level alignments ; 3 . \n\t', '\n\t\t Demonstrate that the inclusion of word-level alignments is more effective than using a bilingual dictionary ; 4 . \n\t', '\n\t\t Show the importance of amplifying the contribution of word-aligned data during parameter estimation . \n\t', '\n\t\t This paper shows that word-level alignments improve the parameter estimates for translation models , which in turn results in improved statistical translation for languages that do not have large sentence-aligned parallel corpora . \n\t', '\n\t\t 2 Parameter Estimation Using Sentence-Aligned Corpora The task of statistical machine translation is to choose the source sentence , e , that is the most probable translation of a given sentence , f , in a foreign language . \n\t', '\n\t\t Rather than choosing e* that directly maximizes p(elf) , \n\t\t']",Positive
"['\n\t\t ( 1 ) e In this equation p(e) is a language model probability and is p(f Ie ) a translation model probability . \n\t', '\n\t\t A series of increasingly sophisticated translation models , referred to as the IBM Models , was defined in \n\t\t']",Positive
"['\n\t\t The translation model , p(fIe) defined as a marginal probability obtained by summing over word-level alignments , a , between the source and target sentences : p(f le ) = X p(f , ale ) . \n\t', '\n\t\t ( 2 ) a While word-level alignments are a crucial component of the IBM models , the model parameters are generally estimated from sentence-aligned parallel corpora without explicit word-level alignment information . \n\t', '\n\t\t The reason for this is that word-aligned parallel corpora do not generally exist . \n\t', '\n\t\t Consequently , word level alignments are treated as hidden variables . \n\t', '\n\t\t To estimate the values of these hidden variables , the expectation maximization ( EM ) framework for maximum likelihood estimation from incomplete data is used \n\t\t']",Positive
"['\n\t\t The previous section describes how the translation probability of a given sentence pair is obtained by summing over all alignments p(f le ) = Ea p(f , ale ) . \n\t', '\n\t\t EM seeks to maximize the marginal log likelihood , logp(fle) , indirectly by iteratively maximizing a bound on this term known as the ex- pected complete log likelihood , ( log p(f , ale))q(a),1 Since only some of the permissible alignments make sense linguistically , we would like EM to use the posterior alignment probabilities calculated in the E-step to weight plausible alignments higher than the large number of bogus alignments which are included in the expected complete log likelihood . \n\t', '\n\t\t This in turn should encourage the parameter adjustments made in the M-step to converge to linguistically plausible values . \n\t', '\n\t\t Since the number of permissible alignments for a sentence grows exponentially in the length of the sentences for the later IBM Models , a large number of informative example sentence pairs are required to distinguish between plausible and implausible alignments . \n\t', '\n\t\t Given sufficient data the distinction occurs because words which are mutual translations appear together more frequently in aligned sentences in the corpus . \n\t', '\n\t\t Given the high number of model parameters and permissible alignments , however , huge amounts of data will be required to estimate reasonable translation models from sentence-aligned data alone . \n\t', '\n\t\t logp(fle) = logX p(f , ale ) ( 3 ) a X=log q(a)p(f,ale) ( 4 ) a q(a) > X q(a) log p(f , ale ) ( 5 ) a q(a) = ( logp(f , ale))q(a) + H(q(a)) where the bound in ( 5 ) is given by Jensen\x92s inequality . \n\t', '\n\t\t By choosing q(a) = p(alf , e ) this bound becomes an equality . \n\t', '\n\t\t This maximization consists of two steps : \x95 E-step : calculate the posterior probability under the current model of every permissi- ble alignment for each sentence pair in the sentence-aligned training corpus ; \x95 M-step : maximize the expected log likelihood under this posterior distribution , ( logp(f , ale))q(a) , with respect to the model\x92s parameters . \n\t', '\n\t\t While in standard maximum likelihood estimation events are counted directly to estimate parameter settings , in EM we effectively collect fractional counts of events ( here permissible alignments weighted by their posterior probability ) , and use these to iteratively update the parameters . \n\t', '\n\t\t 1Here ( \x95)9(.) denotes an expectation with respect to q(\x95) . \n\t', '\n\t\t 3 Parameter Estimation Using Word- and Sentence-Aligned Corpora As an alternative to collecting a huge amount of sentence-aligned training data , by annotating some of our sentence pairs with word-level alignments we can explicitly provide information to highlight plausible alignments and thereby help parameters converge upon reasonable settings with less training data . \n\t', '\n\t\t Since word-alignments are inherent in the IBM translation models it is straightforward to incorporate this information into the parameter estimation procedure . \n\t', '\n\t\t For sentence pairs with explicit word- level alignments marked , fractional counts over all permissible alignments need not be collected . \n\t', '\n\t\t Instead , whole counts are collected for the single hand annotated alignment for each sentence pair which has been word-aligned . \n\t', '\n\t\t By doing this the expected complete log likelihood collapses to a single term , the complete log likelihood ( p(f , ale ) ) , and the E- step is circumvented . \n\t', '\n\t\t The parameter estimation procedure now involves maximizing the likelihood of data aligned only at the sentence level and also of data aligned at the word level . \n\t', '\n\t\t The mixed likelihood function , M , combines the expected information contained in the sentence-aligned data with the complete information contained in the word-aligned data . \n\t', '\n\t\t N3 M = E ( 1 \x97 A)(logp(fs,asles))q(a3) s=1 N. + E A logp(fw , awl ew ) ( 6 ) w=1 Here s and w index the Ns sentence-aligned sentences and Nw word-aligned sentences in our corpora respectively . \n\t', '\n\t\t Thus M combines the expected complete log likelihood and the complete log likelihood . \n\t', '\n\t\t In order to control the relative contributions of the sentence-aligned and word-aligned data in the parameter estimation procedure , we introduce a mixing weight A that can take values between 0 and 1 . \n\t', '\n\t\t 3.1 The impact of word-level alignments The impact of word-level alignments on parameter estimation is closely tied to the structure of the IBM Models . \n\t', '\n\t\t Since translation and word alignment parameters are shared between all sentences , the posterior alignment probability of a source-target word pair in the sentence-aligned section of the corpus that were aligned in the word-aligned section will tend to be relatively high . \n\t', '\n\t\t In this way , the alignments from the word-aligned data effectively percolate through to the sentence- aligned data indirectly constraining the E-step of EM . \n\t', '\n\t\t 3.2 Weighting the contribution of word-aligned data By incorporating A , Equation 6 becomes an interpolation of the expected complete log likelihood provided by the sentence-aligned data and the complete log likelihood provided by word-aligned data . \n\t', '\n\t\t The use of a weight to balance the contributions of unlabeled and labeled data in maximum likelihood estimation was proposed by \n\t\t']",Positive
"['\n\t\t A quantifies our relative confidence in the expected statistics and observed statistics estimated from the sentence- and word-aligned data respectively . \n\t', '\n\t\t Standard maximum likelihood estimation ( MLE ) which weighs all training samples equally , corresponds to an implicit value of lambda equal to the proportion of word-aligned data in the whole of the training set : A =N +N3 . \n\t', '\n\t\t However , having the total amount of sentence-aligned data be much larger than the amount of word-aligned data implies a value of A close to zero . \n\t', '\n\t\t This means that M can be maximized while essentially ignoring the likelihood of the word-aligned data . \n\t', '\n\t\t Since we believe that the explicit word-alignment information will be highly effective in distinguishing plausible alignments in the corpus as a whole , we expect to see benefits by setting A to amplify the contribution of the word- aligned data set particularly when this is a relatively small portion of the corpus . \n\t', '\n\t\t 4 Experimental Design To perform our experiments with word-level alignements we modified GIZA++ , an existing and freely available implementation of the IBM models and HMM variants \n\t\t']",Positive
"['\n\t\t Our modifications involved circumventing the E-step for sentences which had word-level alignments and incorporating these observed alignment statistics in the M-step . \n\t', '\n\t\t The observed and expected statistics were weighted accordingly by A and ( 1\x97 A ) respectively as were their contributions to the mixed log likelihood . \n\t', '\n\t\t In order to measure the accuracy of the predictions that the statistical translation models make under our various experimental settings , we choose the alignment error rate ( AER ) metric , which is defined in \n\t\t']",Positive
"['\n\t\t We also investigated whether improved AER leads to improved translation quality . \n\t', '\n\t\t We used the alignments created during our AER experiments as the input to a phrase-based decoder . \n\t', '\n\t\t We translated a test set of 350 sentences , and used the Bleu metric \n\t\t']",Positive
"['\n\t\t We used the Verbmobil German-English parallel corpus as a source of training data because it has been used extensively in evaluating statistical translation and alignment accuracy . \n\t', '\n\t\t This data set comes with a manually word-aligned set of 350 sentences which we used as our test set . \n\t', '\n\t\t Our experiments additionally required a very large set of word-aligned sentence pairs to be incorporated in the training set . \n\t', '\n\t\t Since previous work has shown that when training on the complete set of 34,000 sentence pairs an alignment error rate as low as 6 % can be achieved for the Verbmobil data , we automatically generated a set of alignments for the entire training data set using the unmodified version of GIZA++ . \n\t', '\n\t\t We wanted to use automatic alignments in lieu of actual hand alignments so that we would be able to perform experiments using large data sets . \n\t', '\n\t\t We ran a pilot experiment to test whether our automatic would produce similar results to manual alignments . \n\t', '\n\t\t We divided our manual word alignments into training and test sets and compared the performance of models trained on human aligned data against models trained on automatically aligned data . \n\t', '\n\t\t A Size of training corpus Model .5k 2k 8k 16k Model1 29.64 24.66 22.64 21.68 HMM 18.74 15.63 12.39 12.04 Model 3 26.07 18.64 14.39 13.87 Model4 20.59 16.05 12.63 12.17 Size of training corpus Model .5k 2k 8k 16k Model1 21.43 18.04 16.49 16.20 HMM 14.42 10.47 9.09 8.80 Model3 20.56 13.25 10.82 10.51 Model4 14.19 10.13 7.87 7.52 Table 1 : Alignment error rates for the various IBM Models trained with sentence-aligned data 100-fold cross validation showed that manual and automatic alignments produced AER results that were similar to each other to within 0.1%.2 Having satisfied ourselves that automatic alignment were a sufficient stand-in for manual alignments , we performed our main experiments which fell into the following categories : 1 . \n\t', '\n\t\t Verifying that the use of word-aligned data has an impact on the quality of alignments predicted by the IBM Models , and comparing the quality increase to that gained by using a bilingual dictionary in the estimation stage . \n\t', '\n\t\t 2. Evaluating whether improved parameter estimates of alignment quality lead to improved translation quality . \n\t', '\n\t\t 3. Experimenting with how increasing the ratio of word-aligned to sentence-aligned data affected the performance . \n\t', '\n\t\t 4. Experimenting with our A parameter which allows us to weight the relative contributions of the word-aligned and sentence-aligned data , and relating it to the ratio experiments . \n\t', '\n\t\t 5. Showing that improvements to AER and translation quality held for another corpus . \n\t', '\n\t\t 5 Results 5.1 Improved alignment quality As a staring point for comparison we trained GIZA++ using four different sized portions of the Verbmobil corpus . \n\t', '\n\t\t For each of those portions we output the most probable alignments of the testing data for Model 1 , the HMM , Model 3 , and Model 2Note that we stripped out probable alignments from our manually produced alignments . \n\t', '\n\t\t Probable alignments are large blocks of words which the annotator was uncertain of how to align . \n\t', '\n\t\t The many possible word-to-word translations implied by the manual alignments led to lower results than with the automatic alignments , which contained fewer word-to-word translation possibilities . \n\t', '\n\t\t Table 2 : Alignment error rates for the various IBM Models trained with word-aligned data 4,3 and evaluated their AERs . \n\t', '\n\t\t Table 1 gives alignment error rates when training on 500 , 2000 , 8000 , and 16000 sentence pairs from Verbmobil corpus without using any word-aligned training data . \n\t', '\n\t\t We obtained much better results when incorporating word-alignments with our mixed likelihood function . \n\t', '\n\t\t Table 2 shows the results for the different corpus sizes , when all of the sentence pairs have been word-aligned . \n\t', '\n\t\t The best performing model in the unmodified GIZA++ code was the HMM trained on 16,000 sentence pairs , which had an alignment error rate of 12.04 % . \n\t', '\n\t\t In our modified code the best performing model was Model 4 trained on 16,000 sentence pairs ( where all the sentence pairs are word-aligned ) with an alignment error rate of 7.52 % . \n\t', '\n\t\t The difference in the best performing models represents a 38 % relative reduction in AER . \n\t', '\n\t\t Interestingly , we achieve a lower AER than the best performing unmodified models using a corpus that is one-eight the size of the sentence-aligned data . \n\t', '\n\t\t Figure 1 show an example of the improved alignments that are achieved when using the word aligned data . \n\t', '\n\t\t The example alignments were held out sentence pairs that were aligned after training on 500 sentence pairs . \n\t', '\n\t\t The alignments produced when the training on word-aligned data are dramatically better than when training on sentence-aligned data . \n\t', '\n\t\t We contrasted these improvements with the improvements that are to be had from incorporating a bilingual dictionary into the estimation process . \n\t', '\n\t\t For this experiment we allowed a bilingual dictionary to constrain which words can act as translations of each other during the initial estimates of translation probabilities ( as described in \n\t\t']",Positive
"['\n\t\t As can be seen in Table 3 , using a dictionary reduces the AER when compared to using GIZA++ without a dictionary , but not as dramatically as integrating the word-alignments . \n\t', '\n\t\t We further tried combining a dictionary with our word-alignments but found that the dictionary results in only very minimal improvements over using word-alignments alone . \n\t', '\n\t\t 3 We used the default training schemes for GIZA++ , and left model smoothing parameters at their default settings . \n\t', '\n\t\t Dann reserviere ich zwei Einzelzimmer , nehme ich mal an . \n\t', '\n\t\t ( a ) Sentence-aligned Dann reserviere ich zwei Einzelzimmer , nehme ich mal an . \n\t', '\n\t\t ( b ) Word-aligned Dann reserviere ich zwei Einzelzimmer , nehme ich mal an . \n\t', '\n\t\t ( c ) Reference Figure 1 : Example alignments using sentence-aligned training data ( a ) , using word-aligned data ( b ) , and a reference manual alignment ( c ) Size of training corpus Model .5k 2k 8k 16k Model1 23.56 20.75 18.69 18.37 HMM 15.71 12.15 9.91 10.13 Model 3 22.11 16.93 13.78 12.33 Model 4 17.07 13.60 11.49 10.77 Table 3 : The improved alignment error rates when using a dictionary instead of word-aligned data to constrain word translations Sentence-aligned Word-aligned Size AER Bleu AER Bleu 500 20.59 0.211 14.19 0.233 2000 16.05 0.247 10.13 0.260 8000 12.63 0.265 7.87 0.278 16000 12.17 0.270 7.52 0.282 Table 4 : Improved AER leads to improved translation quality 5.2 Improved translation quality The fact that using word-aligned data in estimating the parameters for machine translation leads to better alignments is predictable . \n\t', '\n\t\t A more significant result is whether it leads to improved translation quality . \n\t', '\n\t\t In order to test that our improved parameter estimates lead to better translation quality , we used a state-of-the-art phrase-based decoder to translate a held out set of German sentences into English . \n\t', '\n\t\t The phrase-based decoder extracts phrases from the word alignments produced by GIZA++ , and computes translation probabilities based on the frequency of one phrase being aligned with another \n\t\t']",Positive
"['\n\t\t We trained a language model Ratio AER when when A = .9 A = Standard MLE 0.1 11.73 9.40 0.2 10.89 8.66 0.3 10.23 8.13 0.5 8.65 8.19 0.7 8.29 8.03 0.9 7.78 7.78 Table 5 : The effect of weighting word-aligned data more heavily that its proportion in the training data ( corpus size 16000 sentence pairs ) using the 34,000 English sentences from the training set . \n\t', '\n\t\t Table 4 shows that using word-aligned data leads to better translation quality than using sentence- aligned data . \n\t', '\n\t\t Particularly , significantly less data is needed to achieve a high Bleu score when using word alignments . \n\t', '\n\t\t Training on a corpus of 8,000 sentence pairs with word alignments results in a higher Bleu score than when training on a corpus of 16,000 sentence pairs without word alignments . \n\t', '\n\t\t 5.3 Weighting the word-aligned data We have seen that using training data consisting of entirely word-aligned sentence pairs leads to better alignment accuracy and translation quality . \n\t', '\n\t\t However , because manually word-aligning sentence pairs costs more than just using sentence-aligned data , it is unlikely that we will ever want to label an entire corpus . \n\t', '\n\t\t Instead we will likely have a relatively small portion of the corpus word aligned . \n\t', '\n\t\t We want to be sure that this small amount of data labeled with word alignments does not get overwhelmed by a larger amount of unlabeled data . \n\t', '\n\t\t Figure 2 : The effect on AER of varying A for a training corpus of 16K sentence pairs with various proportions of word-alignments Thus we introduced the A weight into our mixed likelihood function . \n\t', '\n\t\t Table 5 compares the natural setting of A ( where it is proportional to the amount of labeled data in the corpus ) to a value that amplifies the contribution of the word-aligned data . \n\t', '\n\t\t Figure 2 shows a variety of values for A . \n\t', '\n\t\t It shows as A increases AER decreases . \n\t', '\n\t\t Placing nearly all the weight onto the word-aligned data seems to be most effective .4 Note this did not vary the training data size \x96 only the relative contributions between sentence- and word-aligned training material . \n\t', '\n\t\t 5.4 Ratio of word- to sentence-aligned data We also varied the ratio of word-aligned to sentence-aligned data , and evaluated the AER and Bleu scores , and assigned high value to A ( = 0.9 ) . \n\t', '\n\t\t Figure 3 shows how AER improves as more word-aligned data is added . \n\t', '\n\t\t Each curve on the graph represents a corpus size and shows its reduction in error rate as more word-aligned data is added . \n\t', '\n\t\t For example , the bottom curve shows the performance of a corpus of 16,000 sentence pairs which starts with an AER ofjust over 12 % with no word-aligned training data and decreases to an AER of 7.5 % when all 16,000 sentence pairs are word-aligned . \n\t', '\n\t\t This curve essentially levels off after 30 % of the data is word-aligned . \n\t', '\n\t\t This shows that a small amount of word-aligned data is very useful , and if we wanted to achieve a low AER , we would only have to label 4,800 examples with their word alignments rather than the entire corpus . \n\t', '\n\t\t Figure 4 shows how the Bleu score improves as more word-aligned data is added . \n\t', '\n\t\t This graph also 4At A = 1 ( not shown in Figure 2 ) the data that is only sentence-aligned is ignored , and the AER is therefore higher . \n\t', '\n\t\t Figure 3 : The effect on AER of varying the ratio of word-aligned to sentence-aligned data Figure 4 : The effect on Bleu of varying the ratio of word-aligned to sentence-aligned data reinforces the fact that a small amount of word- aligned data is useful . \n\t', '\n\t\t A corpus of 8,000 sentence pairs with only 800 of them labeled with word alignments achieves a higher Bleu score than a corpus of 16,000 sentence pairs with no word alignments . \n\t', '\n\t\t 5.5 Evaluation using a larger training corpus We additionally tested whether incorporating word- level alignments into the estimation improved results for a larger corpus . \n\t', '\n\t\t We repeated our experiments using the Canadian Hansards French-English parallel corpus . \n\t', '\n\t\t Figure 6 gives a summary of the improvements in AER and Bleu score for that corpus , when testing on a held out set of 484 hand aligned sentences . \n\t', '\n\t\t On the whole , alignment error rates are higher and Bleu scores are considerably lower for the Hansards corpus . \n\t', '\n\t\t This is probably due to the differences in the corpora . \n\t', '\n\t\t Whereas the Verbmobil corpus has a small vocabulary ( <10,000 per lan- Sentence-aligned Word-aligned Size AER Bleu AER Bleu 500 33.65 0.054 25.73 0.064 2000 25.97 0.087 18.57 0.100 8000 19.00 0.115 14.57 0.120 16000 16.59 0.126 13.55 0.128 Table 6 : Summary results for AER and translation quality experiments on Hansards data guage ) , the Hansards has ten times that many vocabulary items and has a much longer average sentence length . \n\t', '\n\t\t This made it more difficult for us to create a simulated set of hand alignments ; we measured the AER of our simulated alignments at 11.3 % ( which compares to 6.5 % for our simulated alignments for the Verbmobil corpus ) . \n\t', '\n\t\t Nevertheless , the trend of decreased AER and increased Bleu score still holds . \n\t', '\n\t\t For each size of training corpus we tested we found better results using the word-aligned data . \n\t', '\n\t\t 6 Related Work \n\t\t']",Negative
"['\n\t\t Och and Ney do not give any direct analysis of how improved word alignments accuracy contributes toward better translation quality as we do here . \n\t', '\n\t\t \n\t\t']",Negative
"['\n\t\t A number of different methods were tried , but none of them used word-level alignments . \n\t', '\n\t\t Since the best performing system used an unmodified version of Giza++ , we would expected that our modifed version would show enhanced performance . \n\t', '\n\t\t Naturally this would need to be tested in future work . \n\t', '\n\t\t \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t 7 Discussion and Future Work In this paper we show with the appropriate modification of EM significant improvement gains can be had through labeling word alignments in a bilingual corpus . \n\t', '\n\t\t Because of this significantly less data is required to achieve a low alignment error rate or high Bleu score . \n\t', '\n\t\t This holds even when using noisy word alignments such as our automatically created set . \n\t', '\n\t\t One should take our research into account when trying to efficiently create a statistical machine translation system for a language pair for which a parallel corpus is not available . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t In our experience it is quicker to manually word-align translated sentence pairs than to translate a sentence , and word-level alignment can be done by someone who might not be fluent enough to produce translations . \n\t', '\n\t\t It might therefore be possible to achieve a higher performance at a fraction of the cost by hiring a nonprofessional produce word-alignments after a limited set of sentences have been translated . \n\t', '\n\t\t We plan to investigate whether it is feasible to use active learning to select which examples will be most useful when aligned at the word-level . \n\t', '\n\t\t Section 5.4 shows that word-aligning a fraction of sentence pairs in a training corpus , rather than the entire training corpus can still yield most of the benefits described in this paper . \n\t', '\n\t\t One would hope that by selectively sampling which sentences are to be manually word-aligned we would achieve nearly the same performance as word-aligning the entire corpus . \n\t', '\n\t\t Acknowledgements The authors would like to thank Franz Och , Hermann Ney , and Richard Zens for providing the Verbmobil data , and Linear B for providing its phrase-based decoder . \n\t', '\n\t\t References Peter Brown , Stephen Della Pietra , Vincent Della Pietra , and Robert Mercer . \n\t', '\n\t\t 1993. The mathematics of machine translation : Parameter estimation . \n\t', '\n\t\t Computational Linguistics , 19(2):263\x96311 , June . \n\t', '\n\t\t Adrian Corduneanu . \n\t', '\n\t\t 2002. Stable mixing of complete and incomplete information . \n\t', '\n\t\t Master\x92s thesis , Massachusetts Institute of Technology , February . \n\t', '\n\t\t A. P. Dempster , N. M. Laird , and D. B. Rubin . \n\t', '\n\t\t 1977. Maximum likelihood from incomplete data via the EM algorithm . \n\t', '\n\t\t Journal of the Royal Statistical Society , 39(1):1\x9638 , Nov. . \n\t', '\n\t\t Ulrich Germann. 2001 . \n\t', '\n\t\t Building a statistical machine translation system from scratch : How much bang for the buck can we expect ? \n\t', '\n\t\t In ACL 2001 Workshop on Data-Driven Machine Translation , Toulouse , France , July 7 . \n\t', '\n\t\t Philipp Koehn , Franz Josef Och , and Daniel Marcu . \n\t', '\n\t\t 2003. Statistical phrase-based translation . \n\t', '\n\t\t In Proceedings of the HLT/NAACL . \n\t', '\n\t\t I. Dan Melamed . \n\t', '\n\t\t 1998. Manual annotation of translational equivalence : The blinker project . \n\t', '\n\t\t Cognitive Science Technical Report 98/07 , University of Pennsylvania . \n\t', '\n\t\t Rada Mihalcea and Ted Pedersen . \n\t', '\n\t\t 2003. An evaluation exercise for word alignment . \n\t', '\n\t\t In Rada Mihalcea and Ted Pedersen , editors , HLT-NAACL 2003 Workshop : Building and Using Parallel Texts . \n\t', '\n\t\t Kamal Nigam , Andrew K. McCallum , Sebastian Thrun , and Tom M. Mitchell . \n\t', '\n\t\t 2000. Text classification from labeled and unlabeled documents using EM . \n\t', '\n\t\t Machine Learning , 39(2/3):103\x96134 . \n\t', '\n\t\t Franz Josef Och and Hermann Ney . \n\t', '\n\t\t 2003. A systematic comparison of various statistical alignment models . \n\t', '\n\t\t Computational Linguistics , 29(1):19\x9651 , March . \n\t', '\n\t\t Kishore Papineni , Salim Roukos , Todd Ward , and WeiJing Zhu . \n\t', '\n\t\t 2001. Bleu : a method for automatic evaluation of machine translation . \n\t', '\n\t\t IBM Research Report RC22176(W0109-022) , IBM . \n\t', '\n\t\t Philip Resnik and Noah Smith . \n\t', '\n\t\t 2003. The web as a parallel corpus . \n\t', '\n\t\t Computational Linguistics , 29(3):349\x96 380 , September . \n\t', '\n\t\t Finding Ideographic Representations of Japanese Names Written in Latin Script via Language Identification and Corpus Validation Yan Qu Clairvoyance Corporation 5001 Baum Boulevard , Suite 700 Pittsburgh , PA 15213-1854 , USA yqu@clairvoyancecorp.com Abstract Multilingual applications frequently involve dealing with proper names , but names are often missing in bilingual lexicons . \n\t', '\n\t\t This problem is exacerbated for applications involving translation between Latin-scripted languages and Asian languages such as Chinese , Japanese and Korean ( CJK ) where simple string copying is not a solution . \n\t', '\n\t\t We present a novel approach for generating the ideographic representations of a CJK name written in a Latin script . \n\t', '\n\t\t The proposed approach involves first identifying the origin of the name , and then back-transliterating the name to all possible Chinese characters using language-specific mappings . \n\t', '\n\t\t To reduce the massive number of possibilities for computation , we apply a three-tier filtering process by filtering first through a set of attested bigrams , then through a set of attested terms , and lastly through the WWW for a final validation . \n\t', '\n\t\t We illustrate the approach with English-to-Japanese back-transliteration . \n\t', '\n\t\t Against test sets of Japanese given names and surnames , we have achieved average precisions of 73 % and 90 % , respectively . \n\t', '\n\t\t 1 Introduction Multilingual processing in the real world often involves dealing with proper names . \n\t', '\n\t\t Translations of names , however , are often missing in bilingual resources . \n\t', '\n\t\t This absence adversely affects multilingual applications such as machine translation ( MT ) or cross language information retrieval ( CLIR ) for which names are generally good discriminating terms for high IR performance \n\t\t']",Positive
"['\n\t\t For language pairs with different writing systems , such as Japanese and English , and for which simple string-copying of a name from one language to another is not a solution , researchers have studied techniques for transliteration , i.e. , phonetic translation across languages . \n\t', '\n\t\t For example , European names are often transcribed in Japanese using the syllabic Gregory Grefenstette* LIC2M/LIST/CEA 18 , route du Panorama , BP 6 Fontenay-aux-Roses , 92265 France Gregory.Grefenstette@cea.fr katakana alphabet . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t Using similar methods , \n\t\t']",Positive
"['\n\t\t Transliteration of names between alphabetic and syllabic scripts has also been studied for languages such as Japanese/English ( Fujii & Ishikawa , 2001 ) , English/Korean \n\t\t']",Positive
"['\n\t\t In work closest to ours , \n\t\t']",Positive
"['\n\t\t Given a list of identified names , Meng et al . \n\t', '\n\t\t first separated the names into Chinese names and English names . \n\t', '\n\t\t Romanized Chinese names were detected by a leftto-right longest match segmentation method , using the Wade-Giles2 and the pinyin syllable inventories in sequence . \n\t', '\n\t\t If a name could be segmented successfully , then the name was considered a Chinese name . \n\t', '\n\t\t As their spoken document collection had already been transcribed into pinyin , retrieval was based on pinyin-to-pinyin matching ; pinyin to Chinese character conversion was not addressed . \n\t', '\n\t\t Names other than Chinese names were considered as foreign names and were converted into Chinese phonemes using a language model derived from a list of English-Chinese equivalents , both sides of which were represented in phonetic equivalents . \n\t', '\n\t\t * The work was done by the author while at Clairvoyance Corporation . \n\t', '\n\t\t 1 http://www.csse.monash.edu.au/~jwb/edict.html 2 http://lcweb.loc.gov/catdir/pinyin/romcover.html The above English-to-Japanese or English-toChinese transliteration techniques , however , only solve a part of the name translation problem . \n\t', '\n\t\t In multilingual applications such as CLIR and Machine Translation , all types of names must be translated . \n\t', '\n\t\t Techniques for name translation from Latin scripts into CJK scripts often depend on the origin of the name . \n\t', '\n\t\t Some names are not transliterated into a nearly deterministic syllabic script but into ideograms that can be associated with a variety of pronunciations . \n\t', '\n\t\t For example , Chinese , Korean and Japanese names are usually written using Chinese characters ( or kanji ) in Japanese , while European names are transcribed using katakana characters , with each character mostly representing one syllable . \n\t', '\n\t\t In this paper , we describe a method for converting a Japanese name written with a Latin alphabet ( or romanji ) , back into Japanese kanji3 . \n\t', '\n\t\t Transcribing into Japanese kanji is harder than transliteration of a foreign name into syllabic katakana , since one phoneme can correspond to hundreds of possible kanji characters . \n\t', '\n\t\t For example , the sound \x93kou\x94 can be mapped to 670 kanji characters . \n\t', '\n\t\t Our method for back-transliterating Japanese names from English into Japanese consists of the following steps : ( 1 ) language identification of the origins of names in order to know what language- specific transliteration approaches to use , ( 2 ) generation of possible transliterations using sound and kanji mappings from the Unihan database ( to be described in section 3.1 ) and then transliteration validation through a three-tier filtering process by filtering first through a set of attested bigrams , then through a set of attested terms , and lastly through the Web. . \n\t', '\n\t\t The rest of the paper is organized as follows : in section 2 , we describe and evaluate our name origin identifier ; section 3 presents in detail the steps for back transliterating Japanese names written in Latin script into Japanese kanji representations ; section 4 presents the evaluation setup and section 5 discusses the evaluation results ; we conclude the paper in section 6 . \n\t', '\n\t\t 2 Language Identification of Names Given a name in English for which we do not have a translation in a bilingual English-Japanese dictionary , we first have to decide whether the name is of Japanese , Chinese , Korean or some European origin . \n\t', '\n\t\t In order to determine the origin of names , we created a language identifier for names , using a trigram language identification 3 We have applied the same technique to Chinese and Korean names , though the details are not presented here . \n\t', '\n\t\t method \n\t\t']",Positive
"['\n\t\t During training , for Chinese names , we used a list of 11,416 Chinese names together with their frequency information4 . \n\t', '\n\t\t For Japanese names , we used the list of 83,295 Japanese names found in ENAMDICT5 . \n\t', '\n\t\t For English names , we used the list of 88,000 names found at the US . \n\t', '\n\t\t Census site6 . \n\t', '\n\t\t ( We did not obtain any training data for Korean names , so origin identification for Korean names is not available . \n\t', '\n\t\t ) Each list of names7 was converted into trigrams ; the trigrams for each list were then counted and normalized by dividing the count of the trigram by the number of all the trigrams . \n\t', '\n\t\t To identify a name as Chinese , Japanese or English ( Other , actually ) , we divide the name into trigrams , and sum up the normalized trigram counts from each language . \n\t', '\n\t\t A name is identified with the language which provides the maximum sum of normalized trigrams in the word . \n\t', '\n\t\t Table 1 presents the results of this simple trigram-based language identifier over the list of names used for training the trigrams . \n\t', '\n\t\t The following are examples of identification errors : Japanese names recognized as English , e.g. , aa , abason , abire , aebakouson ; Japanese names recognized as Chinese , e.g. , abeseimei , abei , adan , aden , afun , agei , agoin . \n\t', '\n\t\t These errors show that the language identifier can be improved , possibly by taking into account language-specific features , such as the number of syllables in a name . \n\t', '\n\t\t For origin detection of Japanese names , the current method works well enough for a first pass with an accuracy of 92 % . \n\t', '\n\t\t Input names As JAP As CHI As ENG Accuracy Japanese 76816 5265 1212 92 % Chinese 1147 9947 321 87 % English 12115 14893 61701 70 % Table 1 : Accuracy of language origin identification for names in the training set ( JAP , CHI , and ENG stand for Japanese , Chinese , and English , respectively ) 4 http://www.geocities.com/hao510/namelist/ 5 http://www.csse.monash.edu.au/~jwb/ enamdict_doc.html 6 http://www.census.gov/genealogy/names 7 Some names appear in multiple name lists : 452 of the names are found both in the Japanese name list and in the Chinese name list ; 1529 names appear in the Japanese name list and the US Census name list ; and 379 names are found both in the Chinese name list and the US Census list . \n\t', '\n\t\t 3 English-Japanese Back-Transliteration Once the origin of a name in Latin scripts is identified , we apply language-specific rules for back-transliteration . \n\t', '\n\t\t For non-Asian names , we use a katakana transliteration method as described in \n\t\t']",Positive
"['\n\t\t For Japanese and Chinese names , we use the method described below . \n\t', '\n\t\t For example , \x93koizumi\x94 is identified as a name of Japanese origin and thus is back-transliterated to Japanese using Japanese specific phonetic mappings between romanji and kanji characters . \n\t', '\n\t\t 3.1 Romanji-Kanji Mapping To obtain the mappings between kanji characters and their romanji representations , we used the Unihan database , prepared by the Unicode Consortium 8 . \n\t', '\n\t\t The Unihan database , which currently contains 54,728 kanji characters found in Chinese , Japanese , and Korean , provides rich information about these kanji characters , such as the definition of the character , its values in different encoding systems , and the pronunciation(s) of the character in Chinese ( listed under the feature kMandarin in the Unihan database ) , in Japanese ( both the On reading and the Kun reading 9 : kJapaneseKun and kJapaneseOn ) , and in Korean ( kKorean ) . \n\t', '\n\t\t For example , for the kanji character , coded with Unicode hexadecimal character 91D1 , the Unihan database lists 49 features ; we list below its pronunciations in Japanese , Chinese , and Korean : U+91D1 kJapaneseKun KANE U+91D1 kJapaneseOn KIN KON U+91D1 kKorean KIM KUM U+91D1 kMandarin JIN1 JIN4 In the example above , is represented in its Unicode scalar value in the first column , with a feature name in the second column and the values of the feature in the third column . \n\t', '\n\t\t The Japanese Kun reading of is KANE , while the Japanese On readings of is KIN and KON . \n\t', '\n\t\t From the Unicode database , we construct mappings between Japanese readings of a character in romanji and the kanji characters in its Unicode representation . \n\t', '\n\t\t As kanji characters in Japanese names can have either the Kun reading or the On 8 http://www.unicode.org/charts/unihan.html 9 Historically , when kanji characters were introduced into the Japanese writing system , two methods of transcription were used . \n\t', '\n\t\t One is called \x93on-yomi\x94 ( i.e. , On reading ) , where the Chinese sounds of the characters were adopted for Japanese words . \n\t', '\n\t\t The other method is called \x93kun-yomi\x94 ( i.e. , Kun reading ) , where a kanji character preserved its meaning in Chinese , but was pronounced using the Japanese sounds . \n\t', '\n\t\t reading , we consider both readings as candidates for each kanji character . \n\t', '\n\t\t The mapping table has a total of 5,525 entries . \n\t', '\n\t\t A typical mapping is as follows : kou U+4EC0 U+5341 U+554F U+5A09 U+5B58 U+7C50 U+7C58 ...... in which the first field specifies a pronunciation in romanji , while the rest of the fields specifies the possible kanji characters into which the pronunciation can be mapped . \n\t', '\n\t\t There is a wide variation in the distribution of these mappings . \n\t', '\n\t\t For example , kou can be the pronunciation of 670 kanji characters , while the sound katakumi can be mapped to only one kanji character . \n\t', '\n\t\t 3.2 Romanji Name Back-Transliteration In theory , once we have the mappings between romanji characters and the kanji characters , we can first segment a Japanese name written in romanji and then apply the mappings to back-transliterate the romanji characters into all possible kanji representations . \n\t', '\n\t\t However , for some segmentation , the number of the possible kanji combinations can be so large as to make the problem computationally intractable . \n\t', '\n\t\t For example , consider the short Japanese name \x93koizumi.\x94 This name can be segmented into the romanji characters \x93ko-i-zu-mi\x94 using the Romanji-Kanji mapping table described in section 3.1 , but this segmentation then has 182*230*73*49 ( over 149 million ) possible kanji combinations . \n\t', '\n\t\t Here , 182 , 239 , 73 , and 49 represents the numbers of possible kanji characters for the romanji characters \x93ko\x94 , \x93i\x94 , \x93zu\x94 , and \x93mi\x94 , respectively . \n\t', '\n\t\t In this study , we present an efficient procedure for back-transliterating romanji names to kanji characters that avoids this complexity . \n\t', '\n\t\t The procedure consists of the following steps : ( 1 ) romanji name segmentation , ( 2 ) kanji name generation , ( 3 ) kanji name filtering via monolingual Japanese corpus , and ( 4 ) kanjiromanji combination filtering via WWW . \n\t', '\n\t\t Our procedure relies on filtering using corpus statistics to reduce the hypothesis space in the last three steps . \n\t', '\n\t\t We illustrate the steps below using the romanji name \x93koizumi\x94 ( . \n\t', '\n\t\t 3.2.1 Romanji Name Segmentation With the romanji characters from the RomanjiKanji mapping table , we first segment a name recognized as Japanese into sequences of romanji characters . \n\t', '\n\t\t Note that a greedy segmentation method , such as the left-to-right longest match method , often results in segmentation errors . \n\t', '\n\t\t For example , for \x93koizumi\x94 , the longest match segmentation method produces segmentation \x93koi- zu-mi\x94 , while the correct segmentation is \x93koizumi\x94 . \n\t', '\n\t\t Motivated by this observation , we generate all the possible segmentations for a given name . \n\t', '\n\t\t The possible segmentations for \x93koizumi\x94 are : ko-izu mi koi-zu-mi ko-i-zu-mi 3.2.2 Kanji Name Segmentation Using the same Romanji-Kanji mapping table , we obtain the possible kanji combinations for a segmentation of a romanji name produced by the previous step . \n\t', '\n\t\t For the segmentation \x93ko-izumi\x94 , we have a total of 546 ( 182*3 ) combinations ( we use the Unicode scale value to represent the kanji characters and use spaces to separate them ) : U+5C0F U+6CC9 U+53E4 U+6CC9 We do not produce all possible combinations . \n\t', '\n\t\t As we have discussed earlier , such a generation method can produce so many combinations as to make computation infeasible for longer segmentations . \n\t', '\n\t\t To control this explosion , we eliminate unattested combinations using a bigram model of the possible kanji sequences in Japanese . \n\t', '\n\t\t From the Japanese evaluation corpus of the NTCIR-4 CLIR track 10 , we collected bigram statistics by first using a statistical part-of-speech tagger of Japanese \n\t\t']",Positive
"['\n\t\t All valid Japanese terms and their frequencies from the tagger output were extracted . \n\t', '\n\t\t From this term list , we generated kanji bigram statistics ( as well as an attested term list used below in step 3 ) . \n\t', '\n\t\t With this bigram-based model , our hypothesis space is significantly reduced . \n\t', '\n\t\t For example , with the segmentation \x93ko-i-zu-mi\x94 , even though \x93ko-i\x94 can have 182*230 possible combinations , we only retain the 42 kanji combinations that are attested in the corpus . \n\t', '\n\t\t Continuing with the romanji segments \x93i-zu\x94 , we generate the possible kanji combinations for \x93i-zu\x94 that can continue one of the 42 candidates for \x93koi\x94 . \n\t', '\n\t\t This results in only 6 candidates for the segments \x93ko-i-zu\x94 . \n\t', '\n\t\t Lastly , we consider the romanji segments \x93zumi\x94 , and retain with only 4 candidates for the segmentation \x93ko-i-zu-mi\x94 whose bigram sequences are attested in our language model : U+5C0F U+53F0 U+982D U+8EAB U+5B50 U+61 0F U+56F3 U+5B50 U+5C0F U+61 0F U+56F3 U+5B50 U+6545 U+61 0F U+56F3 U+5B50 Thus , for the segmentation \x93ko-i-zu-mi\x94 , the bigram-based language model effectively reduces the hypothesis space from 182*230*73*49 possible kanji combinations to 4 candidates . \n\t', '\n\t\t For the other alternative segmentation \x93koi-zu-mi\x94 , no candidates can be generated by the language model . \n\t', '\n\t\t 3.2.3 Corpus-based Kanji name Filtering In this step , we use a monolingual Japanese corpus to validate whether the kanji name candidates generated by step ( 2 ) are attested in the corpus . \n\t', '\n\t\t Here , we simply use Japanese term list extracted from the segmented NTCIR-4 corpus created for the previous step to filter out unattested kanji combinations . \n\t', '\n\t\t For the segmentation \x93koizumi\x94 , the following kanji combinations are attested in the corpus ( preceded by their frequency in the corpus ) : 4167 koizumi 16 koizumi 4 koizumi None of the four kanji candidates from the alternate segmentation \x93ko-i-zu-mi\x94 is attested in the corpus . \n\t', '\n\t\t While step 2 filters out candidates using bigram sequences , step 3 uses corpus terms in their entirety to validate candidates . \n\t', '\n\t\t 3.2.4 Romanji-Kanji Combination Validation Here , we take the corpus-validated kanji candidates ( but for which we are not yet sure if they correspond to the same reading as the original Japanese name written in romanji ) and use the Web to validate the pairings of kanji-romanji combinations ( e.g. , AND koizumi ) . \n\t', '\n\t\t This is motivated by two observations . \n\t', '\n\t\t First , in contrast to monolingual corpus , Web pages are often mixed- lingual . \n\t', '\n\t\t It is often possible to find a word and its translation on the same Web pages . \n\t', '\n\t\t Second , person names and specialized terminology are among the most frequent mixed-lingual items . \n\t', '\n\t\t Thus , we would expect that the appearance of both representations in close proximity on the same pages gives us more confidence in the kanji representations . \n\t', '\n\t\t For example , with the Google search engine , all three kanji-romanji combinations for \x93koizumi\x94 are attested : 23,600 pages -- koizumi 302 pages -- koizumi 1 page -- koizumi Among the three , the koizumi combination is the most common one , being the name of the current Japanese Prime Minister . \n\t', '\n\t\t 10 http://research.nii.ac.jp/ntcir-ws4/clir/index.html 4 Evaluation In this section , we describe the gold standards and evaluation measures for evaluating the effectiveness of the above method for back- transliterating Japanese names . \n\t', '\n\t\t 4.1 Gold Standards Based on two publicly accessible name lists and a Japanese-to-English name lexicon , we have constructed two Gold Standards . \n\t', '\n\t\t The Japanese-toEnglish name lexicon is ENAMDICT11 , which contains more than 210,000 Japanese-English name translation pairs . \n\t', '\n\t\t Gold Standard \x96 Given Names ( GS-GN ) : to construct a gold standard for Japanese given names , we obtained 7,151 baby names in romanji from http://www.kabalarians.com/ . \n\t', '\n\t\t Of these 7,151 names , 5,115 names have kanji translations in the ENAMDICT12 . \n\t', '\n\t\t We took the 5115 romanji names and their kanji translations in the ENAMDICT as the gold standard for given names . \n\t', '\n\t\t Gold Standard \x96 Surnames ( GS-SN ) : to construct a gold standard for Japanese surnames , we downloaded 972 surnames in romanji from http://business.baylor.edu/Phil_VanAuken/Japanes eSurnames.html . \n\t', '\n\t\t Of these names , 811 names have kanji translations in the ENAMDICT . \n\t', '\n\t\t We took these 811 romanji surnames and their kanji translations in the ENAMDICT as the gold standard for Japanese surnames . \n\t', '\n\t\t 4.2 Evaluation Measures Each name in romanji in the gold standards has at least one kanji representation obtained from the ENAMDICT . \n\t', '\n\t\t For each name , precision , recall , and F measures are calculated as follows : \x95 Precision : number of correct kanji output / total number of kanji output \x95 Recall : number of correct kanji output / total number of kanji names in gold standard \x95 F-measure : 2*Precision*Recall / ( Precision + Recall ) Average Precision , Average Recall , and Average F-measure are computed over all the names in the test sets . \n\t', '\n\t\t 5 Evaluation Results and Analysis 5.1 Effectiveness of Corpus Validation Table 2 and Table 3 present the precision , recall , and F statistics for the gold standards GS-GN and 11 http://mirrors.nihongo.org/monash/ enamdict_doc.html 12 The fact that above 2000 of these names were missing from ENAMDICT is a further justification for a name translation method as described in this paper . \n\t', '\n\t\t GS-SN , respectively . \n\t', '\n\t\t For given names , corpus validation produces the best average precision of 0.45 , while the best average recall is a low 0.27 . \n\t', '\n\t\t With the additional step of Web validation of the romanji-kanji combinations , the average precision increased by 62.2 % to 0.73 , while the best average recall improved by 7.4 % to 0.29 . \n\t', '\n\t\t We observe a similar trend for surnames . \n\t', '\n\t\t The results demonstrate that , through a large , mixed-lingual corpus such as the Web , we can improve both precision and recall for automatically transliterating romanji names back to kanji . \n\t', '\n\t\t Avg Avg F Prec Recall ( 1 ) Corpus 0.45 0.27 0.33 ( 2 ) Web 0.73 0.29 0.38 ( over ( 1 ) ) ( +62.2 % ) ( +7.4 % ) ( +15.2 % ) Table 2 : The best Avg Precision , Avg Recall , and Avg F statistics achieved through corpus validation and Web validation for GS-GN . \n\t', '\n\t\t Avg Avg F Prec Recall ( 1 ) Corpus 0.69 0.44 0.51 ( 2 ) Web 0.90 0.45 0.56 ( over ( 1 ) ) ( +23.3 % ) ( +2.3 % ) ( +9.8 % ) Table 3 : The best Avg Precision , Avg Recall , and Avg F statistics achieved through corpus validation and Web validation for GS-SN . \n\t', '\n\t\t We also observe that the performance statistics for the surnames are significantly higher than those of the given names , which might reflect the different degrees of flexibility in using surnames and given names in Japanese . \n\t', '\n\t\t We would expect that the surnames form a somewhat closed set , while the given names belong to a more open set . \n\t', '\n\t\t This may account for the higher recall for surnames . \n\t', '\n\t\t 5.2 Effectiveness of Corpus Validation If the big , mixed-lingual Web can deliver better validation than the limited-sized monolingual corpus , why not use it at every stage of filtering ? \n\t', '\n\t\t Technically , we could use the Web as the ultimate corpus for validation at any stage when a corpus is required . \n\t', '\n\t\t In practice , however , each Web access involves additional computation time for file IO , network connections , etc. . \n\t', '\n\t\t For example , accessing Google took about 2 seconds per name13 ; gathering 13 We inserted a 1 second sleep between calls to the search engine so as not to overload the engine . \n\t', '\n\t\t statistics for about 30,000 kanji-romanji combinations14 took us around 15 hours . \n\t', '\n\t\t In the procedure described in section 3.2 , we have aimed to reduce computation complexity and time at several stages . \n\t', '\n\t\t In step 2 , we use bigrambased language model from a corpus to reduce the hypothesis space . \n\t', '\n\t\t In step 3 , we use corpus filtering to obtain a fast validation of the candidates , before passing the output to the Web validation in step 4 . \n\t', '\n\t\t Table 4 illustrates the savings achieved through these steps . \n\t', '\n\t\t GS-GN GS-SN All possible 2.0e+017 296,761,622,763 2gram model 21,306,322 2,486,598 ( -99.9 % ) ( -99.9 % ) Corpus 30,457 3,298 validate ( -99.9 % ) ( -99.9 % ) Web validation 20,787 2,769 ( -31.7 % ) ( -16.0 % ) Table 4 : The numbers of output candidates of each step to be passed to the next step . \n\t', '\n\t\t The percentages specify the amount of reduction in hypothesis space . \n\t', '\n\t\t 5.3 Thresholding Effects We have examined whether we should discard the validated candidates with low frequencies either from the corpus or the Web. . \n\t', '\n\t\t The cutoff points examined include initial low frequency range 1 to 10 and then from 10 up to 400 in with increments of 5 . \n\t', '\n\t\t Figure 1 and Figure 2 illustrate that , to achieve best overall performance , it is beneficial to discard candidates with very low frequencies , e.g. , frequencies below 5 . \n\t', '\n\t\t Even though we observe a stabling trend after reaching certain threshold points for these validation methods , it is surprising to see that , for the corpus validation method with GS-GN , with stricter thresholds , average precisions are actually decreasing . \n\t', '\n\t\t We are currently investigating this exception . \n\t', '\n\t\t 5.4 Error Analysis Based on a preliminary error analysis , we have identified three areas for improvements . \n\t', '\n\t\t First , our current method does not account for certain phonological transformations when the On/Kun readings are concatenated together . \n\t', '\n\t\t Consider the name \x93matsuda\x94 ( ) . \n\t', '\n\t\t The segmentation step correctly segmented the romanji to \x93matsu-da\x94 . \n\t', '\n\t\t However , in the Unihan database , 14 At this rate , checking the 21 million combinations remaining after filtering with bigrams using the Web ( without the corpus filtering step ) would take more than a year . \n\t', '\n\t\t the Kun reading of is \x93ta\x94 , while its On reading is \x93den\x94 . \n\t', '\n\t\t Therefore , using the mappings from the Unihan database , we failed to obtain the mapping between the pronunciation \x93da\x94 and the kanji , which resulted in both low precision and recall for \x93matsuda\x94 . \n\t', '\n\t\t This suggests for introducing language-specific phonological transformations or alternatively fuzzy matching to deal with the mismatch problem . \n\t', '\n\t\t Figure 1 : Average precisions achieved via both corpus and corpus+Web validation with different frequency-based cutoff thresholds for GS-GN Figure 2 : Average precisions achieved via both corpus and corpus+Web validation with different frequency-based cutoff thresholds for GS-SN Second , ENAMDICT contains mappings between kanji and romanji that are not available from the Unihan database . \n\t', '\n\t\t For example , for the name \x93hiroshi\x94 in romanji , based on the mappings from the Unihan database , we can obtain two possible segmentations : \x93hiro-shi\x94 and \x93hi-ro-shi\x94 . \n\t', '\n\t\t Our method produces two- and three-kanji character sequences that correspond to these romanji characters . \n\t', '\n\t\t For example , corpus validation produces the following kanji candidates for \x93hiroshi\x94 : Avg Precision - GS_GN 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 corpus+web corpus Threshold for frequency cutoff 0 Avg Precision - GS_SN 1 corpus+web corpus Threshold for frequency cutoff 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 ENAMDCIT , however , in addition to the 2- and 3-character kanji names , also contains 1-character kanji names , whose mappings are not found in the Unihan database , e.g. , Hiroshi Hiroshi Hiroshi Hiroshi Hiroshi Hiroshi This suggests the limitation of relying solely on the Unihan database for building mappings between romanji characters and kanji characters . \n\t', '\n\t\t Other mapping resources , such as ENAMDCIT , should be considered in our future work . \n\t', '\n\t\t Third , because the statistical part-of-speech tagger we used for Japanese term identification does not have a lexicon of all possible names in Japanese , some unknown names , which are incorrectly separated into individual kanji characters , are therefore not available for correct corpus-based validation . \n\t', '\n\t\t We are currently exploring methods using overlapping character bigrams , instead of the tagger-produced terms , as the basis for corpus-based validation and filtering . \n\t', '\n\t\t 6 Conclusions In this study , we have examined a solution to a previously little treated problem of transliterating CJK names written in Latin scripts back into their ideographic representations . \n\t', '\n\t\t The solution involves first identifying the origins of the CJK names and then back-transliterating the names to their respective ideographic representations with language-specific sound-to-character mappings . \n\t', '\n\t\t We have demonstrated that a simple trigram-based language identifier can serve adequately for identifying names of Japanese origin . \n\t', '\n\t\t During back-transliteration , the possibilities can be massive due to the large number of mappings between a Japanese sound and its kanji representations . \n\t', '\n\t\t To reduce the complexity , we apply a three-tier filtering process which eliminates most incorrect candidates , while still achieving an F measure of 0.38 on a test set of given names , and an F measure of 0.56 on a test of surnames . \n\t', '\n\t\t The three filtering steps involve using a bigram model derived from a large segmented Japanese corpus , then using a list of attested corpus terms from the same corpus , and lastly using the whole Web as a corpus . \n\t', '\n\t\t The Web is used to validate the back- transliterations using statistics of pages containing both the candidate kanji translation as well as the original romanji name . \n\t', '\n\t\t Based on the results of this study , our future work will involve testing the effectiveness of the current method in real CLIR applications , applying the method to other types of proper names and other language pairs , and exploring new methods for improving precision and recall for romanji name back-transliteration . \n\t', '\n\t\t In cross-language applications such as English to Japanese retrieval , dealing with a romaji name that is missing in the bilingual lexicon should involve ( 1 ) identifying the origin of the name for selecting the appropriate language-specific mappings , and ( 2 ) automatically generating the back-transliterations of the name in the right orthographic representations ( e.g. , Katakana representations for foreign Latin-origin names or kanji representations for native Japanese names ) . \n\t', '\n\t\t To further improve precision and recall , one promising technique is fuzzy matching \n\t\t']",Positive
"['\n\t\t Lastly , we will explore whether the proposed romanji to kanji back- transliteration approach applies to other types of names such as place names and study the effectiveness of the approach for back- transliterating romanji names of Chinese origin and Korean origin to their respective kanji representations . \n\t', '\n\t\t References Yaser Al-Onaizan and Kevin Knight . \n\t', '\n\t\t 2002. Machine Transliteration of Names in Arabic Text . \n\t', '\n\t\t Proc . \n\t', '\n\t\t of ACL Workshop on Computational Approaches to Semitic Languages William B. Cavnar and John M. Trenkle . \n\t', '\n\t\t 1994. N- gram based text categorization . \n\t', '\n\t\t In 3rd Annual Symposium on Document Analysis and Information Retrieval , 161-175 Atsushi Fujii and Tetsuya Ishikawa . \n\t', '\n\t\t 2001. Japanese/English Cross-Language Information Retrieval : Exploration of Query Translation and Transliteration . \n\t', '\n\t\t Computer and the Humanities , 35( 4 ) : 389\x96420 K. S. Jeong , Sung-Hyon Myaeng , J. S. Lee , and K. S. Choi . \n\t', '\n\t\t 1999. Automatic identification and back-transliteration of foreign words for information retrieval . \n\t', '\n\t\t Information Processing and Management , 35(4) : 523-540 2 hiroshi 10 hiroshi 5 hiroshi 1 hiroshi 2 hiroshi 11 hiroshi 33 hiroshi 311 hiroshi Kevin Knight and Jonathan Graehl . \n\t', '\n\t\t 1998. Machine Transliteration . \n\t', '\n\t\t Computational Linguistics : 24(4) : 599-612 Wen-Cheng Lin , Changhua Yang and Hsin-Hsi Chen . \n\t', '\n\t\t 2003. Foreign Name Backward Transliteration in Chinese-English Cross- Language Image Retrieval , In Proceedings of CLEF 2003 Workshop , Trondheim , Norway . \n\t', '\n\t\t Helen Meng , Wai-Kit Lo , Berlin Chen , and Karen Tang . \n\t', '\n\t\t 2001. Generating Phonetic Cognates to Handel Named Entities in English-Chinese Cross-Language Spoken Document Retrieval . \n\t', '\n\t\t In Proc of the Automatic Speech Recognition and Understanding Workshop ( ASRU 2001 ) Trento , Italy , Dec. . \n\t', '\n\t\t Yan Qu , Gregory Grefenstette , David A. Evans . \n\t', '\n\t\t 2003. Automatic transliteration for Japanese-toEnglish text retrieval . \n\t', '\n\t\t In Proceedings of SIGIR 2003 : 353-360 Yan Qu , Gregory Grefenstette , David A. Hull , David A. Evans , Toshiya Ueda , Tatsuo Kato , Daisuke Noda , Motoko Ishikawa , Setsuko Nara , and Kousaku Arita . \n\t', '\n\t\t 2004. JustsystemClairvoyance CLIR Experiments at NTCIR-4 Workshop . \n\t', '\n\t\t In Proceedings of the NTCIR-4 Workshop . \n\t', '\n\t\t Extracting Regulatory Gene Expression Networks from PubMed Jasmin ^Sari´c Lars J. Jensen Rossitza Ouzounova EML Research gGmbH EMBL EMBL Heidelberg , Germany Heidelberg , Germany Heidelberg , Germany saric@eml-r.org jensen@embl.de ouzounov@embl.de Isabel Rojas Peer Bork EML Research gGmbH EMBL Heidelberg , Germany Heidelberg , Germany rojas@eml-r.org bork@embl.de Abstract We present an approach using syntactosemantic rules for the extraction of relational information from biomedical abstracts . \n\t', '\n\t\t The results show that by overcoming the hurdle of technical terminology , high precision results can be achieved . \n\t', '\n\t\t From abstracts related to baker\x92s yeast , we manage to extract a regulatory network comprised of 441 pairwise relations from 58,664 abstracts with an accuracy of 83\x9690 % . \n\t', '\n\t\t To achieve this , we made use of a resource of gene/protein names considerably larger than those used in most other biology related information extraction approaches . \n\t', '\n\t\t This list of names was included in the lexicon of our retrained part-of-speech tagger for use on molecular biology abstracts . \n\t', '\n\t\t For the domain in question an accuracy of 93.6\x9697.7 % was attained on POS-tags . \n\t', '\n\t\t The method is easily adapted to other organisms than yeast , allowing us to extract many more biologically relevant relations . \n\t', '\n\t\t 1 Introduction and related work A massive amount of information is buried in scientific publications ( more than 500,000 publications per year ) . \n\t', '\n\t\t Therefore , the need for information extraction ( IE ) and text mining in the life sciences is drastically increasing . \n\t', '\n\t\t Most of the ongoing work is being dedicated to deal with PubMedl abstracts . \n\t', '\n\t\t The technical terminology of biomedicine presents the main challenge of applying IE to such a corpus \n\t\t']",Positive
"['\n\t\t The goal of our work is to extract from biological abstracts which proteins are responsible for regulating the expression ( i.e. transcription or translation ) of which genes . \n\t', '\n\t\t This means to extract a specific type of pairwise relations between biological entities . \n\t', '\n\t\t This differs from the BioCreAtIvE competition tasks that aimed at classifying entities ( gene products ) into classes based on Gene Ontology \n\t\t']",Positive
"['\n\t\t A task closely related to ours , which has received some attention over the past five years , is the extraction of protein\x96protein interactions from abstracts . \n\t', '\n\t\t This problem has mainly been addressed by statistical \x93bag of words\x94 approaches \n\t\t']",Negative
"['\n\t\t All of the approaches differ significantly from ours by only attempting to extract the type of interaction and the participating proteins , disregarding agens and patiens . \n\t', '\n\t\t Most NLP based studies tend to have been focused on extraction of events involving one particular verb , e.g. bind \n\t\t']",Negative
"['\n\t\t From a biological point of view , there are two problems with such approaches : 1 ) the meaning of the extracted events 1PubMed is a bibliographic database covering life sciences with a focus on biomedicine , comprising around 12 x 106 articles , roughly half of them including abstract ( ht tp : //www.ncbi.nlm.nih.gov/PubMed/ ) . \n\t', '\n\t\t 2Critical Assessment of Information Extraction systems in Biology , http://www.mitre.org/public/ biocreative/ will depend strongly on the selectional restrictions and 2 ) the same meaning can be expressed using a number of different verbs . \n\t', '\n\t\t In contrast and alike \n\t\t']",Positive
"['\n\t\t The variety in the biological terminology used to describe regulation of gene expression presents a major hurdle to an IE approach ; in many cases the information is buried to such an extent that even a human reader is unable to extract it unless having a scientific background in biology . \n\t', '\n\t\t In this paper we will show that by overcoming the terminological barrier , high precision extraction of entity relations can be achieved within the field of molecular biology . \n\t', '\n\t\t 2 The biological task and our approach To extract relations , one should first recognize the named entities involved . \n\t', '\n\t\t This is particularly difficult in molecular biology where many forms of variation frequently occur . \n\t', '\n\t\t Synonymy is very common due to lack of standardization of gene names ; BYP1 , CIF1 , FDP1 , GGS1 , GLC6 , TPS1 , TSS1 , and YBR126C are all synonyms for the same gene/protein . \n\t', '\n\t\t Additionally , these names are subject to orthographic variation originating from differences in capitalization and hyphenation as well as syntactic variation of multiword terms ( e.g. riboflavin synthetase beta chain = beta chain of riboflavin synthetase ) . \n\t', '\n\t\t Moreover , many names are homonyms since a gene and its gene product are usually named identically , causing cross-over of terms between semantic classes . \n\t', '\n\t\t Finally , paragrammatical variations are more frequent in life science publications than in common English due to the large number of publications by non-native speakers \n\t\t']",Positive
"['\n\t\t Extracting that a protein regulates the expression of a gene is a challenging problem as this fact can be expressed in a variety of ways\x97possibly mentioning neither the biological process ( expression ) nor any of the two biological entities ( genes and proteins ) . \n\t', '\n\t\t Figure 1 shows a simplified ontology providing an overview of the biological entities involved in gene expression , their ontological relationships , and how they can interact with Figure 1 : A simplified ontology for transcription regulation . \n\t', '\n\t\t The background color used for each term signifies its semantic role in relations : regulator ( white ) , target ( black ) , or either ( gray ) . \n\t', '\n\t\t one another . \n\t', '\n\t\t An ontology is a great help when writing extraction rules , as it immediately suggests a large number of relevant relations to be extracted . \n\t', '\n\t\t Examples include \x93promoter contains upstream activating sequence\x94 and \x93transcription regulator binds to promoter\x94 , both of which follow from indirect relationships via binding site . \n\t', '\n\t\t It is often not known whether the regulation takes place at the level of gene transcription or translation or by an indirect mechanism . \n\t', '\n\t\t For this reason , and for simplicity , we decided against trying to extract how the regulation of expression takes place . \n\t', '\n\t\t We do , however , strictly require that the extracted relations provide information about a protein ( the regulator , R ) regulating the expression of a gene ( the target , X ) , for which reason three requirements must be fulfilled : 1 . \n\t', '\n\t\t It must be ascertained that the sentence mentions gene expression . \n\t', '\n\t\t \x93The protein R activates X\x94 fails this requirement , as R might instead activate X post-translationally . \n\t', '\n\t\t Thus , whether the event should be extracted or not depends on the type of the accusative object X ( e.g. gene or gene product ) . \n\t', '\n\t\t Without a head noun specifying the type , X remains ambiguous , leaving the whole relation underspeci- Transcript Upstream activating sequence Transcription regulator Transcription activator Transcription repressor is a part of produces binds to Gene product Stable RNA Upstream repressing sequence Promoter Binding site mRNA Protein Gene fied , for which reason it should not be extracted . \n\t', '\n\t\t It should be noted that two thirds of the gene/protein names mentioned in our corpus are ambiguous for this reason . \n\t', '\n\t\t 2. The identity of the regulator ( R ) must be known . \n\t', '\n\t\t \x93The X promoter activates X expression\x94 fails this requirement , as it is not known which transcription factor activates the expression when binding to the X promoter . \n\t', '\n\t\t Linguistically this implies that noun chunks of certain semantic types should be disallowed as agens . \n\t', '\n\t\t 3. The identity of the target ( X ) must be known . \n\t', '\n\t\t \x93The transcription factor R activates R dependent expression\x94 fails this requirement , as it is not know which gene\x92s expression is dependent on R . \n\t', '\n\t\t The semantic types allowed for patiens should thus also be restricted . \n\t', '\n\t\t The two last requirements are important to avoid extraction from non-informative sentences that\x97 despite them containing no information\x97occur quite frequently in scientific abstracts . \n\t', '\n\t\t The coloring of the entities in Figure 1 helps discern which relations are meaningful and which are not . \n\t', '\n\t\t The ability to genetically modify an organism in experiments brings about further complication to IE : biological texts often mention what takes place when an organism is artificially modified in a particular way . \n\t', '\n\t\t In some cases such modification can reverse part of the meaning of the verb : from the sentence \x93Deletion of R increased X expression\x94 one can conclude that R represses expression of X . \n\t', '\n\t\t The key point is to identify that \x93deletion of R\x94 implies that the sentence describes an experiment in which R has been removed , but that R would normally be present and that the biological impact of R is thus the opposite of what the verb increased alone would suggest . \n\t', '\n\t\t In other cases the verb will lose part of its meaning : \x93Mutation of R increased X expression\x94 implies that R regulates expression X , but we cannot infer whether R is an activator or a repressor . \n\t', '\n\t\t In this case mutation is dealt in a manner similar to deletion in the previous example . \n\t', '\n\t\t Finally , there are those relations that should be completely avoided as they exist only because they have been artificially in troduced through genetic engineering . \n\t', '\n\t\t In our extraction method we address all three cases . \n\t', '\n\t\t We have opted for a rule based approach ( implemented as finite state automata ) to extract the relations for two reasons . \n\t', '\n\t\t The first is , that a rule based approach allows us to directly ensure that the three requirements stated above are fulfilled for the extracted relations . \n\t', '\n\t\t This is desired to attain high accuracy on the extracted relations , which is what matters to the biologist . \n\t', '\n\t\t Hence , we focus in our evaluation on the semantic correctness of our method rather than on its grammatical correctness . \n\t', '\n\t\t As long as grammatical errors do not result in semantic errors , we do not consider it an error . \n\t', '\n\t\t Conversely , even a grammatically correct extraction is considered an error if it is semantically wrong . \n\t', '\n\t\t Our second reason for choosing a rule based approach is that our approach is theory-driven and highly interdisciplinary , involving computational linguists , bioinformaticians , and biologists . \n\t', '\n\t\t The rule based approach allows us to benefit more from the interplay of scientists with different backgrounds , as known biological constraints can be explicitly incorporated in the extraction rules . \n\t', '\n\t\t 3 Methods Table 1 shows an overview of the architecture of our IE system . \n\t', '\n\t\t It is organized in levels such that the output of one level is the input of the next one . \n\t', '\n\t\t The following sections describe each level in detail . \n\t', '\n\t\t 3.1 The corpus The PubMed resource was downloaded on January 19 , 2004 . \n\t', '\n\t\t 58,664 abstracts related to the yeast Saccharomyces cerevisiae were extracted by looking for occurrences of the terms \x93Saccharomyces cerevisiae\x94 , \x93S . \n\t', '\n\t\t cerevisiae\x94 , \x93Baker\x92s yeast\x94 , \x93Brewer\x92s yeast\x94 , and \x93Budding yeast\x94 in the title/abstract or as head of a MeSH term3 . \n\t', '\n\t\t These abstracts were filtered to obtain the 15,777 that mention at least two names ( see section 3.4 ) and subsequently divided into a training and an evaluation set of 9137 and 6640 abstracts respectively . \n\t', '\n\t\t 3Medical Subject Headings ( MeSH ) is a controlled vocabulary for manually annoting PubMed articles . \n\t', '\n\t\t Level Component L0 Tokenization and multiwords Word and sentence boundaries are detected and multiwords are recognized and recomposed to one token . \n\t', '\n\t\t L1 POS-Tagging A part-of-speech tag is assigned to each word ( or multiword ) of the tokenized corpus . \n\t', '\n\t\t L2 Semantic labeling A manually built taxonomy is used to assign semantic labels to tokens . \n\t', '\n\t\t The taxonomy consists of gene names , cue words relevant for entity recognition , and classes of verbs for relation extraction . \n\t', '\n\t\t L3 Named entity chunking Based on the POS-tags and the semantic labels , a cascaded chunk grammar recognizes noun chunks relevant for the gene transcription domain , e.g. [ nxgene The GAL4 gene ] . \n\t', '\n\t\t L4 Relation chunking Relations between entities are recognized , e.g. . \n\t', '\n\t\t The expression of the cytochrome genes CYC1 and CYC7 is controlled by HAP1 . \n\t', '\n\t\t L5 Output and visualization Information is gathered from the recognised patterns and transformed into pre-defined records . \n\t', '\n\t\t From the example in L4 we extract that HAP1 regulates the expression of CYC1 and CYC7 . \n\t', '\n\t\t Table 1 : Overview over the extraction architecture 3.2 Tokenization and multiword detection The process of tokenization consists of two steps \n\t\t']",Positive
['\n\t\t We use the tokenizer developed by Helmut Schmid at IMS ( University of Stuttgart ) because it combines a high accuracy ( 99.56 % on the Brown corpus ) with unsupervised learning ( i.e. no manually labelled data is needed ) \n\t\t'],Positive
"['\n\t\t The determination of token boundaries in technical or scientific texts is one of the main chal lenges within information extraction or retrieval . \n\t', '\n\t\t On the one hand , technical terms contain special characters such as brackets , colons , hyphens , slashes , etc. . \n\t', '\n\t\t On the other hand , they often appear as multiword expressions which makes it hard to detect the left and right boundaries of the terms . \n\t', '\n\t\t Although a lot of work has been invested in the detection of technical terms within biology related texts ( see Nenadi´c et al . ( 2003 ) or \n\t\t']",Positive
"['\n\t\t As we are interested in very special terms and high precision results we opted for multiword detection based on semi-automatical acquisition of multi- words ( see sections 3.4 and 3.5 ) . \n\t', '\n\t\t 3.3 Part-of-speech tagging To improve the accuracy of POS-tagging on PubMed abstracts , TreeTagger \n\t\t']",Positive
"['\n\t\t Furthermore , we expanded the POStagger lexicon with entries relevant for our application such as gene names ( see section 3.4 ) and multiwords ( see section 3.5 ) . \n\t', '\n\t\t As tag set we use the UPenn tag set \n\t\t']",Positive
"['\n\t\t The GENIA 3.0 corpus consists of PubMed abstracts and has 466,179 manually annotated tokens . \n\t', '\n\t\t For our application we made two changes in the annotation . \n\t', '\n\t\t The first one concerns seemingly undecideable cases like in/or annotated as inIcc . \n\t', '\n\t\t These were split into three tokens : in , / , and or each annotated with its own tag . \n\t', '\n\t\t This was done because TreeTagger is not able to annotate two POS-tags for one token . \n\t', '\n\t\t The second set of changes was to adapt the tag set so that vb ... is used for derivates of to be , vh ... for derivates of to have , and vv ... for all other verbs . \n\t', '\n\t\t 3.4 Recognizing gene/protein names To be able to recognize gene/protein names as such , and to associate them with the appropriate database identifiers , a list of synonymous names and identifiers in six eukaryotic model organisms was compiled from several sources ( available from http://www.bork.embl . \n\t', '\n\t\t de/synonyms/ ) . \n\t', '\n\t\t For S. cerevisiae specifically , 51,640 uniquely resolvable names and identi- fiers were obtained from Saccharomyces Genome Database ( SGD ) and SWISS-PROT \n\t\t']",Positive
"['\n\t\t Before matching these names against the POStagged corpus , the list of names was expanded to include different orthographic variants of each name . \n\t', '\n\t\t Firstly , the names were allowed to have various combinations of uppercase and lowercase letters : all uppercase , all lowercase , first letter uppercase , and ( for multiword names ) first letter of each word uppercase . \n\t', '\n\t\t In each of these versions , we allowed whitespace to be replaced by hyphen , and hyphen to be removed or replaced by whites- pace . \n\t', '\n\t\t In addition , from each gene name a possible protein name was generated by appending the letter p . \n\t', '\n\t\t The resulting list containing all orthographic variations comprises 516,799 entries . \n\t', '\n\t\t The orthographically expanded name list was fed into the multiword detection , the POS-tagger lexicon , and was subsequently matched against the POS-tagged corpus to retag gene/protein names as such ( nnpg ) . \n\t', '\n\t\t By accepting only matches to words tagged as common nouns ( nn ) , the problem of homonymy was reduced since e.g. the name MAP can occur as a verb as well . \n\t', '\n\t\t 3.5 Semantic tagging In addition to the recognition of the gene and protein names , we recognize several other terms and annotate them with semantic tags . \n\t', '\n\t\t This set of semantically relevant terms mainly consists of nouns and verbs , as well as some few prepositions like from , or adjectives like dependent . \n\t', '\n\t\t The first main set of terms consists of nouns , which are classified as follows : \x95 Relevant concepts in our ontology : gene , protein , promoter , binding site , transcription factor , etc. ( 153 entries ) . \n\t', '\n\t\t \x95 Relational nouns , like nouns of activation ( e.g. derepression and positive regulation ) , nouns of repression ( e.g. suppression and negative regulation ) , nouns of regulation ( e.g. affect and control ) ( 69 entries ) . \n\t', '\n\t\t \x95 Triggering experimental ( artificial ) contexts : mutation , deletion , fusion , defect , vector , plasmids , etc. ( 11 entries ) . \n\t', '\n\t\t \x95 Enzymes : gyrase , kinase , etc. ( 569 entries ) . \n\t', '\n\t\t \x95 Organism names extracted from the NCBI taxonomy of organisms \n\t\t']",Positive
"['\n\t\t The second set of terms contains 50 verbs and their inflections . \n\t', '\n\t\t They were classified according to their relevance in gene transcription . \n\t', '\n\t\t These verbs are crucial for the extraction of relations between entities : \x95 Verbs of activation e.g. enhance , increase , induce , and positively regulate . \n\t', '\n\t\t \x95 Verbs of repression e.g. block , decrease , downregulate , and down regulate . \n\t', '\n\t\t \x95 Verbs of regulation e.g. affect and control . \n\t', '\n\t\t \x95 Other selected verbs like code ( or encode ) and contain where given their own tags . \n\t', '\n\t\t Each of the terms consisting of more than one word was utilized for multiword recognition . \n\t', '\n\t\t We also have have two additional classes of words to prevent false positive extractions . \n\t', '\n\t\t The first contains words of negation , like not , cannot , etc. . \n\t', '\n\t\t The other contains nouns that are to be distinguished from other common nouns to avoid them being allowed within named entitities , e.g. allele and diploid . \n\t', '\n\t\t 3.6 Extraction of named entities In the preceding steps we classified relevant nouns according to semantic criteria . \n\t', '\n\t\t This allows us to chunk noun phrases generalizing over both POStags and semantic tags . \n\t', '\n\t\t Syntacto-semantic chunking was performed to recognize named entities using cascades of finite state rules implemented as a CASS grammar \n\t\t']",Positive
"['\n\t\t As an example we recognize gene noun phrases : [ nx gene [ dt the ] [ nnpg CYC 1 ] [ gene gene ] [ in in ] [ yeast Saccharomyces cerevisiae ] ] Other syntactic variants , as for example \x93the glucokinase gene GLK1\x94 are recognized too . \n\t', '\n\t\t Similarly , we detect at this early level noun chunks de- noting other biological entities such as proteins , activators , repressors , transcription factors etc. . \n\t', '\n\t\t Subsequently , we recognize more complex noun chunks on the basis of the simpler ones , e.g. promoters , upstream activating/repressing sequences ( UAS/URS ) , binding sites . \n\t', '\n\t\t At this point it becomes important to distinguish between agens and patiens forms of certain entities . \n\t', '\n\t\t Since a binding site is part of a target gene , it can be referred to either by the name of this gene or by the name of the regulator protein that binds to it . \n\t', '\n\t\t It is thus necessary to discriminate between \x93binding site of\x94 and \x93binding site for\x94 . \n\t', '\n\t\t As already mentioned , we have annotated a class of nouns that trigger experimental context . \n\t', '\n\t\t On the basis of these we identify noun chunks mentioning , as for example deletion , mutation , or overexpression of genes . \n\t', '\n\t\t At a fairly late stage we recognize events that can occur as arguments for verbs like \x93expression of\x94 . \n\t', '\n\t\t 3.7 Extraction of relations between entities This step of processing concerns the recognition of three types of relations between the recognized named entities : up-regulation , down-regulation , and ( underspecified ) regulation of expression . \n\t', '\n\t\t We combine syntactic properties ( subcategorization restrictions ) and semantic properties ( selectional restrictions ) of the relevant verbs to map them to one of the three relation types . \n\t', '\n\t\t The following shows a reduced bracketed structure consting of three parts , a promoter chunk , a verbal complex chunk , and a UAS chunk in patiens : [ nx prom the ATR1 promoter region ] [ contain contains ] [ nx uas pt [ dt\x97a a ] [ bs binding site ] [ for for ] [ nx activator the GCN4 activator protein ] ] . \n\t', '\n\t\t From this we extract that the GCN4 protein activates the expression of the ATR1 gene . \n\t', '\n\t\t We identify passive constructs too e.g. \x93RNR1 expression is reduced by CLN1 or CLN2 overexpression\x94 . \n\t', '\n\t\t In this case we extract two pairwise relations , namely that both CLN1 and CLN2 down-regulate the expression of the RNR1 gene . \n\t', '\n\t\t We also identify nominalized relations as exemplified by \x93the binding of GCN4 protein to the SER1 promoter in vitro\x94 . \n\t', '\n\t\t 4 Results Using our relation extraction rules , we were able to extract 422 relation chunks from our complete corpus . \n\t', '\n\t\t Since one entity chunk can mention several different named entities , these corresponded to a total of 597 extracted pairwise relations . \n\t', '\n\t\t However , as several relation chunks may mention the same pairwise relations , this reduces to 441 unique pairwise relations comprised of 126 up-regulations , 90 down-regulations , and 225 regulations of unknown direction . \n\t', '\n\t\t Figure 2 displays these 441 relations as a regulatory network in which the nodes represent genes or proteins and the arcs are expression regulation relations . \n\t', '\n\t\t Known transcription factors according to the Saccharomyces Genome Database ( SGD ) \n\t\t']",Positive
"['\n\t\t From a biological point of view , it is reassuring that these tend to correspond to proteins serving as regulators in our relations . \n\t', '\n\t\t Figure 2 : The extracted network of gene regulation The extracted relations are shown as a directed graph , in which each node corresponds to a gene or protein and each arc represents a pairwise relation . \n\t', '\n\t\t The arcs point from the regulator to the target and the type of regulation is specified by the type of arrow head . \n\t', '\n\t\t Known transcription factors are highlighted as black nodes . \n\t', '\n\t\t 4.1 Evaluation of relation extraction To evaluate the accuracy of the extracted relation , we manually inspected all relations extracted from the evaluation corpus using the TIGERSearch visualization tool \n\t\t']",Positive
"['\n\t\t The accuracy of the relations was evaluated at the semantic rather than the grammatical level . \n\t', '\n\t\t We thus carried out the evaluation in such a way that relations were counted as correct if they extracted the correct biological conclusion , even if the analysis of the sentence is not as to be desired from a linguistic point of view . \n\t', '\n\t\t Conversely , a relation was counted as an error if the biological conclusion was wrong . \n\t', '\n\t\t 75 of the 90 relation chunks ( 83 % ) extracted from the evaluation corpus were entirely correct , meaning that the relation corresponded to expression regulation , the regulator ( R ) and the regulatee ( X ) were correctly identified , and the direction of regulation ( up or down ) was correct if extracted . \n\t', '\n\t\t Further 6 relation chunks extracted the wrong direction of regulation but were otherwise correct ; our accuracy increases to 90 % if allowing for this minor type of error . \n\t', '\n\t\t Approximately half of the errors made by our method stem from overlooked genetic modifications\x97although mentioned in the sentence , the extracted relation is not biologically relevant . \n\t', '\n\t\t 4.2 Entity recognition For the sake of consistency , we have also evaluated our ability to correctly identify named entities at the level of semantic rather than grammatical correctness . \n\t', '\n\t\t Manual inspection of 500 named entities from the evaluation corpus revealed 14 errors , which corresponds to an estimated accuracy ofjust over 97 % . \n\t', '\n\t\t Surprisingly , many of these errors were commited when recognizing proteins , for which our accuracy was only 95 % . \n\t', '\n\t\t Phrases such as \x93telomerase associated protein\x94 ( which got confused with \x93telomerase protein\x94 itself ) were responsible for about half of these errors . \n\t', '\n\t\t Among the 153 entities involved in relations no errors were detected , which is fewer than expected from our estimated accuracy on entity recognition ( 99 % confidence according to hypergeometric test ) . \n\t', '\n\t\t This suggests that the templates used for relation extraction are unlikely to match those sen tence constructs on which the entity recognition goes wrong . \n\t', '\n\t\t False identification of named entities are thus unlikely to have an impact on the accuracy of relation extraction . \n\t', '\n\t\t 4.3 POS-tagging and tokenization We compared the POS-tagging performance of two parameter files on 55,166 tokens from the GE- NIA corpus that were not used for retraining . \n\t', '\n\t\t Using the retrained tagger , 93.6 % of the tokens were correctly tagged , 4.1 % carried questionable tags ( e.g. confusing proper nouns for common nouns ) , and 2.3 % were clear tagging errors . \n\t', '\n\t\t This compares favourably to the 85.7 % correct , 8.5 % questionable tags , and 5.8 % errors obtained when using the Standard English parameter file . \n\t', '\n\t\t Retraining thus reduced the error rate more than two-fold . \n\t', '\n\t\t Of 198 sentences evaluated , the correct sentence boundary was detected in all cases . \n\t', '\n\t\t In addition , three abbreviations incorrectly resulted in sentence marker , corresponding to an overall precision of 98.5 % . \n\t', '\n\t\t 5 Conclusions We have developed a method that allows us to extract information on regulation of gene expression from biomedical abstracts . \n\t', '\n\t\t This is a highly relevant biological problem , since much is known about it although this knowledge has yet to be collected in a database . \n\t', '\n\t\t Also , knowledge on how gene expression is regulated is crucial for interpreting the enormous amounts of gene expression data produced by high-throughput methods like spotted microarrays and GeneChips . \n\t', '\n\t\t Although we developed and evaluated our method on abstracts related to baker\x92s yeast only , we have successfully applied the method to other organisms including humans ( to be published elsewhere ) . \n\t', '\n\t\t The main adaptation required was to replace the list of synonymous gene/protein names to reflect the change of organism . \n\t', '\n\t\t Furthermore , we also intend to reuse the recognition of named entities to extract other , specific types of interactions between biological entities . \n\t', '\n\t\t Acknowledgments The authors wish to thank Sean Hooper for help with Figure 2. Jasmin ^Sari´c is funded by the Klaus Tschira Foundation gGmbH , Heidelberg ( http : //www.kts.villa-bosch.de ) . \n\t', '\n\t\t Lars Juhl Jensen is funded by the Bundesministerium f¨ur Forschung und Bildung , BMBF-01-GG-9817 . \n\t', '\n\t\t References S. Abney . \n\t', '\n\t\t 1996. Partial parsing via finite-state cascades . \n\t', '\n\t\t In Proceedings of the ESSLLI \x9296 Robust Parsing Workshop , pages 8\x9615 , Prague , Czech Republic . \n\t', '\n\t\t M. Ashburner , C. A. Ball , J. A. Blake , D. Botstein , H. Butler , J. M. Cherry , A. P. Davis , K. Dolinski , S. S. Dwight , J. T. Eppig , M. A. Harris , D. P. Hill , L. Issel-Tarver , A. Kasarskis , S. Lewis , J. C. Matese , J. E. Richardson , M. Ringwald , G. M. Rubin , and G. Sherlock . \n\t', '\n\t\t 2000. Gene Ontology : tool for the unification of biology . \n\t', '\n\t\t Nature Genetics , 25:25\x9629 . \n\t', '\n\t\t C. Blaschke , M. A. Andrade , C. Ouzounis , and A. Valencia . \n\t', '\n\t\t 1999. Automatic extraction of biological information from scientific text : protein \x96protein interactions . \n\t', '\n\t\t In Proc. , Intelligent Systems for Molecular Biology , volume 7 , pages 60\x9667 , Menlo Park , CA . \n\t', '\n\t\t AAAI Press . \n\t', '\n\t\t B. Boeckmann , A. Bairoch , R. Apweiler , M. C. Blatter , A. Estreicher , E. Gasteiger , M. J. Martin , K Michoud , C. O\x92Donovan , I. Phan , S. Pilbout , and M. Schneider . \n\t', '\n\t\t 2003. The SWISS-PROT protein knowledgebase and its supplement TrEMBL in 2003 . \n\t', '\n\t\t Nucleic Acids Res. , 31:365\x96370 . \n\t', '\n\t\t S. S. Dwight , M. A. Harris , K. Dolinski , C. A. Ball , G. Binkley , K. R. Christie , D. G. Fisk , L. IsselTarver , M. Schroeder , G. Sherlock , A. Sethuraman , S. Weng , D. Botstein , and J. M. Cherry . \n\t', '\n\t\t 2002. Saccharomyces Genome Database ( SGD ) provides secondary gene annotation using the Gene Ontology ( GO ) . \n\t', '\n\t\t Nucleic Acids Res. , 30:69\x9672 . \n\t', '\n\t\t C. Friedman , P. Kra , H. Yu , M. Krauthammer , and A. Rzhetsky . \n\t', '\n\t\t 2001. GENIES : a natural-language processing system for the extraction of molecular pathways from journal articles . \n\t', '\n\t\t Bioinformatics , 17 Suppl . \n\t', '\n\t\t 1:S74\x96S82 . \n\t', '\n\t\t G. Grefenstette and P. Tapanainen . \n\t', '\n\t\t 1994. What is a word , what is a sentence ? \n\t', '\n\t\t problems of tokenization . \n\t', '\n\t\t In The 3rd International Conference on Computational Lexicography , pages 79\x9687 . \n\t', '\n\t\t J. R. Hobbs . \n\t', '\n\t\t 2003. Information extraction from biomedical text . \n\t', '\n\t\t J. Biomedical Informatics . \n\t', '\n\t\t J.-D. Kim , T. Ohta , Y. Tateisi , and J. Tsujii . \n\t', '\n\t\t 2003. GE- NIA corpus\x97a semantically annotated corpus for bio-textmining . \n\t', '\n\t\t Bioinformatics , 19 suppl . \n\t', '\n\t\t 1:i180\x96 i182 . \n\t', '\n\t\t W. Lezius . \n\t', '\n\t\t 2002. TIGERSearch\x97ein Suchwerkzeug f¨ur Baumbanken . \n\t', '\n\t\t In S. Busemann , editor , Proceedings der 6 . \n\t', '\n\t\t Konferenz zur Verarbeitung natrlicher Sprache ( KONVENS 2002 ) , Saarbr¨ucken , Germany . \n\t', '\n\t\t E. M. Marcotte , I. Xenarios , and D. Eisenberg . \n\t', '\n\t\t 2001. Mining literature for protein \x96protein interactions . \n\t', '\n\t\t Bioinformatics , 17:359\x96363 . \n\t', '\n\t\t G. Nenadi´c , S. Rice , I. Spasi´c , S. Ananiadou , and B. Stapley . \n\t', '\n\t\t 2003. Selecting text features for gene name classification : from documents to terms . \n\t', '\n\t\t In S. Ananiadou and J. Tsujii , editors , Proceedings of the ACL 2003 Workshop on Natural Language Processing in Biomedicine , pages 121\x96128 . \n\t', '\n\t\t R. Netzel , Perez-Iratxeta C. , P. Bork , and M. A. Andrade . \n\t', '\n\t\t 2003. The way we write . \n\t', '\n\t\t EMBO Rep. , 4:446\x96451 . \n\t', '\n\t\t J. Pustejovsky , J. Casta\x98no , J. Zhang , M. Kotecki , and B. Cochran . \n\t', '\n\t\t 2002 . \n\t', '\n\t\t Robust relational parsing over biomedical literature : Extracting inhibit relations . \n\t', '\n\t\t In Proceedings of the Seventh Pacific Symposium on Biocomputing , pages 362\x96373 , Hawaii . \n\t', '\n\t\t World Scientific . \n\t', '\n\t\t B. Santorini . \n\t', '\n\t\t 1991. Part-of-speech tagging guidelines for the penn treebank project . \n\t', '\n\t\t Technical report , University of Pennsylvania . \n\t', '\n\t\t H. Schmid . \n\t', '\n\t\t 1994. Probabilistic part-of-speech tagging using decision trees . \n\t', '\n\t\t In International Conference on New Methods in Language Processing , Manchester , UK . \n\t', '\n\t\t H. Schmid . \n\t', '\n\t\t 2000. Unsupervised learning of period disambiguation for tokenisation . \n\t', '\n\t\t Technical report , Institut fr Maschinelle Sprachverarbeitung , University of Stuttgart . \n\t', '\n\t\t J. Thomas , D. Milward , C. Ouzounis , S. Pulman , and M. Carroll . \n\t', '\n\t\t 2000. Automatic extraction of protein interactions from scientific abstracts . \n\t', '\n\t\t In Proceedings of the Fifth Pacific Symposium on Biocomputing , pages 707\x96709 , Hawaii . \n\t', '\n\t\t World Scientific . \n\t', '\n\t\t D. L. Wheeler , D. M. Church , R. Edgar , S. Feder- hen , W. Helmberg , Madden T. L. , Pontius J. U. , Schuler G. D. , Schriml L. M. , E. Sequeira , T. O. Suzek , T. A. Tatusova , and L. Wagner . \n\t', '\n\t\t 2004. Database resources of the national center for biotechnology information : update . \n\t', '\n\t\t Nucleic Acids Res. , 32:D35\x9640 . \n\t', '\n\t\t K. Yamamoto , T. Kudo , A. Konagaya , and Y. Matsumoto . \n\t', '\n\t\t 2003. Protein name tagging for biomedical annotation in text . \n\t', '\n\t\t In S. Ananiadou and J. Tsujii , editors , Proceedings of the ACL 2003 Workshop on Natural Language Processing in Biomedicine , pages 65\x9672 . \n\t', '\n\t\t Linguistic Profiling for Author Recognition and Verification Hans van Halteren Language and Speech , Univ . \n\t', '\n\t\t of Nijmegen P.O. Box 9103 NL-6500 HD , Nijmegen , The Netherlands hvh@let.kun.nl Abstract A new technique is introduced , linguistic profiling , in which large numbers of counts of linguistic features are used as a text profile , which can then be compared to average profiles for groups of texts . \n\t', '\n\t\t The technique proves to be quite effective for authorship verification and recognition . \n\t', '\n\t\t The best parameter settings yield a False Accept Rate of 8.1 % at a False Reject Rate equal to zero for the verification task on a test corpus of student essays , and a 99.4 % 2-way recognition accuracy on the same corpus . \n\t', '\n\t\t 1 Introduction There are several situations in language research or language engineering where we are in need of a specific type of extra-linguistic information about a text ( document ) and we would like to determine this information on the basis of linguistic properties of the text . \n\t', '\n\t\t Examples are the determination of the language variety or genre of a text , or a classification for document routing or information retrieval . \n\t', '\n\t\t For each of these applications , techniques have been developed focusing on specific aspects of the text , often based on frequency counts of functions words in linguistics and of content words in language engineering . \n\t', '\n\t\t In the technique we are introducing in this paper , linguistic profiling , we make no a priori choice for a specific type of word ( or more complex feature ) to be counted . \n\t', '\n\t\t Instead , all possible features are included and it is determined by the statistics for the texts under consideration , and the distinction to be made , how much weight , if any , each feature is to receive . \n\t', '\n\t\t Furthermore , the frequency counts are not used as absolute values , but rather as deviations from a norm , which is again determined by the situation at hand . \n\t', '\n\t\t Our hypothesis is that this technique can bring a useful contribution to all tasks where it is necessary to distinguish one group of texts from another . \n\t', '\n\t\t In this paper the technique is tested for one specific type of group , namely the group of texts written by the same author . \n\t', '\n\t\t 2 Tasks and Application Scenarios Traditionally , work on the attribution of a text to an author is done in one of two environments . \n\t', '\n\t\t The first is that of literary and/or historical research where attribution is sought for a work of unknown origin ( e.g. Mosteller & Wallace , 1984 ; Holmes , 1998 ) . \n\t', '\n\t\t As secondary information generally identifies potential authors , the task is authorship recognition : selection of one author from a set of known authors . \n\t', '\n\t\t Then there is forensic linguistics , where it needs to be determined if a suspect did or did not write a specific , probably incriminating , text ( e.g. Broeders , 2001 ; Chaski , 2001 ) . \n\t', '\n\t\t Here the task is authorship verification : confirming or denying authorship by a single known author . \n\t', '\n\t\t We would like to focus on a third environment , viz . \n\t', '\n\t\t that of the handling of large numbers of student essays . \n\t', '\n\t\t For some university courses , students have to write one or more essays every week and submit them for grading . \n\t', '\n\t\t Authorship recognition is needed in the case the sloppy student , who forgets to include his name in the essay . \n\t', '\n\t\t If we could link such an essay to the correct student ourselves , this would prevent delays in handling the essay . \n\t', '\n\t\t Authorship verification is needed in the case of the fraudulous student , who has decided that copying is much less work than writing an essay himself , which is only easy to spot if the original is also submitted by the original author . \n\t', '\n\t\t In both scenarios , the test material will be sizable , possibly around a thousand words , and at least several hundred . \n\t', '\n\t\t Training material can be sufficiently available as well , as long as text collection for each student is started early enough . \n\t', '\n\t\t Many other authorship verification scenarios do not have the luxury of such long stretches of test text . \n\t', '\n\t\t For now , however , we prefer to test the basic viability of linguistic profiling on such longer stretches . \n\t', '\n\t\t Afterwards , further experiments can show how long the test texts need to be to reach an acceptable recognition/verification quality . \n\t', '\n\t\t 2.1 Quality Measures For recognition , quality is best expressed as the percentage of correct choices when choosing between N authors , where N generally depends on the attribution problem at hand . \n\t', '\n\t\t We will use the percentage of correct choices between two authors , in order to be able to compare with previous work . \n\t', '\n\t\t For verification , quality is usually expressed in terms of erroneous decisions . \n\t', '\n\t\t When the system is asked to verify authorship for the actual author of a text and decides that the text was not written by that author , we speak of a False Reject . \n\t', '\n\t\t The False Reject Rate ( FRR ) is the percentage of cases in which this happens , the percentage being taken from the cases which should be accepted . \n\t', '\n\t\t Similarly , the False Accept Rate ( FAR ) is the percentage of cases where somebody who has not written the test text is accepted as having written the text . \n\t', '\n\t\t With increasing threshold settings , FAR will go down , while FRR goes up . \n\t', '\n\t\t The behaviour of a system can be shown by one of several types of FAR/FRR curve , such as the Receiver Operating Characteristic ( ROC ) . \n\t', '\n\t\t Alternatively , if a single number is preferred , a popular measure is the Equal Error Rate ( EER ) , viz . \n\t', '\n\t\t the threshold value where FAR is equal to FRR . \n\t', '\n\t\t However , the EER may be misleading , since it does not take into account the consequences of the two types of errors . \n\t', '\n\t\t Given the example application , plagiarism detection , we do not want to reject , i.e. accuse someone of plagiarism , unless we are sure . \n\t', '\n\t\t So we would like to measure the quality of the system with the False Accept Rate at the threshold at which the False Reject Rate becomes zero . \n\t', '\n\t\t 2.2 The Test Corpus Before using linguistic profiling for any real task , we should test the technique on a benchmark corpus . \n\t', '\n\t\t The first component of the Dutch Authorship Benchmark Corpus ( ABC-NL1 ) appears to be almost ideal for this purpose . \n\t', '\n\t\t It contains widely divergent written texts produced by first- year and fourth-year students of Dutch at the University of Nijmegen . \n\t', '\n\t\t The ABC-NL1 consists of 72 Dutch texts by 8 authors , controlled for age and educational level of the authors , and for register , genre and topic of the texts . \n\t', '\n\t\t It is assumed that the authors\x92 language skills were advanced , but their writing styles were as yet at only weakly developed and hence very similar , unlike those in literary attribution problems . \n\t', '\n\t\t Each author was asked to write nine texts of about a page and a half . \n\t', '\n\t\t In the end , it turned out that some authors were more productive than others , and that the text lengths varied from 628 to 1342 words . \n\t', '\n\t\t The authors did not know that the texts were to be used for authorship attribution studies , but instead assumed that their writing skill was measured . \n\t', '\n\t\t The topics for the nine texts were fixed , so that each author produced three argumentative non-fiction texts , on the television program Big Brother , the unification of Europe and smoking , three descriptive non-fiction texts , about soccer , the ( then ) upcoming new millennium and the most recent book they read , and three fiction texts , namely a fairy tale about Little Red Riding Hood , a murder story at the university and a chivalry romance . \n\t', '\n\t\t The ABC-NL1 corpus is not only well-suited because of its contents . \n\t', '\n\t\t It has also been used in previously published studies into authorship attribution . \n\t', '\n\t\t A \x91traditional\x92 authorship attribution method , i.e. using the overall relative frequencies of the fifty most frequent function words and a Principal Components Analysis ( PCA ) on the correlation matrix of the corresponding 50- dimensional vectors , fails completely \n\t\t']",Positive
"['\n\t\t The use of Linear Discriminant Analysis ( LDA ) on overall frequency vectors for the 50 most frequent words achieves around 60 % correct attributions when choosing between two authors , which can be increased to around 80 % by the application of cross-sample entropy weighting \n\t\t']",Positive
"['\n\t\t Weighted Probability Distribution Voting ( WPDV ) modeling on the basis of a very large number of features achieves 97.8 % correct attributions ( van Halteren et al. , To Appear ) . \n\t', '\n\t\t Although designed to produce a hard recognition task , the latter result show that very high recognition quality is feasible . \n\t', '\n\t\t Still , this appears to be a good test corpus to examine the effectiveness of a new technique . \n\t', '\n\t\t 3 Linguistic Profiling In linguistic profiling , the occurrences in a text are counted of a large number of linguistic features , either individual items or combinations of items . \n\t', '\n\t\t These counts are then normalized for text length and it is determined how much ( i.e. how many standard deviations ) they differ from the mean observed in a profile reference corpus . \n\t', '\n\t\t For the authorship task , the profile reference corpus consists of the collection of all attributed and non-attributed texts , i.e. the entire ABC-NL1 corpus . \n\t', '\n\t\t For each text , the deviation scores are combined into a profile vector , on which a variety of distance measures can be used to position the text in relation to any group of other texts . \n\t', '\n\t\t 3.1 Features Many types of linguistic features can be profiled , such as features referring to vocabulary , lexical patterns , syntax , semantics , pragmatics , information content or item distribution through a text . \n\t', '\n\t\t However , we decided to restrict the current experiments to a few simpler types of features to demonstrate the overall techniques and methodology for profiling before including every possible type of feature . \n\t', '\n\t\t In this paper , we first show the results for lexical features and continue with syntactic features , since these are the easiest ones to extract automatically for these texts . \n\t', '\n\t\t Other features will be the subject of further research . \n\t', '\n\t\t 3.2 Authorship Score Calculation In the problem at hand , the system has to decide if an unattributed text is written by a specific author , on the basis of attributed texts by that and other authors . \n\t', '\n\t\t We test our system\x92s ability to make this distinction by means of a 9-fold cross- validation experiment . \n\t', '\n\t\t In each set of runs of the system , the training data consists of attributed texts for eight of the nine essay topics . \n\t', '\n\t\t The test data consists of the unattributed texts for the ninth essay topic . \n\t', '\n\t\t This means that for all runs , the test data is not included in the training data and is about a different topic than what is present in the training material . \n\t', '\n\t\t During each run within a set , the system only receives information about whether each training text is written by one specific author . \n\t', '\n\t\t All other texts are only marked as \x93not by this author\x94 . \n\t', '\n\t\t 3.3 Raw Score The system first builds a profile to represent text written by the author in question . \n\t', '\n\t\t This is simply the featurewise average of the profile vectors of all text samples marked as being written by the author in question . \n\t', '\n\t\t The system then determines a raw score for all text samples in the list . \n\t', '\n\t\t Rather than using the normal distance measure , we opted for a non-symmetric measure which is a weighted combination of two factors : a ) the difference between sample score and author score for each feature and b ) the sample score by itself . \n\t', '\n\t\t This makes it possible to assign more importance to features whose count deviates significantly from the norm . \n\t', '\n\t\t The following distance formula is used : ^T = ( E |TiAi| D |Ti| S ) 1/(D+S) In this formula , Ti and Ai are the values for the ith feature for the text sample profile and the author profile respectively , and D and S are the weighting factors that can be used to assign more or less importance to the two factors described . \n\t', '\n\t\t We will see below how the effectiveness of the measure varies with their setting . \n\t', '\n\t\t The distance measure is then transformed into a score by the formula ScoreT = ( E |Ti|(D+S)) 1/(D+S) \x96 AT In this way , the score will grow with the similarity between text sample profile and author profile . \n\t', '\n\t\t Also , the first component serves as a correction factor for the length of the text sample profile vector . \n\t', '\n\t\t 3.4 Normalization and Renormalization The order of magnitude of the score values varies with the setting of D and S . \n\t', '\n\t\t Furthermore , the values can fluctuate significantly with the sample collection . \n\t', '\n\t\t To bring the values into a range which is suitable for subsequent calculations , we express them as the number of standard deviations they differ from the mean of the scores of the text samples marked as not being written by the author in question . \n\t', '\n\t\t In the experiments described in this paper , a rather special condition holds . \n\t', '\n\t\t In all tests , we know that the eight test samples are comparable in that they address the same topic , and that the author to be verified produced exactly one of the eight test samples . \n\t', '\n\t\t Under these circumstances , we should expect one sample to score higher than the others in each run , and we can profit from this knowledge by performing a renormalization , viz . \n\t', '\n\t\t to the number of standard deviations the score differs from the mean of the scores of the unattributed samples . \n\t', '\n\t\t However , this renormalization only makes sense in the situation that we have a fixed set of authors who each produced one text for each topic . \n\t', '\n\t\t This is in fact yet a different task than those mentioned above , say authorship sorting . \n\t', '\n\t\t Therefore , we will report on the results with renormalization , but only as additional information . \n\t', '\n\t\t The main description of the results will focus on the normalized scores . \n\t', '\n\t\t 4 Profiling with Lexical Features The most straightforward features that can be used are simply combinations of tokens in the text . \n\t', '\n\t\t 4.1 Lexical features Sufficiently frequent tokens , i.e. those that were observed at least a certain amount of times ( in this case 5 ) in some language reference corpus ( in this case the Eindhoven corpus ; uit den Boogaart , 1975 ) are used as features by themselves . \n\t', '\n\t\t For less frequent tokens we determine a token pattern consisting of the sequence of character types , e.g. , the token \x93Uefa-cup\x94 is represented by the pattern \x93#L#6+/CL-L\x94 , where the first \x93L\x94 indicates low frequency , 6+ the size bracket , and the sequence \x93CL-L\x94 a capital letter followed by one or more lower case letters followed by a hyphen and again one or more lower case letters . \n\t', '\n\t\t For lower case words , the final three letters of the word are included too , e.g. \x93waarmaken\x94 leads to \x93#L#6+/L/ken\x94 . \n\t', '\n\t\t These patterns have been originally designed for English and Dutch and will probably have to be extended when other languages are being handled . \n\t', '\n\t\t In addition to the form of the token , we also use the potential syntactic usage of the token as a feature . \n\t', '\n\t\t We apply the first few modules of a morphosyntactic tagger ( in this case Wotan-Lite ; Van Halteren et al. , 2001 ) to the text , which determine which word class tags could apply to each token . \n\t', '\n\t\t For known words , the tags are taken from a lexicon ; for unknown words , they are estimated on the basis of the word patterns described above . \n\t', '\n\t\t The three ( if present ) most likely tags are combined into a feature , e.g. \x93niet\x94 leads to \x93#H#Adv(stell,onverv)-N(ev,neut)\x94 and \x93waarmaken\x94 to \x93#L#V(inf)-N(mv,neut)- V(verldw , onverv)\x94 . \n\t', '\n\t\t Note that the most likely tags are determined on the basis of the token itself and that the context is not consulted . \n\t', '\n\t\t The modules of the tagger which do context dependent disambiguation are not applied . \n\t', '\n\t\t Op top of the individual token and tag features we use all possible bi- and trigrams which can be built with them , e.g. the token combination \x93kon niet waarmaken\x94 leads to features such as \x93wcw=#H#kon#H#Adv(stell,onverv)-N(ev,neut) #L#6+/L/ken\x94 . \n\t', '\n\t\t Since the number of features quickly grows too high for efficient processing , we filter the set of features by demanding that a feature occurs in a set minimum number of texts in the profile reference corpus ( in this case two ) . \n\t', '\n\t\t A feature which is filtered out instead contributes to a rest category feature , e.g. the feature above would contribute to \x93wcw=<OTHER>\x94 . \n\t', '\n\t\t For the current corpus , this filtering leads to a feature set of about 100K features . \n\t', '\n\t\t The lexical features currently also include features for utterance length . \n\t', '\n\t\t Each utterance leads to two such features , viz . \n\t', '\n\t\t the exact length ( e.g. \x93len=15\x94 ) and the length bracket ( e.g. \x93len=10- 19\x94 ) . \n\t', '\n\t\t 4.2 Results with lexical features A very rough first reconnaissance of settings for D and S suggested that the best results could be achieved with D between 0.1 and 2.4 and S between 0.0 and 1.0 . \n\t', '\n\t\t Further examination of this area leads to FAR FRR=0 scores ranging down to around 15 % . \n\t', '\n\t\t Figure 1 shows the scores at various settings for D and S . \n\t', '\n\t\t The z-axis is inverted ( i.e. 1 - FAR FRR=0 is used ) to show better scores as peaks rather than troughs . \n\t', '\n\t\t The most promising area is the ridge along the trough at D=0.0 , S=0.0 . \n\t', '\n\t\t A closer investigation of this area shows that the best settings are D=0.575 and S=0.15 . \n\t', '\n\t\t The FAR FRR=0 score here is 14.9 % , i.e. there is a threshold setting such that if all texts by the authors themselves are accepted , only 14.9 % of texts by other authors are falsely accepted . \n\t', '\n\t\t The very low value for S is surprising . \n\t', '\n\t\t It indicates that it is undesirable to give too much attention to features which deviate much in the sample being measured ; still , in the area in question , the score does peak at a positive S value , indicating that some such weighting does have effect . \n\t', '\n\t\t Successful low scores for S can also be seen in the hill leading around D=1.0 , S=0.3 , which peaks at an FAR FRR=0 score of around 17 percent . \n\t', '\n\t\t From the shape of the surface it would seem that an investigation of the area across the S=0.0 divide might still be worthwhile , which is in contradiction with the initial finding that negative values produce no useful results . \n\t', '\n\t\t 5 Beyond Lexical Features As stated above , once the basic viability of the technique was confirmed , more types of features would be added . \n\t', '\n\t\t As yet , this is limited to syntactic features . \n\t', '\n\t\t We will first describe the system quality using only syntactic features , and then describe the results when using lexical and syntactic features in combination . \n\t', '\n\t\t 5.1 Syntactic Features We used the Amazon parser to derive syntactic constituent analyses of each utterance \n\t\t']",Positive
"['\n\t\t We did not use the full rewrites , but rather constituent N-grams . \n\t', '\n\t\t The N-grams used were : \x95 left hand side label , examining constituent occurrence \x95 left hand side label plus one label from the right hand side , examining dominance \x95 left hand side plus label two labels from the right hand side , in their actual order , examining dominance and linear precedence Figure 1 : The variation of FAR ( or rather 1-FAR ) as a function of D and S , with D ranging from 0.1 to 2.4 and S from 0.0 to 1.0 . \n\t', '\n\t\t For each label , two representations are used . \n\t', '\n\t\t The first is only the syntactic constituent label , the second is the constituent label plus the head word . \n\t', '\n\t\t This is done for each part of the N-grams independently , leading to 2 , 4 and 8 features respectively for the three types of N-gram . \n\t', '\n\t\t Furthermore , each feature is used once by itself , once with an additional marking for the depth of the rewrite in the analysis tree , once with an additional marking for the length of the rewrite , and once with both these markings . \n\t', '\n\t\t This means another multiplication factor of four for a total of 8 , 16 and 32 features respectively . \n\t', '\n\t\t After filtering for minimum number of observations , again at least an observation in two different texts , there are about 900K active syntactic features , nine times as many as for the lexical features . \n\t', '\n\t\t Investigation of the results for various settings has not been as exhaustive as for the lexical features . \n\t', '\n\t\t The best settings so far , D=1.3 , S=1.4 , yield an FAR FRR=0 of 24.8 % , much worse than the 14.9 % seen for lexical features . \n\t', '\n\t\t 5.2 Combining Lexical and Syntactic Features From the FAR FRR=0 score , it would seem that syntactic features are not worth pursuing any fur- ther , since they perform much worse than lexical ones . \n\t', '\n\t\t However , they might still be useful if we combine their scores with those for the lexical features . \n\t', '\n\t\t For now , rather than calculating new combined profiles , we just added the scores from the two individual systems . \n\t', '\n\t\t The combination of the best two individual systems leads to an FAR FRR=0 of 10.3 % , a solid improvement over lexical features by themselves . \n\t', '\n\t\t However , the best individual systems are not necessarily the best combiners . \n\t', '\n\t\t The best combination systems produce FAR FRR=0 measurements down to 8.1 % , with settings in different parts of the parameter space . \n\t', '\n\t\t It should be observed that the improvement gained by combination is linked to the chosen quality measure . \n\t', '\n\t\t If we examine the ROC-curves for several types of systems ( plotting the FAR against the FRR ; Figure 2 ) , we see that the combination curves as a whole do not differ much from the lexical feature curve . \n\t', '\n\t\t In fact , the EER for the \x91best\x92 combination system is worse than that for the best lexical feature system . \n\t', '\n\t\t This means that we should be very much aware of the relative importance of FAR and FRR in any specific application when determining the \x91optimal\x92 features and parameters . \n\t', '\n\t\t 6 Parameter Settings A weak point in the system so far is that there is no automatic parameter selection . \n\t', '\n\t\t The best results reported above are the ones at optimal settings . \n\t', '\n\t\t One would hope that optimal settings on training/tuning data will remain good settings for new data . \n\t', '\n\t\t Further experiments on other data will have to shed more light on this . \n\t', '\n\t\t Another choice which cannot yet be made automatically is that of a threshold . \n\t', '\n\t\t So far , the presentation in this paper has been based on a single threshold for all author/text combinations . \n\t', '\n\t\t That there is an enormous potential for improvement can be shown by assuming a few more informed methods of threshold selection . \n\t', '\n\t\t The first method uses the fact that , in our experiments , there are always one true and seven false authors . \n\t', '\n\t\t This means we can choose the threshold at some point below the highest of the eight scores . \n\t', '\n\t\t We can hold on to the single threshold strategy if we first renormalize , as described Figure 2 : ROC ( FAR plotted against FRR ) for a varying threshold at good settings of D and S for different types of features . \n\t', '\n\t\t The top pane shows the whole range ( 0 to 1 ) for FAR and FRR . \n\t', '\n\t\t The bottom pane shows the area from 0.0 to 0.2. in Section 3.4 , and then choose a single value to threshold the renormalized values against . \n\t', '\n\t\t The second method assumes that we will be able to find an optimal threshold for each individual run of the system . \n\t', '\n\t\t The maximum effect of this can be estimated with an oracle providing the optimal threshold . \n\t', '\n\t\t Basically , since the oracle threshold will be at the score for the text by the author , we Lexical Features Syntactic Features Combination Single threshold 14.9 % 24.8 % 8.1 % Single 9.3 % 6.0 % 2.4 % threshold after renormalization Oracle thresh- 0.8 % 1.6 % 0.2 % old per run Table 1 : Best FAR FRR=0 scores for verification with various feature types and threshold selection mechanisms . \n\t', '\n\t\t are examining how many texts by other authors score better than the text by the actual author . \n\t', '\n\t\t Table 1 compares the results for the best settings for these two new scenarios with the results presented above . \n\t', '\n\t\t Renormalizing already greatly improves the results . \n\t', '\n\t\t Interestingly , in this scenario , the syntactic features outperform the lexical ones , something which certainly merits closer investigation after the parameter spaces have been charted more extensively . \n\t', '\n\t\t The full potential of profiling becomes clear in the Oracle threshold scenario , which shows extremely good scores . \n\t', '\n\t\t Still , this potential will yet have to be realized by finding the right automatic threshold determination mechanism . \n\t', '\n\t\t 7 Comparison to Previous Authorship Attribution Work Above , we focused on the authorship verification task , since it is the harder problem , given that the potential group of authors is unknown . \n\t', '\n\t\t However , as mentioned in Section 2 , previous work with this data has focused on the authorship recognition problem , to be exact on selecting the correct author out of two potential authors . \n\t', '\n\t\t We repeat the previously published results in Table 2 , together with linguistic profiling scores , both for the 2- way and for the 8-way selection problem . \n\t', '\n\t\t To do attribution with linguistic profiling , we calculated the author scores for each author from the set for a given text , and then selected the author with the highest score . \n\t', '\n\t\t The results are shown 2-way errors /504 2-way percent correct 8-way errors /72 8-way percent correct 50 func- ± 50 % tion words , PCA followed by LDA ± 60 % LDA with cross- sample entropy weighting ± 80 % all tokens , WPDV modeling 97.8 % Lexical 6 98.8 % 5 93 % Syntactic 14 98.2 % 10 86 % Combined 3 99.4 % 2 97 % Lexical ( renorm . \n\t', '\n\t\t ) 1 99.8 % 1 99 % Syntactic ( renorm . \n\t', '\n\t\t ) 4 99.2 % 3 96 % Combined ( renorm . \n\t', '\n\t\t ) 0 100.0 % 0 100 % Table 2 : Authorship recognition quality for various methods . \n\t', '\n\t\t in Table 2 , using lexical or syntactic features or both , and with and without renormalization . \n\t', '\n\t\t The Oracle scenario is not applicable as we are comparing rather than thresholding . \n\t', '\n\t\t In each case , the best results are not just found at a single parameter setting , but rather over a larger area in the parameter space . \n\t', '\n\t\t This means that the choice of optimal parameters will be more robust with regard to changes in authors and text types . \n\t', '\n\t\t We also observe that the optimal settings for recognition are very different from those for verification . \n\t', '\n\t\t A more detailed examination of the results is necessary to draw conclusions about these differences , which is again not possible until the parameter spaces have been charted more exhaustively . \n\t', '\n\t\t All results with normalized scores are already better than the previously published results . \n\t', '\n\t\t When applying renormalization , which might be claimed to be justified in this particular authorship attribution problem , the combination system reaches the incredible level of making no mistakes at all . \n\t', '\n\t\t 8 Conclusion Linguistic profiling has certainly shown its worth for authorship recognition and verification . \n\t', '\n\t\t At the best settings found so far , a profiling system using combination of lexical and syntactic features is able select the correct author for 97 % of the texts in the test corpus . \n\t', '\n\t\t It is also able to perform the verification task in such a way that it rejects no texts that should be accepted , while accepting only 8.1 % of the texts that should be rejected . \n\t', '\n\t\t Using additional knowledge about the test corpus can improve this to 100 % and 2.4 % . \n\t', '\n\t\t The next step in the investigation of linguistic profiling for this task should be a more exhaustive charting of the parameter space , and especially the search for an automatic parameter selection procedure . \n\t', '\n\t\t Another avenue of future research is the inclusion of even more types of features . \n\t', '\n\t\t Here , however , it would be useful to define an even harder verification task , as the current system scores already very high and further improvements might be hard to measure . \n\t', '\n\t\t With the current corpus , the task might be made harder by limiting the size of the test texts . \n\t', '\n\t\t Other corpora might also serve to provide more obstinate data , although it must be said that the current test corpus was already designed specifically for this purpose . \n\t', '\n\t\t Use of further corpora will also help with parameter space charting , as they will show the similarities and/or differences in behaviour between data sets . \n\t', '\n\t\t Finally , with the right types of corpora , the worth of the technique for actual application scenarios could be investigated . \n\t', '\n\t\t So there are several possible routes to further improvement . \n\t', '\n\t\t Still , the current quality of the system is already such that the system could be applied as is . \n\t', '\n\t\t Certainly for authorship recognition and verification , as we hope to show by our par- ticipation in Patrick Juola\x92s Ad-hoc Authorship Attribution Contest ( to be presented at ALLC/ACH 2004 ) , for language verification ( cf. van Halteren and Oostdijk , 2004 ) , and possibly also for other text classification tasks , such as language or language variety recognition , genre recognition , or document classification for IR purposes . \n\t', '\n\t\t References Harald Baayen , Hans van Halteren , Anneke Neijt , and Fiona Tweedie . \n\t', '\n\t\t 2002. An Experiment in Authorship Attribution . \n\t', '\n\t\t Proc . \n\t', '\n\t\t JADT 2002 , pp. 69-75 . \n\t', '\n\t\t Ton Broeders . \n\t', '\n\t\t 2001. Forensic Speech and Audio Analysis , Forensic Linguistics 1998-2001 \x96 A Review . \n\t', '\n\t\t Proc . \n\t', '\n\t\t 13th Interpol Forensic Science Symposium , Lyon , France . \n\t', '\n\t\t C. Chaski . \n\t', '\n\t\t 2001. Empirical Evaluations of Language- Based Author Identification Techniques . \n\t', '\n\t\t Forensic Linguistics 8(1) : 1-65 . \n\t', '\n\t\t Peter Arno Coppen . \n\t', '\n\t\t 2003. Rejuvenating the Amazon parser . \n\t', '\n\t\t Poster presentation CLIN2003 , Antwerp , Dec. 19 , 2003 . \n\t', '\n\t\t David Holmes . \n\t', '\n\t\t 1998 . \n\t', '\n\t\t Authorship attribution . \n\t', '\n\t\t Literary and Linguistic Computing 13(3):111-117 . \n\t', '\n\t\t F. Mosteller , and D.L. Wallace . \n\t', '\n\t\t 1984. Applied Bayesian and Classical Inference in the Case of the Federalist Papers ( 2nd edition ) . \n\t', '\n\t\t Springer Verlag , New York . \n\t', '\n\t\t P. C. Uit den Boogaart . \n\t', '\n\t\t 1975. Woordfrequenties in geschreven en gesproken Nederlands . \n\t', '\n\t\t Oosthoek , Scheltema & Holkema , Utrecht . \n\t', '\n\t\t Hans van Halteren , Jakub Zavrel , and Walter Daelemans . \n\t', '\n\t\t 2001. Improving accuracy in word class tagging through the combination of machine learning systems . \n\t', '\n\t\t Computational Linguistics 27(2):199-230 . \n\t', '\n\t\t Hans van Halteren and Nelleke Oostdijk , 2004 . \n\t', '\n\t\t Linguistic Profiling of Texts for the Purpose of Language Verification . \n\t', '\n\t\t Proc . \n\t', '\n\t\t COLING 2004 . \n\t', '\n\t\t Hans van Halteren , Marco Haverkort , Harald Baayen , Anneke Neijt , and Fiona Tweedie . \n\t', '\n\t\t To appear . \n\t', '\n\t\t New Machine Learning Methods Demonstrate the Existence of a Human Stylome . \n\t', '\n\t\t Journal of Quantitative Linguistics . \n\t', '\n\t\t An Empirical Study of Information Synthesis Tasks Enrique Amig´o Julio Gonzalo Victor Peinado Anselmo Pe\x98nas Felisa Verdejo Departamento de Lenguajes y Sistemas Inform´aticos Universidad Nacional de Educaci´on a Distancia c/Juan del Rosal , 16 - 28040 Madrid - Spain {enrique,julio,victor,anselmo,felisa}@lsi.uned.es Abstract This paper describes an empirical study of the \x93Information Synthesis\x94 task , defined as the process of ( given a complex information need ) extracting , organizing and inter-relating the pieces of information contained in a set of relevant documents , in order to obtain a comprehensive , non redundant report that satisfies the information need . \n\t', '\n\t\t Two main results are presented : a ) the creation of an Information Synthesis testbed with 72 reports manually generated by nine subjects for eight complex topics with 100 relevant documents each ; and b ) an empirical comparison of similarity metrics between reports , under the hypothesis that the best metric is the one that best distinguishes between manual and automatically generated reports . \n\t', '\n\t\t A metric based on key concepts overlap gives better results than metrics based on n-gram overlap ( such as ROUGE ) or sentence overlap . \n\t', '\n\t\t 1 Introduction A classical Information Retrieval ( IR ) system helps the user finding relevant documents in a given text collection . \n\t', '\n\t\t In most occasions , however , this is only the first step towards fulfilling an information need . \n\t', '\n\t\t The next steps consist of extracting , organizing and relating the relevant pieces of information , in order to obtain a comprehensive , non redundant report that satisfies the information need . \n\t', '\n\t\t In this paper , we will refer to this process as Information Synthesis . \n\t', ""\n\t\t It is normally understood as an ( intellectually challenging ) human task , and per- haps the Google Answer Service ' is the best gen- eral purpose illustration of how it works . \n\t"", '\n\t\t In this service , users send complex queries which cannot be answered simply by inspecting the first two or three documents returned by a search engine . \n\t', '\n\t\t These are a couple of real , representative examples : a ) I\x92m looking for information concerning the history of text compression both before and with computers . \n\t', '\n\t\t lhttp://answers.google.com b ) Provide an analysis on the future of web browsers , if any . \n\t', '\n\t\t Answers to such complex information needs are provided by experts which , commonly , search the Internet , select the best sources , and assemble the most relevant pieces of information into a report , organizing the most important facts and providing additional web hyperlinks for further reading . \n\t', '\n\t\t This Information Synthesis task is understood , in Google Answers , as a human task for which a search engine only provides the initial starting point . \n\t', '\n\t\t Our midterm goal is to develop computer assistants that help users to accomplish Information Synthesis tasks . \n\t', '\n\t\t From a Computational Linguistics point of view , Information Synthesis can be seen as a kind of topic-oriented , informative multi-document summarization , where the goal is to produce a single text as a compressed version of a set of documents with a minimum loss of relevant information . \n\t', '\n\t\t Unlike indicative summaries ( which help to determine whether a document is relevant to a particular topic ) , informative summaries must be helpful to answer , for instance , factual questions about the topic . \n\t', '\n\t\t In the remainder of the paper , we will use the term \x93reports\x94 to refer to the summaries produced in an Information Synthesis task , in order to distinguish them from other kinds of summaries . \n\t', '\n\t\t Topic-oriented multi-document summarization has already been studied in other evaluation initiatives which provide testbeds to compare alternative approaches \n\t\t']",Negative
"['\n\t\t Unfortunately , those studies have been restricted to very small summaries ( around 100 words ) and small document sets ( 10- 20 documents ) . \n\t', '\n\t\t These are relevant summarization tasks , but hardly representative of the Information Synthesis problem we are focusing on . \n\t', '\n\t\t The first goal of our work has been , therefore , to create a suitable testbed that permits qualitative and quantitative studies on the information synthesis task . \n\t', '\n\t\t Section 2 describes the creation of such a testbed , which includes the manual generation of 72 reports by nine different subjects across 8 complex topics with 100 relevant documents per topic . \n\t', '\n\t\t Using this testbed , our second goal has been to compare alternative similarity metrics for the Information Synthesis task . \n\t', '\n\t\t A good similarity metric provides a way of evaluating Information Synthesis systems ( comparing their output with manually generated reports ) , and should also shed some light on the common properties of manually generated reports . \n\t', '\n\t\t Our working hypothesis is that the best metric will best distinguish between manual and automatically generated reports . \n\t', '\n\t\t We have compared several similarity metrics , including a few baseline measures ( based on document , sentence and vocabulary overlap ) and a state- of-the-art measure to evaluate summarization systems , ROUGE \n\t\t']",Positive
"['\n\t\t We also introduce another proximity measure based on key concept overlap , which turns out to be substantially better than ROUGE for a relevant class of topics . \n\t', '\n\t\t Section 3 describes these metrics and the experimental design to compare them ; in Section 4 , we analyze the outcome of the experiment , and Section 5 discusses related work . \n\t', '\n\t\t Finally , Section 6 draws the main conclusions of this work . \n\t', '\n\t\t 2 Creation of an Information Synthesis testbed We refer to Information Synthesis as the process of generating a topic-oriented report from a nontrivial amount of relevant , possibly interrelated documents . \n\t', '\n\t\t The first goal of our work is the generation of a testbed ( ISCORPUS ) with manually produced reports that serve as a starting point for further empirical studies and evaluation of information synthesis systems . \n\t', '\n\t\t This section describes how this testbed has been built . \n\t', '\n\t\t 2.1 Document collection and topic set The testbed must have a certain number of features which , altogether , differentiate the task from current multi-document summarization evaluations : Complex information needs . \n\t', '\n\t\t Being Information Synthesis a step which immediately follows a document retrieval process , it seems natural to start with standard IR topics as used in evaluation conferences such as TREC2 , CLEF3 or NTCIR4 . \n\t', '\n\t\t The title/description/narrative topics commonly used in such evaluation exercises are specially well suited for an Information Synthesis task : they are complex 2http://trec.nist.gov 3http://www.clef-campaign.org 4http://research.nii.ac.jp/ntcir/ and well defined , unlike , for instance , typical web queries . \n\t', '\n\t\t We have selected the Spanish CLEF 2001-2003 news collection testbed \n\t\t']",Positive
"['\n\t\t Out of the CLEF topic set , we have chosen the eight topics with the largest number of documents manually judged as relevant from the assessment pools . \n\t', '\n\t\t We have slightly reworded the topics to change the document retrieval focus ( \x93Find documents that...\x94 ) into an information synthesis wording ( \x93Generate a report about...\x94 ) . \n\t', '\n\t\t Table 1 shows the eight selected topics . \n\t', '\n\t\t C042 : Generate a report about the invasion of Haiti by UN/US soldiers . \n\t', '\n\t\t C045 : Generate a report about the main negotiators of the Middle East peace treaty between Israel and Jordan , giving detailed information on the treaty . \n\t', '\n\t\t C47 : What are the reasons for the military intervention of Russia in Chechnya ? \n\t', '\n\t\t C48 : Reasons for the withdrawal of United Nations ( UN ) peace- keeping forces from Bosnia . \n\t', '\n\t\t C050 : Generate a report about the uprising of Indians in Chiapas ( Mexico ) . \n\t', '\n\t\t C085 : Generate a report about the operation \x93Turquoise\x94 , the French humanitarian program in Rwanda . \n\t', '\n\t\t C056 : Generate a report about campaigns against racism in Europe . \n\t', '\n\t\t C080 : Generate a report about hunger strikes attempted in order to attract attention to a cause . \n\t', '\n\t\t Table 1 : Topic set This set of eight CLEF topics has two differentiated subsets : in a majority of cases ( first six topics ) , it is necessary to study how a situation evolves in time ; the importance of every event related to the topic can only be established in relation with the others . \n\t', '\n\t\t The invasion of Haiti by UN and USA troops ( C042 ) is an example of such a topic . \n\t', '\n\t\t We will refer to them as \x93Topic Tracking\x94 ( TT ) reports , because they resemble the kind of topics used in such task . \n\t', '\n\t\t The last two questions ( 56 and 80 ) , however , resemble Information Extraction tasks : essentially , the user has to detect and describe instances of a generic event ( cases of hunger strikes and campaigns against racism in Europe ) ; hence we will refer to them as \x93IE\x94 reports . \n\t', '\n\t\t Topic tracking reports need a more elaborated treatment of the information in the documents , and therefore are more interesting from the point of view of Information Synthesis . \n\t', '\n\t\t We have , however , decided to keep the two IE topics ; first , because they also reflect a realistic synthesis task ; and second , because they can provide contrastive information as compared to TT reports . \n\t', '\n\t\t Large document sets . \n\t', '\n\t\t All the selected CLEF topics have more than one hundred documents judged as relevant by the CLEF assessors . \n\t', '\n\t\t For homogeneity , we have restricted the task to the first 100 documents for each topic ( using a chronological order ) . \n\t', '\n\t\t Complex reports . \n\t', '\n\t\t The elaboration of a comprehensive report requires more space than is allowed in current multi-document summarization experiences . \n\t', '\n\t\t We have established a maximum of fifty sentences per summary , i.e. , half a sentence per document . \n\t', '\n\t\t This limit satisfies three conditions : a ) it is large enough to contain the essential information about the topic , b ) it requires a substantial compression effort from the user , and c ) it avoids defaulting to a \x93first sentence\x94 strategy by lazy ( or tired ) users , because this strategy would double the maximum size allowed . \n\t', '\n\t\t We decided that the report generation would be an extractive task , which consists of selecting sentences from the documents . \n\t', '\n\t\t Obviously , a realistic information synthesis process also involves rewriting and elaboration of the texts contained in the documents . \n\t', '\n\t\t Keeping the task extractive has , however , two major advantages : first , it permits a direct comparison to automatic systems , which will typically be extractive ; and second , it is a simpler task which produces less fatigue . \n\t', '\n\t\t 2.2 Generation of manual reports Nine subjects between 25 and 35 years-old were recruited for the manual generation of reports . \n\t', '\n\t\t All of them self-reported university degrees and a large experience using search engines and performing information searches . \n\t', '\n\t\t All subjects were given an in-place detailed description of the task in order to minimize divergent interpretations . \n\t', '\n\t\t They were told that , in a first step , they had to generate reports with a maximum of information about every topic within the fifty sentence space limit . \n\t', '\n\t\t In a second step , which would take place six months afterwards , they would be examined from each of the eight topics . \n\t', '\n\t\t The only documentation allowed during the exam would be the reports generated in the first phase of the experiment . \n\t', '\n\t\t Subjects scoring best would be rewarded . \n\t', '\n\t\t These instructions had two practical effects : first , the competitive setup was an extra motivation for achieving better results . \n\t', '\n\t\t And second , users tried to take advantage of all available space , and thus most reports were close to the fifty sentences limit . \n\t', '\n\t\t The time limit per topic was set to 30 minutes , which is tight for the information synthesis task , but prevents the effects of fatigue . \n\t', '\n\t\t We implemented an interface to facilitate the generation of extractive reports . \n\t', '\n\t\t The system displays a list with the titles of relevant documents in chronological order . \n\t', '\n\t\t Clicking on a title displays the full document , where the user can select any sentence(s) and add them to the final report . \n\t', '\n\t\t A different frame displays the selected sentences ( also in chronological order ) , together with one bar indicating the remaining time and another bar indicating the remaining space . \n\t', '\n\t\t The 50 sentence limit can be temporarily exceeded and , when the 30 minute limit has been reached , the user can still remove sentences from the report until the sentence limit is reached back . \n\t', '\n\t\t 2.3 Questionnaires After summarizing every topic , the following questionnaire was filled in by every user : \x95 Who are the main people involved in the topic ? \n\t', '\n\t\t \x95 What are the main organizations participating in the topic ? \n\t', '\n\t\t \x95 What are the key factors in the topic ? \n\t', '\n\t\t Users provided free-text answers to these questions , with their freshly generated summary at hand . \n\t', '\n\t\t We did not provide any suggestions or constraints at this point , except that a maximum of eight slots were available per question ( i.e. a maximum of 8X3 = 24 key concepts per topic , per user ) . \n\t', '\n\t\t This is , for instance , the answer of one user for the topic 42 about the invasion of Haiti by UN and USA troops in 1994 : People Organizations Jean Bertrand Aristide ONU ( UN ) Clinton EEUU ( USA ) Raoul Cedras OEA ( OAS ) Philippe Biambi Michel Josep Francois Factors militares golpistas ( coup attempting soldiers ) golpe militar ( coup attempt ) restaurar la democracia ( reinstatement of democracy ) Finally , a single list of key concepts is generated for each topic , joining all the different answers . \n\t', '\n\t\t Redundant concepts ( e.g. \x93war\x94 and \x93conflict\x94 ) were inspected and collapsed by hand . \n\t', '\n\t\t These lists of key concepts constitute the gold standard for the similarity metric described in Section 3.2.5 . \n\t', '\n\t\t Besides identifying key concepts , users also filled in the following questionnaire : \x95 Were you familiarized with the topic ? \n\t', '\n\t\t \x95 Was it hard for you to elaborate the report ? \n\t', '\n\t\t \x95 Did you miss the possibility of introducing annotations or rewriting parts of the report by hand ? \n\t', '\n\t\t \x95 Do you consider that you generated a good report ? \n\t', '\n\t\t \x95 Are you tired ? \n\t', '\n\t\t Out of the answers provided by users , the most remarkable facts are that : \x95 only in 6 % of the cases the user missed \x93a lot\x94 the possibility of rewriting/adding comments to the topic . \n\t', '\n\t\t The fact that reports are made extractively did not seem to be a significant problem for our users . \n\t', '\n\t\t \x95 in 73 % of the cases , the user was quite or very satisfied about his summary . \n\t', '\n\t\t These are indications that the practical constraints imposed on the task ( time limit and extractive nature of the summaries ) do not necessarily compromise the representativeness of the testbed . \n\t', '\n\t\t The time limit is very tight , but the temporal arrangement of documents and their highly redundant nature facilitates skipping repetitive material ( some pieces of news are discarded just by looking at the title , without examining the content ) . \n\t', '\n\t\t 2.4 Generation of baseline reports We have automatically generated baseline reports in two steps : \x95 For every topic , we have produced 30 tentative baseline reports using DUC style criteria : \x96 18 summaries consist only of picking the first sentence out of each document in 18 different document subsets . \n\t', '\n\t\t The subsets are formed using different strategies , e.g. the most relevant documents for the query ( according to the Inquery search engine ) , one document per day , the first or last 50 documents in chronological order , etc. \x96 The other 12 summaries consist of a ) picking the first n sentences out of a set of selected documents ( with different values for n and different sets of documents ) and b ) taking the full content of a few documents . \n\t', '\n\t\t In both cases , document sets are formed with similar criteria as above . \n\t', '\n\t\t \x95 Out of these 30 baseline reports , we have selected the 10 reports which have the highest sentence overlap with the manual summaries . \n\t', '\n\t\t The second step increases the quality of the baselines , making the task of differentiating manual and baseline reports more challenging . \n\t', '\n\t\t 3 Comparison of similarity metrics Formal aspects of a summary ( or report ) , such as legibility , grammatical correctness , informativeness , etc. , can only be evaluated manually . \n\t', '\n\t\t However , automatic evaluation metrics can play a useful role in the evaluation of how well the information from the original sources is preserved \n\t\t']",Positive
['\n\t\t Previous studies have shown that it is feasible to evaluate the output of summarization systems automatically \n\t\t'],Positive
"['\n\t\t The process is based in similarity metrics between texts . \n\t', '\n\t\t The first step is to establish a ( manual ) reference summary , and then the automatically generated summaries are ranked according to their similarity to the reference summary . \n\t', '\n\t\t The challenge is , then , to define an appropriate proximity metric for reports generated in the information synthesis task . \n\t', '\n\t\t 3.1 How to compare similarity metrics without human judgments ? \n\t', '\n\t\t The QARLA estimation In tasks such as Machine Translation and Summarization , the quality of a proximity metric is measured in terms of the correlation between the ranking produced by the metric , and a reference ranking produced by human judges . \n\t', '\n\t\t An optimal similarity metric should produce the same ranking as human judges . \n\t', '\n\t\t In our case , acquiring human judgments about the quality of the baseline reports is too costly , and probably cannot be done reliably : a fine-grained evaluation of 50-sentence reports summarizing sets of 100 documents is a very complex task , which would probably produce different rankings from different judges . \n\t', '\n\t\t We believe there is a cheaper and more robust way of comparing similarity metrics without using human assessments . \n\t', '\n\t\t We assume a simple hypothesis : the best metric should be the one that best discriminates between manual and automatically generated reports . \n\t', '\n\t\t In other words , a similarity metric that cannot distinguish manual and automatic reports cannot be a good metric . \n\t', '\n\t\t Then , all we need is an estimation of how well a similarity metric separates manual and automatic reports . \n\t', '\n\t\t We propose to use the probability that , given any manual report MTe f , any other manual report M is closer to MTe f than any other automatic report A : QARLA(sim) = P(sim(M , MTe f ) > sim(A , MTef ) ) where M , MTe f E M , A E A where M is the set of manually generated reports , A is the set of automatically generated reports , and \x93sim\x94 is the similarity metric being evaluated . \n\t', '\n\t\t We refer to this value as the QARLA5 estimation . \n\t', '\n\t\t QARLA has two interesting features : \x95 No human assessments are needed to compute QARLA. . \n\t', '\n\t\t Only a set of manually produced summaries and a set of automatic summaries , for each topic considered . \n\t', '\n\t\t This reduces the cost of creating the testbed and , in addition , eliminates the possible bias introduced by human judges . \n\t', '\n\t\t \x95 It is easy to collect enough data to achieve statistically significant results . \n\t', '\n\t\t For instance , our testbed provides 720 combinations per topic to estimate QARLA probability ( we have nine manual plus ten automatic summaries per topic ) . \n\t', '\n\t\t A good QARLA value does not guarantee that a similarity metric will produce the same rankings as human judges , but a good similarity metric must have a good QARLA value : it is unlikely that a measure that cannot distinguish between manual and automatic summaries can still produce high- quality rankings of automatic summaries by comparison to manual reference summaries . \n\t', '\n\t\t 3.2 Similarity metrics We have compared five different metrics using the QARLA estimation . \n\t', '\n\t\t The first three are meant as baselines ; the fourth is the standard similarity metric used to evaluate summaries ( ROUGE ) ; and the last one , introduced in this paper , is based on the overlapping of key concepts . \n\t', '\n\t\t 3.2.1 Baseline 1 : Document co-selection metric The following metric estimates the similarity of two reports from the set of documents which are represented in both reports ( i.e. at least one sentence in each report belongs to the document ) . \n\t', '\n\t\t DocSim(M ,. , M ) _ IDoc(~) n Doc(M) I where Mr is the reference report , M a second re- port and Doc(Mr) , Doc(M) are the documents to which the sentences in Mr , M belong to. 5Quality criterion for reports evaluation metrics 3.2.2 Baselines 2 and 3 : Sentence co-selection The more sentences in common between two reports , the more similar their content will be . \n\t', '\n\t\t We can measure Recall ( how many sentences from the reference report are also in the contrastive report ) and Precision ( how many sentences from the contrastive report are also in the reference report ) : SentenceSimR(M ,. , M ) _ IS(M,.) n S(M)I IS(M,.)I SentenceSimP(M ,. , M ) _ IS(M,.) n S(M)I IS(M)I where S(Mr) , S(M) are the sets of sentences in the reports Mr ( reference ) and M ( contrastive ) . \n\t', '\n\t\t 3.2.3 Baseline 4 : Perplexity A language model is a probability distribution over word sequences obtained from some training corpora ( see e.g. \n\t\t']",Positive
"['\n\t\t Perplexity is a measure of the degree of surprise of a text or corpus given a language model . \n\t', '\n\t\t In our case , we build a language model LM(Mr) for the refer- ence report Mr , and measure the perplexity of the contrastive report M as compared to that language model : PerplexitySim(M ,. , M ) _ 1 Perp(LM(M,.) , M ) We have used the Good-Turing discount algorithm to compute the language models \n\t\t']",Positive
"['\n\t\t Note that this is also a baseline metric , because it only measures whether the content of the contrastive report is compatible with the reference report , but it does not consider the coverage : a single sentence from the reference report will have a low perplexity , even if it covers only a small fraction of the whole report . \n\t', '\n\t\t This problem is mitigated by the fact that we are comparing reports of approximately the same size and without repeated sentences . \n\t', '\n\t\t 3.2.4 ROUGE metric The distance between two summaries can be established as a function of their vocabulary ( unigrams ) and how this vocabulary is used ( n-grams ) . \n\t', '\n\t\t From this point of view , some of the measures used in the evaluation of Machine Translation systems , such as BLEU \n\t\t']",Positive
"['\n\t\t BLEU is based in the precision and n-gram co-ocurrence between an automatic translation and a reference manual translation . \n\t', '\n\t\t \n\t\t']",Negative
"['\n\t\t Indeed , some of the characteristics that define a good translation are not related with the features of a good summary ; then Lin and Hovy proposed a recall- based variation of BLEU , known as ROUGE . \n\t', '\n\t\t The idea is the same : the quality of a proposed summary can be calculated as a function of the n-grams in common between the units of a model summary . \n\t', '\n\t\t The units can be sentences or discourse units : ECE{MU} En-gramEC Count where MU is the set of model units , Count n is the maximum number of n-grams co-ocurring in a peer summary and a model unit , and Count is the number of n-grams in the model unit . \n\t', '\n\t\t It has been established that unigram and bigram based metrics permit to create a ranking of automatic summaries better ( more similar to a human-produced ranking ) than n-grams with n > 2 . \n\t', '\n\t\t For our experiment , we have only considered unigrams ( lemmatized words , excluding stop words ) , which gives good results with standard summaries \n\t\t']",Positive
"['\n\t\t 3.2.5 Key concepts metric Two summaries generated by different subjects may differ in the documents that contribute to the summary , in the sentences that are chosen , and even in the information that they provide . \n\t', '\n\t\t In our Information Synthesis settings , where topics are complex and the number of documents to summarize is large , it is likely to expect that similarity measures based on document , sentence or n-gram overlap do not give large similarity values between pairs of manually generated summaries . \n\t', '\n\t\t Our hypothesis is that two manual reports , even if they differ in their information content , will have the same ( or very similar ) key concepts ; if this is true , comparing the key concepts of two reports can be a better similarity measure than the previous ones . \n\t', '\n\t\t In order to measure the overlap of key concepts between two reports , we create a vector ~kc for every report , such that every element in the vector represents the frequency of a key concept in the report in relation to the size of the report : kc(M)Z = freq(CZ , M ) Iwords(M)I being f req(CZ , M ) the number of times the key concept CZ appears in the report M , and Iwords ( M ) I the number of words in the report . \n\t', '\n\t\t The key concept similarity NICOS ( Nuclear Informative Concept Similarity ) between two reports M and Mr can then be defined as the inverse of the Euclidean distance between their associated concept vectors : I ~kc(Mr) \x97 ~kc(M) I In our experiment , the dimensions of kc vectors correspond to the list of key concepts provided by our test subjects ( see Section 2.3 ) . \n\t', '\n\t\t This list is our gold standard for every topic . \n\t', '\n\t\t 4 Experimental results Figure 1 shows , for every topic ( horizontal axis ) , the QARLA estimation obtained for each similarity metric , i.e. , the probability of a manual report being closer to other manual report than to an automatic report . \n\t', '\n\t\t Table 2 shows the average QARLA measure across all topics . \n\t', '\n\t\t Metric TT topics IE topics Perplexity 0.19 0.60 DocSim 0.20 0.34 SentenceSimR 0.29 0.52 SentenceSimP 0.38 0.57 ROUGE 0.54 0.53 NICOS 0.77 0.52 Table 2 : Average QARLA For the six TT topics , the key concept similarity NICOS performs 43 % better than ROUGE , and all baselines give poor results ( all their QARLA probabilities are below chance , QARLA < 0.5 ) . \n\t', '\n\t\t A non- parametric Wilcoxon sign test confirms that the difference between NICOS and ROUGE is highly significant ( p < 0.005 ) . \n\t', '\n\t\t This is an indication that the Information Synthesis task , as we have defined it , should not be studied as a standard summarization problem . \n\t', '\n\t\t It also confirms our hypothesis that key concepts tend to be stable across different users , and may help to generate the reports . \n\t', '\n\t\t The behavior of the two Information Extraction ( IE ) topics is substantially different from TT topics . \n\t', '\n\t\t While the ROUGE measure remains stable ( 0.53 versus 0.54 ) , the key concept similarity is much worse with IE topics ( 0.52 versus 0.77 ) . \n\t', '\n\t\t On the other hand , all baselines improve , and some of them ( SentenceSim precision and perplexity ) give better results than both ROUGE and NICOS . \n\t', '\n\t\t Of course , no reliable conclusion can be obtained from only two IE topics . \n\t', '\n\t\t But the observed differences suggest that TT and IE may need different approaches , both to the automatic generation of reports and to their evaluation . \n\t', '\n\t\t ROUGE _ E CE{MU} En-gramEC Countm , 1 NICOS(M , Mr ) = Figure 1 : Comparison of similarity metrics by topic One possible reason for this different behavior is that IE topics do not have a set of consistent key concepts ; every case of a hunger strike , for instance , involves different people , organizations and places . \n\t', '\n\t\t The average number of different key concepts is 18.7 for TT topics and 28.5 for IE topics , a differ- ence that reveals less agreement between subjects , supporting this argument . \n\t', '\n\t\t 5 Related work Besides the measures included in our experiment , there are other criteria to compare summaries which could as well be tested for Information Synthesis : Annotation of relevant sentences in a corpus . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t The paper describes the creation of an evaluation corpus in which the most relevant sentences in a set of related news were annotated . \n\t', '\n\t\t Summaries are evaluated with a measure called \x93novel recall\x94 , based in sentences selected by a summarization system and sentences manually associated to events in the corpus . \n\t', '\n\t\t The agreement rate between subjects in the identification of key events and the sentence annotation does not correspond with the agreement between reports that we have obtained in our experiments . \n\t', '\n\t\t There are , at least , two reasons to explain this : \x95 \n\t\t']",Positive
"['\n\t\t \x95 Although there are topics in both experiments , the information needs in our testbed are more complex ( e.g. motivations for the invasion of Chechnya ) Factoids . \n\t', '\n\t\t One of the problems in the evaluation of summaries is the versatility of human language . \n\t', '\n\t\t Two different summaries may contain the same information . \n\t', '\n\t\t In \n\t\t']",Positive
"['\n\t\t They also annotate the composition , generalization and implication relations between extracted factoids . \n\t', '\n\t\t The resulting measure is different from unigram based similarity . \n\t', '\n\t\t The main problem of factoids , as compared to other metrics , is that they require a costly manual processing of the summaries to be evaluated . \n\t', '\n\t\t 6 Conclusions In this paper , we have reported an empirical study of the \x93Information Synthesis\x94 task , defined as the process of ( given a complex information need ) extracting , organizing and relating the pieces of information contained in a set of relevant documents , in order to obtain a comprehensive , non redundant report that satisfies the information need . \n\t', '\n\t\t We have obtained two main results : \x95 The creation of an Information Synthesis testbed ( ISCORPUS ) with 72 reports manually generated by 9 subjects for 8 complex topics with 100 relevant documents each . \n\t', '\n\t\t \x95 The empirical comparison of candidate metrics to estimate the similarity between reports . \n\t', '\n\t\t Our empirical comparison uses a quantitative criterion ( the QARLA estimation ) based on the hypothesis that a good similarity metric will be able to distinguish between manual and automatic reports . \n\t', '\n\t\t According to this measure , we have found evidence that the Information Synthesis task is not a standard multi-document summarization problem : state-of- the-art similarity metrics for summaries do not perform equally well with the reports in our testbed . \n\t', '\n\t\t Our most interesting finding is that manually generated reports tend to have the same key concepts : a similarity metric based on overlapping key concepts ( NICOS ) gives significantly better results than metrics based on language models , n-gram coocurrence and sentence overlapping . \n\t', '\n\t\t This is an indication that detecting relevant key concepts is a promising strategy in the process of generating reports . \n\t', '\n\t\t Our results , however , has also some intrinsic limitations . \n\t', '\n\t\t Firstly , manually generated summaries are extractive , which is good for comparison purposes , but does not faithfully reflect a natural process of human information synthesis . \n\t', '\n\t\t Another weakness is the maximum time allowed per report : 30 minutes seems too little to examine 100 documents and extract a decent report , but allowing more time would have caused an excessive fatigue to users . \n\t', '\n\t\t Our volunteers , however , reported a medium to high satisfaction with the results of their work , and in some occasions finished their task without reaching the time limit . \n\t', '\n\t\t ISCORPUS is available at : http://nlp.uned.es/ISCORPUS Acknowledgments This research has been partially supported by a grant of the Spanish Government , project HERMES ( TIC-2000-0335-C03-01 ) . \n\t', '\n\t\t We are indebted to E. Hovy for his comments on an earlier version of this paper , and C. Y. Lin for his assistance with the ROUGE measure . \n\t', '\n\t\t Thanks also to our volunteers for their valuable cooperation . \n\t', '\n\t\t References P. Clarkson and R. Rosenfeld . \n\t', '\n\t\t 1997. Statistical language modeling using the CMU-Cambridge toolkit . \n\t', '\n\t\t In Proceeding of Eurospeech \x9297 , Rhodes , Greece . \n\t', '\n\t\t J. Goldstein , V. O. Mittal , J. G. Carbonell , and J. P. Callan . \n\t', '\n\t\t 2000. Creating and Evaluating Multi-Document Sentence Extract Summaries . \n\t', '\n\t\t In Proceedings of Ninth International Conferences on Information Knowledge Management ( CIKM´00 ) , pages 165\x96172 , McLean , VA . \n\t', '\n\t\t H. V. Halteren and S. Teufel . \n\t', '\n\t\t 2003. Examining the Consensus between Human Summaries : Initial Experiments with Factoids Analysis . \n\t', '\n\t\t In HLT/NAACL-2003 Workshop on Automatic Summarization , Edmonton , Canada . \n\t', '\n\t\t V. Khandelwal , R. Gupta , and J. Allan . \n\t', '\n\t\t 2001. An Evaluation Corpus for Temporal Summarization . \n\t', '\n\t\t In Proceedings of the First International Conference on Human Language Technology Research ( HLT 2001 ) , Tolouse , France . \n\t', '\n\t\t C. Lin and E. H. Hovy . \n\t', '\n\t\t 2003. Automatic Evaluation of Summaries Using N-gram Co-ocurrence Statistics . \n\t', '\n\t\t In Proceeding of the 2003 Language Technology Conference ( HLT-NAACL 2003 ) , Edmonton , Canada . \n\t', '\n\t\t I. Mani . \n\t', '\n\t\t 2001. Automatic Summarization , volume 3 of Natural Language Processing . \n\t', '\n\t\t John Benjamins Publishing Company , Amsterdam/Philadelphia . \n\t', '\n\t\t C. D. Manning and H. Schutze . \n\t', '\n\t\t 1999. Foundations of statistical natural language processing . \n\t', '\n\t\t MIT Press , Cambridge Mass. P. . \n\t', '\n\t\t Over . \n\t', '\n\t\t 2003. Introduction to DUC-2003 : An Intrinsic Evaluation of Generic News Text Summarization Systems . \n\t', '\n\t\t In Proceedings of Workshop on Automatic Summarization ( DUC 2003 ) . \n\t', '\n\t\t K. Papineni , S. Roukos , T. Ward , and W. Zhu . \n\t', '\n\t\t 2002. Bleu : a method for automatic evaluation of machine translation . \n\t', '\n\t\t In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics ( ACL ) , pages 311\x96 318 , Philadelphia . \n\t', '\n\t\t C. Peters , M. Braschler , J. Gonzalo , and M. Kluck , editors . \n\t', '\n\t\t 2002. Evaluation of Cross-Language Information Retrieval Systems , volume 2406 of Lecture Notes in Computer Science . \n\t', '\n\t\t SpringerVerlag , Berlin-Heidelberg-New York . \n\t', '\n\t\t D. R. Radev , J. Hongyan , and M. Budzikowska . \n\t', '\n\t\t 2000. Centroid-Based Summarization of Multiple Documents : Sentence Extraction , Utility- Based Evaluation , and User Studies . \n\t', '\n\t\t In Proceedings of the Workshop on Automatic Summarization at the 6th Applied Natural Language Processing Conference and the 1st Conference of the North American Chapter of the Association for Computational Linguistics , Seattle , WA , April . \n\t', '\n\t\t Mining metalinguistic activity in corpora to create lexical resources using Information Extraction techniques : the MOP system Carlos Rodríguez Penagos Language Engineering Group , Engineering Institute UNAM , Ciudad Universitaria A.P. 70-472 Coyoacán 04510 Mexico City , México CRodriguezP@iingen.unam.mx Abstract This paper describes and evaluates MOP , an IE system for automatic extraction of metalinguistic information from technical and scientific documents . \n\t', '\n\t\t We claim that such a system can create special databases to bootstrap compilation and facilitate update of the huge and dynamically changing glossaries , knowledge bases and ontologies that are vital to modern-day research . \n\t', '\n\t\t 1 Introduction Availability of large-scale corpora has made it possible to mine specific knowledge from free or semi-structured text , resulting in what many consider by now a reasonably mature NLP technology . \n\t', '\n\t\t Extensive research in Information Extraction ( IE ) techniques , especially with the series of Message Understanding Conferences of the nineties , has focused on tasks such as creating and updating databases of corporate join ventures or terrorist and guerrilla attacks , while the ACQUILEX project used similar methods for creating lexical databases using the highly structured environment of machine-readable dictionary entries and other resources . \n\t', '\n\t\t Gathering knowledge from unstructured text often requires manually crafting knowledge- engineering rules both complex and deeply dependent of the domain at hand , although some successful experiences using learning algorithms have been reported \n\t\t']",Positive
['\n\t\t Although mining specific semantic relations and subcategorization information from free-text has been successfully carried out in the past \n\t\t'],Positive
"['\n\t\t A good example of this NLP-based processing need is the MedLine abstract database maintained by the National Library of Medicine1 ( NLM ) , which incorporates around 40,000 Health Sciences papers each month . \n\t', '\n\t\t Researchers depend on these electronic resources to keep abreast of their rapidly changing field . \n\t', '\n\t\t In order to maintain and update vital indexing references such as the Unified Medical Language System ( UMLS ) resources , the MeSH and SPECIALIST vocabularies , the NLM staff needs to review 400,000 highly-technical papers each year . \n\t', '\n\t\t Clearly , neology detection , terminological information update and other tasks can benefit from applications that automatically search text for information , e.g. , when a new term is introduced or an existing one is modified due to data or theory-driven concerns , or , in general , when new information about sublanguage usage is being put forward . \n\t', '\n\t\t But the usefulness of robust NLP applications for special-domain text goes beyond glossary updates . \n\t', '\n\t\t The kind of categorization information implicit in many definitions can help improve anaphora resolution , semantic typing or acronym identification in these corpora , as well as enhance \x93semantic rerendering\x94 of special-domain ontologies and thesaurii \n\t\t']",Positive
"['\n\t\t In this paper we describe and evaluate the MOP2 IE system , implemented to automatically create Metalinguistic Information Databases ( MIDs ) from large collections of special-domain 1 http://www.nlm.nih.gov/ 2 Metalinguistic Operation Processor research papers . \n\t', '\n\t\t Section 2 will lay out the theory , methodology and the empirical research grounding the application , while Section 3 will describe the first phase of the MOP tasks : accurate location of good candidate metalinguistic sentences for further processing . \n\t', '\n\t\t We experimented both with manually coded rules and with learning algorithms for this task . \n\t', '\n\t\t Section 4 focuses on the problem of identifying and organizing into a useful database structure the different linguistic constituents of the candidate predications , a phase similar to what are known in the IE literature as Named-Entity recognition , Element and Scenario template fill-up tasks . \n\t', '\n\t\t Finally , Section 5 discusses results and problems of our experiments , as well as future lines of research . \n\t', '\n\t\t 2 Metalanguage and term evolution in scientific disciplines 2.1 Explicit Metalinguistic Operations Preliminary empirical work to explore how researchers modify the terminological framework of their highly complex conceptual systems , included manual review of a corpus of 19 sociology articles ( 138,183 words ) published in various British , American and Canadian academic journals with strict peer-review policies . \n\t', '\n\t\t We look at how term manipulation was done as well as how metalinguistic activity was signaled in text , both by lexical and paralinguistic means . \n\t', '\n\t\t Some of the indicators found included verbs and verbal phrases like called , known as , defined as , termed , coined , dubbed , and descriptors such as term and word . \n\t', '\n\t\t Other non-lexical markers included quotation marks , apposition and text formatting . \n\t', '\n\t\t A collection of potential metalinguistic patterns identified in the exploratory Sociology corpus was expanded ( using other verbal tenses and forms ) to 116 queries sent to the scientific and learned domains of the British National Corpus . \n\t', '\n\t\t The resulting 10,937 sentences were manually classified as metalinguistic or otherwise , with 5,407 ( 49.6 % of total ) found to be truly metalinguistic sentences . \n\t', '\n\t\t The presence of three components described below ( autonym , informative segment and markers/operators ) was the criteria for classification . \n\t', '\n\t\t Reliability of human subjects for this task has not been reported in the literature , and was not evaluated in our experiments . \n\t', '\n\t\t Careful analysis of this extensive corpus presented some interesting facts about what we have termed \x93Explicit Metalinguistic Operations\x94 ( or EMOs ) in specialized discourse : A ) EMOs usually do not follow the genus- differentia scheme of aristotelian definitions , nor conform to the rigid and artificial structure of dictionary entries . \n\t', '\n\t\t More often than not , specific information about language use and term definition is provided by sentences such as : ( 1 ) This means that they ingest oxygen from the air via fine hollow tubes , known as tracheae , in which the term trachea is linked to the description fine hollow tubes in the context of a globally nonmetalinguistic sentence . \n\t', '\n\t\t Partial and heterogeneous information , rather that a complete definition , are much more common . \n\t', '\n\t\t B ) Introduction of metalinguistic information in discourse is highly regular , regardless of the specific domain . \n\t', '\n\t\t This can be credited to the fact that the writer needs to mark these sentences for special processing by the reader , as they dissect across two different semiotic levels : a metalanguage and its object language , to use the terminology of logic where these concepts originate.3 Its constitutive markedness means that most of the times these sentences will have at least two indicators present , for example a verb and a descriptor , or quotation marks , or even have preceding sentences that announce them in some way . \n\t', '\n\t\t These formal and cognitive properties of EMOs facilitate the task of locating them accurately in text . \n\t', '\n\t\t C ) EMOs can be further analyzed into 3 distinct components , each with its own properties and linguistic realizations : i ) An autonym ( see note 3 ) : One or more self- referential lexical items that are the logical or grammatical subject of a predication that needs not be a complete grammatical sentence . \n\t', '\n\t\t 3 At a very basic semiotic level natural language has to be split ( at least methodologically ) into two distinct systems that share the same rules and elements : a metalanguage , which is a language that is used to talk about another one , and an object language , which in turn can refer to and describe objects in the mind or in the physical world . \n\t', '\n\t\t The two are isomorphic and this accounts for reflexivity , the property of referring to itself , as when linguistic items are mentioned instead of being used normally in an utterance . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t ii ) An informative segment : a contribution of relevant information about the meaning , status , coding or interpretation of a linguistic unit . \n\t', '\n\t\t Informative segments constitute what we state about the autonymical element . \n\t', '\n\t\t iii ) Markers/Operators : Elements used to mark or made prominent whole discourse operation , on account of its non-referential , metalinguistic nature . \n\t', '\n\t\t They are usually lexical , typographic or pragmatic elements that articulate autonyms and informative segments into a predication . \n\t', ""\n\t\t Thus , in a sentence such as ( 2 ) , the [ autonym ] is marked in square brackets , the { informational segment } in curly brackets and the <marker- operators> in angular brackets : ( 2 ) { The bit sequences representing quanta of knowledge } <will be called \x93>[Kenes]<\x94> , { a neologism intentionally similar to ' genes ' } . \n\t"", '\n\t\t 2.2 Defaults , knowledge and knowledge of language The 5,400 metalinguistic sentences from our BNC-based test corpus ( henceforth , the EMO corpus ) reflect an important aspect of scientific sublanguages , and of the scientific enterprise in general . \n\t', '\n\t\t Whenever scientists and scholars advance the state of the art of a discipline , the language they use has to evolve and change , and this buildup is carried out under metalinguistic control . \n\t', '\n\t\t Previous knowledge is transformed into new scientific common ground and ontological commitments are introduced and defended when semantic reference is established . \n\t', '\n\t\t That is why when we want to structure and acquire new knowledge we have to go through a resource-costly cognitive process that integrates , within coherent conceptual structures , a considerable amount of new and very complex lexical items and terms . \n\t', '\n\t\t It has to be pointed out that non-specialized language is not abundant4 in these kinds of meta- linguistic exchanges because ( unless in the context of language acquisition ) we usually rely on a lexical competence that , although subsequently modified and enhanced , reaches the plateau of a generalized lexicon relatively early in our adult life . \n\t', '\n\t\t Technical terms can be thought of as semantic anomalies , in the sense that they are ad hoc 4 Our study shows that they represent between 1 and 6 % of all sentences across different domains . \n\t', '\n\t\t constructs strongly bounded to a model , a domain or a context , and are not , by definition , part of the far larger linguistic competence from a first native language . \n\t', '\n\t\t The information provided by EMOs is not usually inferable from previous one available to the speaker\x92s community or expert group , and does not depend on general language competence by itself , but nevertheless is judged important and relevant enough to warrant the additional processing effort involved . \n\t', '\n\t\t Conventional resources like lexicons and dictionaries compile established meaning definitions . \n\t', '\n\t\t They can be seen as repositories of the default , core lexical information of words or terms used by a community ( that is , the information available to an average , idealized speaker ) . \n\t', '\n\t\t A Metalinguistic Information Database ( MID ) , on the other hand , compiles the real-time data provided by metalanguage analysis of leading-edge research papers , and can be conceptualized as an anti-dictionary : a listing of exceptions , special contexts and specific usage , of instances where meaning , value or pragmatic conditions have been spotlighted by discourse for cognitive reasons . \n\t', '\n\t\t The non-default and highly relevant information from MIDs could provide the material for new interpretation rules in reasoning applications , when inferences won\x92t succeed because the states of the lexicoconceptual system have changed . \n\t', '\n\t\t When interpreting text , regular lexical information is applied by default under normal conditions , but more specific pragmatic or discursive information can override it if necessary , or if context demands so ( Lascarides & Copestake , 1995 ) . \n\t', '\n\t\t A neologism or a word in an unexpected technical sense could stump a NLP system that assumes it will be able to use default information from a machine-readable dictionary . \n\t', '\n\t\t 3 Locating metalinguistic information in text : two approaches When implementingan IE application to mine metalinguistic information from text , the first issue to tackle is how to obtain a reliable set of candidate sentences from free text for input into the next phases of extraction . \n\t', '\n\t\t From our initial corpus analysis we selected 44 patterns that showed the best reliability for being EMO indicators . \n\t', '\n\t\t We start our processing5 by tokenizing text , which then is 5 Our implementation is Python-based , using the run through a cascade of finite-state devices based on identification patterns that extract a candidate set for filtering . \n\t', '\n\t\t Our filtering strategies in effect distinguish between useful results such as ( 3 ) from non-metalinguistic instances like ( 4 ) : ( 3 ) Since the shame that was elicited by the coding procedure was seldom explicitly mentioned by the patient or the therapist , Lewis called it unacknowledged shame . \n\t', '\n\t\t ( 4 ) It was Lewis ( 1971;1976 ) who called attention to emotional elements in what until then had been construed as a perceptual phenomenon . \n\t', '\n\t\t For this task , we experimented with two strategies : First , we used corpus-based collocations to discard non-metalinguistic instances , for example the presence of attention in sentence ( 4 ) next to the marker called . \n\t', '\n\t\t Since immediate co-text seems important for this classification task , we also implemented learning algorithms that were trained on a subset from our EMO corpus , using as vectors either POS tags or word forms , at 1 , 2 , and 3 positions adjacent before and after our markers . \n\t', '\n\t\t These approaches are representative of wider paradigmatic approaches to NLP : symbolic and statistic techniques , each with their own advantages and limitations . \n\t', '\n\t\t Our evaluations of the MOP system are based on test runs over 3 document sets : a ) our original exploratory corpus of sociology research papers [ 5581 sentences , 243 EMOs ] ; b ) an online histology textbook [ 5146 sentences , 69 EMOs ] ; and c ) a small sample from the MedLine abstract database [ 1403 sentences , 10 EMOs ] . \n\t', '\n\t\t Using collocational information , our first approach fared very well , presenting good precision numbers , but not so encouraging recall . \n\t', '\n\t\t The sociology corpus , for example , gave 0.94 precision ( P ) and 0.68 recall ( R ) , while the histology one presented 0.9 P and 0.5 R . \n\t', '\n\t\t These low recall numbers reflect the fact that we only selected a subset of the most reliable and common metalinguistic patterns , and our list is not exhaustive . \n\t', '\n\t\t Example ( 5 ) shows one kind of metalinguistic sentence ( with a copulative structure ) attested in corpora , NLTK toolkit ( nltk.sf.net ) developed by E. Loper and S. Byrd at the University of Pennsylvania , although we have replaced stochastic POS taggers with an implementation of the Brill algorithm by Hugo Liu at MIT . \n\t', ""\n\t\t Our output files follow XML standards to ensure transparency , portability and accessibility but that the system does not attempt to extract or process : ( 5 ) \x93Intercursive\x94 power , on the other hand , is power in Weber 's sense of constraint by an actor or group of actors over others . \n\t"", '\n\t\t In order to better compare our two strategies , we decided to also zoom in on a more limited subset of verb forms for extraction ( namely , calls , called , call ) , which presented ratios of metalinguistic relevance in our MOP corpus , ranging from 100 % positives ( for the pattern so called + quotation marks ) to 77 % ( called , by itself ) to 31 % ( call ) . \n\t', '\n\t\t Restricted to these verbs , our metrics show precision and recall rates of around 0.97 , and an overall F-measure of 0.97.6 Of 5581 sentences ( 96 of which were metalinguistic sentences signaled by our cluster of verbs ) , 83 were extracted , with 13 ( or 15.6 % of candidates ) filtered-out by collocations . \n\t', '\n\t\t For our learning experiments ( an approach we have called contextual feature language models ) , we selected two well-known algorithms that showed promise for this classification task.7 The naive Bayes ( NB ) algorithm estimates the conditional probability of a set of features given a label , using the product of the probabilities of the individual features given that label . \n\t', '\n\t\t The Maximum Entropy model establishes a probability distribution that favors entropy , or uniformity , subject to the constraints encoded in the feature-label correlation . \n\t', '\n\t\t When training our ME classifiers , Generalized ( GISMax ) and Improved Iterative Scaling ( IISMax ) algorithms are used to estimate the optimal maximum entropy of a feature set , given a corpus . \n\t', ""\n\t\t 1,371 training sentences were converted into labeled vectors , for example using 3 positions and POS tags : ( ' VB WP NNP ' , ' calls ' , ' DT NN NN ' ) /'YES'@[102] . \n\t"", '\n\t\t The different number of positions considered to the left and right of the markers in our training corpus , as well as the nature of the features selected ( there are many more word-types than POS tags ) ensured that our 3-part vector introduced a wide range of features against our 2 possible YES-NO labels for processing by our algorithms . \n\t', '\n\t\t Although our test runs using only collocations showed initially that structural regulari- 6 With a ß factor of 1.0 , and within the sociology document set 7 see \n\t\t']",Positive
"['\n\t\t Nevertheless , stochastic approaches that used short range features did perform very well , in line with the hand-coded approach . \n\t', '\n\t\t The results of the different algorithms , restricted to the lexeme call , are presented in Table 1 , while Figures 1 and 2 present best results in the learning experiments for the complete set of patterns used in the collocation approach , over two of our evaluation corpora . \n\t', '\n\t\t Type Positions Tags/ Words Features Accuracy Precision Recall GISMaz 1 W 1254 0.97 0.96 0.98 IISMaz 1 T 136 0.95 0.96 0.94 IISMaz 1 W 1252 0.92 0.97 0.9 GISMaz 1 T 138 0.91 0.9 0.96 GISMaz 2 T 796 0.88 0.93 0.92 IISMaz 2 T 794 0.86 0.95 0.89 IISMaz 3 W 4290 0.87 0.85 0.98 GISMaz 3 W 4292 0.87 0.85 0.98 IISMaz 2 W 3186 0.86 0.87 0.95 GISMaz 2 W 3188 0.86 0.87 0.95 NB 1 T 136 0.88 0.97 0.84 NB 2 T 794 0.87 0.96 0.84 NB 3 W 4290 0.73 0.86 0.77 Table 1 . \n\t', '\n\t\t Best metrics for \x93call\x94 lexeme sorted by F-measure and classifier accuracy Figure 1 . \n\t', '\n\t\t Best metrics for Sociology corpus 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 8 Legend : P : Precision ; R : Recall ; F : F-Measure . \n\t', '\n\t\t NB : naïve Bayes ; IIS : Maximum Entropy trained with Improved Iterative Scaling ; GIS : Maximum Entropy trained with Generalized Iterative Scaling . \n\t', '\n\t\t ( Positions/Feature type ) R P F NB ( 3/T ) IIS ( 1/W ) GIS ( 1/W ) Figure 2 . \n\t', '\n\t\t Best metrics for Histology corpus R P F NB ( 3/W ) IIS ( 3/W ) GIS ( 1/W ) 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 Figures 1 & 2 . \n\t', '\n\t\t Best results for filtering algorithms.8 Both Knowledge-Engineering and supervised learning approaches can be adequate for extraction of metalinguistic sentences , although learning algorithms can be helpful when procedural rules have not been compiled ; they also allow easier transport of systems to new thematic domains . \n\t', '\n\t\t We plan further research into stochastic approaches to fine tune them for the task . \n\t', '\n\t\t One issue that merits special attention is why some of the algorithms and features work well with one corpus , but not so well with another . \n\t', '\n\t\t This fact is in line with observations in \n\t\t']",Positive
"['\n\t\t A hybrid approach that combines hand-crafted collocations with classifiers customized to each pattern\x92s behavior and morpho-syntactic contexts in corpora might offer better results in future experiments . \n\t', '\n\t\t 4 Processing EMOs to compile metalinguistic information databases Once we have extracted candidate EMOs , the MOP system conforms to a general processing architecture shown in Figure 3. POS tagging is followed by shallow parsing that attempts limited PP-attachment . \n\t', '\n\t\t The resulting chunks are then tagged semantically as Autonyms , Agents , Markers , Anaphoric elements or simply as Noun Chunks , using heuristics based on syntactic , pragmatic and argument structure observation of the extraction patterns . \n\t', '\n\t\t Next , a predicate processing phase selects the most likely surface realization of informational segments , autonyms and makers-operators , and proceeds to fill the templates in our databases . \n\t', '\n\t\t This was done by following different processing routes customized for each pattern using corpus analysis as well as FrameNet data from Name conferral and Name bearing frames to establish relevant arguments and linguistic realizations . \n\t', '\n\t\t Figure 3. MOP Architecture As mentioned earlier , informational segments present many realizations that distance them from the clarity , completeness and conciseness of lexicographic entries . \n\t', '\n\t\t In fact , they may show up as full-fledged clauses ( 6 ) , as inter- or intrasentential anaphoric elements ( 7 and 8 , the first one a relative clause ) , supply a categorization descriptor ( 9 ) , or even ( 10 ) restrict themselves semantically to what we could call a sententiallyunrealized \x93existential variable\x94 ( with logical form x ) indicating only that certain discourse entity is being introduced . \n\t', '\n\t\t ( 6 ) In 1965 the term soliton was coined to describe waves with this remarkable behaviour . \n\t', '\n\t\t ( 7 ) This leap brings cultural citizenship in line with what has been called the politics of citizenship . \n\t', '\n\t\t ( 8 ) They are called \x93endothermic compounds.\x94 ( 9 ) One of the most enduring aspects of all social theories are those conceptual entities known as structures or groups . \n\t', '\n\t\t ( 10 ) A , so called cell-type-specific TF can be used by closely related cells , e.g. , in erythrocytes and megakaryocytes . \n\t', '\n\t\t We have not included an anaphora-resolution module in our present system , so that instances 7 , 8 and 10 will only display in the output as unresolved surface element or as existential variable place-holders,9 but these issues will be explored in future versions of the system . \n\t', '\n\t\t Nevertheless , much more common occurrences as in ( 11 ) and ( 12 ) are enough to create MIDs quite useful for lexicographers and for NLP lexical resources . \n\t', '\n\t\t ( 11 ) The Jovian magnetic field exerts an influence out to near a surface , called the "" magnetopause "" . \n\t', '\n\t\t ( 12 ) Here we report the discovery of a soluble decoy receptor , termed decoy receptor 3 ( DcR3 ) ... \n\t', '\n\t\t The correct database entry for example 12 is presented in Table 4 . \n\t', '\n\t\t Reference : MedLine sample # 6 Autonym : decoy receptor 3 ( DcR3 ) Information a soluble decoy receptor Markers/ Operators : termed Table 4 . \n\t', '\n\t\t Sample entry of MID The final processing stage presents metrics shown in Figure 4 , using a ß factor of 1.0 to estimate F-measures . \n\t', '\n\t\t To better reflect overall performance in all template slots , we introduced a threshold of similarity of 65 % for comparison between a golden standard slot entry and the one provided by the application . \n\t', '\n\t\t Thus , if the autonym or the informational segment is at least 2/3 of the correct response , it is counted as a positive , in many cases leveling the field for the expected errors in the prepositional phrase- or acronym- attachment algorithms , but accounting for a ( basically ) correct selection of superficial sentence segments . \n\t', '\n\t\t 9 For sentence ( 8 ) the system would retrieve a previous sentence : ( \x93A few have positive enthalpies offormation\x94 ) . \n\t', '\n\t\t to define \x93endothermic compounds\x94 . \n\t', '\n\t\t Database template fillup Corpus MID Candidate Filtering Collocations \x95 Learning Candidate extraction Semantic labeling Tokenization POS tagging & Partial parsing Precision Recall Precision Recall Precision Recall Informational Segments Global Autonyms Figure 4. Metrics for 3 corpora ( # of Records/Global F-Measure ) 1 0.9 0.8 0.7 0.6 Histology ( 35/0.71 ) Sociology ( 143/0.77 ) MedLine ( 10/0.78 ) 5 Results , comparisons and discussion The DEFINDER system \n\t\t']",Positive
"['\n\t\t First , DEFINDER examines user-oriented documents that are bound to contain fully-developed definitions for the layman , as the general goal of the PERSIVAL project is to present medical information to patients in a less technical language than the one of reference literature . \n\t', '\n\t\t MOP focuses on leading-edge research papers that present the less predictable informational templates of highly technical language . \n\t', '\n\t\t Secondly , by the very nature of DEFINDER\x92s goals their qualitative evaluation criteria include readability , usefulness and completeness as judged by lay subjects , criteria which we have not adopted here . \n\t', '\n\t\t Neither have we determined coverage against existing online dictionaries , as they have done . \n\t', '\n\t\t Taking into account the above-mentioned differences between the two systems\x92 methods and goals , MOP compares well with the 0.8 Precision and 0.75 Recall of DEFINDER . \n\t', '\n\t\t While the resulting MOP \x93definitions\x94 generally do not present high readability or completeness , these informational segments are not meant to be read by laymen , but used by domain lexicographers reviewing existing glossaries for neological change , or , for example , in machine-readable form by applications that attempt automatic categorization for semantic rerendering of an expert ontology , since definitional contexts provide sortal information as a natural part of the process of precisely situating a term or concept against the meaning network of interrelated lexical items . \n\t', '\n\t\t The Metalinguistic Information Databases in their present form are not , in full justice , lexical knowledge bases comparable with the highly-structured and sophisticated resources that use inheritance and typed features , like LKB \n\t\t']",Positive
"['\n\t\t MIDs are semi-structured resources ( midway between raw corpora and structured lexical bases ) that can be further processed to convert them into usable data sources , along the lines suggested by \n\t\t']",Positive
"['\n\t\t Another interesting possibility is to use a dynamically-updated MID to trace the conceptual and terminological evolution of a discipline . \n\t', '\n\t\t We believe that low recall rates in our tests are in part due to the fact that we are dealing with the wider realm of metalinguistic information , as opposed to structured definitional sentences that have been distilled by an expert for consumer- oriented documents . \n\t', '\n\t\t We have opted in favor of exploiting less standardized , non-default metalinguistic information that is being put forward in text because it can\x92t be assumed to be part of the collective expert-domain competence ( Section 2.1 ) . \n\t', '\n\t\t In doing so , we have exposed our system to the less predictable and highly charged lexical environment of leading-edge research literature , the cauldron where knowledge and terminological systems are forged in real time , and where scienti- fic meaning and interpretation are constantly debated , modified and agreed . \n\t', '\n\t\t We have not performed major customization of the system ( like enriching the tagging lexicon with medical terms ) , in order to preserve the ability to use the system across different domains . \n\t', '\n\t\t Domain customization may improve metrics , but at a cost for portability . \n\t', '\n\t\t The implementation we have described here undoubtedly shows room for improvement in some areas , including : adding other patterns for better overall recall rates , deeper parsing for more accurate semantic typing of sentence arguments , etc. . \n\t', '\n\t\t Also , the issue of which learning algorithms can better perform the initial filtering of EMO candidates is still very much an open question . \n\t', '\n\t\t Applications that can turn MIDs into truly useful lexical resources by further processing them need to be written . \n\t', '\n\t\t We plan to continue development of our proof-of-concept system to explore those areas . \n\t', '\n\t\t DEFINDER and MOP both show great potential as robust lexical acquisition systems capable of handling the vast electronic resources available today to researchers and laymen alike , helping to make them more accessible and useful . \n\t', '\n\t\t In doing so , they are also fulfilling the promise of NLP techniques as mature and practical technologies . \n\t', '\n\t\t References ACQUILEX projects , final report available at : http://www.cl.cam.ac.uk/Research/NL/acquilex/ Berger , A. , S. Della Pietra et al. , 1996 . \n\t', '\n\t\t A Maximum Entropy Approach to Natural Language Processing . \n\t', '\n\t\t Computational Linguistics , vol. 22 , no . \n\t', '\n\t\t 1. Carnap , R. 1934 . \n\t', '\n\t\t The Logical Syntax of Lan- guage . \n\t', '\n\t\t Routledge and Kegan , Londres 1964. Cartier , E. 1998 . \n\t', '\n\t\t Analyse Automatique des textes : l\x92example des informations définitoires . \n\t', '\n\t\t RIFRA 1998 . \n\t', '\n\t\t Sfax , Tunisia . \n\t', '\n\t\t Chieu , Hai Leong , Ng , Hwee Tou , & Lee , Yoong Keok . \n\t', '\n\t\t 2003. Closing the Gap : Learning-Based Information Extraction Rivaling Knowledge- Engineering Methods . \n\t', '\n\t\t 41st ACL . \n\t', '\n\t\t Sapporo , Japan . \n\t', '\n\t\t Copestake , A. , Sanfilippo , A. , Briscoe , T. and de Pavia , V. 1993 . \n\t', '\n\t\t The ACQUILEX LKB : An introduction . \n\t', '\n\t\t In : Inheritance , Defaults and the Lexicon . \n\t', '\n\t\t Cambridge University Press . \n\t', '\n\t\t Fisher , D. , S. Soderland , J. McCarthy , F. Feng , and W. Lehnert . \n\t', '\n\t\t 1995. Description of the UMass system as used for MUC-6 . \n\t', '\n\t\t In Proceedings of MUC-6 Hearst , M. 1998 . \n\t', '\n\t\t Automated discovery of wordnet relations . \n\t', '\n\t\t In Christiane Fellbaum , editor , WordNet : An Electronic Lexical Database . \n\t', '\n\t\t MIT Press , Cambridge , MA Klavans , J. and S. Muresan . \n\t', '\n\t\t 2001. Evaluation of the DEFINDER System for Fully Automatic Glossary Construction , proceedings of the American Medical Informatics Association Symposium 2001 Lascarides , A. and Copestake A. 1995 . \n\t', '\n\t\t The Pragmatics of Word Meaning , Proceedings of the AAAI Spring Symposium Series : Representation and Acquisition of Lexical Knowledge : Polysemy , Ambiguity and Generativity , Stanford CA . \n\t', '\n\t\t Manning , Ch. 1993 . \n\t', '\n\t\t Automatic acquisition of a large subcategorization dictionary from corpora , In Proceedings of the 31st ACL , Columbus , OH . \n\t', '\n\t\t Nigam , K. , Lafferty , J. , and McCallum , A. 1999 . \n\t', '\n\t\t Using Maximum Entropy for Text Classification , IJCAI-99 Workshop on Machine Learning for Information Filtering , pp. 61-67 Pustejovsky J. , A. Rumshisky and J. Castaño . \n\t', '\n\t\t 2002. Rerendering Semantic Ontologies : Automatic Extensions to UMLS through Corpus Analytics . \n\t', '\n\t\t LREC 2002 Workshop on Ontologies and Lexical Knowledge Bases . \n\t', '\n\t\t Las Palmas , Canary Islands , Spain . \n\t', '\n\t\t Ratnaparkhi A. 1997 . \n\t', '\n\t\t A Simple Introduction to Maximum Entropy Models for Natural Language Processing , TR 97-08 , Institute for Research in Cognitive Science , University of Pennsylvania Rey-Debove , J. 1978 . \n\t', '\n\t\t Le Métalangage . \n\t', '\n\t\t Le Robert , Paris . \n\t', '\n\t\t Rodríguez , C. 2001 . \n\t', '\n\t\t Parsing Metalinguistic Knowledge from Texts , Selected papers from CICLING-2000 Collection in Computer Science ( CCC ) ; National Polytechnic Institute ( IPN ) , Mexico . \n\t', '\n\t\t Vossen , P. and Copestake , A. 1993 . \n\t', '\n\t\t Untangling Definition Structure into Knowledge Representation . \n\t', '\n\t\t In : Inheritance , Defaults and the Lexicon . \n\t', '\n\t\t Optimizing Typed Feature Structure Grammar Parsing through Non-Statistical Indexing Cosmin Munteanu and Gerald Penn University of Toronto 10 King\x92s College Rd. Toronto M5S 3G4 Canada mcosmin,gpenn @cs.toronto.edu Abstract This paper introduces an indexing method based on static analysis of grammar rules and type signatures for typed feature structure grammars ( TFSGs ) . \n\t', '\n\t\t The static analysis tries to predict at compile-time which feature paths will cause unification failure during parsing at run-time . \n\t', '\n\t\t To support the static analysis , we introduce a new classification of the instances of variables used in TFSGs , based on what type of structure sharing they create . \n\t', '\n\t\t The indexing actions that can be performed during parsing are also enumerated . \n\t', '\n\t\t Non-statistical indexing has the advantage of not requiring training , and , as the evaluation using large-scale HPSGs demonstrates , the improvements are comparable with those of statistical optimizations . \n\t', '\n\t\t Such statistical optimizations rely on data collected during training , and their performance does not always compensate for the training costs . \n\t', '\n\t\t 1 Introduction Developing efficient all-paths parsers has been a long-standing goal of research in computational linguistics . \n\t', '\n\t\t One particular class still in need of parsing time improvements is that of TFSGs . \n\t', '\n\t\t While simpler formalisms such as context-free grammars ( CFGs ) also face slow all-paths parsing times when the size of the grammar increases significantly , TFSGs ( which generally have fewer rules than large- scale CFGs ) become slow as a result of the complex structures used to describe the grammatical categories . \n\t', '\n\t\t In HPSGs \n\t\t']",Positive
"['\n\t\t This has been a barrier in transferring CFGsuccessful techniques to TFSG parsing . \n\t', '\n\t\t For TFSG chart parsers , one of the most time- consuming operations is the retrieval of categories from the chart during rule completion ( closing of constituents in the chart under a grammar rule ) . \n\t', '\n\t\t Looking in the chart for a matching edge for a daughter is accomplished by attempting unifications with edges stored in the chart , resulting in many failed unifications . \n\t', '\n\t\t The large and complex structure of TFS descriptions \n\t\t']",Negative
"['\n\t\t Thus , failing unifications must be avoided during retrieval from the chart . \n\t', '\n\t\t To our knowledge , there have been only four methods proposed for improving the retrieval component of TFSG parsing . \n\t', '\n\t\t One \n\t\t']",Positive
"['\n\t\t The second , a statistical method known as quick- check \n\t\t']",Positive
"['\n\t\t This was measured as providing up to a 50 % improvement in parse times on the English Resource Grammar ( Flickinger , 1999 , ERG ) . \n\t', '\n\t\t The third \n\t\t']",Positive
"['\n\t\t This was found to improve parse times on the ALE HPSG by up to 33 % . \n\t', '\n\t\t The problem with these statistical methods is that the improvements in parsing times may not justify the time spent on profiling , particularly during grammar development . \n\t', '\n\t\t The static analysis method introduced here does not use profiling , although it does not preclude it either . \n\t', '\n\t\t Indeed , an evaluation of statistical methods would be more relevant if measured on top of an adequate extent of non-statistical optimizations . \n\t', '\n\t\t Although quick-check is thought to produce parsing time improvements , its evaluation used a parser with only a superficial static analysis of chart indexing . \n\t', '\n\t\t That analysis , rule filtering \n\t\t']",Positive
"['\n\t\t True indexing organizes the data ( in this case , chart edges ) to avoid unnecessary retrievals altogether , does not require the operations that it performs to be repeated once full unification is deemed necessary , and offers the support for easily adding information extracted from further static analysis of the grammar rules , while maintaining the same indexing strategy . \n\t', '\n\t\t Flexibility is one of the reasons for the successful employment of indexing in databases \n\t\t']",Positive
"['\n\t\t In this paper , we present a general scheme for indexing TFS categories during parsing ( Section 3 ) . \n\t', '\n\t\t We then present a specific method for statically analyzing TFSGs based on the type signature and the structure of category descriptions in the grammar rules , and prove its soundness and completeness ( Section 4.2.1 ) . \n\t', '\n\t\t We describe a specific indexing strategy based on this analysis ( Section 4 ) , and evaluate it on two large-scale TFSGs ( Section 5 ) . \n\t', '\n\t\t The result is a purely non-statistical method that is competitive with the improvements gained by statistical optimizations , and is still compatible with further statistical improvements . \n\t', '\n\t\t 2 TFSG Terminology TFSs are used as formal representatives of rich grammatical categories . \n\t', '\n\t\t In this paper , the formalism from \n\t\t']",Positive
"['\n\t\t A TFSG is defined relative to a fixed set of types and set of features , along with constraints , called appropriateness conditions . \n\t', '\n\t\t These are collectively known as the type signature ( Figure 3 ) . \n\t', '\n\t\t For each type , appropriateness specifies all and only the features that must have values defined in TFSs of that type . \n\t', '\n\t\t It also specifies the types of the values that those features can take . \n\t', '\n\t\t The set of types is partially ordered , and has a unique most general type ( \x96 \x93bottom\x94 ) . \n\t', '\n\t\t This order is called subsumption ( ) : more specific ( higher ) types inherit appropriate features from their more general ( lower ) supertypes . \n\t', '\n\t\t Two types t1 and t2 unify ( t1 t2 ) iff they have a least upper bound in the hierarchy . \n\t', '\n\t\t Besides a type signature , TFSGs contain a set of grammar ( phrase ) rules and lexical descriptions . \n\t', '\n\t\t A simple example of a lexical description is : john SYNSEM : SYN : np SEM : j , while an example of a phrase rule is given in Figure 1. SYN : s SEM : VPSem AGENT : NPSem SYN : np AGR : Agr SEM : NPSem , SYN : vp AGR : Agr SEM : VPSem . \n\t', '\n\t\t Figure 1 : A phrase rule stating that the syntactic category s can be combined from np and vp if their values for agr are the same . \n\t', '\n\t\t The semantics of s is that of the verb phrase , while the semantics of the noun phrase serves as agent . \n\t', '\n\t\t 2.1 Typed Feature Structures A TFS ( Figure 2 ) is like a recursively defined record in a programming language : it has a type and features with values that can be TFSs , all obeying the appropriateness conditions of the type signature . \n\t', '\n\t\t TFSs can also be seen as rooted graphs , where arcs correspond to features and nodes to substructures . \n\t', '\n\t\t A node typing function 0 q associates a type to every node q in a TFS . \n\t', '\n\t\t Every TFS F has a unique starting or root node , qF . \n\t', '\n\t\t For a given TFS , the feature value partial function S f q specifies the node reachable from q by feature f when one exists . \n\t', '\n\t\t The path value partial function S 7c q specifies the node reachable from q by following a path of features 7c when one exists . \n\t', '\n\t\t TFSs can be unified as well . \n\t', '\n\t\t The result represents the most general consistent combination of the information from two TFSs . \n\t', '\n\t\t That information includes typing ( by unifying the types ) , feature values ( by recursive unification ) , and structure sharing ( by an equivalence closure taken over the nodes of the arguments ) . \n\t', '\n\t\t For large TFSs , unification is computationally expensive , since all the nodes of the two TFSs are visited . \n\t', '\n\t\t In this process , many nodes are collapsed into equivalence classes because of structure sharing . \n\t', '\n\t\t A node x in a TFS F with root qF and a node x in a TFS F with root qF are equivalent ( ) with respect to F F iff x qF and x qF , or if there is a path 7c such that SF F 7c qF x and SF F 7cqF x . \n\t', '\n\t\t Figure 2 : A TFS . \n\t', '\n\t\t Features are written in uppercase , while types are written with bold-face lowercase . \n\t', '\n\t\t Structure sharing is indicated by numerical tags , such as [ 1 ] . \n\t', '\n\t\t masculine feminine neuter singular plural first second third Figure 3 : A type signature . \n\t', '\n\t\t For each type , appropriateness declares the features that must be defined on TFSs of that type , along with the type restrictions applying to their values . \n\t', '\n\t\t index index THROWER : THROWN : PERSON : pers num gend NUMBER : GENDER : gend num pers throwing index throwing THROWER : index PERSON : third [1]singular NUMBER : THROWN : index GENDER : masculine third PERSON : [ 1 ] NUMBER : neuter GENDER : 2.2 Structure Sharing in Descriptions TFSGs are typically specified using descriptions , which logically denote sets of TFSs . \n\t', '\n\t\t Descriptions can be more terse because they can assume all of the information about their TFSs that can be inferred from appropriateness . \n\t', '\n\t\t Each non-disjunctive description can be associated with a unique most general feature structure in its denotation called a most general satisfier ( MGSat ) . \n\t', '\n\t\t While a formal presentation can be found in \n\t\t']",Positive
"['\n\t\t Descriptions can also contain variables , such as Nr . \n\t', '\n\t\t Structure sharing is enforced in descriptions through the use of variables . \n\t', '\n\t\t In TFSGs , the scope of a variable extends beyond a single description , resulting in structure sharing between different TFSs . \n\t', '\n\t\t In phrase structure rules ( Figure 1 ) , this sharing can occur between different daughter categories in a rule , or between a mother and a daughter . \n\t', '\n\t\t Unless the term description is explicitly used , we will use \x93mother\x94 and \x93daughter\x94 to refer to the MGSat of a mother or daughter description . \n\t', '\n\t\t We can classify instances of variables based on what type of structure sharing they create . \n\t', '\n\t\t Internal variables are the variables that represent internal structure sharing ( such as in Figure 2 ) . \n\t', '\n\t\t The occurrences of such variables are limited to a single category in a phrase structure rule . \n\t', '\n\t\t External variables are the variables used to share structure between categories . \n\t', '\n\t\t If a variable is used for structure sharing both inside a category and across categories , then it is also considered an external variable . \n\t', '\n\t\t For a specific category , two kinds of external variable instances can be distinguished , depending on their occurrence relative to the parsing control strategy : active external variables and inactive external variables . \n\t', '\n\t\t Active external variables are instances of external variables that are shared between the description of a category D and one or more descriptions of categories in the same rule as D visited by the parser before D as the rule is extended ( completed ) . \n\t', '\n\t\t Inactive external variables are the external variable instances that are not active . \n\t', '\n\t\t For example , in bottom-up left-to-right parsing , all of a mother\x92s external variable instances would be active because , being external , they also occur in one of the daughter descriptions . \n\t', '\n\t\t Similarly , all of the leftmost daughter\x92s external variable instances would be inactive because this is the first description used by the parser . \n\t', '\n\t\t In Figure 1 , Agr is an active external variable in the second daughter , but it is inactive in the first daughter . \n\t', '\n\t\t The active external variable instances are important for path indexing ( Section 4.2 ) , because they represent the points at which the parser must copy structure between TFSs . \n\t', '\n\t\t They are therefore substructures that must be provided to a rule by the parsing chart if these unifications could potentially fail . \n\t', '\n\t\t They also represent shared nodes in the MGSats of a rule\x92s category descriptions . \n\t', '\n\t\t In our definitions , we assume without loss of generality that parsing proceeds bottom-up , with left-to-right of rule daughters . \n\t', '\n\t\t This is the ALE system\x92s \n\t\t']",Positive
"['\n\t\t Definition 1 . \n\t', '\n\t\t If D1 Dn are daughter de- scriptions in a rule and the rules are extended from left to right , then Ext MGSat Di is the set of nodes shared between MGSat Di and MGSat D1 MGSat Di 1 . \n\t', '\n\t\t For a mother description M , Ext MGSat M is the set of nodes shared with any daughter in the same rule . \n\t', '\n\t\t Because the completion of TFSG rules can cause the categories to change in structure ( due to external variable sharing ) , we need some extra notation to refer to a phrase structure rule\x92s categories at different times during a single application of that rule . \n\t', '\n\t\t By M we symbolize the mother M after M\x92s rule is completed ( all of the rule\x92s daughters are matched with edges in the chart ) . \n\t', '\n\t\t D symbolizes the daughter D after all daughters to D\x92s left in D\x92s rule were unified with edges from the chart . \n\t', '\n\t\t An important relation exists between M and M : if qM is M\x92s root and ^ qM is M\x92s root , then^x M^x which S 7c qM x and S 7c qM x , 6 x 6 x . \n\t', '\n\t\t In other words , extending the rule extends the information states of its categories monotonically . \n\t', '\n\t\t A similar relation exists between D and D . \n\t', '\n\t\t The set of all nodes x in M such that 7c for which S 7c qM x and S 7c qM x will be denoted by x 1 ( and like- wise for nodes in D ) . \n\t', '\n\t\t There may be more than one node in x 1 because of unifications that occur during the extension of M to M. 3 The Indexing Timeline Indexing can be applied at several moments during parsing . \n\t', '\n\t\t We introduce a general strategy for indexed parsing , with respect to what actions should be taken at each stage . \n\t', '\n\t\t Three main stages can be identified . \n\t', '\n\t\t The first one consists of indexing actions that can be taken off-line ( along with other optimizations that can be performed at compile-time ) . \n\t', '\n\t\t The second and third stages refer to actions performed at run time . \n\t', '\n\t\t M such that 7c for Stage 1 . \n\t', '\n\t\t In the off-line phase , a static analysis of grammar rules can be performed . \n\t', '\n\t\t The complete content of mothers and daughters may not be accessible , due to variables that will be instantiated during parsing , but various sources of information , such as the type signature , appropriateness specifications , and the types and features of mother and daughter descriptions , can be analyzed and an appropriate indexing scheme can be specified . \n\t', '\n\t\t This phase of indexing may include determining : ( 1a ) which daughters in which rules will certainly not unify with a specific mother , and ( 1b ) what information can be extracted from categories during parsing that can constitute indexing keys . \n\t', '\n\t\t It is desirable to perform as much analysis as possible off-line , since the cost of any action taken during run time prolongs the parsing time . \n\t', '\n\t\t Stage 2 . \n\t', '\n\t\t During parsing , after a rule has been completed , all variables in the mother have been extended as far as they can be before insertion into the chart . \n\t', '\n\t\t This offers the possibility of further investigating the mother\x92s content and extracting supplemental information from the mother that contributes to the indexing keys . \n\t', '\n\t\t However , the choice of such investigative actions must be carefully studied , since it might burden the parsing process . \n\t', '\n\t\t Stage 3 . \n\t', '\n\t\t While completing a rule , for each daughter a matching edge is searched in the chart . \n\t', '\n\t\t At this moment , the daughter\x92s active external variables have been extended as far as they can be before unification with a chart edge . \n\t', '\n\t\t The information identified in stage ( 1b ) can be extracted and unified as a precursor to the remaining steps involved in category unification . \n\t', '\n\t\t These steps also take place at this stage . \n\t', '\n\t\t 4 TFSG Indexing To reduce the time spent on failures when searching for an edge in the chart , each edge ( edge\x92s category ) has an associated index key which uniquely identifies the set of daughter categories that can potentially match it . \n\t', '\n\t\t When completing a rule , edges unifying with a specific daughter are searched for in the chart . \n\t', '\n\t\t Instead of visiting all edges in the chart , the daughter\x92s index key selects a restricted number of edges for traversal , thus reducing the number of unification attempts . \n\t', '\n\t\t The passive edges added to the chart represent specializations of rules\x92 mothers . \n\t', '\n\t\t When a rule is completed , its mother M is added to the chart according to M\x92s indexing scheme , which is the set of index keys of daughters that might possibly unify with M . \n\t', '\n\t\t The index is implemented as a hash , where the hash function applied to a daughter yields the daughter\x92s index key ( a selection of chart edges ) . \n\t', '\n\t\t For a passive edge representing M , M\x92s indexing scheme provides the collection of hash entries where it will be added . \n\t', '\n\t\t Each daughter is associated with a unique index key . \n\t', '\n\t\t During parsing , a specific daughter is searched for in the chart by visiting only those edges that have a matching key , thus reducing the time needed for traversing the chart . \n\t', '\n\t\t The index keys can be computed off-line ( when daughters are indexed by position ) , or during parsing . \n\t', '\n\t\t 4.1 Positional Indexing In positional indexing , the index key for each daughter is represented by its position ( rule number and daughter position in the rule ) . \n\t', '\n\t\t The structure of the index can be determined at compile-time ( first stage ) . \n\t', '\n\t\t For each mother M in the grammar , a collection L M RZ Dj daughters that can match M is created ( M\x92s indexing scheme ) , where each element of L M represents the rule number RZ and daughter position Dj inside rule RZ ( 1 j arity RZ ) of a category that can match with M . \n\t', '\n\t\t For TFSGs it is not possible to compute off-line the exact list of mother-daughter matching pairs , but it is possible to rule out certain non-unifiable pairs before parsing \x97 a compromise that pays off with a very low index management time . \n\t', '\n\t\t During parsing , each time an edge ( representing a rule\x92s mother M ) is added to the chart , it is inserted into the hash entries associated with the positions RZ Dj from the list L M ( the number of entries where M is inserted is L M ) . \n\t', '\n\t\t The entry associated with the key RZ Dj will contain only categories that can possibly unify with the daughter at position RZ Dj in the grammar . \n\t', '\n\t\t Because our parsing algorithm closes categories depth-first under leftmost daughter matching , only daughters DZ with i 2 are searched for in the chart ( and consequently , indexed ) . \n\t', '\n\t\t We used the EFD-based modification of this algorithm \n\t\t']",Positive
"['\n\t\t Without this , the cost of copying TFS categories would have overwhelmed the benefit of the index . \n\t', '\n\t\t 4.2 Path Indexing Path indexing is an extension of positional indexing . \n\t', '\n\t\t Although it shares the same underlying principle as the path indexing used in automated reasoning \n\t\t']",Positive
"['\n\t\t Path indexing differs from quick-check in that it identifies these paths by a static analysis of grammar rules , performed off-line and with no training required . \n\t', '\n\t\t Path indexing is also built on top of positional indexing , therefore the vector of types can be different for each potentially unifiable mother- daughter pair . \n\t', '\n\t\t 4.2.1 Static Analysis of Grammar Rules Similar to the abstract interpretation used in program verification \n\t\t']",Positive
"['\n\t\t It tries to identify nodes in a mother that carry no relevant information with respect to unification with a particular daughter . \n\t', '\n\t\t For a mother M unifiable with a daughter D , these nodes will be grouped in a set StaticCut M D. Intuitively , these nodes can be left out or ignored while computing the unification of M and D . \n\t', '\n\t\t The StaticCut can be divided into two subsets : StaticCut M D RigidCut M D VariableCut M D The RigidCut represents nodes that can be left out because neither they , nor one of their S^-ancestors , can have their type values changed by means of external variable sharing . \n\t', '\n\t\t The VariableCut represents nodes that are either externally shared , or have an externally shared ancestor , but still can be left out . \n\t', '\n\t\t Definition 2. RigidCut M D is the largest subset of nodes x M such that , y D for which x y : 1. x Ext M,y Ext D , 2.^x Ms. t. 7c s. t. S 7c x x , x Ext M^ , and 3. y D s. t. 7c s. t. S 7c y y , y Ext D. Definition 3. VariableCut is the largest subset of nodes x M such that : 1. x RigidCut M D , and 2. y D for which x y , s 6 x t 6 y , s t exists . \n\t', '\n\t\t In words , a node can be left out even if it is externally shared ( or has an externally shared ancestor ) if all possible types this node can have unify with all possible types its corresponding nodes in D can have . \n\t', '\n\t\t Due to structure sharing , the types of nodes in M and D can change during parsing , by being specialized to one of their subtypes . \n\t', '\n\t\t Condition 2 ensures that the types of these nodes will remain compatible ( have a least upper bound ) , even if they specialize during rule completion . \n\t', '\n\t\t An intuitive example ( real-life examples cannot be reproduced here \x97 a category in a typical TFSG can have hundreds of nodes ) is presented in Figure 4 . \n\t', '\n\t\t Figure 4 : Given the above type signature , mother M and daughter D ( externally shared nodes are pointed to by dashed arrows ) , nodes x1 x2 and x3 from M can be left out when unifying M with D during parsing . \n\t', '\n\t\t x1 and x3 RigidCut M D , while x2 VariableCut M D ( 9 y2 can promote only to t7 , thus x2 and y2 will always be compatible ) . \n\t', '\n\t\t x4 is not included in the StaticCut , because if ^ y5 promotes to t5 , then ^ y4 will promote to t5 ( not unifiable with t3 ) . \n\t', ""\n\t\t When computing the unification between a mother and a daughter during parsing , the same outcome ( success or failure ) will be reached by using a reduced representation of the mother ( M'`D ) , with nodes in StaticCut M D removed from M. Proposition 1 . \n\t"", ""\n\t\t For a mother M and a daughter D , ifM D before parsing , and ^M ( as an edge in the chart ) and D exist , then during parsing : ( 1 ) M'`D D M D , ( 2 ) M'`D D M D. Proof . \n\t"", ""\n\t\t The second part ( M'`D D M D ) of Proposition 1 has a straightforward proof : if M'`D D , then z M'`D D such that t for which^x ^z t 6 x . \n\t"", ""\n\t\t Since M'`D M , z M D such that t for which^x ^z^ t 6 x , and therefore , M D. The first part of the proposition will be proven by showing that^z M D , a consistent type can be assigned to ^z , where ^z is the set of nodes in M and D equivalent to z with respect to the unification of M and D.1 Three lemmata need to be formulated : Lemma 1 . \n\t"", '\n\t\t Ifx M and x ^x 1 , then 6 x 6x . \n\t', '\n\t\t Similarly , for y D , y ^y 1 , 6 y 6 y. Lemma 2 . \n\t', '\n\t\t If types t0 t1 tn are such that t0 t0 i 1 n , t0 ti , then t t0 such that i 1 n , t ti . \n\t', ""\n\t\t ' Because we do not assume inequated TFSs \n\t\t""]",Positive
"['\n\t\t t0 F:t6 t5 G:t5 t3 t4 t2 J:t5 G:t1 H:t6 I:t3 K:t1 t7 ts t1 t6 t1 t5 X1 M y1 D F : H : F : H : K : t7 t7 X3 X2 t6 y3 t6 y5 t1 y2 I : G : X4 t3 G : G : y4 t1 Lemma 3 . \n\t', '\n\t\t Ifx Mandy D for which x y , then x x 1 y y 1 such that x y . \n\t', '\n\t\t In proving the first part of Proposition 1 , four cases are identified : Case A : z^ M 1 and z D 1 , Case B : z M 1 and z D 1 , Case C : z^ M 1 and z D 1 , Case D : z M 1 and z D 1 . \n\t', '\n\t\t Case A is trivial , and D is a generalization of B and C. Case B. It will be shown that t Type such that y z D and for x z M , t 6 y and t 6x . \n\t', ""\n\t\t Subcase B.i : x M x M'`D . \n\t"", '\n\t\t y z^ D , y x . \n\t', '\n\t\t Therefore , according to Lemma 3 , x x 1 y y 1 such that x y . \n\t', '\n\t\t Thus , according to Condition 2 of Definition 3 , s 6 y t 6 x , s t . \n\t', '\n\t\t But according to Lemma 1 , 6 y 6 y and 6 x 6 x . \n\t', '\n\t\t Therefore , y z D , s 6 y , t 6 x , s t , and hence , y z D t 6 x t 6 y . \n\t', '\n\t\t Thus , according to Lemma 2 , t 6x y z D,t 6y . \n\t', ""\n\t\t Subcase B.ii : x Mx M'`D . \n\t"", ""\n\t\t Since M'`D D , t 6 x such that y z D , t 6 y . \n\t"", '\n\t\t Case C. It will be shown that t 6 y such that^x z , t 6 x . \n\t', '\n\t\t Let y z^ D . \n\t', ""\n\t\t The set z^ M can be divided into two subsets : SZZ x z^M x M'`D , and SZ x z M x M x M'`D , and x VariableCut M D. If x were in RigidCut M D , then necessarily z^ M would be 1 . \n\t"", ""\n\t\t Since SZZ M'`D and M'`D D , then t 6 y such that^x SZZ t 6 x ( * ) . \n\t"", '\n\t\t How- ever,^x SZZ , x y . \n\t', '\n\t\t Therefore , according to Lemma 3,^x SZZ x x 1 y y 1 such that x y . \n\t', '\n\t\t Thus , since x VariableCut M D , Condition 2 of Definition 3 holds , and therefore , accord- ing to Lemma 1 , ^s1 6 x^s2 6 y s1 s2 . \n\t', '\n\t\t More than this , since t 6 y ( for the type t from ( * ) ) , s1 6 x s2 t s1 s2 , and hence , s2 t s2 6 x . \n\t', '\n\t\t Thus , according to Lemma 2 and to ( * ) , t t 6^y such that^x SZZ t 6 x Thus , t such that^x z^ , t 6 x . \n\t', '\n\t\t While Proposition 1 could possibly be used by grammar developers to simplify TFSGs themselves at the source-code level , here we only exploit it for internally identifying index keys for more efficient chart parsing with the existing grammar . \n\t', '\n\t\t There may be better static analyses , and better uses of this static analysis . \n\t', '\n\t\t In particular , future work will focus on using static analysis to determine smaller representations ( by cutting nodes in Static Cuts ) of the chart edges themselves . \n\t', '\n\t\t 4.2.2 Building the Path Index The indexing schemes used in path indexing are built on the same principles as those in positional indexing . \n\t', '\n\t\t The main difference is the content of the indexing keys , which now includes a third element . \n\t', '\n\t\t Each mother M has its indexing scheme defined as : L M RZ Dj VZ i . \n\t', '\n\t\t The pair RZ Dj is the positional index key ( as in positional indexing ) , while VZ i is the path index vector containing type values extracted from M . \n\t', '\n\t\t A different set of types is extracted for each mother-daughter pair . \n\t', '\n\t\t So , path indexing uses a two-layer indexing method : the positional key for daughters , and types extracted from the typed feature structure . \n\t', '\n\t\t Each daughter\x92s index key is now given by L Dj RZ VZ i , where RZ is the rule number of a potentially matching mother , and VZ i is the path index vector containing types extracted from Dj . \n\t', '\n\t\t The types extracted for the indexing vectors are those of nodes found at the end of indexing paths . \n\t', '\n\t\t A path 7c is an indexing path for a mother- daughter pair M D iff : ( 1 ) 7c is defined for both M and D , ( 2 ) x StaticCut M D f s.t. S f x S 7c qM ( qM is M\x92s root ) , and ( 3 ) S 7c qM StaticCut M D. Indexing paths are the \x93frontiers\x94 of the non-statically-cut nodes of M . \n\t', '\n\t\t A similar key extraction could be performed during Stage 2 of indexing ( as outlined in Section 3 ) , using M rather than M . \n\t', '\n\t\t We have found that this online path discovery is generally too expensive to be performed during parsing , however . \n\t', '\n\t\t As stated in Proposition 1 , the nodes in StaticCut M D do not affect the success/failure of M D. Therefore , the types of first nodes not included in StaticCut M D along each path 7c that stems from the root of M and D are in- cluded in the indexing key , since these nodes might contribute to the success/failure of the unification . \n\t', '\n\t\t It should be mentioned that the vectors VZ i are filled with values extracted from M after M\x92s rule is completed , and from D after all daughters to the left of D are unified with edges in the chart . \n\t', '\n\t\t As an example , assuming that the indexing paths are THROWER:PERSON , THROWN , and THROWN:GENDER , the path index vector for the TFS shown in Figure 2 is third index neuter . \n\t', '\n\t\t 4.2.3 Using the Path Index Inserting and retrieving edges from the chart using path indexing is similar to the general method presented at the beginning of this section . \n\t', '\n\t\t The first layer of the index is used to insert a mother as an edge into appropriate chart entries , according to the positional keys for the daughters it can match . \n\t', '\n\t\t Along with the mother , its path index vector is inserted into the chart . \n\t', '\n\t\t When searching for a matching edge for a daughter , the search is restricted by the first indexing layer to a single entry in the chart ( labeled with the positional index key for the daughter ) . \n\t', '\n\t\t The second layer restricts searches to the edges that have a compatible path index vector . \n\t', '\n\t\t The compatibility is defined as type unification : the type pointed to by the element Vi j n of an edge\x92s vector VZ j should unify with the type pointed to by the element Vi j n of the path index vector Vi j of the daughter on position Dj in a rule Ri . \n\t', '\n\t\t 5 Experimental Evaluation Two TFSGs were used to evaluate the performance of indexing : a pre-release version of the MERGE grammar , and the ALE port of the ERG ( in its final form ) . \n\t', '\n\t\t MERGE is an adaptation of the ERG which uses types more conservatively in favour of relations , macros and complex-antecedent constraints . \n\t', '\n\t\t This pre-release version has 17 rules , 136 lexical items , 1157 types , and 144 introduced features . \n\t', '\n\t\t The ERG port has 45 rules , 1314 lexical entries , 4305 types and 155 features . \n\t', '\n\t\t MERGE was tested on 550 sentences of lengths between 6 and 16 words , extracted from the Wall Street Journal annotated parse trees ( where phrases not covered by MERGE\x92s vocabulary were replaced by lexical entries having the same parts of speech ) , and from MERGE\x92s own test corpus . \n\t', '\n\t\t ERG was tested on 1030 sentences of lengths between 6 and 22 words , extracted from the Brown Corpus and from the Wall Street Journal annotated parse trees . \n\t', '\n\t\t Rather than use the current version of ALE , TFSs were encoded as Prolog terms as prescribed in \n\t\t']",Positive
"['\n\t\t This was extended to allow for the enforcement of type constraints during TFS unification . \n\t', '\n\t\t Types were encoded as attributed variables in SICStus Prolog \n\t\t']",Positive
"['\n\t\t 5.1 Positional and path indexing evaluation The average and best improvements in parsing times of positional and path indexing over the same EFDbased parser without indexing are presented in Table 1 . \n\t', '\n\t\t The parsers were implemented in SICStus 3 . \n\t', '\n\t\t 10.1 for Solaris 8 , running on a Sun Server with 16 GB of memory and 4 UltraSparc v.9 processors at 1281 MHz . \n\t', '\n\t\t For MERGE , parsing times range from 10 milliseconds to 1.3 seconds . \n\t', '\n\t\t For ERG , parsing times vary between 60 milliseconds and 29.2 seconds . \n\t', '\n\t\t Positional Index Path Index average best average best MERGE 1.3 % 50 % 1.3 % 53.7 % ERG 13.9 % 36.5 % 12 % 41.6 % Table 1 : Parsing time improvements of positional and path indexing over the non-indexed EFD parser . \n\t', '\n\t\t 5.2 Comparison with statistical optimizations Non-statistical optimizations can be seen as a first step toward a highly efficient parser , while statistical optimization can be applied as a second step . \n\t', '\n\t\t However , one of the purposes of non-statistical indexing is to eliminate the burden of training while offering comparable improvements in parsing times . \n\t', '\n\t\t A quick-check parser was also built and evaluated and the set-up times for the indexed parsers and the quick-check parser were compared ( Table 2 ) . \n\t', '\n\t\t Quick-check was trained on a 300-sentence training corpus , as prescribed in \n\t\t']",Positive
"['\n\t\t The training corpus included 150 sentences also used in testing . \n\t', '\n\t\t The number of paths in path indexing is different for each mother-daughter pair , ranging from 1 to 43 over the two grammars . \n\t', '\n\t\t Positional Path Index Quick Check Index Compiling grammar 6\x9230\x94 Compiling index 2\x94 1\x9233\x94 - Training - - 3h28\x9214\x94 Total set-up time : 6\x9232\x94 8\x923\x94 3h34\x9244\x94 Table 2 : The set-up times for non-statistically indexed parsers and statistically optimized parsers for MERGE . \n\t', '\n\t\t As seen in Table 3 , quick-check alone surpasses positional and path indexing for the ERG . \n\t', '\n\t\t However , it is outperformed by them on the MERGE , recording slower times than even the baseline . \n\t', '\n\t\t But the combination of quick-check and path indexing is faster than quick-check alone on both grammars . \n\t', '\n\t\t Path indexing at best provided no decrease in performance over positional indexing alone in these experiments , attesting to the difficulty of maintaining efficient index keys in an implementation . \n\t', '\n\t\t Positional Indexing Path Quick Check Quick + Indexing Path MERGE 1.3 % 1.3 % -4.5 % -4.3 % ERG 13.9 % 12 % 19.8 % 22 % Table 3 : Comparison of average improvements over non- indexed parsing among all parsers . \n\t', '\n\t\t The quick-check evaluation presented in \n\t\t']",Positive
"['\n\t\t Quick-check has an additional advantage in the present comparison , because half of the training sentences were included in the test corpus . \n\t', '\n\t\t While quick-check improvements on the ERG confirm other reports on this method , it must be Grammar Successful unifications Failed unifications Failure rate reduction ( vs. no index ) EFD Positional Path Index Quick Check Positional Path Index Quick Check non-indexed Index Index MERGE 159 755 699 552 370 7.4 % 26.8 % 50.9 % ERG 1078 215083 109080 108610 18040 49.2 % 49.5 % 91.6 % Table 4 : The number of successful and failed unifications for the non-indexed , positional indexing , path indexing , and quick-check parsers , over MERGE and ERG ( collected on the slowest sentence in the corresponding test sets . \n\t', '\n\t\t ) noted that quick-check appears to be parochially very well-suited to the ERG ( indeed quick-check was developed alongside testing on the ERG ) . \n\t', '\n\t\t Although the recommended first 30 most probable failure-causing paths account for a large part of the failures recorded in training on both grammars ( 94 % for ERG and 97 % for MERGE ) , only 51 paths caused failures at all for MERGE during training , compared to 216 for the ERG . \n\t', '\n\t\t Further training with quick-check for determining a better vector length for MERGE did not improve its performance . \n\t', '\n\t\t This discrepancy in the number of failure-causing paths could be resulting in an overfitted quick-check vector , or , perhaps the 30 paths chosen for MERGE really are not the best 30 ( quick-check uses a greedy approximation ) . \n\t', '\n\t\t In addition , as shown in Table 4 , the improvements made by quick-check on the ERG are explained by the drastic reduction of ( chart lookup ) unification failures during parsing relative to the other methods . \n\t', '\n\t\t It appears that nothing short of a drastic reduction is necessary to justify the overhead of maintaining the index , which is the largest for quick-check because some of its paths must be traversed at run-time \x97 path indexing only uses paths available at compile-time in the grammar source . \n\t', '\n\t\t Note that path indexing outperforms quick-check on MERGE in spite of its lower failure reduction rate , because of its smaller overhead . \n\t', '\n\t\t 6 Conclusions and Future Work The indexing method proposed here is suitable for several classes of unification-based grammars . \n\t', '\n\t\t The index keys are determined statically and are based on an a priori analysis of grammar rules . \n\t', '\n\t\t A major advantage of such indexing methods is the elimination of the lengthy training processes needed by statistical methods . \n\t', '\n\t\t Our experimental evaluation demonstrates that indexing by static analysis is a promising alternative to optimizing parsing with TFSGs , although the time consumed by on-line maintenance of the index is a significant concern \x97 echoes of an observation that has been made in applications of term indexing to databases and programming languages \n\t\t']",Positive
"['\n\t\t Further work on efficient implementations and data structures is therefore required . \n\t', '\n\t\t Indexing by static analysis of grammar rules combined with statistical methods also can provide a higher aggregate benefit . \n\t', '\n\t\t The current static analysis of grammar rules used as a basis for indexing does not consider the effect of the universally quantified constraints that typically augment the signature and grammar rules . \n\t', '\n\t\t Future work will investigate this extension as well . \n\t', '\n\t\t References B. Carpenter and G. Penn. 1996 . \n\t', '\n\t\t Compiling typed attribute-value logic grammars . \n\t', '\n\t\t In H. Bunt and M. Tomita , editors , Recent Advances in Parsing Technologies , pages 145\x96168 . \n\t', '\n\t\t Kluwer . \n\t', '\n\t\t B. Carpenter . \n\t', '\n\t\t 1992. The Logic of Typed Feature Structures . \n\t', '\n\t\t Cambridge University Press . \n\t', '\n\t\t P. Cousot and R. Cousot . \n\t', '\n\t\t 1992. Abstract interpretation and application to logic programs . \n\t', '\n\t\t Journal ofLogic Programming , 13(2\x963) . \n\t', '\n\t\t R. Elmasri and S. Navathe . \n\t', '\n\t\t 2000. Fundamentals of database systems . \n\t', '\n\t\t Addison-Wesley . \n\t', '\n\t\t D. Flickinger . \n\t', '\n\t\t 1999. The English Resource Grammar . \n\t', '\n\t\t http://lingo.stanford.edu/erg.html . \n\t', '\n\t\t P. Graf . \n\t', '\n\t\t 1996. Term Indexing . \n\t', '\n\t\t Springer . \n\t', '\n\t\t B. Kiefer , H.U. Krieger , J. Carroll , and R. Malouf . \n\t', '\n\t\t 1999. A bag of useful techniques for efficient and robust parsing . \n\t', '\n\t\t In Proceedings of the 37th Annual Meeting of the ACL . \n\t', '\n\t\t R. Malouf , J. Carrol , and A. Copestake . \n\t', '\n\t\t 2000. Efficient feature structure operations without compilation . \n\t', '\n\t\t Natural Language Engineering , 6(1) . \n\t', '\n\t\t G. Penn and C. Munteanu . \n\t', '\n\t\t 2003. A tabulation- based parsing method that reduces copying . \n\t', '\n\t\t In Proceedings of the 41st Annual Meeting of the ACL , Sapporo , Japan . \n\t', '\n\t\t G. Penn. 1999a . \n\t', '\n\t\t An optimised Prolog encoding of typed feature structures . \n\t', '\n\t\t Technical Report 138 , SFB 340 , T¨ubingen . \n\t', '\n\t\t G. Penn. 1999b . \n\t', '\n\t\t Optimising don\x92t-care non- determinism with statistical information . \n\t', '\n\t\t Technical Report 140 , SFB 340 , T¨ubingen . \n\t', '\n\t\t C. Pollard and I. Sag . \n\t', '\n\t\t 1994. Head-driven Phrase Structure Grammar . \n\t', '\n\t\t The University of Chicago Press . \n\t', '\n\t\t I.V. Ramakrishnan , R. Sekar , and A. Voronkov . \n\t', '\n\t\t 2001. Term indexing . \n\t', '\n\t\t In Handbook of Automated Reasoning , volume II , chapter 26. Elsevier Science . \n\t', '\n\t\t Swedish Institute of Computer Science . \n\t', '\n\t\t 2004. SIC- Stus Prolog 3.11.0. http://www.sics.se/sicstus . \n\t', '\n\t\t Head-Driven Parsing for Word Lattices Bob Carpenter Alias I , Inc. . \n\t', '\n\t\t Brooklyn , NY , USA carp@alias-i.com Christopher Collins Department of Computer Science University of Toronto Toronto , ON , Canada ccollins@cs.utoronto.ca Gerald Penn Department of Computer Science University of Toronto Toronto , ON , Canada gpenn@cs.utoronto.ca Abstract We present the first application of the head-driven statistical parsing model of \n\t\t']",Positive
"['\n\t\t The model is adapted to an online left to right chart-parser for word lattices , integrating acoustic , n-gram , and parser probabilities . \n\t', '\n\t\t The parser uses structural and lexical dependencies not considered by n- gram models , conditioning recognition on more linguistically-grounded relationships . \n\t', '\n\t\t Experiments on the Wall Street Journal treebank and lattice corpora show word error rates competitive with the standard n-gram language model while extracting additional structural information useful for speech understanding . \n\t', '\n\t\t 1 Introduction The question of how to integrate high-level knowledge representations of language with automatic speech recognition ( ASR ) is becoming more important as ( 1 ) speech recognition technology matures , ( 2 ) the rate of improvement of recognition accuracy decreases , and ( 3 ) the need for additional information ( beyond simple transcriptions ) becomes evident . \n\t', '\n\t\t Most of the currently best ASR systems use an n-gram language model of the type pioneered by \n\t\t']",Positive
"['\n\t\t Recently , research has begun to show progress towards application of new and better models of spoken language \n\t\t']",Positive
"['\n\t\t Our goal is integration of head-driven lexicalized parsing with acoustic and n-gram models for speech recognition , extracting high-level structure from speech , while simultaneously selecting the best path in a word lattice . \n\t', '\n\t\t Parse trees generated by this process will be useful for automated speech understanding , such as in higher semantic parsing \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t Grammar productions are conditioned on headwords . \n\t', '\n\t\t The conditioning context is thus more focused than that of a large n-gram covering the same span , so the sparse data problems arising from the sheer size of the parameter space are less pressing . \n\t', '\n\t\t However , sparse data problems arising from the limited availability of annotated training data become a problem . \n\t', '\n\t\t We test the head-driven statistical lattice parser with word lattices from the NIST HUB-1 corpus , which has been used by others in related work \n\t\t']",Positive
"['\n\t\t Parse accuracy and word error rates are reported . \n\t', '\n\t\t We present an analysis of the effects of pruning and heuristic search on efficiency and accuracy and note several simplifying assumptions common to other reported experiments in this area , which present challenges for scaling up to real- world applications . \n\t', '\n\t\t This work shows the importance of careful algorithm and data structure design and choice of dynamic programming constraints to the efficiency and accuracy of a head-driven probabilistic parser for speech . \n\t', '\n\t\t We find that the parsing model of \n\t\t']",Positive
"['\n\t\t In the following section , we present a review of recent works in high-level language modelling for speech recognition . \n\t', '\n\t\t We describe the word lattice parser developed in this work in Section 3 . \n\t', '\n\t\t Section 4 is a description of current evaluation metrics , and suggestions for new metrics . \n\t', '\n\t\t Experiments on strings and word lattices are reported in Section 5 , and conclusions and opportunities for future work are outlined in Section 6 . \n\t', '\n\t\t 2 Previous Work The largest improvements in word error rate ( WER ) have been seen with n-best list rescoring . \n\t', '\n\t\t The best n hypotheses of a simple speech recognizer are processed by a more sophisticated language model and re-ranked . \n\t', '\n\t\t This method is algorithmically simpler than parsing lattices , as one can use a model developed for strings , which need not operate strictly . \n\t', '\n\t\t left to right . \n\t', '\n\t\t However , we confirm the observation of \n\t\t']",Positive
['\n\t\t \n\t\t'],Negative
"['\n\t\t Word predictions of the structured language model are conditioned on the two previous phrasal heads not yet contained in a bigger constituent . \n\t', '\n\t\t This is a computationally intensive process , as the dependencies considered can be of arbitrarily long distances . \n\t', '\n\t\t All possible sentence prefixes are considered at each extension step . \n\t', '\n\t\t \n\t\t']",Positive
['\n\t\t Our work is different from \n\t\t'],Positive
"['\n\t\t Bottom-up chart parsing , through various forms of extensions to the CKY algorithm , has been applied to word lattices for speech recognition \n\t\t']",Positive
"['\n\t\t Full acoustic and n-best lattices filtered by trigram scores have been parsed . \n\t', '\n\t\t \n\t\t']",Positive
['\n\t\t Unlike \n\t\t'],Negative
"['\n\t\t 3 Word Lattice Parser Parsing models based on headword dependency relationships have been reported , such as the structured language model of \n\t\t']",Negative
['\n\t\t These models use much less conditioning information than the parsing models of \n\t\t'],Positive
['\n\t\t In this section we outline the adaptation of the \n\t\t'],Positive
"['\n\t\t The intended action of the parser is illustrated in Figure 1 , which shows parse trees built directly upon a word lattice . \n\t', '\n\t\t 3.1 Parameterization The parameterization of model II of \n\t\t']",Positive
"['\n\t\t Parameters are S S. NP NP VP NN IN CC NNP AUX IN DT NN MD VB rise arise Figure 1 : Example of a partially-parsed word lattice . \n\t', '\n\t\t Different paths through the lattice are simultaneously parsed . \n\t', '\n\t\t The example shows two final parses , one of low probability ( S ) and one of high probability ( S ) . \n\t', '\n\t\t maximum likelihood estimates of conditional probabilities \x97 the probability of some event of interest ( e.g. , a left-modifier attachment ) given a context ( e.g. , parent non-terminal , distance , headword ) . \n\t', '\n\t\t One notable difference between the word lattice parser and the original implementation of \n\t\t']",Positive
"['\n\t\t The conditioning context of the parsing model parameters includes POS tagging . \n\t', '\n\t\t \n\t\t']",Positive
['\n\t\t As the tagger of \n\t\t'],Positive
"['\n\t\t We rely on the tag assigned by the parsing model in all cases . \n\t', '\n\t\t Edges created by the bottom-up parsing are assigned a score which is the product of the inside and outside probabilities of the \n\t\t']",Positive
"['\n\t\t 3.2 Parsing Algorithm The algorithm is a variation of probabilistic online , bottom-up , left-to-right Cocke-KasamiYounger parsing similar to \n\t\t']",Positive
"['\n\t\t Our parser produces trees ( bottom-up ) in a right- branching manner , using unary extension and binary adjunction . \n\t', '\n\t\t Starting with a proposed headword , left modifiers are added first using right-branching , then right modifiers using left-branching . \n\t', '\n\t\t Word lattice edges are iteratively added to the agenda . \n\t', '\n\t\t Complete closure is carried out , and the next word edge is added to the agenda . \n\t', '\n\t\t This process is repeated until all word edges are read from the . \n\t', '\n\t\t speculation in tokyo was that the yen could and the unit lattice , and at least one complete parse is found . \n\t', '\n\t\t Edges are each assigned a score , used to rank parse candidates . \n\t', '\n\t\t For parsing of strings , the score for a chart edge is the product of the scores of any child edges and the score for the creation of the new edge , as given by the model parameters . \n\t', '\n\t\t This score , defined solely by the parsing model , will be referred to as the parser score . \n\t', '\n\t\t The total score for chart edges for the lattice parsing task is a combination of the parser score , an acoustic model score , and a trigram model score . \n\t', '\n\t\t Scaling factors follow those of \n\t\t']",Positive
['\n\t\t 3.3 Smoothing and Pruning The parameter estimation techniques ( smoothing and back-off ) of \n\t\t'],Positive
"['\n\t\t Additional techniques are required to prune the search space of possible parses , due to the complexity of the parsing algorithm and the size of the word lattices . \n\t', '\n\t\t The main technique we employ is a variation of the beam search of \n\t\t']",Positive
"['\n\t\t The total score ( combined acoustic and language model scores ) of candidate edges are compared against edge with the same span and category . \n\t', '\n\t\t Proposed edges with score outside the beam are not added to the chart . \n\t', '\n\t\t The drawback to this process is that we can no longer guarantee that a model-optimal solution will be found . \n\t', '\n\t\t In practice , these heuristics have a negative effect on parse accuracy , but the amount of pruning can be tuned to balance relative time and space savings against precision and recall degradation \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t We experiment with several variable beam ( \x88b ) sizes , where the beam is some function of a base beam ( b ) and the edge width ( the number of terminals dominated by an edge ) . \n\t', '\n\t\t The base beam starts at a low beam size and increases iteratively by a specified increment if no parse is found . \n\t', '\n\t\t This allows parsing to operate quickly ( with a minimal number of edges added to the chart ) . \n\t', '\n\t\t However , if many iterations are required to obtain a parse , the utility of starting with a low beam and iterating becomes questionable \n\t\t']",Positive
"['\n\t\t The base beam is limited to control the increase in the chart size . \n\t', '\n\t\t The selection of the base beam , beam increment , and variable beam function is governed by the familiar speed/accuracy trade-off.1 The variable beam function found to allow fast convergence with minimal loss of accuracy is : b b\x88 ( 1 ) log w 2 2 1Details of the optimization can be found in \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
['\n\t\t The technique is employed by \n\t\t'],Positive
"['\n\t\t We adapt this idea to a single stage process . \n\t', '\n\t\t Due to the restrictions of beam search and thresholds , the first parse found by the model may not be the model optimal parse ( i.e. , we cannot guarantee best-first search ) . \n\t', '\n\t\t We therefore employ a form of overparsing \x97 once a complete parse tree is found , we further extend the base beam by the beam increment and parse again . \n\t', '\n\t\t We continue this process as long as extending the beam results in an improved best parse score . \n\t', '\n\t\t 4 Expanding the Measures of Success Given the task of simply generating a transcription of speech , WER is a useful and direct way to measure language model quality for ASR . \n\t', '\n\t\t WER is the count of incorrect words in hypothesis W\x88 per word in the true string W . \n\t', '\n\t\t For measurement , we must assume prior knowledge of W and the best alignment of the reference and hypothesis strings.2 Errors are categorized as insertions , deletions , or substitutions . \n\t', '\n\t\t Word Error Rate 100Insertions Substitutions Deletions ( 2 ) Total Words in Correct Transcript It is important to note that most models \x97 \n\t\t']",Positive
"['\n\t\t Sentence error rate is the percentage of sentences for which the proposed utterance has at least one error . \n\t', '\n\t\t Models ( such as ours ) which optimize prediction of test sentences Wt , generated by the source , minimize the sentence error . \n\t', '\n\t\t Thus even though WER is useful practically , it is formally not the appropriate measure for the commonly used language models . \n\t', '\n\t\t Unfortunately , as a practical measure , sentence error rate is not as useful \x97 it is not as fine-grained as WER . \n\t', '\n\t\t Perplexity is another measure of language model quality , measurable independent of ASR performance \n\t\t']",Positive
"['\n\t\t Perplexity is related to the entropy of the source model which the language model attempts to estimate . \n\t', '\n\t\t These measures , while informative , do not capture success of extraction of high-level information from speech . \n\t', '\n\t\t Task-specific measures should be used in tandem with extensional measures such as perplexity and WER . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t parsing for speech recognition , discusses a modelling trade-off between producing parse trees and producing strings . \n\t', '\n\t\t Most models are evaluated either with measures of success for parsing or for word recognition , but rarely both . \n\t', '\n\t\t Parsing models are difficult to implement as word-predictive language models due to their complexity . \n\t', '\n\t\t Generative random sampling is equally challenging , so the parsing correlate of perplexity is not easy to measure . \n\t', '\n\t\t Traditional ( i.e. , n-gram ) language models do not produce parse trees , so parsing metrics are not useful . \n\t', '\n\t\t However , \n\t\t']",Positive
['\n\t\t Weighted WER \n\t\t'],Positive
['\n\t\t We will adopt the testing strategy of \n\t\t'],Positive
"['\n\t\t Use of weighted WER and development of methods to simultaneously measure WER and parse accuracy remain a topic for future research . \n\t', '\n\t\t 5 Experiments The word lattice parser was evaluated with several metrics \x97 WER , labelled precision and recall , crossing brackets , and time and space resource usage . \n\t', '\n\t\t Following \n\t\t']",Positive
"['\n\t\t We optimized settings ( thresholds , variable beam function , base beam value ) for parsing using development test data consisting of strings for which we have annotated parse trees . \n\t', '\n\t\t The parsing accuracy for parsing word lattices was not directly evaluated as we did not have annotated parse trees for comparison . \n\t', '\n\t\t Furthermore , standard parsing measures such as labelled precision and recall are not directly applicable in cases where the number of words differs between the proposed parse tree and the gold standard . \n\t', '\n\t\t Results show scores for parsing strings which are lower than the original implementation of \n\t\t']",Positive
"['\n\t\t The WER scores for this , the first application of the \n\t\t']",Positive
['\n\t\t 3Parse trees are commonly scored with the PARSEVAL set of metrics \n\t\t'],Positive
"['\n\t\t 5.1 Parsing Strings The lattice parser can parse strings by creating a single-path lattice from the input ( all word transitions are assigned an input score of 1.0 ) . \n\t', '\n\t\t The lattice parser was trained on sections 02-21 of the Wall Street Journal portion of the Penn Treebank \n\t\t']",Positive
"['\n\t\t Final testing was carried out on section 00 , and the PARSEVAL measures \n\t\t']",Positive
['\n\t\t The scores for our experiments are lower than the scores of the original implementation of model II \n\t\t'],Positive
"['\n\t\t This difference is likely due in part to differences in POS tagging . \n\t', '\n\t\t Tag accuracy for our model was 93.2 % , whereas for the original implementation of \n\t\t']",Positive
"['\n\t\t In addition to different tagging strategies for unknown words , mentioned above , we restrict the tag-set considered by the parser for each word to those suggested by a simple first-stage tagger.4 By reducing the tag-set considered by the parsing model , we reduce the search space and increase the speed . \n\t', '\n\t\t However , the simple tagger used to narrow the search also introduces tagging error . \n\t', '\n\t\t The utility of the overparsing extension can be seen in Table 1 . \n\t', '\n\t\t Each of the PARSEVAL measures improves when overparsing is used . \n\t', '\n\t\t 5.2 Parsing Lattices The success of the parsing model as a language model for speech recognition was measured both by parsing accuracy ( parsing strings with annotated reference parses ) , and by WER . \n\t', '\n\t\t WER is measured by parsing word lattices and comparing the sentence yield of the highest scoring parse tree to the reference transcription ( using NIST SCLITE for alignment and error calculation).5 We assume the parsing performance achieved by parsing strings carries over approximately to parsing word lattices . \n\t', '\n\t\t Two different corpora were used in training the parsing model on word lattices : sections 02-21 of the WSJ Penn Treebank ( the same sections as used to train the model for parsing strings ) [ 1 million words ] 4The original implementation \n\t\t']",Positive
"['\n\t\t 5To properly model language using a parser , one should sum parse tree scores for each sentence hypothesis , and choose the sentence with the best sum of parse tree scores . \n\t', '\n\t\t We choose the yield of the parse tree with the highest score . \n\t', '\n\t\t Summation is too computationally expensive given the model \x97we do not even generate all possible parse trees , but instead restrict generation using dynamic programming . \n\t', '\n\t\t Exp . \n\t', '\n\t\t OP LP ( % ) LR ( % ) CB 0 CB ( % ) 2 CB ( % ) Ref N 88.7 89.0 0.95 65.7 85.6 1 N 79.4 80.6 1.89 46.2 74.5 2 Y 80.8 81.4 1.70 44.3 80.4 Table 1 : Results for parsing section 0 ( 40 words ) of the WSJ Penn Treebank : OP = overparsing , LP/LR = labelled precision/recall . \n\t', '\n\t\t CB is the average number of crossing brackets per sentence . \n\t', '\n\t\t 0 CB , 2 CB are the percentage of sentences with 0 or 2 crossing brackets respectively . \n\t', '\n\t\t Ref is model II of \n\t\t']",Positive
['\n\t\t section \x931987\x94 of the BLLIP corpus \n\t\t'],Positive
"['\n\t\t As the memory usage of our model corresponds directly to the amount of training data used , we were restricted by available memory to use only one section ( 1987 ) of the total corpus . \n\t', '\n\t\t Using the BLLIP corpus , we expected to get lower quality parse results due to the higher parse error of the corpus , when compared to the manually annotated Penn Treebank . \n\t', '\n\t\t The WER was expected to improve , as the BLLIP corpus has much greater lexical coverage . \n\t', '\n\t\t The training corpora were modified using a utility by Brian Roark to convert newspaper text to speech- like text , before being used as training input to the model . \n\t', '\n\t\t Specifically , all numbers were converted to words ( 6 0 sixty ) and all punctuation was removed . \n\t', '\n\t\t We tested the performance of our parser on the word lattices from the NIST HUB-1 evaluation task of 1993 . \n\t', '\n\t\t The lattices are derived from a set of utterances produced from Wall Street Journal text \x97 the same domain as the Penn Treebank and the BLLIP training data . \n\t', '\n\t\t The word lattices were previously pruned to the 50-best paths by Brian Roark , using the A* decoding of \n\t\t']",Positive
"['\n\t\t The word lattices of the HUB-1 corpus are directed acyclic graphs in the HTK Standard Lattice Format ( SLF ) , consisting of a set of vertices and a set of edges . \n\t', '\n\t\t Vertices , or nodes , are defined by a time-stamp and labelled with a word . \n\t', '\n\t\t The set of labelled , weighted edges , represents the word utterances . \n\t', '\n\t\t A word w is hypothesized over edge e if e ends at a vertex v labelled w. Edges are associated with transition probabilities and are labelled with an acoustic score and a language model score . \n\t', '\n\t\t The lattices of the HUB- 6The sentences of the HUB-1 corpus are a subset of those in BLLIP . \n\t', '\n\t\t We removed all HUB-1 sentences from the BLLIP corpus used in training . \n\t', '\n\t\t 1 corpus are annotated with trigram scores trained using a 20 thousand word vocabulary and 40 million word training sample . \n\t', '\n\t\t The word lattices have a unique start and end point , and each complete path through a lattice represents an utterance hypothesis . \n\t', '\n\t\t As the parser operates in a left-to-right manner , and closure is performed at each node , the input lattice edges must be processed in topological order . \n\t', '\n\t\t Input lattices were sorted before parsing . \n\t', '\n\t\t This corpus has been used in other work on syntactic language modelling \n\t\t']",Positive
"['\n\t\t The word lattices of the HUB-1 corpus are annotated with an acoustic score , a , and a trigram probability , lm , for each edge . \n\t', '\n\t\t The input edge score stored in the word lattice is : log PZnpyd ^log a ^log lm ( 3 ) where a is the acoustic score and lm is the trigram score stored in the lattice . \n\t', '\n\t\t The total edge weight in the parser is a scaled combination of these scores with the parser score derived with the model parameters : log w ^ log a ^ log lm s ( 4 ) where w is the edge weight , and s is the score assigned by the parameters of the parsing model . \n\t', '\n\t\t We optimized performance on a development subset of test data , yielding ^ 1 16 and ^ 1 . \n\t', '\n\t\t There is an important difference in the tokenization of the HUB-1 corpus and the Penn Treebank format . \n\t', '\n\t\t Clitics ( i.e. , he\x92s , wasn\x92t ) are split from their hosts in the Penn Treebank ( i.e. , he \x92s , was n\x92 t ) , but not in the word lattices . \n\t', '\n\t\t The Tree- bank format cannot easily be converted into the lattice format , as often the two parts fall into different parse constituents . \n\t', '\n\t\t We used the lattices modified by \n\t\t']",Positive
['\n\t\t We followed \n\t\t'],Positive
"['\n\t\t The model was tested with and without overparsing . \n\t', '\n\t\t We see from Table 2 that overparsing has little effect on the WER . \n\t', '\n\t\t The word sequence most easily parsed by the model ( i.e. , generating the first complete parse tree ) is likely also the word sequence found by overparsing . \n\t', '\n\t\t Although overparsing may have little effect on WER , we know from the experiments on strings that overparsing increases parse accuracy . \n\t', '\n\t\t This introduces a speed-accuracy tradeoff : depending on what type of output is required from the model ( parse trees or strings ) , the additional time and resource requirements of overparsing may or may not be warranted . \n\t', '\n\t\t 5.3 Parsing N-Best Lattices vs. N-Best Lists The application of the model to 50-best word lattices was compared to rescoring the 50-best paths individually ( 50-best list parsing ) . \n\t', '\n\t\t The results are presented in Table 2 . \n\t', '\n\t\t The cumulative number of edges added to the chart per word for n-best lists is an order of magnitude larger than for corresponding n-best lattices , in all cases . \n\t', '\n\t\t As the WERs are similar , we conclude that parsing n-best lists requires more work than parsing n-best lattices , for the same result . \n\t', '\n\t\t Therefore , parsing lattices is more efficient . \n\t', '\n\t\t This is because common substrings are only considered once per lattice . \n\t', '\n\t\t The amount of computational savings is dependent on the density of the lattices \x97 for very dense lattices , the equivalent n-best list parsing will parse common substrings up to n times . \n\t', '\n\t\t In the limit of lowest density , a lattice may have paths without overlap , and the number of edges per word would be the same for the lattice and lists . \n\t', '\n\t\t 5.4 Time and Space Requirements The algorithms and data structures were designed to minimize parameter lookup times and memory usage by the chart and parameter set \n\t\t']",Positive
"['\n\t\t To increase parameter lookup speed , all parameter values are calculated for all levels of back-off at training time . \n\t', '\n\t\t By contrast , \n\t\t']",Positive
"['\n\t\t The implementation was then optimized using a memory and processor profiler and debugger . \n\t', '\n\t\t Parsing the complete set of HUB-1 lattices ( 213 sentences , a total of 3,446 words ) on average takes approximately 8 hours , on an Intel Pentium 4 ( 1.6GHz ) Linux system , using 1GB memory . \n\t', '\n\t\t Memory requirements for parsing lattices is vastly greater than equivalent parsing of a single sentence , as chart size increases with the number of divergent paths in a lattice . \n\t', '\n\t\t Additional analysis of resource issues can be found in \n\t\t']",Positive
"['\n\t\t 5.5 Comparison to Previous Work The results of our best experiments for lattice- and list-parsing are compared with previous results in Table 3 . \n\t', '\n\t\t The oracle WER7 for the HUB-1 corpus is 3.4 % . \n\t', '\n\t\t For the pruned 50-best lattices , the oracle WER is 7.8 % . \n\t', '\n\t\t We see that by pruning the lattices using the trigram model , we already introduce additional error . \n\t', '\n\t\t Because of the memory usage and time required for parsing word lattices , we were unable to test our model on the original \x93acoustic\x94 HUB-1 lattices , and are thus limited by the oracle WER of the 50-best lattices , and the bias introduced by pruning using a trigram model . \n\t', '\n\t\t Where available , we also present comparative scores of the sentence error rate ( SER ) \x97 the percentage of sentences in the test set for which there was at least one recognition error . \n\t', '\n\t\t Note that due to the small ( 213 samples ) size of the HUB-1 corpus , the differences seen in SER may not be significant . \n\t', '\n\t\t We see an improvement in WER for our pars- ing model alone ( ^ ^ 0 ) trained on 1 million words of the Penn Treebank compared to a trigram model trained on the same data \x97 the \x93Treebank Trigram\x94 noted in Table 3 . \n\t', '\n\t\t This indicates that the larger context considered by our model allows for performance improvements over the trigram model alone . \n\t', '\n\t\t Further improvement is seen with the com- bination of acoustic , parsing , and trigram scores ( ^ 1 16^ 1 ) . \n\t', '\n\t\t However , the combination of the parsing model ( trained on 1M words ) with the lattice trigram ( trained on 40M words ) resulted in a higher WER than the lattice trigram alone . \n\t', '\n\t\t This indicates that our 1M word training set is not sufficient to permit effective combination with the lattice trigram . \n\t', '\n\t\t When the training of the head-driven parsing model was extended to the BLLIP 1987 corpus ( 20M words ) , the combination of models ( ^ 1 16^ 1 ) achieved additional improvement in WER over the lattice trigram alone . \n\t', '\n\t\t The current best-performing models , in terms of WER , for the HUB-1 corpus , are the models of \n\t\t']",Positive
"['\n\t\t However , n-best list parsing , as seen in our evaluation , requires repeated analysis of common subsequences , a less efficient process than directly parsing the word lattice . \n\t', '\n\t\t The reported results of \n\t\t']",Positive
"['\n\t\t Hall and John- 7The WER of the hypothesis which best matches the true utterance , i.e. , the lowest WER possible given the hypotheses set . \n\t', '\n\t\t Training Size Lattice/List OP WER Number of Edges ( per word ) S D I T 1M Lattice N 10.4 3.3 1.5 15.2 1788 1M List N 10.4 3.2 1.4 15.0 10211 1M Lattice Y 10.3 3.2 1.4 14.9 2855 1M List Y 10.2 3.2 1.4 14.8 16821 20M Lattice N 9.0 3.1 1.0 13.1 1735 20M List N 9.0 3.1 1.0 13.1 9999 20M Lattice Y 9.0 3.1 1.0 13.1 2801 20M List Y 9.0 3.3 0.9 13.3 16030 Table 2 : Results for parsing HUB-1 n-best word lattices and lists : OP = overparsing , S = substutitions ( % ) , D = deletions ( % ) , I = insertions ( % ) , T = total WER ( % ) . \n\t', '\n\t\t Variable beam function : b\x88 b log w 2 2 . \n\t', '\n\t\t Training corpora : 1M = Penn Treebank sections 02-21 ; 20M = BLLIP section 1987 . \n\t', '\n\t\t Model n-best List/Lattice Training Size WER ( % ) SER ( % ) Oracle ( 50-best lattice ) Lattice 7.8 \n\t\t']",Positive
"['\n\t\t SER = sentence error rate . \n\t', '\n\t\t WER = word error rate . \n\t', '\n\t\t \x93Speech-like\x94 transformations were applied to all training corpora . \n\t', '\n\t\t \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t son ( 2003 ) does not use the lattice trigram scores directly . \n\t', '\n\t\t However , as in other works , the lattice trigram is used to prune the acoustic lattice to the 50 best paths . \n\t', '\n\t\t The difference in WER between our parser and those of \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t We achieve 73.2/76.5 % LP/LR on section 23 of the Penn Treebank , compared to 82.9/82.4 % LP/LR of \n\t\t']",Positive
['\n\t\t Another contributing factor to the accuracy of \n\t\t'],Positive
['\n\t\t The low WER of \n\t\t'],Positive
['\n\t\t 6 Conclusions In this work we present an adaptation of the parsing model of \n\t\t'],Positive
"['\n\t\t The system was evaluated over two sets of data : strings and word lattices . \n\t', '\n\t\t As PARSEVAL measures are not applicable to word lattices , we measured the parsing accuracy using string input . \n\t', '\n\t\t The resulting scores were lower than that original implementation of the model . \n\t', '\n\t\t Despite this , the model was successful as a language model for speech recognition , as measured by WER and ability to extract high-level information . \n\t', '\n\t\t Here , the system performs better than a simple n-gram model trained on the same data , while simultaneously providing syntactic information in the form of parse trees . \n\t', '\n\t\t WER scores are comparable to related works in this area . \n\t', '\n\t\t The large size of the parameter set of this parsing model necessarily restricts the size of training data that may be used . \n\t', '\n\t\t In addition , the resource requirements currently present a challenge for scaling up from the relatively sparse word lattices of the NIST HUB-1 corpus ( created in a lab setting by professional readers ) to lattices created with spontaneous speech in non-ideal conditions . \n\t', '\n\t\t An investigation into the relevant importance of each parameter for the speech recognition task may allow a reduction in the size of the parameter space , with minimal loss of recognition accuracy . \n\t', '\n\t\t A speedup may be achieved , and additional training data could be used . \n\t', '\n\t\t Tuning of parameters using EM has lead to improved WER for other models . \n\t', '\n\t\t We encourage investigation of this technique for lexicalized head-driven lattice parsing . \n\t', '\n\t\t Acknowledgements This research was funded in part by the Natural Sciences and Engineering Research Council ( NSERC ) of Canada . \n\t', '\n\t\t Advice on training and test data was provided by Keith Hall of Brown University . \n\t', '\n\t\t References L. R. Bahl , F. Jelinek , and R. L. Mercer . \n\t', '\n\t\t 1983. A maximum likelihood approach to continuous speech recognition . \n\t', '\n\t\t IEEE Transactions on Pattern Analysis and Machine Intelligence , 5:179\x96190 . \n\t', '\n\t\t E. Black , S. Abney , D. Flickenger , C. Gdaniec , R. Grishman , P. Harrison , D. Hindle , R. Ingria , F. Jelinek , J. Klavans , M. Liberman , M. Marcus , S. Roukos , B. Santorini , and T. Strzalkowski . \n\t', '\n\t\t 1991. A procedure for quantitatively comparing the syntactic coverage of English grammars . \n\t', '\n\t\t In Proceedings of Fourth DARPA Speech and Natural Language Workshop , pages 306\x96 311 . \n\t', '\n\t\t J.-C. Chappelier and M. Rajman . \n\t', '\n\t\t 1998. A practical bottom-up algorithm for on-line parsing with stochas- tic context-free grammars . \n\t', '\n\t\t Technical Report 98-284 , Swiss Federal Institute of Technology , July . \n\t', '\n\t\t Eugene Charniak , Sharon Goldwater , and Mark Johnson . \n\t', '\n\t\t 1998. Edge-Based Best-First Chart Parsing . \n\t', '\n\t\t In 6th Annual Workshop for Very Large Corpora , pages 127\x96133 . \n\t', '\n\t\t Eugene Charniak , Don Blaheta , Niyu Ge , Keith Hall , John Hale , and Mark Johnson . \n\t', '\n\t\t 1999. BLLIP 1987-89 WSJ Corpus Release 1 . \n\t', '\n\t\t Linguistic Data Consortium . \n\t', '\n\t\t Eugene Charniak . \n\t', '\n\t\t 2000. A maximum-entropy-inspired parser . \n\t', '\n\t\t In Proceedings of the 2000 Conference of the North American Chapter of the Association for Computational Linguistics , pages 132\x96129 , New Brunswick , U.S.A. . \n\t', '\n\t\t Eugene Charniak . \n\t', '\n\t\t 2001. Immediate-head parsing for language models . \n\t', '\n\t\t In Proceedings of the 39th Annual Meeting of the ACL . \n\t', '\n\t\t Ciprian Chelba and Frederick Jelinek . \n\t', '\n\t\t 2000. Structured language modeling . \n\t', '\n\t\t Computer Speech and Language , 14:283\x96332 . \n\t', '\n\t\t Ciprian Chelba . \n\t', '\n\t\t 2000. Exploiting Syntactic Structure for Natural Language Modeling . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , Johns Hopkins University . \n\t', '\n\t\t Christopher Collins . \n\t', '\n\t\t 2004. Head-Driven Probabilistic Parsingfor Word Lattices . \n\t', '\n\t\t M.Sc . \n\t', '\n\t\t thesis , University of Toronto . \n\t', '\n\t\t Michael Collins . \n\t', '\n\t\t 1999. Head-Driven Statistical Models for Natural Language Parsing . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , University of Pennsylvania . \n\t', '\n\t\t Joshua Goodman . \n\t', '\n\t\t 1997. Global thresholding and multiple-pass parsing . \n\t', '\n\t\t In Proceedings ofthe 2nd Conference on Empirical Methods in Natural Language Processing . \n\t', '\n\t\t Keith Hall and Mark Johnson . \n\t', '\n\t\t 2003. Language modeling using efficient best-first bottom-up parsing . \n\t', '\n\t\t In Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop . \n\t', '\n\t\t Frederick Jelinek . \n\t', '\n\t\t 1997. Information Extraction From Speech And Text . \n\t', '\n\t\t MIT Press . \n\t', '\n\t\t Lidia Mangu , Eric Brill , and Andreas Stolcke . \n\t', '\n\t\t 2000. Finding consensus in speech recognition : Word error minimization and other applications of confusion networks . \n\t', '\n\t\t Computer Speech and Language , 14(4):373\x96 400 . \n\t', '\n\t\t Hwee Tou Ng and John Zelle . \n\t', '\n\t\t 1997. Corpus-based approaches to semantic interpretation in natural language processing . \n\t', '\n\t\t AI Magazine , 18:45\x9654 . \n\t', '\n\t\t A. Ratnaparkhi . \n\t', '\n\t\t 1996. A maximum entropy model for part-of-speech tagging . \n\t', '\n\t\t In Conference on Empirical Methods in Natural Language Processing , May . \n\t', '\n\t\t Mosur K. Ravishankar . \n\t', '\n\t\t 1997. Some results on search complexity vs accuracy . \n\t', '\n\t\t In DARPA Speech Recognition Workshop , pages 104\x96107 , February . \n\t', '\n\t\t Brian Roark . \n\t', '\n\t\t 2001 . \n\t', '\n\t\t Robust Probabilistic Predictive Syntactic Processing : Motivations , Models , and Applications . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , Brown University . \n\t', '\n\t\t Brian Roark . \n\t', '\n\t\t 2002. Markov parsing : Lattice rescoring with a statistical parser . \n\t', '\n\t\t In Proceedings of the 40th Annual Meeting of the ACL , pages 287\x96294 . \n\t', '\n\t\t Ann Taylor , Mitchell Marcus , and Beatrice Santorini , 2003 . \n\t', '\n\t\t The Penn TreeBank : An Overview , chapter 1. Kluwer , Dordrecht , The Netherlands . \n\t', '\n\t\t Hans Weber , J¨org Spilker , and G¨unther G¨orz . \n\t', '\n\t\t 1997. Parsing n best trees from a word lattice . \n\t', '\n\t\t Kunstliche Intelligenz , pages 279\x96288 . \n\t', '\n\t\t Peng Xu , Ciprian Chelba , and Frederick Jelinek . \n\t', '\n\t\t 2002. A study on richer syntactic dependencies in structured language modeling . \n\t', '\n\t\t In Proceedings of the 40th Annual Meeting of the ACL , pages 191\x96198 . \n\t', '\n\t\t Balancing Clarity and Efficiency in Typed Feature Logic through Delaying Gerald Penn University of Toronto 10 King\x92s College Rd. Toronto M5S 3G4 Canada gpenn@cs.toronto.edu Abstract The purpose of this paper is to re-examine the balance between clarity and efficiency in HPSG design , with particular reference to the design decisions made in the English Resource Grammar ( LinGO , 1999 , ERG ) . \n\t', '\n\t\t It is argued that a simple generalization of the conventional delay statements used in logic programming is sufficient to restore much of the functionality and concomitant benefit that the ERG elected to forego , with an acceptable although still perceptible computational cost . \n\t', '\n\t\t 1 Motivation By convention , current HPSGs consist , at the very least , of a deductive backbone of extended phrase structure rules , in which each category is a description of a typed feature structure ( TFS ) , augmented with constraints that enforce the principles of grammar . \n\t', '\n\t\t These principles typically take the form of statements , \x93for all TFSs , 0 holds,\x94 where 0 is usually an implication . \n\t', '\n\t\t Historically , HPSG used a much richer set of formal descriptive devices , however , mostly on analogy to developments in the use of types and description logics in programming language theory ( A¨ít-Ka´ci , 1984 ) , which had served as the impetus for HPSG\x92s invention \n\t\t']",Positive
"['\n\t\t This included logic-programming-style relations ( H¨ohfeld and Smolka , 1988 ) , a powerful description language in which expressions could denote sets of TFSs through the use of an explicit disjunction operator , and the full expressive power of implications , in which antecedents of the above- mentioned 0 principles could be arbitrarily complex . \n\t', '\n\t\t Early HPSG-based natural language processing systems faithfully supported large chunks of this richer functionality , in spite of their inability to handle it efficiently \x97 so much so that when the designers of the ERG set out to select formal descriptive devices for their implementation with the aim of \x93balancing clarity and efficiency,\x94 \n\t\t']",Negative
"['\n\t\t The ERG uses only phrase-structure rules and type-antecedent constraints , pushing all would-be description-level disjunctions into its type system or rules . \n\t', '\n\t\t In one respect , this choice was successful , because it did at least achieve a respectable level of efficiency . \n\t', '\n\t\t But the ERG\x92s selection of functionality has acquired an almost liturgical status within the HPSG community in the intervening seven years . \n\t', '\n\t\t Keeping this particular faith , moreover , comes at a considerable cost in clarity , as will be argued below . \n\t', '\n\t\t This paper identifies what it is precisely about this extra functionality that we miss ( modularity , Section 2 ) , determines what it would take at a minimum computationally to get it back ( delaying , Section 3 ) , and attempts to measure exactly how much that minimal computational overhead would cost ( about 4 ps per delay , Section 4 ) . \n\t', '\n\t\t This study has not been undertaken before ; the ERG designers\x92 decision was based on largely anecdotal accounts of performance relative to then-current implementations that had not been designed with the intention of minimizing this extra cost ( indeed , the ERG baseline had not yet been devised ) . \n\t', '\n\t\t 2 Modularity : the cost in clarity Semantic types and inheritance serve to organize the constraints and overall structure of an HPSG grammar . \n\t', '\n\t\t This is certainly a familiar , albeit vague justification from programming languages research , but the comparison between HPSG and modern programming languages essentially ends with this statement . \n\t', '\n\t\t Programming languages with inclusional polymorphism ( subtyping ) invariably provide functions or relations and allow these to be reified as methods within user-defined subclasses/subtypes . \n\t', '\n\t\t In HPSG , however , values of features must necessarily be TFSs themselves , and the only method ( implicitly ) provided by the type signature to act on these values is unification . \n\t', '\n\t\t In the absence of other methods and in the absence of an explicit disjunction operator , the type signature itself has the responsibility of not only declaring definitional sub- fi n-wh-fi ll-rel-clinf-wh-fi ll-rel-cl red-rel-cl simp-inf-rel-cl fi n-hd-fi ll-ph inf-hd-fi ll-ph wh-rel-cl non-wh-rel-cl hd-fi ll-ph hd-comp-ph inter-cl rel-cl hd-adj-ph hd-nexus-ph clause non-hd-ph hd-ph headed phrase phrase Figure 1 : Relative clauses in the ERG ( partial ) . \n\t', '\n\t\t class relationships , but expressing all other non- definitional disjunctions in the grammar ( as subtyping relationships ) . \n\t', '\n\t\t It must also encode the necessary accoutrements for implementing all other necessary means of combination as unification , such as difference lists for appending lists , or the so-called qeq constraints of Minimal Recursion Semantics \n\t\t']",Positive
"['\n\t\t Unification , furthermore , is an inherently non- modular , global operation because it can only be defined relative to the structure of the entire partial order of types ( as a least upper bound ) . \n\t', '\n\t\t Of course , some partial orders are more modularizable than others , but legislating the global form that type signatures must take on is not an easy property to enforce without more local guidance . \n\t', '\n\t\t The conventional wisdom in programming languages research is indeed that types are responsible for mediating the communication between modules . \n\t', '\n\t\t A simple type system such as HPSG\x92s can thus only mediate very simple communication . \n\t', '\n\t\t Modern programming languages incorporate some degree of parametric polymorphism , in addition to subtyping , in order to accommodate more complex communication . \n\t', '\n\t\t To date , HPSG\x92s use of parametric types has been rather limited , although there have been some recent attempts to apply them to the ERG \n\t\t']",Positive
"['\n\t\t Without this , one obtains type signatures such as Figure 1 ( a portion of the ERG\x92s for relative clauses ) , in which both the semantics of the subtyping links themselves ( normally , subset inclusion ) and the multi-dimensionality of the empirical domain\x92s analysis erode into a collection of arbitrary naming conventions that are difficult to validate or modify . \n\t', '\n\t\t A more avant-garde view of typing in programming languages research , inspired by the Curry- Howard isomorphism , is that types are equivalent to relations , which is to say that a relation can mediate communication between modules through its arguments , just as a parametric type can through its parameters . \n\t', '\n\t\t The fact that we witness some of these mediators as types and others as relations is simply an intensional reflection of how the grammar writer thinks of them . \n\t', '\n\t\t In classical HPSG , relations were generally used as goals in some proof resolution strategy ( such as Prolog\x92s SLD resolution ) , but even this has a parallel in the world of typing . \n\t', '\n\t\t Using the type signature and principles of Figure 2 , for ex- appendbase appendrec Arg1 : e list Arg1:ne list Junk:append append Arg1 : list Arg2 : list Arg3 : list 1 appendbase=~ Arg2 : L n Arg3 : L. appendrec=~ Arg1 : [ HIL1 ] n Arg2 : L2 n Arg3 : [ HIL3 ] n Junk : ( append n A1 : L1 n A2 : L2 n Arg3 : L3 ) . \n\t', '\n\t\t Figure 2 : Implementing SLD resolution over the append relation as sort resolution . \n\t', '\n\t\t ample , we can perform proof resolution by attempting to sort resolve every TFS to a maximally specific type . \n\t', '\n\t\t This is actually consistent with HPSG\x92s use of feature logic , although most TFS-based NLP systems do not sort resolve because type inference under sort resolution is NP-complete \n\t\t']",Positive
"['\n\t\t Phrase structure rules , on the other hand , while they can be encoded inside a logic programming relation , are more naturally viewed as algebraic generators . \n\t', '\n\t\t In this respect , they are more similar to the immediate subtyping declarations that grammar writers use to specify type signatures \x97 both chart parsing and transitive closure are instances of all- source shortest-path problems on the same kind of algebraic structure , called a closed semi-ring . \n\t', '\n\t\t The only notion of modularity ever proven to hold of phrase structure rule systems \n\t\t']",Positive
"['\n\t\t 3 Delaying : the missing link of functionality If relations are used in the absence of recursive data structures , a grammar could be specified using relations , and the relations could then be unfolded off- line into relation-free descriptions . \n\t', '\n\t\t In this usage , relations are just macros , and not at all inefficient . \n\t', '\n\t\t Early HPSG implementations , however , used quite a lot of recursive structure where it did not need to be , and the structures they used , such as lists , buried important data deep inside substructures that made parsing much slower . \n\t', '\n\t\t Provided that grammar writers use more parsimonious structures , which is a good idea even in the absence of relations , there is nothing wrong with the speed of logic programming relations \n\t\t']",Positive
"['\n\t\t Recursive datatypes are also prone to non- termination problems , however . \n\t', '\n\t\t This can happen when partially instantiated and potentially recursive data structures are submitted to a proof resolution procedure which explores the further instantiations of these structures too aggressively . \n\t', '\n\t\t Although this problem has received significant attention over the last fifteen years in the constraint logic programming ( CLP ) community , no true CLP implementation yet exists for the logic of typed feature structures ( Carpenter , 1992 , LTFS ) . \n\t', '\n\t\t Some aspects of general solution strategies , including incremental entailment simplification ( A¨ít-Kaci et al. , 1992 ) , deterministic goal expansion \n\t\t']",Positive
['\n\t\t The CUF implementation \n\t\t'],Positive
"['\n\t\t In the remainder of this section , a method is presented for reducing delays on any inequationfree description , including variables and disjunctions , to the SICStus Prolog when/2 primitive ( Sections 3.4 ) . \n\t', '\n\t\t This method takes full advantage of the restrictions inherent to LTFS ( Section 3.1 ) to maximize run-time efficiency . \n\t', '\n\t\t In addition , by delaying calls to subgoals individually rather than the ( universally quantified ) relation definitions themselves , ) we can also use delays to postpone non-deterministic search on disjunctive descriptions ( Section 3.3 ) and to implement complex- antecedent constraints ( Section 3.2 ) . \n\t', '\n\t\t As a result , this single method restores all of the functionality we were missing . \n\t', '\n\t\t For simplicity , it will be assumed that the target language of our compiler is Prolog itself . \n\t', '\n\t\t This is inconsequential to the general proposal , although implementing logic programs in Prolog certainly involves less effort . \n\t', ""\n\t\t ' Delaying relational definitions is a subcase of this func- tionality , which can be made more accessible through some extra syntactic sugar . \n\t"", '\n\t\t 3.1 Restrictions inherent to LTFS LTFS is distinguished by its possession of appropriateness conditions that mediate the occurrence of features and types in these records . \n\t', '\n\t\t Appropriateness conditions stipulate , for every type , a finite set of features that can and must have values in TFSs of that type . \n\t', '\n\t\t This effectively forces TFSs to be finite- branching terms with named attributes . \n\t', '\n\t\t Appropriateness conditions also specify a type to which the value of an appropriate feature is restricted ( a value restriction ) . \n\t', '\n\t\t These conditions make LTFS very convenient for linguistic purposes because the combination of typing with named attributes allows for a very terse description language that can easily make reference to a sparse amount of information in what are usually extremely large structures/records : Definition : Given afinite meet semi-lattice of types , Type , a fixedfinite set offeatures , Feat , and a countable set of variables , Var , 4b is the least set of descriptions that contains : \x95 v , v E Var , \x95~,~EType , \x95 F : 0 , F E Feat , 0 E 4b , \x95 01 A 02 , 01 , 02 E 4b , and \x95 01 V 02 , 01 , 02 E 4b . \n\t', '\n\t\t A nice property of this description language is that every non-disjunctive description with a non- empty denotation has a unique most general TFS in its denotation . \n\t', '\n\t\t This is called its most general satisfier . \n\t', '\n\t\t We will assume that appropriateness guarantees that there is a unique most general type , Intro(F) to which a given feature , F , is appropriate . \n\t', '\n\t\t This is called unique feature introduction . \n\t', '\n\t\t Where unique feature introduction is not assumed , it can be added automatically in O ( F · T ) time , where F is the number of features and T is the number of types \n\t\t']",Positive
"['\n\t\t Meet semi-latticehood can also be restored automatically , although this involves adding exponentially many new types in the worst case . \n\t', ""\n\t\t 3.2 Complex Antecedent Constraints It will be assumed here that all complex-antecedent constraints are implicitly universally quantified , and are of the form : a=~ ( ' yAp ) where a , -y are descriptions from the core description language , 4b , and p is drawn from a definite clause language of relations , whose arguments are also descriptions from 4b . \n\t"", '\n\t\t As mentioned above , the ERG uses the same form , but where a can only be a type description , T , and p is the trivial goal , true . \n\t', '\n\t\t The approach taken here is to allow for arbitrary antecedents , a , but still to interpret the implications of principles using subsumption by a , i.e. , for every TFS ( the implicit universal quantification is still there ) , either the consequent holds , or the TFS is not subsumed by the most general satisfier of a . \n\t', '\n\t\t The subsumption convention dates back to the TDL ( Krieger and Sch¨afer , 1994 ) and ALE \n\t\t']",Positive
['\n\t\t The Con- Troll constraint solver \n\t\t'],Negative
"['\n\t\t Within CLP more broadly , there is some related work on guarded constraints \n\t\t']",Positive
"['\n\t\t In most CLP , constraints on a class of terms or objects must be explicitly posted to a store for each member of that class . \n\t', '\n\t\t If a constraint is not posted for a particular term , then it does not apply to that term . \n\t', '\n\t\t The subsumption-based approach is sound with respect to the classical interpretation of implication for those principles where the classical interpretation really is the correct one . \n\t', '\n\t\t For completeness , some additional resolution method ( in the form of a logic program with relations ) must be used . \n\t', '\n\t\t As is normally the case in CLP , deductive search is used alongside constraint resolution . \n\t', '\n\t\t Under such assumptions , our principles can be converted to : trigger(a) ==> . \n\t', '\n\t\t v A whenfs((v = a ) , ( ( v = ~)~p ) ) Thus , with an implementation of type-antecedent constraints and an implementation of when f s / 2 ( Section 3.3 ) , which delays the goal in its second argument until v is subsumed by ( one of ) the most general satisfier(s) of description a , all that remains is a method for finding the trigger , the most efficient type antecedent to use , i.e. , the most general one that will not violate soundness . \n\t', '\n\t\t trigger(a) can be defined as follows : \x95 trigger(v) = L , \x95 trigger(~) 7 , = T , \x95 trigger(F : 0 ) = Intro ( F ) , \x95 trigger(01 ~ 02 ) = trigger(01) U trigger(02) , and \x95 trigger(01V02) = trigger(01)fltrigger(02) , where U and n are respectively unification and generalization in the type semi-lattice . \n\t', '\n\t\t In this and the next two subsections , we can use Figure 3 as a running example of the various stages of compilation of a typical complex-antecedent constraint , namely the Finiteness Marking Principle for German ( 1 ) . \n\t', '\n\t\t This constraint is stated relative to the signature shown in Figure 4 . \n\t', '\n\t\t The description to the left of the arrow in Figure 3 ( 1 ) selects TFSs whose substructure on the path SYNSEM:LOC:CAT satisfies two requirements : its HEAD value has type verb , and its MARKING value has type fin . \n\t', '\n\t\t The principle says that every TFS that satisfies that description must also have a SYNSEM : LOC : CAT : HEAD : VFORM value of type bse . \n\t', '\n\t\t To find the trigger in Figure 3 ( 1 ) , we can observe that the antecedent is a feature value description ( F:0 ) , so the trigger is Intro(SYNSEM) , the unique introducer of the SYNSEM feature , which happens to be the type sign . \n\t', '\n\t\t We can then transform this constraint as above ( Figure 3 ( 2 ) ) . \n\t', '\n\t\t The cons and goal operators in (2)\x96(5) are ALE syntax , used respectively to separate the type antecedent of a constraint from the description component of the consequent ( in this case , just the variable , x ) , and to separate the description component of the consequent from its relational attachment . \n\t', '\n\t\t We know that any TFS subsumed by the original antecedent will also be subsumed by the most general TFS of type sign , because sign introduces SYNSEM . \n\t', '\n\t\t 3.3 Reducing Complex Conditionals Let us now implement our delay predicate , whenfs ( V=Desc , Goal ) . \n\t', '\n\t\t Without loss of generality , it can be assumed that the first argument is actually drawn from a more general conditional language , including those of the form Vi = Desci closed under conjunction and disjunction . \n\t', '\n\t\t It can also be assumed that the variables of each Desci are distinct . \n\t', '\n\t\t Such a complex conditional can easily be converted into a normal form in which each atomic conditional contains a non-disjunctive description . \n\t', '\n\t\t Conjunction and disjunction of atomic conditionals then reduce as follows ( using the Prolog convention of comma for AND and semi-colon for OR ) : whenfs((VD1,VD2),Goal) :- whenfs(VD1,whenfs(VD2,Goal)) . \n\t', '\n\t\t whenfs((VD1;VD2),Goal) :- whenfs(VD1,(Trigger = 0 -> Goal ; true ) ) , whenfs(VD2,(Trigger = 1 -> Goal ; true ) ) . \n\t', '\n\t\t The binding of the variable Trigger is necessary to ensure that Goal is only resolved once in case the ( 1 ) synsem:loc:cat : ( head : verb , marking : fin ) =#- synsem:loc : cat : head : vform:bse . \n\t', '\n\t\t ( 2 ) sign cons X goal whenfs((X=synsem:loc:cat:(head:verb,marking:fin)) , ( X=synsem:loc:cat:head:vform:bse ) ) . \n\t', '\n\t\t ( 3 ) sign cons X goal whentype(sign,X,(farg(synsem,X,SynVal) , whentype(synsem,SynVal,(farg(loc,SynVal,LocVal) , whentype(local,LocVal,(farg(cat,LocVal,CatVal) , whenfs((CatVal=(head:verb,marking:fin)) , ( X=synsem:loc:cat:head:vform:bse ) ) ) ) ) ) ) ) . \n\t', '\n\t\t ( 4 ) sign cons X goal ( whentype(sign,X,(farg(synsem,X,SynVal) , whentype(synsem,SynVal,(farg(loc,SynVal,LocVal) , whentype(local,LocVal,(farg(cat,LocVal,CatVal) , whentype(category,CatVal,(farg(head,CatVal,HdVal) , whentype(verb,HdVal , whentype(category,CatVal,(farg(marking,CatVal,MkVal) , whentype(fin,MkVal , ( X=synsem:loc:cat:head:vform:bse ) ) ) ) ) ) ) ) ) ) ) ) ) ) . \n\t', '\n\t\t ( 5 ) sign cons X goal ( farg(synsem,X,SynVal) , farg(loc,SynVal,LocVal) , farg(cat,LocVal,CatVal) , farg(head,CatVal,HdVal) , whentype(verb,HdVal,(farg(marking,CatVal,MkVal) , whentype(fin,MkVal , ( X=synsem:loc:cat:head:vform:bse ) ) ) ) ) . \n\t', '\n\t\t ( 6 ) sign ( e list( ) , e list( ) , SynVal , DelayVar ) ( 7 ) whentype(Type,FS,Goal) :- functor(FS,CurrentType,Arity) , ( sub type ( Type , CurrentType ) -> call(Goal) ; arg(Arity,FS,DelayVar) , whentype(Type,DelayVar,Goal)) . \n\t', '\n\t\t Figure 4 : Part of the signature underlying the constraint in Figure 3 . \n\t', '\n\t\t Figure 3 : Reduction stages for the Finiteness Marking Principle . \n\t', '\n\t\t bse ind fin inf verb noun vform marking head VFORM:vform sign CAT:category QRETR:list QSTORE:list SYNSEM:synsem synsem LOC:local category HEAD:head MARKING:marking local goals for both conditionals eventually unsuspend . \n\t', '\n\t\t For atomic conditionals , we must thread two extra arguments , Vs In , and V s O u t , which track which variables have been seen so far . \n\t', '\n\t\t Delaying on atomic type conditionals is implemented by a special whentype/3 primitive ( Section 3.4 ) , and feature descriptions reduce using unique feature introduction : whenfs(V=T,Goal,Vs,Vs) :- type(T) -> whentype ( T , V , Goal ) . \n\t', '\n\t\t whenfs(V=(F:Desc),Goal,VsIn,VsOut):- unique introducer ( F , Intro ) , whentype(Intro,V , ( farg(F,V,FVal) , whenfs(FVal=Desc,Goal,VsIn , VsOut ) ) ) . \n\t', '\n\t\t farg ( F , V , FVal ) binds FVal to the argument position of V that corresponds to the feature F once V has been instantiated to a type for which F is appropriate . \n\t', '\n\t\t In the variable case , whenf s / 4 simply binds the variable when it first encounters it , but subsequent occurrences of that variable create a suspension using Prolog when/2 , checking for identity with the previous occurrences . \n\t', '\n\t\t This implements a primitive delay on structure sharing ( Section 3.4 ) : whenfs(V=X,Goal,VsIn,VsOut):- var ( X ) , ( select(VsIn,X,VsOut) -> % not first X - - wait when ( ?= ( V , X ) , ( ( V==X ) -> call(Goal) ; true ) ) ; % first X - - bind VsOut=VsIn,V=X,call(Goal)) . \n\t', '\n\t\t In practice , whe n f s / 2 can be partially evaluated by a compiler . \n\t', '\n\t\t In the running example , Figure 3 , we can compile the when f s / 2 subgoal in ( 2 ) into simpler whentype/2 subgoals , that delay until X reaches a particular type . \n\t', '\n\t\t The second case of when f s / 4 tells us that this can be achieved by successively waiting for the types that introduce each of the features , SYNSEM , LOC , and CAT . \n\t', '\n\t\t As shown in Figure 4 , those types are sign , synsem and local , respectively ( Figure 3 ( 3 ) ) . \n\t', '\n\t\t The description that CatVal is suspended on is a conjunction , so we successively suspend on each conjunct . \n\t', '\n\t\t The type that introduces both HEAD and MARKING is category ( 4 ) . \n\t', '\n\t\t In practice , static analysis can greatly reduce the complexity of the resulting relational goals . \n\t', '\n\t\t In this case , static analysis of the type system tells us that all four of these whentype/2 calls can be eliminated ( 5 ) , since X must be a sign in this context , synsem is the least appropriate type of any SYNSEM value , local is the least appropriate type of any LOC value , and category is the least appropriate type of any CAT value . \n\t', '\n\t\t 3.4 Primitive delay statements The two fundamental primitives typically provided for Prolog terms , e.g. , by SICStus Prolog when/2 , are : ( 1 ) suspending until a variable is instantiated , and ( 2 ) suspending until two variables are equated or inequated . \n\t', '\n\t\t The latter corresponds exactly to structure-sharing in TFSs , and to shared variables in descriptions ; its implementation was already discussed in the previous section . \n\t', '\n\t\t The former , if carried over directly , would correspond to delaying until a variable is promoted to a type more specific than L , the most general type in the type semi- lattice . \n\t', '\n\t\t There are degrees of instantiation in LTFS , however , corresponding to long subtyping chains that terminate in L . \n\t', '\n\t\t A more general and useful primitive in a typed language with such chains is suspending until a variable is promoted to a particular type . \n\t', '\n\t\t whentype ( Type , X , Goal ) , i.e. , delaying subgoal Goal until variable X reaches Type , is then the non-universally-quantified cousin of the type-antecedent constraints that are already used in the ERG . \n\t', '\n\t\t How whentype ( Type , X , Goal ) is implemented depends on the data structure used for TFSs , but in Prolog they invariably use the underlying Prolog implementation of when/2 . \n\t', '\n\t\t In ALE , for example , TFSs are represented with reference chains that extend every time their type changes . \n\t', '\n\t\t One can simply wait for a variable position at the end of this chain to be instantiated , and then compare the new type to Type . \n\t', '\n\t\t Figure 3 ( 6 ) shows a schematic representation of a sign-typed TFS with SYNSEM value SynVal , and two other appropriate feature values . \n\t', '\n\t\t Acting upon this as its second argument , the corresponding definition of whentype ( Type , X , Goal ) in Figure 3 ( 7 ) delays on the variable in the extra , fourth argument position . \n\t', '\n\t\t This variable will be instantiated to a similar term when this TFS promotes to a subtype of sign . \n\t', '\n\t\t As described above , delaying until the antecedent of the principle in Figure 3 ( 1 ) is true or false ultimately reduces to delaying until various feature values attain certain types using whentype/3 . \n\t', '\n\t\t A TFS may not have substructures that are specific enough to determine whether an antecedent holds or not . \n\t', '\n\t\t In this case , we must wait until it is known whether the antecedent is true or false before applying the consequent . \n\t', '\n\t\t If we reach a deadlock , where several constraints are suspended on their antecedents , then we must use another resolution method to begin testing more specific extensions of the TFS in turn . \n\t', '\n\t\t The choice of these other methods characterizes a true CLP solution for LTFS , all of which are enabled by the method presented in this paper . \n\t', '\n\t\t In the case of the signature in Figure 4 , one of these methods may test whether a marking-typed substructure is consistent with either fin or inf . \n\t', '\n\t\t If it is consistent with fin , then this branch of the search may unsuspend the Finiteness Marking Principle on a sign-typed TFS that contains this substructure . \n\t', '\n\t\t 4 Measuring the cost of delaying How much of a cost do we pay for using delaying ? \n\t', '\n\t\t In order to answer this question definitively , we would need to reimplement a large-scale grammar which was substantially identical in every way to the ERG but for its use of delay statements . \n\t', '\n\t\t The construction of such a grammar is outside the scope of this research programme , but we do have access to MERGE,2 which was designed to have the same extensional coverage of English as the ERG . \n\t', '\n\t\t Internally , the MERGE is quite unlike the ERG . \n\t', '\n\t\t Its TFSs are far larger because each TFS category carries inside it the phrase structure daughters of the rule that created it . \n\t', '\n\t\t It also has far fewer types , more feature values , a heavy reliance on lists , about a third as many phrase structure rules with daughter categories that are an average of 32 % larger , and many more constraints . \n\t', '\n\t\t Because of these differences , this version of MERGE runs on average about 300 times slower than the ERG . \n\t', '\n\t\t On the other hand , MERGE uses delaying for all three of the purposes that have been discussed in this paper : complex antecedents , explicit when f s / 2 calls to avoid non-termination problems , and explicit whe n f s / 2 calls to avoid expensive non- deterministic searches . \n\t', '\n\t\t While there is currently no delay-free grammar to compare it to , we can pop open the hood on our implementation and measure delaying relative to other system functions on MERGE with its test suite . \n\t', '\n\t\t The results are shown in Figure 5 . \n\t', '\n\t\t These results show that while the per call Function avg . \n\t', '\n\t\t avg . \n\t', '\n\t\t # calls per sent . \n\t', '\n\t\t µs avg . \n\t', '\n\t\t % / call parse time PS rules 1458 410 0.41 Chart access 13.3 13426 0.12 Relations 4.0 1380288 1.88 Delays 2.6 3633406 6.38 Path compression 2.0 955391 1.31 Constraints 1.6 1530779 1.62 Unification 1.5 37187128 38.77 Dereferencing 0.5 116731777 38.44 Add type MGSat 0.3 5131391 0.97 Retrieve feat . \n\t', '\n\t\t val . \n\t', '\n\t\t 0.02 19617973 0.21 Figure 5 : Run-time allocation of functionality in MERGE . \n\t', '\n\t\t Times were measured on an HP Omni- book XE3 laptop with an 850MHz Pentium II processor and 512MB of RAM , running SICStus Prolog 3.11.0 on Windows 98 SE . \n\t', '\n\t\t cost of delaying is on a par with other system functions such as constraint enforcement and relational goal resolution , delaying takes between three and five times more of the percentage of sentence parse 2The author sincerely thanks Kordula DeKuthy and Detmar Meurers for their assistance in providing the version of MERGE ( 0.9.6 ) and its test suite ( 1347 sentences , average word length 6.3 , average chart size 410 edges ) for this evaluation . \n\t', '\n\t\t MERGE is still under development . \n\t', '\n\t\t time because it is called so often . \n\t', '\n\t\t This reflects , in part , design decisions of the MERGE grammar writers , but it also underscores the importance of having an efficient implementation of delaying for large- scale use . \n\t', '\n\t\t Even if delaying could be eliminated entirely from this grammar at no cost , however , a 6 % reduction in parsing speed would not , in the present author\x92s view , warrant the loss of modularity in a grammar of this size . \n\t', '\n\t\t 5 Conclusion It has been shown that a simple generalization of conventional delay statements to LTFS , combined with a subsumption-based interpretation of implicational constraints and unique feature introduction are sufficient to restore much of the functionality and concomitant benefit that has been routinely sacrificed in HPSG in the name of parsing efficiency . \n\t', '\n\t\t While a definitive measurement of the computational cost of this functionality has yet to emerge , there is at least no apparent indication from the experiments that we can conduct that disjunction , complex antecedents and/or a judicious use of recursion pose a significant obstacle to tractable grammar design when the right control strategy ( CLP with subsumption testing ) is adopted . \n\t', '\n\t\t References H. A¨ít-Kaci , A. Podelski , and G. Smolka . \n\t', '\n\t\t 1992. A feature-based constraint system for logic programming with entailment . \n\t', '\n\t\t In Proceedings of the International Conference on Fifth Generation Computer Systems . \n\t', '\n\t\t H. A¨ít-Ka´ci . \n\t', '\n\t\t 1984. A Lattice-theoretic Approach to Computation based on a Calculus ofPartially Ordered Type Structures . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , University of Pennsylvania . \n\t', '\n\t\t B. Carpenter and G. Penn. 1996 . \n\t', '\n\t\t Compiling typed attribute-value logic grammars . \n\t', '\n\t\t In H. Bunt and M. Tomita , editors , Recent Advances in Parsing Technologies , pages 145\x96168 . \n\t', '\n\t\t Kluwer . \n\t', '\n\t\t B. Carpenter . \n\t', '\n\t\t 1992. The Logic of Typed Feature Structures . \n\t', '\n\t\t Cambridge . \n\t', '\n\t\t A. Copestake , D. Flickinger , C. Pollard , and I. Sag . \n\t', '\n\t\t 2003 . \n\t', '\n\t\t Minimal Recursion Semantics : An introduction . \n\t', '\n\t\t Journal submission , November 2003 . \n\t', '\n\t\t J. Doerre , M. Dorna , J. Junger , and K. Schneider , 1996 . \n\t', '\n\t\t The CUF User\x92s Manual . \n\t', '\n\t\t IMS Stuttgart , 2.0 edition . \n\t', '\n\t\t J. Doerre . \n\t', '\n\t\t 1993. Generalizing Earley deduction for constraint-based grammars . \n\t', '\n\t\t Technical Report R1.2.A , DYANA Deliverable . \n\t', '\n\t\t D. Flickinger . \n\t', '\n\t\t 2000. On building a more efficient grammar by exploiting types . \n\t', '\n\t\t Natural Language Engineering , 6(1):15\x9628 . \n\t', '\n\t\t T. Goetz and W.D. Meurers . \n\t', '\n\t\t 1997. Interleaving universal principles and relational constraints over typed feature logic . \n\t', '\n\t\t In Proceedings of the 35th ACL / 8th EACL , pages 1\x968 . \n\t', '\n\t\t M. H¨ohfeld and G. Smolka . \n\t', '\n\t\t 1988 . \n\t', '\n\t\t Definite relations over constraint languages . \n\t', '\n\t\t LILOG Report 53 , IBM Deutschland . \n\t', '\n\t\t H.-U. Krieger and J. Nerbone . \n\t', '\n\t\t 1991. Feature-based inheritance networks for computational lexicons . \n\t', '\n\t\t In Proceedings of the ACQUILEX Workshop on Default Inheritance in the Lexicon , number 238 in University of Cambridge , Computer Laboratory Technical Report . \n\t', '\n\t\t H.-U. Krieger and U. Sch¨afer . \n\t', '\n\t\t 1994. TDL \x97 a type description language for HPSG part 1 : Overview . \n\t', '\n\t\t Technical Report RR-94-37 , Deutsches Forschungszentrum f¨ur K¨unstliche Intelligenz ( DFKI ) , November . \n\t', '\n\t\t LinGO . \n\t', '\n\t\t 1999. The LinGO grammar and lexicon . \n\t', '\n\t\t Available on-line at http://lingo.stanford.edu . \n\t', '\n\t\t G. Penn and K. Hoetmer . \n\t', '\n\t\t 2003. In search of epistemic primitives in the english resource grammar . \n\t', '\n\t\t In Proceedings of the 10th International Conference on Head-driven Phrase Structure Grammar , pages 318\x96337 . \n\t', '\n\t\t G. Penn. 2001 . \n\t', '\n\t\t Tractability and structural closures in attribute logic signatures . \n\t', '\n\t\t In Proceedings of the 39th ACL , pages 410\x96417 . \n\t', '\n\t\t C. J. Pollard . \n\t', '\n\t\t 1998. Personal communiciation to the author . \n\t', '\n\t\t G. Smolka . \n\t', '\n\t\t 1991. Residuation and guarded rules for constraint logic programming . \n\t', '\n\t\t Technical Report RR-91-13 , DFKI . \n\t', '\n\t\t G. Smolka . \n\t', '\n\t\t 1994. A calculus for higher-order concurrent constraint programming with deep guards . \n\t', '\n\t\t Technical Report RR-94-03 , DFKI . \n\t', '\n\t\t P. Van Roy . \n\t', '\n\t\t 1990. Can Logic Programming Execute as Fast as Imperative Programming ? \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , University of California , Berkeley . \n\t', '\n\t\t S. Wintner . \n\t', '\n\t\t 2002. Modular context-free grammars . \n\t', '\n\t\t Grammars , 5(1):41\x9663 . \n\t', '\n\t\t Minimal Recursion Semantics as Dominance Constraints : Translation , Evaluation , and Analysis Ruth Fuchss,1 Alexander Koller,1 Joachim Niehren,2 and Stefan Thater1 1 Dept. of Computational Linguistics , Saarland University , Saarbrücken , Germany 2 INRIA Futurs , Lille , France {fuchss,koller,stth}@coli.uni-sb.de Abstract We show that a practical translation of MRS descriptions into normal dominance constraints is feasible . \n\t', '\n\t\t We start from a recent theoretical translation and verify its assumptions on the outputs of the English Resource Grammar ( ERG ) on the Redwoods corpus . \n\t', '\n\t\t The main assumption of the translation\x97 that all relevant underspecified descriptions are nets\x97is validated for a large majority of cases ; all non-nets computed by the ERG seem to be systematically incomplete . \n\t', '\n\t\t 1 Introduction Underspecification is the standard approach to dealing with scope ambiguity \n\t\t']",Positive
"['\n\t\t The readings of underspecified expressions are represented by compact and concise descriptions , instead of being enumerated explicitly . \n\t', '\n\t\t Underspecified descriptions are easier to derive in syntax-semantics interfaces \n\t\t']",Positive
['\n\t\t Two important underspecification formalisms in the recent literature are Minimal Recursion Semantics ( MRS ) \n\t\t'],Positive
"['\n\t\t MRS is the under- specification language which is used in large-scale HPSG grammars , such as the English Resource Grammar ( ERG ) \n\t\t']",Positive
['\n\t\t The main advantage of dominance constraints is that they can be solved very efficiently \n\t\t'],Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t This translation clarified the precise relationship between these two related formalisms , and made the powerful meta-theory of dominance constraints accessible to MRS . \n\t', '\n\t\t Their goal was to also make the large grammars for MRS * Supported by the CHORUS project of the SFB 378 of the DFG . \n\t', '\n\t\t and the efficient constraint solvers for dominance constraints available to the other formalism . \n\t', '\n\t\t However , Niehren and Thater made three technical assumptions : 1. that EP-conjunction can be resolved in a preprocessing step ; 2. that the qeq relation in MRS is simply dominance ; 3 . \n\t', '\n\t\t and ( most importantly ) that all linguistically correct and relevant MRS expressions belong to a certain class of constraints called nets . \n\t', '\n\t\t This means that it is not obvious whether their result can be immediately applied to the output of practical grammars like the ERG . \n\t', '\n\t\t In this paper , we evaluate the truth of these assumptions on the MRS expressions which the ERG computes for the sentences in the Redwoods Tree- bank \n\t\t']",Positive
"['\n\t\t The main result of our evaluation is that 83 % of the Redwoods sentences are indeed nets , and 17 % aren\x92t . \n\t', '\n\t\t A closer analysis of the non-nets reveals that they seem to be systematically incomplete , i. e. they predict more readings than the sentence actually has . \n\t', '\n\t\t This supports the claim that all linguistically correct MRS expressions are indeed nets . \n\t', '\n\t\t We also verify the other two assumptions , one empirically and one by proof . \n\t', '\n\t\t Our results are practically relevant because dominance constraint solvers are much faster and have more predictable runtimes when solving nets than the LKB solver for MRS \n\t\t']",Negative
"['\n\t\t In addition , nets might be useful as a debugging tool to identify potentially problematic semantic outputs when designing a grammar . \n\t', '\n\t\t Plan of the Paper . \n\t', '\n\t\t We first recall the definitions of MRS ( § 2 ) and dominance constraints ( § 3 ) . \n\t', '\n\t\t We present the translation from MRS-nets to dominance constraints ( § 4 ) and prove that it can be extended to MRS-nets with EP-conjunction ( § 5 ) . \n\t', '\n\t\t Finally we evaluate the net hypothesis and the qeq assumption on the Redwoods corpus , and compare runtimes ( § 6 ) . \n\t', '\n\t\t 2 Minimal Recursion Semantics This section presents a definition of Minimal Recursion Semantics ( MRS ) \n\t\t']",Positive
"['\n\t\t Full MRS with qeq-semantics , top handles , and event variables will be discussed in the last paragraph . \n\t', '\n\t\t MRS Syntax . \n\t', '\n\t\t MRS constraints are conjunctive formulas over the following vocabulary : 1 . \n\t', '\n\t\t An infinite set of variables ranged over by h . \n\t', '\n\t\t Variables are also called handles . \n\t', '\n\t\t 2. An infinite set of constants x,y,z denoting indivual variables of the object language . \n\t', '\n\t\t 3. A set of function symbols ranged over by P , and a set of quantifier symbols ranged over by Q . \n\t', '\n\t\t Pairs Qx are further function symbols . \n\t', '\n\t\t 4. The binary predicate symbol \x91=q\x92 . \n\t', '\n\t\t MRS constraints have three kinds of literals , two kinds of elementary predications ( EPs ) in the first two lines and handle constraints in the third line : 1. h:P(x1,...,xn,h1 , ... , hm),where n,m>0 2. h : Qx(h1,h2) 3. h1 =q h2 In EPs , label positions are on the left of \x91:\x92 and argument positions on the right . \n\t', '\n\t\t Let Mbe a set of literals . \n\t', '\n\t\t The label set lab(M) contains all handles of M that occur in label but not in argument position , and the argument handle set arg(M) contains all handles of M that occur in argument but not in label position . \n\t', '\n\t\t Definition 1 ( MRS constraints ) . \n\t', ""\n\t\t An MRS constraint ( MRS for short ) is a finite set M of MRS- literals such that : M1 every handle occurs at most once in argument position in M , M2 handle constraints h =q h ' always relate argument handles h to labels h ' , and M3 for every constant ( individual variable ) x in argument position in M there is a unique literal of the formh : Qx(h1,h2) in M . \n\t"", '\n\t\t We say that an MRS M is compact if every handle h in M is either a label or an argument handle . \n\t', '\n\t\t Compactness simplifies the following proofs , but it is no serious restriction in practice . \n\t', '\n\t\t We usually represent MRSs as directed graphs : the nodes of the graph are the handles of the MRS , EPs are represented as solid lines , and handle constraints are represented as dotted lines . \n\t', '\n\t\t For instance , the following MRS is represented by the graph on the left of Fig . \n\t', '\n\t\t 1. { h5 : somey(h6,h8),h7 : book(y),h1 : everyx(h2,h4) , h3 : student(x),h9 : read(x,y),h2 =q h3,h6 =q h7 } Figure 1 : An MRS and its two configurations . \n\t', '\n\t\t Note that the relation between bound variables and their binders is made explicit by binding edges drawn as dotted lines ( cf. C2 below ) ; transitively redundand binding edges ( e. g. , from somey to booky ) however are omited . \n\t', '\n\t\t MRS Semantics . \n\t', '\n\t\t Readings of underspecified representations correspond to configurations of MRS constraints . \n\t', '\n\t\t Intuitively , a configuration is an MRS where all handle constraints have been resolved by plugging the \x93tree fragments\x94 into each other . \n\t', ""\n\t\t Let M be an MRS and h , h ' be handles in M . \n\t"", ""\n\t\t We say that h immediately outscopes h ' in M if there is an EP in M with label h and argument handle h ' , and we say that h outscopes h ' in M if the pair ( h , h ' ) belongs to the reflexive transitive closure of the immediate outscope relation of M. Definition 2 ( MRS configurations ) . \n\t"", ""\n\t\t An MRS M is a configuration if it satisfies conditions C 1 and C2 : C 1 The graph of M is a tree of solid edges : ( i ) all handles are labels i. e. , arg ( M ) = 0 and M contains no handle constraints , ( ii ) handles don\x92t properly outscope themselve , and ( iii ) all handles are pairwise connected by EPs in M. C2 If h : Qx(h1,h2) and h ' : P( ... , x , ... ) belong to M , then h outscopes h ' in M i. e. , binding edges in the graph of M are transitively redundant . \n\t"", ""\n\t\t We say that a configuration M is configuration of an MRS M ' if there exists a partial substitution 6 : lab ( M ' ) --~ arg(M') that states how to identify labels with argument handles of M ' so that : C3 M= { 6(E) I E is an EP inM ' } , and C4 for all h =q h ' in M ' , h outscopes 6(h') in M . \n\t"", '\n\t\t The value 6(E) is obtained by substituting all labels in dom((Y) in E while leaving all other handels unchanged . \n\t', '\n\t\t The MRS on the left of Fig . \n\t', '\n\t\t 1 , for instance , has two configurations given to the right . \n\t', '\n\t\t EP-conjunctions . \n\t', '\n\t\t Definitions 1 and 2 generalize the idealized definition of MRS of \n\t\t']",Positive
"['\n\t\t An MRS M contains an EP-conjunction if it contains different EPs with the same label h.The intuition is that EP-conjunctions are interpreted by object language conjunctions . \n\t', '\n\t\t studentx booky readx.y booky readx.y readx.y studentx everyx somey somey everyx somey booky everyx studentx P1 , P2 { h1 : P1 (h2),h1 : P2(h3),h4 : P3 h2 =q h4 , h3=q h4 } P3 Figure 2 : An unsolvable MRS with EP-conjunction Figure 3 : A solvable MRS without merging-free configaration Fig . \n\t', '\n\t\t 2 shows an MRS with an EP-conjunction and its graph . \n\t', '\n\t\t The function symbols of both EPs are conjoined and their arguments are merged into a set . \n\t', '\n\t\t The MRS does not have configurations since the argument handles of the merged EPs cannot jointly outscope the node P4 . \n\t', '\n\t\t We call a configuration merging if it contains EP- conjunctions , and merging -free otherwise . \n\t', '\n\t\t Merging configurations are needed to solve EP-conjuctions such as { h : P1 , h : P2 } . \n\t', '\n\t\t Unfortunately , they can also solve MRSs without EP-conjunctions , such as the MRS in Fig . \n\t', '\n\t\t 3. The unique configuration of this MRS is a merging configuration : the labels of P1 and P2 must be identified with the only available argument handle . \n\t', '\n\t\t The admission of merging configurations may thus have important consequences for the solution space of arbitrary MRSs . \n\t', '\n\t\t Standard MRS . \n\t', '\n\t\t Standard MRS requires three further extensions : ( i ) qeq-semantics , ( ii ) top- handles , and ( iii ) event variables . \n\t', '\n\t\t These extensions are less relevant for our comparision . \n\t', '\n\t\t The qeq-semantics restricts the interpretation of handle constraints beyond dominance . \n\t', ""\n\t\t Let M be an MRS with handles h , h ' . \n\t"", ""\n\t\t We say that h is qeq h ' in M if either h = h ' , or there is an EP h : Qx ( h0 , h1 ) in M and h1 is qeq h ' in M . \n\t"", '\n\t\t Every qeq-configuration is a configuration as defined above , but not necessarily vice versa . \n\t', '\n\t\t The qeq-restriction is relevant in theory but will turn out unproblematic in practice ( see § 6 ) . \n\t', '\n\t\t Standard MRS requires the existence of top handles in all MRS constraints . \n\t', '\n\t\t This condition doesn\x92t matter for MRSs with connected graphs ( see \n\t\t']",Positive
"['\n\t\t MRSs with unconnected graphs clearly do not play any role in practical underspecified semantics . \n\t', ""\n\t\t Finally , MRSs permit events variables e , e ' as a second form of constants . \n\t"", '\n\t\t They are treated equally to individual variables except that they cannot be bound by quantifiers . \n\t', '\n\t\t 3 Dominance Constraints Dominance constraints are a general framework for describing trees . \n\t', '\n\t\t For scope underspecification , they are used to describe the syntax trees of object language formulas . \n\t', '\n\t\t Dominance constraints are the core language underlying CLLS \n\t\t']",Positive
"['\n\t\t Syntax and semantics . \n\t', '\n\t\t We assume a possibly infinite signature E = { f,g , ... } of function symbols with fixed arities ( written ar(f)) and an infinite set of variables ranged over by X , Y , Z . \n\t', ""\n\t\t A dominance constraint q ) is a conjunction of dominance , inequality , and labeling literals of the following form , where ar(f) = n : q)::=Xa^ Y |X=~Y |X : f(X1,...,Xn) | q)^q ) ' Dominance constraints are interpreted over finite constructor trees i. e. , ground terms constructed from the function symbols in E . \n\t"", '\n\t\t We identify ground terms with trees that are rooted , ranked , edge- ordered and labeled . \n\t', '\n\t\t A solution for a dominance constraint q ) consists of a tree i and an assign- ment a that maps the variables in q ) to nodes of i such that all constraints are satisfied : labeling lit- erals X : f(X1,...,Xn) are satisfied iff a(X) is la- beled with f and its daughters are a(X1 ) , ... , a(Xn) in this order ; dominance literals X a^ Y are satisfied iff a(X) dominates a(Y) in i ; and inequality literals X =~ Y are satisfied iff a(X) and a(Y) are distinct nodes . \n\t', '\n\t\t Solved forms . \n\t', '\n\t\t Satisfiable dominance constraints have infinitely many solutions . \n\t', '\n\t\t Constraint solvers for dominance constraints therefore do not enumerate solutions but solved forms i. e. , \x93tree shaped\x94 constraints . \n\t', '\n\t\t To this end , we consider ( weakly ) normal dominance constraints \n\t\t']",Positive
"['\n\t\t We call a variable a hole of q ) if it occurs in argu- ment position in q ) and a root of q ) otherwise . \n\t', '\n\t\t Definition 3 . \n\t', '\n\t\t A dominance constraint q ) is normal if it satisfies the following conditions . \n\t', '\n\t\t N1 ( a ) each variable of q ) occurs at most once in the labeling literals of q ) . \n\t', '\n\t\t ( b ) each variable of q ) occurs at least once in the labeling literals of q ) . \n\t', '\n\t\t N2 for distinct roots X and Y of q ) , X =~ Y is in q ) . \n\t', '\n\t\t N3 ( a ) ifX a^ Y occurs in q ) , Y is a root in q ) . \n\t', '\n\t\t ( b ) ifX a^ Y occurs in q ) , X is a hole in q ) . \n\t', '\n\t\t We call q ) weakly normal if it satisfies the above properties except for N1 ( b ) and N3 ( b ) . \n\t', '\n\t\t Note that Definition 3 imposes compactness : the height of tree fragments is always one . \n\t', '\n\t\t This is not P2 , P3 P2 P3 P1 configures P1 booky read,,y read~,y student , every , somey student , booky read,,y every , somey student , somey booky every , Figure 4 : A normal dominance constraint ( left ) and its two solved forms ( right ) . \n\t', '\n\t\t a serious restriction , as weakly normal dominance constraints can be compactified , provided that dominance links relate either roots or holes with roots . \n\t', '\n\t\t Weakly normal dominance constraints q ) can be represented by dominance graphs . \n\t', '\n\t\t The dominance graph of q ) is a directed graph G = ( V , ET U+ ED ) de- fined as follows . \n\t', '\n\t\t The nodes of G are the variables of q ) . \n\t', ""\n\t\t Labeling literals X : f ( X1 , ... , Xk ) are represented by tree edges ( X,Xi ) E ET , for 1 < i < k , and dominance literals X a* X ' are represented by dominance edges ( X,X' ) E ED . \n\t"", '\n\t\t Inequality literals are not represented in the graph . \n\t', '\n\t\t In pictures , labeling literals are drawn with solid lines and dominance edges with dotted lines . \n\t', '\n\t\t We say that a constraint q ) is in solved form if its graph is in solved form . \n\t', '\n\t\t A graph G is in solved form iff it is a forest . \n\t', ""\n\t\t The solved forms of G are solved forms G ' which are more specific than G i. e. , they differ only in their dominance edges and the reachability relation of G extends the reachability of G ' . \n\t"", '\n\t\t A minimal solved form is a solved form which is minimal with respect to specificity . \n\t', '\n\t\t Simple solvedforms are solved forms where every hole has exactly one outgoing dominance edge . \n\t', '\n\t\t Fig . \n\t', '\n\t\t 4 shows as a concrete example the translation of the MRS description in Fig . \n\t', '\n\t\t 1 together with its two minimal solved forms . \n\t', '\n\t\t Both solved forms are simple . \n\t', '\n\t\t 4 Translating Merging-Free MRS-Nets This section defines MRS-nets without EP- conjunctions , and sketches their translation to normal dominance constraints . \n\t', '\n\t\t We define nets equally for MRSs and dominance constraints . \n\t', '\n\t\t The key semantic property of nets is that different notions of solutions coincide . \n\t', '\n\t\t In this section , we show that merging-free configurations coincides to minimal solved forms . \n\t', '\n\t\t § 5 generalizes the translation by adding EP-conjunctions and permitting merging semantics . \n\t', '\n\t\t Pre-translation . \n\t', ""\n\t\t An MRS constraint M can be represented as a corresponding dominance con- straint q)M as follows : The variables of q)M are the handles of M , and the literals of q)M correspond ( a ) strong ( b ) weak ( c ) island Figure 5 : Fragment Schemata of Nets those of M in the following sence : h : P(x1 , ... , xn , h1 , ... , hk ) H h : Px1,...,xn ( h1 , ... , hk ) h : Qx(h1,h2) H h : Qx(h1,h2) h=qh'Hha*h' Additionally , dominance literals h a* h ' are added to q)Mfor all h,h's.t.h : Qx(h1,h2) and h':P(...,x , ... ) belong to M ( cf. C2 ) , and literals h =~ h ' are added to q)M for all h , h ' in distinct label position in M. Lemma 1 . \n\t"", '\n\t\t If a compact MRS M does not contain EP-conjunctions then q)M is weakly normal , and the graph of M is the transitive reduction of the graph of q)M . \n\t', '\n\t\t Nets . \n\t', '\n\t\t A hypernormal path \n\t\t']",Positive
"['\n\t\t Let q ) be a weakly normal dominance constraint and let G be the constraint graph of q ) . \n\t', ""\n\t\t We say that q ) is a dominance net if the transitive reduction G ' of G is a net . \n\t"", ""\n\t\t G ' is a net if every tree fragment F of G ' satisfies one of the following three conditions , illustrated in Fig . \n\t"", '\n\t\t 5 : Strong . \n\t', '\n\t\t Every hole of F has exactly one outgoing dominance edge , and there is no weak root-to-root dominance edge . \n\t', '\n\t\t Weak . \n\t', '\n\t\t Every hole except for the last one has exactly one outgoing dominance edge ; the last hole has no outgoing dominance edge , and there is exactly one weak root-to-root dominance edge . \n\t', '\n\t\t Island . \n\t', '\n\t\t The fragment has one hole X , and all variables which are connected toXby dominance edges are connected by a hypernormal path in the graph where F has been removed . \n\t', '\n\t\t We say that an MRS M is an MRS-net if the pre- translation of its literals results in a dominance net q)M . \n\t', '\n\t\t We say that an MRS-net M is connected if q)M is connected ; q)M is connected if the graph of q)M is connected . \n\t', '\n\t\t Note that this notion of MRS-nets implies that MRS-nets cannot contain EP-conjunctions as otherwise the resulting dominance constraint would not be weakly normal . \n\t', '\n\t\t § 5 shows that EP-conjunctions ... ... ... ... ... can be resolved i. e. , MRSs with EP-conjunctions can be mapped to corresponding MRSs without EP- conjunctions . \n\t', '\n\t\t If M is an MRS-net ( without EP-conjunctions ) , then M can be translated into a corresponding dominance constraint q ) by first pre-translating M into a q)M and then normalizing q)M by replacing weak root-to-root dominance edges in weak fragments by dominance edges which start from the open last hole . \n\t', '\n\t\t Theorem 1 \n\t\t']",Positive
"['\n\t\t Let Mbe an MRS and q)M be the translation of M . \n\t', '\n\t\t If M is a connected MRS-net , then the merging-free configurations of M bijectively correspond to the minimal solved forms of the q)M . \n\t', '\n\t\t The following section generalizes this result to MRS-nets with a merging semantics . \n\t', '\n\t\t 5 Merging and EP-Conjunctions We now show that if an MRS is a net , then all its configurations are merging-free , which in particular means that the translation can be applied to the more general version of MRS with a merging semantics . \n\t', '\n\t\t Lemma 2 \n\t\t']",Positive
"['\n\t\t All minimal solved forms of a connected dominance net are simple . \n\t', '\n\t\t Lemma 3 . \n\t', '\n\t\t If all solved forms of a normal dominance constraint are simple , then all of its solved forms are minimal . \n\t', '\n\t\t Theorem 2 . \n\t', '\n\t\t The configurations of an MRS-net M are merging-free . \n\t', '\n\t\t Proof . \n\t', '\n\t\t Let M , be a configuration of M and let 6 be the underlying substitution . \n\t', '\n\t\t We construct a solved form q)M , as follows : the labeling literals of q)M , are the pre-translations of the EPs in M , and q)M , has a dominance literal h , a* h iff ( h , h , ) E 6 , and inequal- ity literals X =~ Y for all distinct roots in q)M , . \n\t', '\n\t\t By condition C 1 in Def . \n\t', '\n\t\t 2 , the graph of M , is a tree , hence the graph of q)M , must also be a tree i. e. , q)M , is a solved form . \n\t', '\n\t\t q)M , must also be more spe- cific than the graph of q)M because the graph of M , satisfies all dominance requirements of the handle constraints in M , hence q)M , is a solved form of q)M . \n\t', '\n\t\t M clearly solved q)M , . \n\t', '\n\t\t By Lemmata 2 and 3 , q)M , must be simple and minimal because q)M is a net . \n\t', '\n\t\t But then M , cannot contain EP-conjunctions i. e. , M , is merging-free . \n\t', '\n\t\t The merging semantics of MRS is needed to solve EP-conjunctions . \n\t', '\n\t\t As we have seen , the merging semantics is not relevant for MRS constraints which are nets . \n\t', '\n\t\t This also verifies Niehren and Thater\x92s ( 2003 ) assumption that EP-conjunctions are \x93syntactic sugar\x94 which can be resolved in a preprocessing step : EP-conjunctions can be resolved by exhaustively applying the following rule which adds new literals to make the implicit conjunction explicit : h : E1(h1,...,hn),h : E2(h,1,...,h,m) ~ h : \x91E1&E2\x92(h1,...,hn,h,1,...,h,m) , where E ( h 1 , ... , hn ) stands for anEP with argument handles h1 , ... , hn , and where \x91E1 &E2\x92 is a complex function symbol . \n\t', '\n\t\t If this rule is applied exhaustively to an MRS M , we obtain an MRS M , without EP- conjunctions . \n\t', '\n\t\t It should be intuitively clear that the configurations of M and M , correspond ; Therefore , the configurations of M also correspond to the minimal solved forms of the translation of M ,. 6 Evaluation The two remaining assumptions underlying the translation are the \x93net-hypothesis\x94 that all linguistically relevant MRS expressions are nets , and the \x93qeq-hypothesis\x94 that handle constraints can be given a dominance semantics practice . \n\t', '\n\t\t In this section , we empirically show that both assumptions are met in practice . \n\t', '\n\t\t As an interesting side effect , we also compare the run-times of the constraint-solvers we used , and we find that the dominance constraint solver typically outperforms the MRS solver , often by significant margins . \n\t', '\n\t\t Grammar and Resources . \n\t', '\n\t\t We use the English Resource Grammar ( ERG ) , a large-scale HPSG grammar , in connection with the LKB system , a grammar development environment for typed feature grammars \n\t\t']",Positive
"['\n\t\t We use the system to parse sentences and output MRS constraints which we then translate into dominance constraints . \n\t', '\n\t\t As a test corpus , we use the Redwoods Treebank \n\t\t']",Positive
"['\n\t\t We exclude the sentences that cannot be parsed due to memory capacities or words and grammatical structures that are not included in the ERG , or which produce ill-formed MRS expressions ( typically violating M1 ) and thus base our evaluation on a corpus containing 6242 sentences . \n\t', '\n\t\t In case of syntactic ambiguity , we only use the first reading output by the LKB system . \n\t', '\n\t\t To enumerate the solutions of MRS constraints and their translations , we use the MRS solver built into the LKB system and a solver for weakly normal dominance constraints \n\t\t']",Positive
"['\n\t\t 6.1 Relevant Constraints are Nets We check for 6242 constraints whether they constitute nets . \n\t', '\n\t\t It turns out that 5200 ( 83.31 % ) constitute nets while 1042 ( 16.69 % ) violate one or more net- conditions . \n\t', '\n\t\t Non-nets . \n\t', '\n\t\t The evaluation shows that the hypothesis that all relevant constraints are nets seems to be falsified : there are constraints that are not nets . \n\t', '\n\t\t However , a closer analysis suggests that these constraints are incomplete and predict more readings than the sentence actually has . \n\t', '\n\t\t This can also be illustrated with the average number of solutions : For the Redwoods corpus in combination with the ERG , nets have 1836 solutions on average , while non-nets have 14039 solutions , which is a factor of 7.7 . \n\t', '\n\t\t The large number of solutions for non-nets is due to the \x93structural weakness\x94 of non-nets ; often , non-nets have only merging configurations . \n\t', '\n\t\t Non-nets can be classified into two categories ( see Fig . \n\t', '\n\t\t 6 ) : The first class are violated \x93strong\x94 fragments which have holes without outgoing dominance edge and without a corresponding root-toroot dominance edge . \n\t', '\n\t\t The second class are violated \x93island\x94 fragments where several outgoing dominance edges from one hole lead to nodes which are not hypernormally connected . \n\t', '\n\t\t There are two more possibilities for violated \x93weak\x94 fragments\x97 having more than one weak dominance edge or having a weak dominance edge without empty hole\x97 , but they occur infrequently ( 4.4 % ) . \n\t', '\n\t\t If those weak fragments were normalized , they would constitute violated island fragments , so we count them as such . \n\t', '\n\t\t 124 ( 11.9 % ) of the non-nets contain empty holes , 762 ( 73.13 % ) contain violated island fragments , and 156 ( 14.97 % ) contain both . \n\t', '\n\t\t Those constraints that contain only empty holes and no violated island fragments cannot be configured , as in configurations , all holes must be filled . \n\t', '\n\t\t Fragments with open holes occur frequently , but not in all contexts , for constraints representing for example time specifications ( e. g. , \x93from nine to twelve\x94 or \x93a three o\x92clock flight\x94 ) or intensional expressions ( e.g. , \x93Is it?\x94 or \x93I suppose\x94 ) . \n\t', '\n\t\t Ill- Figure 7 : An MRS for \x93A sauna and a cafeteria are available\x94 ( top ) and two of sixteen merging configurations ( below ) . \n\t', '\n\t\t Figure 8 : The \x93repaired\x94 MRS from Fig . \n\t', '\n\t\t 7 formed island fragments are often triggered by some kind of coordination , like \x93a restaurant and/or a sauna\x94 or \x93a hundred and thirty Marks\x94 , also implicit ones like \x93one hour thirty minutes\x94 or \x93one thirty\x94 . \n\t', '\n\t\t Constraints with both kinds of violated fragments emerge when there is some input that yields an open hole and another part of the input yields a violated island fragment ( for example in constructions like \x93from nine to eleven thirty\x94 or \x93the ten o\x92clock flight Friday or Thursday\x94 , but not necessarily as obviously as in those examples ) . \n\t', '\n\t\t The constraint on the left in Fig . \n\t', '\n\t\t 7 gives a concrete example for violated island fragments . \n\t', '\n\t\t The topmost fragment has outgoing dominance edges to otherwise unconnected subconstraints ^1 and q)2 . \n\t', '\n\t\t Under the merging-free semantics of the MRS dialect used in \n\t\t']",Positive
"['\n\t\t However , standard MRS has merging configuration where holes can be filled more than once . \n\t', '\n\t\t For the constraint in Fig . \n\t', '\n\t\t 7 this means that \x93available\x94 can be merged in almost ( a ) open hole ( b ) ill-formed island ... cafeteria , , saunas and~,,,,s saunas , available , and~,,,,s prop prop prop a , , available , , a , , as cafeteria , , as cafeteriax saunas and,~x~s available , prop ax as everywhere , only restricted by the \x93qeq-semantics\x94 which forbids for instance \x93available\x94 to be merged with \x93sauna.\x94 In fact , the MRS constraint solver derives sixteen configurations for the constraint , two of which are given in Fig . \n\t', '\n\t\t 7 , although the sentence has only two scope readings . \n\t', '\n\t\t We conjecture that non-nets are semantically \x93incomplete\x94 in the sense that certain constraints are missing . \n\t', '\n\t\t For instance , an alternative analysis for the above constraint is given in Fig . \n\t', '\n\t\t 8. The constraint adds an additional argument handle to \x93and\x94 and places a dominance edge from this handle to \x93available.\x94 In fact , the constraint is a net ; it has exactly two readings . \n\t', '\n\t\t 6.2 Qeq is dominance For all nets , the dominance constraint solver calculates the same number of solutions as the MRS solver does , with 3 exceptions that hint at problems in the syntax-semantics interface . \n\t', '\n\t\t As every configuration that satisfies proper qeq-constraints is also a configuration if handle constraints are interpreted under the weaker notion of dominance , the solutions computed by the dominance constraint solver and the MRS solver must be identical for every constraint . \n\t', '\n\t\t This means that the additional expressivity of proper qeq-constraints is not used in practice , which in turn means that in practice , the translation is sound and correct even for the standard MRS notion of solution , given the constraint is a net . \n\t', '\n\t\t 6.3 Comparison of Runtimes The availability of a large body of underspecified descriptions both in MRS and in dominance constraint format makes it possible to compare the solvers for the two underspecification formalisms . \n\t', '\n\t\t We measured the runtimes on all nets using a Pentium III CPU at 1.3 GHz . \n\t', '\n\t\t The tests were run in a multi-user environment , but as the MRS and dominance measurements were conducted pairwise , conditions were equal for every MRS constraint and corresponding dominance constraint . \n\t', '\n\t\t The measurements for all MRS-nets with less than thirty dominance edges are plotted in Fig . \n\t', '\n\t\t 9. Inputs are grouped according to the constraint size . \n\t', '\n\t\t The filled circles indicate average runtimes within each size group for enumerating all solutions using the dominance solver , and the empty circles indicate the same for the LKB solver . \n\t', '\n\t\t The brackets around each point indicate maximum and minimum runtimes in that group . \n\t', '\n\t\t Note that the vertical axis is logarithmic . \n\t', '\n\t\t We excluded cases in which one or both of the solvers did not return any results : There were 173 sentences ( 3.33 % of all nets ) on which the LKB solver ran out of memory , and 1 sentence ( 0.02 % ) that took the dominance solver more than two minutes to solve . \n\t', '\n\t\t The graph shows that the dominance constraint solver is generally much faster than the LKB solver : The average runtime is less by a factor of 50 for constraints of size 10 , and this grows to a factor of 500 for constraints of size 25 . \n\t', '\n\t\t Our experiments show that the dominance solver outperforms the LKB solver on 98 % the cases . \n\t', '\n\t\t In addition , its runtimes are much more predictable , as the brackets in the graph are also shorter by two or three orders of magnitude , and the standard deviation is much smaller ( not shown ) . \n\t', '\n\t\t 7 Conclusion We developed Niehren and Thater\x92s ( 2003 ) theoretical translation into a practical system for translating MRS into dominance constraints , applied it systematically to MRSs produced by English Resource Grammar for the Redwoods treebank , and evaluated the results . \n\t', '\n\t\t We showed that : 1. most \x93real life\x94 MRS expressions are MRS- nets , which means that the translation is correct in these cases ; 2. for nets , merging is not necessary ( or even possible ) ; 3 . \n\t', '\n\t\t the practical translation works perfectly for all MRS-nets from the corpus ; in particular , the =q relation can be taken as synonymous with dominance in practice . \n\t', '\n\t\t Because the translation works so well in practice , we were able to compare the runtimes of MRS and dominance constraint solvers on the same inputs . \n\t', '\n\t\t This evaluation shows that the dominance constraint solver outperforms the MRS solver and displays more predictable runtimes . \n\t', '\n\t\t A researcher working with MRS can now solve MRS nets using the efficient dominance constraint solvers . \n\t', '\n\t\t A small but significant number of the MRS constraints derived by the ERG are not nets . \n\t', '\n\t\t We have argued that these constraints seem to be systematically incomplete , and their correct completions are indeed nets . \n\t', '\n\t\t A more detailed evaluation is an important task for future research , but if our \x93net hypothesis\x94 is true , a system that tests whether all outputs of a grammar are nets ( or a formal \x93safety criterion\x94 that would prove this theoretically ) could be a useful tool for developing and debugging grammars . \n\t', '\n\t\t From a more abstract point of view , our evaluation contributes to the fundamental question of what expressive power an underspecification formalism needs . \n\t', '\n\t\t It turned out that the distinction between qeq 0 5 10 15 20 25 30 Size ( number of dominance edges ) Figure 9 : Comparison of runtimes for the MRS and dominance constraint solvers . \n\t', '\n\t\t DC solver ( LEDA ) MRS solver 1e+06 100000 10000 1000 100 10 1 and dominance hardly plays a role in practice . \n\t', '\n\t\t If the net hypothesis is true , it also follows that merging is not necessary because EP-conjunctions can be converted into ordinary conjunctions . \n\t', '\n\t\t More research along these lines could help unify different under- specification formalisms and the resources that are available for them . \n\t', '\n\t\t Acknowledgments We are grateful to Ann Copestake for many fruitful discussions , and to our reviewers for helpful comments . \n\t', '\n\t\t References H. Alshawi and R. Crouch . \n\t', '\n\t\t 1992. Monotonic semantic interpretation . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 30th ACL , pages 32\x9639 . \n\t', '\n\t\t Ernst Althaus , Denys Duchier , Alexander Koller , Kurt Mehlhorn , Joachim Niehren , and Sven Thiel . \n\t', '\n\t\t 2003. An efficient graph algorithm for dominance constraints . \n\t', '\n\t\t Journal of Algorithms , 48:194\x96219 . \n\t', '\n\t\t Manuel Bodirsky , Denys Duchier , Joachim Niehren , and Sebastian Miele . \n\t', '\n\t\t 2004. An efficient algorithm for weakly normal dominance constraints . \n\t', '\n\t\t In ACM-SIAM Symposium on Discrete Algorithms . \n\t', '\n\t\t The ACM Press . \n\t', '\n\t\t Ann Copestake and Dan Flickinger . \n\t', '\n\t\t 2000. An open-source grammar development environment and broad-coverage english grammar using HPSG . \n\t', '\n\t\t In Conference on Language Resources and Evaluation . \n\t', '\n\t\t Ann Copestake , Dan Flickinger , Rob Malouf , Susanne Riehemann , and Ivan Sag . \n\t', '\n\t\t 1995. Translation using Minimal Recursion Semantics . \n\t', '\n\t\t Leuven . \n\t', '\n\t\t Ann Copestake , Alex Lascarides , and Dan Flickinger . \n\t', '\n\t\t 2001. An algebra for semantic construction in constraint-based grammars . \n\t', '\n\t\t In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics , pages 132\x96139 , Toulouse , France . \n\t', '\n\t\t Ann Copestake , Dan Flickinger , Carl Pollard , and Ivan Sag . \n\t', '\n\t\t 2004 . \n\t', '\n\t\t Minimal recursion semantics : An introduction . \n\t', '\n\t\t Journal ofLanguage and Computation . \n\t', '\n\t\t To appear . \n\t', '\n\t\t Ann Copestake . \n\t', '\n\t\t 2002. Implementing Typed Feature Structure Grammars . \n\t', '\n\t\t CSLI Publications , Stanford , CA . \n\t', '\n\t\t Markus Egg , Alexander Koller , and Joachim Niehren . \n\t', '\n\t\t 2001. The Constraint Language for Lambda Structures . \n\t', '\n\t\t Logic , Language , and Information , 10:457\x96485 . \n\t', '\n\t\t K. Mehlhorn and S. Näher . \n\t', '\n\t\t 1999. The LEDA Platform of Combinatorial and Geometric Computing . \n\t', '\n\t\t Cambridge University Press , Cambridge . \n\t', '\n\t\t See also http://www.mpi-sb.mpg.de/LEDA/ . \n\t', '\n\t\t Joachim Niehren and Stefan Thater . \n\t', '\n\t\t 2003. Bridging the gap between underspecification formalisms : Minimal recursion semantics as dominance constraints . \n\t', '\n\t\t In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics . \n\t', '\n\t\t Stephan Oepen , Kristina Toutanova , Stuart Shieber , Christopher Manning , Dan Flickinger , and Thorsten Brants . \n\t', '\n\t\t 2002. The LinGO Redwoods treebank : Motivation and preliminary applications . \n\t', '\n\t\t In Proceedings of the 19th International Conference on Computational Linguistics ( COLING\x9202 ) , pages 1253\x961257 . \n\t', '\n\t\t Manfred Pinkal . \n\t', '\n\t\t 1996. Radical underspecification . \n\t', '\n\t\t In 10th Amsterdam Colloquium , pages 587\x96606 . \n\t', '\n\t\t Learning with Unlabeled Data for Text Categorization Using Bootstrapping and Feature Projection Techniques Youngjoong Ko Jungyun Seo Dept. of Computer Science , Sogang Univ . \n\t', '\n\t\t Dept. of Computer Science , Sogang Univ . \n\t', '\n\t\t Sinsu-dong 1 , Mapo-gu Sinsu-dong 1 , Mapo-gu Seoul , 121-742 , Korea Seoul , 121-742 , Korea kyj@nlpzodiac.sogang.ac.kr seojy@ccs.sogang.ac.kr Abstract A wide range of supervised learning algorithms has been applied to Text Categorization . \n\t', '\n\t\t However , the supervised learning approaches have some problems . \n\t', '\n\t\t One of them is that they require a large , often prohibitive , number of labeled training documents for accurate learning . \n\t', '\n\t\t Generally , acquiring class labels for training data is costly , while gathering a large quantity of unlabeled data is cheap . \n\t', '\n\t\t We here propose a new automatic text categorization method for learning from only unlabeled data using a bootstrapping framework and a feature projection technique . \n\t', '\n\t\t From results of our experiments , our method showed reasonably comparable performance compared with a supervised method . \n\t', '\n\t\t If our method is used in a text categorization task , building text categorization systems will become significantly faster and less expensive . \n\t', '\n\t\t 1 Introduction Text categorization is the task of classifying documents into a certain number of pre-defined categories . \n\t', '\n\t\t Many supervised learning algorithms have been applied to this area . \n\t', '\n\t\t These algorithms today are reasonably successful when provided with enough labeled or annotated training examples . \n\t', '\n\t\t For example , there are Naive Bayes \n\t\t']",Negative
"['\n\t\t However , the supervised learning approach has some difficulties . \n\t', '\n\t\t One key difficulty is that it requires a large , often prohibitive , number of labeled training data for accurate learning . \n\t', '\n\t\t Since a labeling task must be done manually , it is a painfully time-consuming process . \n\t', '\n\t\t Furthermore , since the application area of text categorization has diversified from newswire articles and web pages to E-mails and newsgroup postings , it is also a difficult task to create training data for each application area \n\t\t']",Positive
"['\n\t\t In this light , we consider learning algorithms that do not require such a large amount of labeled data . \n\t', '\n\t\t While labeled data are difficult to obtain , unlabeled data are readily available and plentiful . \n\t', '\n\t\t Therefore , this paper advocates using a bootstrapping framework and a feature projection technique with just unlabeled data for text categorization . \n\t', '\n\t\t The input to the bootstrapping process is a large amount of unlabeled data and a small amount of seed information to tell the learner about the specific task . \n\t', '\n\t\t In this paper , we consider seed information in the form of title words associated with categories . \n\t', '\n\t\t In general , since unlabeled data are much less expensive and easier to collect than labeled data , our method is useful for text categorization tasks including online data sources such as web pages , E-mails , and newsgroup postings . \n\t', '\n\t\t To automatically build up a text classifier with unlabeled data , we must solve two problems ; how we can automatically generate labeled training documents ( machine-labeled data ) from only title words and how we can handle incorrectly labeled documents in the machine-labeled data . \n\t', '\n\t\t This paper provides solutions for these problems . \n\t', '\n\t\t For the first problem , we employ the bootstrapping framework . \n\t', '\n\t\t For the second , we use the TCFP classifier with robustness from noisy data \n\t\t']",Positive
"['\n\t\t How can labeled training data be automatically created from unlabeled data and title words ? \n\t', '\n\t\t Maybe unlabeled data don\x92t have any information for building a text classifier because they do not contain the most important information , their category . \n\t', '\n\t\t Thus we must assign the class to each document in order to use supervised learning approaches . \n\t', '\n\t\t Since text categorization is a task based on pre-defined categories , we know the categories for classifying documents . \n\t', '\n\t\t Knowing the categories means that we can choose at least a representative title word of each category . \n\t', '\n\t\t This is the starting point of our proposed method . \n\t', '\n\t\t As we carry out a bootstrapping task from these title words , we can finally get labeled training data . \n\t', '\n\t\t Suppose , for example , that we are interested in classifying newsgroup postings about specially \x91Autos\x92 category . \n\t', '\n\t\t Above all , we can select \x91automobile\x92 as a title word , and automatically extract keywords ( \x91car\x92 , \x91gear\x92 , \x91transmission\x92 , \x91sedan\x92 , and so on ) using co-occurrence information . \n\t', '\n\t\t In our method , we use context ( a sequence of 60 words ) as a unit of meaning for bootstrapping from title words ; it is generally constructed as a middle size of a sentence and a document . \n\t', '\n\t\t We then extract core contexts that include at least one of the title words and the keywords . \n\t', '\n\t\t We call them centroid-contexts because they are regarded as contexts with the core meaning of each category . \n\t', '\n\t\t From the centroidcontexts , we can gain many words contextually co- occurred with the title words and keywords : \x91driver\x92 , \x91clutch\x92 , \x91trunk\x92 , and so on . \n\t', '\n\t\t They are words in first-order co-occurrence with the title words and the keywords . \n\t', '\n\t\t To gather more vocabulary , we extract contexts that are similar to centroid-contexts by a similarity measure ; they contain words in second-order co-occurrence with the title words and the keywords . \n\t', '\n\t\t We finally construct context-cluster of each category as the combination of centroid-contexts and contexts selected by the similarity measure . \n\t', '\n\t\t Using the context-clusters as labeled training data , a Naive Bayes classifier can be built . \n\t', '\n\t\t Since the Naive Bayes classifier can label all unlabeled documents for their category , we can finally obtain labeled training data ( machine-labeled data ) . \n\t', '\n\t\t When the machine-labeled data is used to learn a text classifier , there is another difficult in that they have more incorrectly labeled documents than manually labeled data . \n\t', '\n\t\t Thus we develop and employ the TCFP classifiers with robustness from noisy data . \n\t', '\n\t\t The rest of this paper is organized as follows . \n\t', '\n\t\t Section 2 reviews previous works . \n\t', '\n\t\t In section 3 and 4 , we explain the proposed method in detail . \n\t', '\n\t\t Section 5 is devoted to the analysis of the empirical results . \n\t', '\n\t\t The final section describes conclusions and future works . \n\t', '\n\t\t 2 Related Works In general , related approaches for using unlabeled data in text categorization have two directions ; One builds classifiers from a combination of labeled and unlabeled data \n\t\t']",Positive
"['\n\t\t Nigam studied an Expected Maximization ( EM ) technique for combining labeled and unlabeled data for text categorization in his dissertation . \n\t', '\n\t\t He showed that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training data with a large pool of unlabeled data . \n\t', '\n\t\t Bennet and Demiriz achieved small improvements on some UCI data sets using SVM . \n\t', '\n\t\t It seems that SVMs assume that decision boundaries lie between classes in low-density regions of instance space , and the unlabeled examples help find these areas . \n\t', '\n\t\t Slonim suggested clustering techniques for unsupervised document classification . \n\t', '\n\t\t Given a collection of unlabeled data , he attempted to find clusters that are highly correlated with the true topics of documents by unsupervised clustering methods . \n\t', '\n\t\t In his paper , Slonim proposed a new clustering method , the sequential Information Bottleneck ( sIB ) algorithm . \n\t', '\n\t\t 3 The Bootstrapping Algorithm for Creating Machine-labeled Data The bootstrapping framework described in this paper consists of the following steps . \n\t', '\n\t\t Each module is described in the following sections in detail . \n\t', '\n\t\t 1. Preprocessing : Contexts are separated from unlabeled documents and content words are extracted from them . \n\t', '\n\t\t 2. Constructing context-clusters for training : - Keywords of each category are created - Centroid-contexts are extracted and verified - Context-clusters are created by a similarity measure 3 . \n\t', '\n\t\t Learning Classifier : Naive Bayes classifier are learned by using the context-clusters 3.1 Preprocessing The preprocessing module has two main roles : extracting content words and reconstructing the collected documents into contexts . \n\t', '\n\t\t We use the Brill POS tagger to extract content words \n\t\t']",Positive
"['\n\t\t Generally , the supervised learning approach with labeled data regards a document as a unit of meaning . \n\t', '\n\t\t But since we can use only the title words and unlabeled data , we define context as a unit of meaning and we employ it as the meaning unit to bootstrap the meaning of each category . \n\t', '\n\t\t In our system , we regard a sequence of 60 content words within a document as a context . \n\t', '\n\t\t To extract contexts from a document , we use sliding window techniques \n\t\t']",Positive
"['\n\t\t The window is a slide from the first word of the document to the last in the size of the window ( 60 words ) and the interval of each window ( 30 words ) . \n\t', '\n\t\t Therefore , the final output of preprocessing is a set of context vectors that are represented as content words of each context . \n\t', '\n\t\t 3.2 Constructing Context-Clusters for Training At first , we automatically create keywords from a title word for each category using co-occurrence information . \n\t', '\n\t\t Then centroid-contexts are extracted using the title word and keywords . \n\t', '\n\t\t They contain at least one of the title and keywords . \n\t', '\n\t\t Finally , we can gain more information of each category by assigning remaining contexts to each context- cluster using a similarity measure technique ; the remaining contexts do not contain any keywords or title words . \n\t', '\n\t\t 3.2.1 Creating Keyword Lists The starting point of our method is that we have title words and collected documents . \n\t', '\n\t\t A title word can present the main meaning of each category but it could be insufficient in representing any category for text categorization . \n\t', '\n\t\t Thus we need to find words that are semantically related to a title word , and we define them as keywords of each category . \n\t', '\n\t\t The score of semantic similarity between a title word , T , and a word , W , is calculated by the cosine metric as follows : ( 1 ) where t ; and w ; represent the occurrence ( binary value : 0 or 1 ) of words T and W in ;-th document respectively , and n is the total number of documents in the collected documents . \n\t', '\n\t\t This method calculates the similarity score between words based on the degree of their co-occurrence in the same document . \n\t', '\n\t\t Since the keywords for text categorization must have the power to discriminate categories as well as similarity with the title words , we assign a word to the keyword list of a category with the maximum similarity score and recalculate the score of the word in the category using the following formula : ( 2 ) where Tma. , is the title word with the maximum similarity score with a word W , c ma. , is the category of the title word Tma. , , and Tsecond ma. , is other title word with the second high similarity score with the word W . \n\t', '\n\t\t This formula means that a word with high ranking in a category has a high similarity score with the title word of the category and a high similarity score difference with other title words . \n\t', '\n\t\t We sort out words assigned to each category according to the calculated score in descending order . \n\t', '\n\t\t We then choose top m words as keywords in the category . \n\t', '\n\t\t Table 1 shows the list of keywords ( top 5 ) for each category in the WebKB data set . \n\t', '\n\t\t Table 1 . \n\t', '\n\t\t The list of keywords in the WebKB data set Category course Title Word Keywords faculty project student course assignments , hours , instructor , professor project student class , fall associate , ph.d , fax , interests , publications system , systems , research , software , information graduate , computer , science , page , university 3.2.2 Extracting and Verifying Centroid-Contexts We choose contexts with a keyword or a title word of a category as centroid-contexts . \n\t', '\n\t\t Among centroid-contexts , some contexts could not have good features of a category even though they include the keywords of the category . \n\t', '\n\t\t To rank the importance of centroid-contexts , we compute the importance score of each centroid-context . \n\t', '\n\t\t First of all , weights ( W;j ) of word w ; in j-th category are calculated using Term Frequency ( TF ) within a category and Inverse Category Frequency ( ICF ) \n\t\t']",Positive
"['\n\t\t Using word weights ( W;j ) calculated by formula 3 , the score of a centroid-context ( Sk ) in j-th category ( cj ) is computed as follows : ( 4 ) where N is the number of words in the centroidcontext . \n\t', '\n\t\t As a result , we obtain a set of words in first- order co-occurrence from centroid-contexts of each category . \n\t', '\n\t\t 3.2.3 Creating Context-Clusters We gather the second-order co-occurrence information by assigning remaining contexts to the context-cluster of each category . \n\t', '\n\t\t For the assigning criterion , we calculate similarity between remaining contexts and centroid-contexts of each category . \n\t', '\n\t\t Thus we employ the similarity measure technique by \n\t\t']",Positive
"['\n\t\t In our method , a part of this technique is reformed for our purpose and remaining contexts are assigned to each context-cluster by that revised technique . \n\t', '\n\t\t 1 ) Measurement of word and context similarities As similar words tend to appear in similar contexts , we can compute the similarity by using contextual information . \n\t', '\n\t\t Words and contexts play complementary roles . \n\t', '\n\t\t Contexts are similar to the extent that they contain similar words , and words are similar to the extent that they appear in similar contexts \n\t\t']",Positive
"['\n\t\t This definition is circular . \n\t', '\n\t\t Thus it is applied iteratively using two matrices , WSM and CSM . \n\t', '\n\t\t Each category has a word similarity matrix WSMn and a context similarity matrix CSMn . \n\t', '\n\t\t In each iteration n , we update WSMn , whose rows and columns are labeled by all content words encountered in the centroid-contexts of each category and input remaining contexts . \n\t', '\n\t\t In that matrix , the cell ( ij ) holds a value between 0 and 1 , indicating the extent to which the i-th word is contextually similar to the j-th word . \n\t', '\n\t\t Also , we keep and update a CSMn , which holds similarities among contexts . \n\t', '\n\t\t The rows of CSMn correspond to the remaining contexts and the columns to the centroid-contexts . \n\t', '\n\t\t In this paper , the number of input contexts of row and column in CSM is limited to 200 , considering execution time and memory allocation , and the number of iterations is set as 3 . \n\t', '\n\t\t To compute the similarities , we initialize WSMn to the identity matrix . \n\t', '\n\t\t The following steps are iterated until the changes in the similarity values are small enough . \n\t', '\n\t\t 1. Update the context similarity matrix CSMn , using the word similarity matrix WSMn . \n\t', '\n\t\t 2. Update the word similarity matrix WSMn , using the context similarity matrix CSMn . \n\t', '\n\t\t 2 ) Affinity formulae To simplify the symmetric iterative treatment of similarity between words and contexts , we define an auxiliary relation between words and contexts as affinity . \n\t', '\n\t\t Affinity formulae are defined as follows \n\t\t']",Positive
"['\n\t\t Every word has some affinity to the context , and the context can be represented by a vector indicating the affinity of each word to it . \n\t', '\n\t\t 3 ) Similarity formulae The similarity of W1 to W2 is the average affinity of the contexts that include W1 to W2 , and the similarity of a context X1 to X2 is a weighted average of the affinity of the words in X1 to X2 . \n\t', '\n\t\t Similarity formulae are defined as follows : ( 7 ) else ( 8 ) The weights in formula 7 are computed as reflecting global frequency , log-likelihood factors , and part of speech as used in \n\t\t']",Positive
"['\n\t\t The sum of weights in formula 8 , which is a reciprocal number of contexts that contain W1 , is 1 . \n\t', '\n\t\t 4 ) Assigning remaining contexts to a category We decided a similarity value of each remaining context for each category using the following method : ( 9 ) In formula 9 , i ) X is a remaining context , ii ) is a category set , and iii ) ccc ={S1 , ... , Sn}is a controid-contexts set of category ci . \n\t', '\n\t\t Each remaining context is assigned to a category which has a maximum similarity value . \n\t', '\n\t\t But there may exist noisy remaining contexts which do not belong to any category . \n\t', '\n\t\t To remove these noisy remaining contexts , we set up a dropping threshold using normal distribution of similarity values as follows \n\t\t']",Positive
"['\n\t\t Finally , a remaining context is assigned to the context-cluster of any category when the category has a maximum similarity above the dropping threshold value . \n\t', '\n\t\t In this paper , we empirically use a 15 % threshold value from an experiment using a validation set . \n\t', '\n\t\t 3.3 Learning the Naive Bayes Classifier Using Context-Clusters conventional classifiers in using machine-labeled data . \n\t', '\n\t\t In above section , we obtained labeled training data : context-clusters . \n\t', '\n\t\t Since training data are labeled as the context unit , we employ a Naive Bayes classifier because it can be built by estimating the word probability in a category , but not in a document . \n\t', '\n\t\t That is , the Naive Bayes classifier does not require labeled data with the unit of documents unlike other classifiers . \n\t', '\n\t\t We use the Naive Bayes classifier with minor modifications based on Kullback-Leibler Divergence \n\t\t']",Positive
"['\n\t\t We classify a document di according to the following formula : ( 11 ) where i ) n is the number of words in document di , ii ) wt is the t-th word in the vocabulary , iii ) N(wt,di) is the frequency of word wt in document di . \n\t', '\n\t\t Here , the Laplace smoothing is used to estimate the probability of word wt in class cj and the probability of class cj as follows : ( 13 ) where N(wt,cc) is the count of the number of times word wt occurs in the context-cluster ( Gcj ) of category cj. 4 Using a Feature Projection Technique for Handling Noisy Data of Machine-labeled Data We finally obtained labeled data of a documents unit , machine-labeled data . \n\t', '\n\t\t Now we can learn text classifiers using them . \n\t', '\n\t\t But since the machine- labeled data are created by our method , they generally include far more incorrectly labeled documents than the human-labeled data . \n\t', '\n\t\t Thus we employ a feature projection technique for our method . \n\t', '\n\t\t By the property of the feature projection technique , a classifier ( the TCFP classifier ) can have robustness from noisy data \n\t\t']",Positive
"['\n\t\t As seen in our experiment results , TCFP showed the highest performance among The TCFP classifier with robustness from noisy data Here , we simply describe the TCFP classifier using the feature projection technique ( Ko and Seo , 2002 ; 2004 ) . \n\t', '\n\t\t In this approach , the classification knowledge is represented as sets of projections of training data on each feature dimension . \n\t', '\n\t\t The classification of a test document is based on the voting of each feature of that test document . \n\t', '\n\t\t That is , the final prediction score is calculated by accumulating the voting scores of all features . \n\t', '\n\t\t First of all , we must calculate the voting ratio of each category for all features . \n\t', '\n\t\t Since elements with a high TF-IDF value in projections of a feature must become more useful classification criteria for the feature , we use only elements with TF-IDF values above the average TF-IDF value for voting . \n\t', '\n\t\t And the selected elements participate in proportional voting with the same importance as the TF-IDF value of each element . \n\t', '\n\t\t The voting ratio of each category cj in a feature tm is calculated by the following formula : ( 14 ) In formula 14 , w(tm , d ) is the weight of term tm in document d , Im denotes a set of elements selected for voting and y(cj , t m ( l ) ) ^ { 0.1 } is a function ; if the category for an element tm(l) is equal to cj , the output value is 1 . \n\t', '\n\t\t Otherwise , the output value is 0 . \n\t', '\n\t\t Next , since each feature separately votes on feature projections , contextual information is missing . \n\t', '\n\t\t Thus we calculate co-occurrence frequency of features in the training data and modify TF-IDF values of two terms ti and tj in a test document by co-occurrence frequency between them ; terms with a high co-occurrence frequency value have higher term weights . \n\t', '\n\t\t Finally , the voting score of each category cj in the m-th feature tm of a test document d is calculated by the following formula : ( 15 ) where tw(tm,d) denotes a modified term weight by the co-occurrence frequency and X Qm ) denotes 2 the calculated ^ statistics value of tm. ( 12 ) Table 2 . \n\t', '\n\t\t The top micro-avg F1 scores and precision-recall breakeven points of each method . \n\t', '\n\t\t OurMethod OurMethod OurMethod OurMethod OurMethod OurMethod Newsgroups WebKB Reuters ( basis ) ( NB ) 83.46 73.22 88.23 ( Rocchio ) ( LNN ) 79.95 68.04 85.65 ( SVM ) 82.49 73.74 87.41 ( TCFP ) 86.19 75.47 89.09 79.36 83 73.63 75.28 88.62 86.26 The outline of the TCFP classifier is as follow : 5 Empirical Evaluation 5.1 Data Sets and Experimental Settings To test our method , we used three different kinds of data sets : UseNet newsgroups ( 20 Newsgroups ) , web pages ( WebKB ) , and newswire art icles ( Reuters 21578 ) . \n\t', '\n\t\t For fair evaluation in Newsgroups and WebKB , we employed the five- fold cross-validation method . \n\t', '\n\t\t The Newsgroups data set , collected by Ken Lang , contains about 20,000 articles evenly divided among 20 UseNet discussion groups \n\t\t']",Positive
"['\n\t\t In this paper , we used only 16 categories after removing 4 categories : three miscellaneous categories ( talk.politics.misc , talk.religion.misc , and comp.os.ms-windows.misc ) and one duplicate ing category ( comp.sys . \n\t', '\n\t\t ibm.pc.hardware ) . \n\t', '\n\t\t The second data set comes from the WebKB project at CMU \n\t\t']",Positive
"['\n\t\t This data set contains web pages gathered fr om university computer science departments . \n\t', '\n\t\t The Reuters 21578 Distribution 1.0 data set consists of 12,902 articles and 90 topic categories from the Reuters newswire . \n\t', '\n\t\t Like other study in \n\t\t']",Positive
"['\n\t\t About 25 % documents from training data of each data set are selected for a validation set . \n\t', '\n\t\t We applied a statistical feature selection method ( ^2 statistics ) to a preprocessing stage for each classifier \n\t\t']",Positive
"['\n\t\t As performance measures , we followed the standard definition of recall , precision , an d F1 measure . \n\t', '\n\t\t For evaluation performance average across categories , we used the micro-averaging method \n\t\t']",Positive
"['\n\t\t Results on Reuters are reported as precision-recall breakeven points , which is a stan dard information retrieval measure for binary classification \n\t\t']",Positive
"['\n\t\t Title words in our experiment are selected according to category names of each data set ( see Table 1 as an example ) . \n\t', '\n\t\t 5.2 Experimental Results 5.2.1 Observing the Performance According to the Number of Keywords First of all , we determine the number of keywords in our method using the validation set . \n\t', '\n\t\t The number of keywords is limited by the top m-th keyword from the ordered list of each category . \n\t', '\n\t\t Figure 1 displays the performance at different number of keywords ( fr om 0 to 20 ) in each data set . \n\t', '\n\t\t Figure 1 . \n\t', '\n\t\t The comparison of performance according to the number of keywords We set the number of keywords to 2 in Newsgroups , 5 in WebKB , and 3 in Reuters empirically . \n\t', '\n\t\t Generally , we recommend that the number of keywords be between 2 an d 5. 5.2.2 Comparing our Method Using TCFP with those Using other Classifiers In this section , we prove the superiority of TCFP over the other classifiers ( SVM , kNN , Naive Bayes ( NB ) , Roccio ) in training data with much noisy data such as machine-labeled data . \n\t', '\n\t\t As shown in Table 2 , we obtained the best performan ce in using TCFP at all three data sets . \n\t', '\n\t\t mean Let us define the notations . \n\t', '\n\t\t OurMethod(basis) denotes the Naive Bayes classifier using labeled contexts an d OurMethod(NB) denotes the Naive Bayes classifier using machine-labeled data as 1. input : test document : d =<t1 , t2 , ... , tn> 2. main process For each feature ti tw(ti,d) is calculated For each feature ti For each category c ; vote[c;]=vote[c;]+vs(c;,ti) by Formula 15 prediction = arg max vote[c ; ] training data . \n\t', '\n\t\t The same manner is applied for other classifiers . \n\t', '\n\t\t OurMethod(TCFP) achieved more advanced scores than OurMethod(basis) : 6.83 in Newsgroups , 1.84 in WebKB , and 0.47 in Reuters . \n\t', '\n\t\t 5.2.3 Comparing with the Supervised Naive Bayes Classifier For this experiment , we consider two possible cases for labeling task . \n\t', '\n\t\t The first task is to label a part of collected documents and the second is to label all of them . \n\t', '\n\t\t As the first task , we built up a new training data set ; it consists of 500 different documents randomly chosen from appropriate categories like the experiment in \n\t\t']",Positive
"['\n\t\t As a result , we report performances from two kinds of Naive Bayes classifiers which are learned from 500 training documents and the whole training documents respectively . \n\t', '\n\t\t Table 3 . \n\t', '\n\t\t The comparison of our method and the supervised NB classifier OurMethod NB NB Newsgroups WebKB Reuters ( TCFP ) 86.19 75.47 89.09 ( 500 ) 72.68 74.1 82.1 ( All ) 91.72 85.29 91.64 In Table 3 , the results of our method are higher than those of NB(500) and are comparable to those of NB(All) in all data sets . \n\t', '\n\t\t Especially , the result in Reuters reached 2.55 close to that of NB(All) though it used the whole labeled training data . \n\t', '\n\t\t 5.2.4 Enhancing our Method from Choosing Keywords by Human The main problem of our method is that the performance depends on the quality of the keywords and title words . \n\t', '\n\t\t As we have seen in Table 3 , we obtained the worst performance in the WebKB data set . \n\t', '\n\t\t In fact , title words and keywords of each category in the WebKB data set also have high frequency in other categories . \n\t', '\n\t\t We think these factors contribute to a comparatively poor performance of our method . \n\t', '\n\t\t If keywords as well as title words are supplied by humans , our method may achieve higher performance . \n\t', '\n\t\t However , choosing the proper keywords for each category is a much difficult task . \n\t', '\n\t\t Moreover , keywords from developers , who have insufficient knowledge about an application domain , do not guarantee high performance . \n\t', '\n\t\t In order to overcome this problem , we propose a hybrid method for choosing keywords . \n\t', '\n\t\t That is , a developer obtains 10 candidate keywords from our keyword extraction method and then they can choose proper keywords from them . \n\t', '\n\t\t Table 4 shows the results from three data sets . \n\t', '\n\t\t Table 4 . \n\t', '\n\t\t The comparison of our method and enhancing method Newsgroups WebKB Reuters OurMethod Enhancing ( TCFP ) ) Improvement ( TCFP ) 86.23 77.59 89.52 +0.04 +2.12 +0.43 86.19 75.47 89.09 As shown in Table 4 , especially we could achieve significant improvement in the WebKb data set . \n\t', '\n\t\t Thus we find that the new method for choosing keywords is more useful in a domain with confused keywords between categories such as the WebKB data set . \n\t', '\n\t\t 5.2.5 Comparing with a Clustering Technique In related works , we presented two approaches using unlabeled data in text categorization ; one approach combines unlabeled data and labeled data , and the other approach uses the clustering technique for text categorization . \n\t', '\n\t\t Since our method does not use any labeled data , it cannot be fairly compared with the former approaches . \n\t', '\n\t\t Therefore , we compare our method with a clustering technique . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t In his experiments , the sIB algorithm was superior to other clustering algorithms . \n\t', '\n\t\t As we set the same experimental settings as in Slonim\x92s experiments and conduct experiments , we verify that our method outperforms ths sIB algorithm . \n\t', '\n\t\t In our experiments , we used the micro-averaging precision as performance measure and two revised data sets : revised_ In revised_ NG , the categories of Newsgroups were united with respect to 10 meta-categories : five comp categories , three politics categories , two sports categories , three religions categories , and two transportation categories into five big meta- categories . \n\t', '\n\t\t The revised_ Reuters used the 10 most frequent categories in the Reuters 21578 corpus under the ModApte split . \n\t', '\n\t\t As shown in Table 5 , our method shows 6.65 advanced score in revised_ NG and 3.2 advanced score in revised_ Reuters . \n\t', '\n\t\t Table 5 . \n\t', '\n\t\t The comparison of our method and sIB revised NG revised_Reuters sIB OurMethod Improvement 79.5 ( TCFP ) +6.65 85.8 86.15 +3.2 89 Reuters . \n\t', '\n\t\t These data sets were revised in the same way according to Slonim\x92s paper as follows : NG , revised _ 6 Conclusions and Future Works This paper has addressed a new unsupervised or semi-unsupervised text categorization method . \n\t', '\n\t\t Though our method uses only title words and unlabeled data , it shows reasonably comparable performance in comparison with that of the supervised Naive Bayes classifier . \n\t', '\n\t\t Moreover , it outperforms a clustering method , sIB . \n\t', '\n\t\t Labeled data are expensive while unlabeled data are inexpensive and plentiful . \n\t', '\n\t\t Therefore , our method is useful for low-cost text categorization . \n\t', '\n\t\t Furthermore , if some text categorization tasks require high accuracy , our method can be used as an assistant tool for easily creating labeled training data . \n\t', '\n\t\t Since our method depends on title words and keywords , we need additional studies about the characteristics of candidate words for title words and keywords according to each data set . \n\t', '\n\t\t Acknowledgement This work was supported by grant No . \n\t', '\n\t\t R01-2003- 000-11588-0 from the basic Research Program of the KOSEF References K. Bennett and A. Demiriz , 1999 , Semi-supervised Support Vector Machines , Advances in Neural Information Processing Systems 11 , pp. 368-374 . \n\t', '\n\t\t E. Brill , 1995 , Transformation-Based Error-driven Learning and Natural Language Processing : A Case Study in Part of Speech Tagging , Computational Linguistics , Vol.21 , No. 4 . \n\t', '\n\t\t K. Cho and J. Kim , 1997 , Automatic Text Categorization on Hierarchical Category Structure by using ICF ( Inverse Category Frequency ) Weighting , In Proc . \n\t', '\n\t\t of KISS conference , pp. 507-510 . \n\t', '\n\t\t M. Craven , D. DiPasquo , D. Freitag , A. McCallum , T. Mitchell , K. Nigam , and S. Slattery , 2000 , Learning to construct knowledge bases from the World Wide Web , Artificial Intelligence , 118(1-2) , pp. 69-113 . \n\t', '\n\t\t T. Joachims , 1998 , Text Categorization with Support Vector Machines : Learning with Many Relevant Features . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of ECML , pp. 137-142 . \n\t', '\n\t\t Y. Karov and S. Edelman , 1998 , Similarity-based Word Sense Disambiguation , Computational Linguistics , Vol. 24 , No. 1 , pp. 41-60 . \n\t', '\n\t\t Y. Ko and J. Seo , 2000 , Automatic Text Categorization by Unsupervised Learning , In Proc . \n\t', '\n\t\t of COLING\x922000 , pp. 453-459 . \n\t', '\n\t\t Y. Ko and J. Seo , 2002 , Text Categorization using Feature Projections , In Proc . \n\t', '\n\t\t of COLING \x922002 , pp. 467-473 . \n\t', '\n\t\t Y. Ko and J. Seo , 2004 , Using the Feature Projection Technique based on the Normalized Voting Method for Text Classification , Information Processing and Management , Vol. 40 , No. 2 , pp. 191-208 . \n\t', '\n\t\t D.D. Lewis , R.E. Schapire , J.P. Callan , and R. Papka , 1996 , Training Algorithms for Linear Text Classifiers . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of SIGIR\x9296 , pp.289-297 . \n\t', '\n\t\t Y. Maarek , D. Berry , and G. Kaiser , 1991 , An Information Retrieval Approach for Automatically Construction Software Libraries , IEEE Transaction on Software Engineering , Vol. 17 , No. 8 , pp. 800- 813 . \n\t', '\n\t\t A. McCallum and K. Nigam , 1998 , A Comparison of Event Models for Naive Bayes Text Classification . \n\t', '\n\t\t AAAI \x9298 workshop on Learning for Text Categorization , pp. 41-48 . \n\t', '\n\t\t K. P. Nigam , A. McCallum , S. Thrun , and T. Mitchell , 1998 , Learning to Classify Text from Labeled and Unlabeled Documents , In Proc . \n\t', '\n\t\t of AAAI-98 . \n\t', '\n\t\t K. P. Nigam , 2001 , Using Unlabeled Data to Improve Text Classification , The dissertation for the degree of Doctor of Philosophy . \n\t', '\n\t\t N. Slonim , N. Friedman , and N. Tishby , 2002 , Unsupervised Document Classification using Sequential Information Maximization , In Proc . \n\t', '\n\t\t of SIGIR\x9202 , pp. 129-136 . \n\t', '\n\t\t Y. Yang and J. P. Pedersen . \n\t', '\n\t\t 1997 , Feature selection in statistical leaning of text categorization . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of ICML\x9297 , pp. 412-420 . \n\t', '\n\t\t Y. Yang , S. Slattery , and R. Ghani . \n\t', '\n\t\t 2002 , A study of approaches to hypertext categorization , Journal of Intelligent Information Systems , Vol. 18 , No. 2 . \n\t', '\n\t\t The Sentimental Factor : Improving Review Classification via Human-Provided Information Philip Beineke*and Trevor Hastie Shivakumar Vaithyanathan Dept. of Statistics IBM Almaden Research Center Stanford University 650 Harry Rd. Stanford , CA 94305 San Jose , CA 95120-6099 Abstract Sentiment classification is the task of labeling a review document according to the polarity of its prevailing opinion ( favorable or unfavorable ) . \n\t', '\n\t\t In approaching this problem , a model builder often has three sources of information available : a small collection of labeled documents , a large collection of unlabeled documents , and human understanding of language . \n\t', '\n\t\t Ideally , a learning method will utilize all three sources . \n\t', '\n\t\t To accomplish this goal , we generalize an existing procedure that uses the latter two . \n\t', '\n\t\t We extend this procedure by re-interpreting it as a Naive Bayes model for document sentiment . \n\t', '\n\t\t Viewed as such , it can also be seen to extract a pair of derived features that are linearly combined to predict sentiment . \n\t', '\n\t\t This perspective allows us to improve upon previous methods , primarily through two strategies : incorporating additional derived features into the model and , where possible , using labeled data to estimate their relative influence . \n\t', '\n\t\t 1 Introduction Text documents are available in ever-increasing numbers , making automated techniques for information extraction increasingly useful . \n\t', '\n\t\t Traditionally , most research effort has been directed towards \x93objective\x94 information , such as classification according to topic ; however , interest is growing in producing information about the opinions that a document contains ; for instance , \n\t\t']",Positive
"['\n\t\t In March , 2004 , the American Association for Artificial Intelligence held a symposium in this area , entitled \x93Exploring Affect and Attitude in Text.\x94 One task in opinion extraction is to label a review document d according to its prevailing sentiment s E { ^1 , 1 } ( unfavorable or favorable ) . \n\t', '\n\t\t Several previous papers have addressed this problem by building models that rely exclusively upon labeled documents , e.g. \n\t\t']",Negative
"['\n\t\t By learning models from labeled data , one can apply familiar , powerful techniques directly ; however , in practice it may be difficult to obtain enough labeled reviews to learn model parameters accurately . \n\t', '\n\t\t A contrasting approach \n\t\t']",Negative
"['\n\t\t This makes it possible to use a large underlying corpus \x96 in this case , the entire Internet as seen through the AltaVista search engine . \n\t', '\n\t\t As a result , estimates for model parameters are subject to a relatively small amount of random variation . \n\t', '\n\t\t The corresponding drawback to such an approach is that its predictions are not validated on actual documents . \n\t', '\n\t\t In machine learning , it has often been effective to use labeled and unlabeled examples in tandem , e.g. \n\t\t']",Positive
"['\n\t\t Turney\x92s model introduces the further consideration of incorporating human-provided knowledge about language . \n\t', '\n\t\t In this paper we build models that utilize all three sources : labeled documents , unlabeled documents , and human-provided information . \n\t', '\n\t\t The basic concept behind Turney\x92s model is quite simple . \n\t', '\n\t\t The \x93sentiment orientation\x94 \n\t\t']",Positive
"['\n\t\t These words serve as \x93anchors\x94 for positive and negative sentiment . \n\t', '\n\t\t Words that co-occur more frequently with one anchor than the other are themselves taken to be predictive of sentiment . \n\t', '\n\t\t As a result , information about a pair of words is generalized to many words , and then to documents . \n\t', '\n\t\t In the following section , we relate this model with Naive Bayes classification , showing that Turney\x92s classifier is a \x93pseudo-supervised\x94 approach : it effectively generates a new corpus of labeled documents , upon which it fits a Naive Bayes classifier . \n\t', '\n\t\t This insight allows the procedure to be represented as a probability model that is linear on the logistic scale , which in turn suggests generalizations that are developed in subsequent sections . \n\t', '\n\t\t 2 A Logistic Model for Sentiment 2.1 Turney\x92s Sentiment Classifier In Turney\x92s model , the \x93sentiment orientation\x94 Q of word w is estimated as follows . \n\t', '\n\t\t Q(w) = log N(w , excellent ) /Nexcellent ( 1 ) N(w,poor) /Npoor Here , Na is the total number of sites on the Internet that contain an occurrence of a \x96 a feature that can be a word type or a phrase . \n\t', '\n\t\t N(w,a) is the number of sites in which features w and a appear \x93near\x94 each other , i.e. in the same passage of text , within a span of ten words . \n\t', '\n\t\t Both numbers are obtained from the hit count that results from a query of the AltaVista search engine . \n\t', '\n\t\t The rationale for this estimate is that words that express similar sentiment often co-occur , while words that express conflicting sentiment co- occur more rarely . \n\t', '\n\t\t Thus , a word that co-occurs more frequently with excellent than poor is estimated to have a positive sentiment orientation . \n\t', '\n\t\t To extrapolate from words to documents , the estimated sentiment s\x88 E { -1 , 1 } of a review document d is the sign of the average sentiment orientation of its constituent features . \n\t', ""\n\t\t ' To represent this estimate formally , we introduce the following notation : W is a \x93dictionary\x94 of features : ( w1 , ... , wp ) . \n\t"", '\n\t\t Each feature\x92s respective sentiment orientation is represented as an entry in the vector ~\x88 of length p : \x88~j = \x88~(wj) ( 2 ) Given a collection of n review documents , the i-th each di is also represented as a vector of length p , with dij equal to the number of times that feature w j occurs in di . \n\t', '\n\t\t The length of a document is its total number of features , Idi I = E~3=1 dij . \n\t', ""\n\t\t Turney\x92s classifier for the i-th document\x92s sentiment si can now be written : \x88si = si Ej=1 &jdij sign Idi ' ( 3 ) Using a carefully chosen collection of features , this classifier produces correct results on 65.8 % of a collection of 120 movie reviews , where 60 are labeled positive and 60 negative . \n\t"", '\n\t\t Although this is not a particularly encouraging result , movie reviews tend to be a difficult domain . \n\t', '\n\t\t Accuracy on sentiment classification in other domains exceeds 80 % \n\t\t']",Positive
"[""\n\t\t ' Note that not all words or phrases need to be considered as features . \n\t"", '\n\t\t In \n\t\t']",Positive
"['\n\t\t 2.2 Naive Bayes Classification Bayes\x92 Theorem provides a convenient framework for predicting a binary response s E { -1 , 1 } from a feature vector x : Pr(s = 1 x = Pr(xIs = 1)7r1 ( 4 ) ( I ) EkCI-1,11 Pr(xIs = k)7rk For a labeled sample of data ( xi , si ) , i = 1 , ... , n , a class\x92s marginal probability 7rk can be estimated trivially as the proportion of training samples belonging to the class . \n\t', '\n\t\t Thus the critical aspect of classification by Bayes\x92 Theorem is to estimate the conditional distribution of x given s. Naive Bayes simplifies this problem by making a \x93naive\x94 assumption : within a class , the different feature values are taken to be independent of one another . \n\t', '\n\t\t Pr(xIs) = fj Pr(xjIs) ( 5 ) j As a result , the estimation problem is reduced to univariate distributions . \n\t', '\n\t\t \x95 Naive Bayes for a Multinomial Distribution We consider a \x93bag of words\x94 model for a document that belongs to class k , where features are assumed to result from a sequence of I di I independent multinomial draws with outcome probability vector ilk = ( gk1,...,gkp ) . \n\t', '\n\t\t Given a collection of documents with labels , ( di , si ) , i = 1 , ... , n , a natural estimate for gkj is the fraction of all features in documents of class k that equal wj : Ei:si=k dij ( 6 ) Ei:si=k IdiI In the two-class case , the logit transformation provides a revealing representation of the class posterior probabilities of the Naive Bayes model . \n\t', '\n\t\t ~logit(sId) °A ~Pr(s = 1Id ) ( 7 ) = ( 8 ) log ~Pr(s = -1Id ) p \x887r1 \x88g1j log +1 : dj log \x887r-1 \x88g-1j j=1 ~p dj\x88~j ( 9 ) \x88~0 + j=1 \x887r1 where \x88~0 = log ( 10 ) \x887r-1 \x88~j = \x88g1j ( 11 ) log \x88g-1j \x88gkj = Observe that the estimate for the logit in Equation 9 has a simple structure : it is a linear function of d . \n\t', '\n\t\t Models that take this form are commonplace in classification . \n\t', '\n\t\t 2.3 Turney\x92s Classifier as Naive Bayes Although Naive Bayes classification requires a labeled corpus of documents , we show in this section that Turney\x92s approach corresponds to a Naive Bayes model . \n\t', '\n\t\t The necessary documents and their corresponding labels are built from the spans of text that surround the anchor words excellent and poor . \n\t', '\n\t\t More formally , a labeled corpus may be produced by the following procedure : 1 . \n\t', '\n\t\t For a particular anchor ak , locate all of the sites on the Internet where it occurs . \n\t', '\n\t\t 2. From all of the pages within a site , gather the features that occur within ten words of an occurrence of ak , with any particular feature included at most once . \n\t', '\n\t\t This list comprises a new \x93document,\x94 representing that site.2 3 . \n\t', '\n\t\t Label this document +1 if ak = excellent , -1 if ak = poor . \n\t', '\n\t\t When a Naive Bayes model is fit to the corpus described above , it results in a vector ~\x88 of length p , consisting of coefficient estimates for all features . \n\t', '\n\t\t In Propositions 1 and 2 below , we show that Turney\x92s estimates of sentiment orientation ~\x88 are closely related to \x88~ , and that both estimates produce identical classifiers . \n\t', '\n\t\t Proposition 1 Proposition 2 Turney\x92s classifier is identical to a Naive Bayes classifier fit on this corpus , with 7r1 = 7r-1 = 0.5 . \n\t', '\n\t\t Proof : A Naive Bayes classifier typically assigns an observation to its most probable class . \n\t', '\n\t\t This is equivalent to classifying according to the sign of the estimated logit . \n\t', '\n\t\t So for any document , we must show that both the logit estimate and the average sentiment orientation are identical in sign . \n\t', '\n\t\t When 7r1 = 0 . \n\t', '\n\t\t 5 , ~0 = 0 . \n\t', '\n\t\t Thus the estimated logit is logit(sId) = ~p \x88~jdj ( 18 ) j=1 = C1 ~p \x88~jdj ( 19 ) j=1 This is a positive multiple of Turney\x92s classifier ( Equation 3 ) , so they clearly match in sign . \n\t', '\n\t\t ^ 3 A More Versatile Model 3.1 Desired Extensions By understanding Turney\x92s model within a Naive Bayes framework , we are able to interpret its output as a probability model for document classes . \n\t', '\n\t\t In the presence of labeled examples , this insight also makes it possible to estimate the intercept term ~0 . \n\t', '\n\t\t Further , we are able to view this model as a member of a broad class : linear estimates for the logit . \n\t', '\n\t\t This understanding facilitates further extensions , in particular , utilizing the following : ~\x88 = C1\x88~ ( 12 ) Nexc./~i:si=1 IdiI Npoor/ Ei:si=-1 IdiI ( 13 ) where C1 = 1 . \n\t', '\n\t\t Labeled documents 2 . \n\t', '\n\t\t More anchor words Proof : Because a feature is restricted to at most one occurrence in a document , E dij = N(w,ak) ( 14 ) i:si=k 2If both anchors occur on a site , then there will actually be two documents , one for each sentiment The reason for using labeled documents is straightforward ; labels offer validation for any chosen model . \n\t', '\n\t\t Using additional anchors is desirable in part because it is inexpensive to produce lists of words that are believed to reflect positive sentiment , perhaps by reference to a thesaurus . \n\t', '\n\t\t In addition , a single anchor may be at once too general and too specific . \n\t', '\n\t\t An anchor may be too general in the sense that many common words have multiple meanings , and not all of them reflect a chosen sentiment orientation . \n\t', '\n\t\t For example , poor can refer to an objective economic state that does not necessarily express negative sentiment . \n\t', '\n\t\t As a result , a word such as income appears 4.18 times as frequently with poor as excellent , even though it does not convey negative sentiment . \n\t', '\n\t\t Similarly , excellent has a technical Then from Equations 6 and 11 : \x88~j = log \x88q1j ( 15 ) \x88q-1j N(w,poor)/ ~ i:si=-1IdiI N(w,exc.)/ ~i:si=1 IdiI ( 16 ) = C1 \x88~j ( 17 ) = log meaning in antiquity trading , which causes it to appear 3.34 times as frequently with furniture . \n\t', '\n\t\t An anchor may also be too specific , in the sense that there are a variety of different ways to express sentiment , and a single anchor may not capture them all . \n\t', '\n\t\t So a word like pretentious carries a strong negative sentiment but co-occurs only slightly more frequently ( 1.23 times ) with excellent than poor . \n\t', '\n\t\t Likewise , fascination generally reflects a positive sentiment , yet it appears slightly more frequently ( 1.06 times ) with poor than excellent . \n\t', '\n\t\t 3.2 Other Sources of Unlabeled Data The use of additional anchors has a drawback in terms of being resource-intensive . \n\t', '\n\t\t A feature set may contain many words and phrases , and each of them requires a separate AltaVista query for every chosen anchor word . \n\t', '\n\t\t In the case of 30,000 features and ten queries per minute , downloads for a single anchor word require over two days of data collection . \n\t', '\n\t\t An alternative approach is to access a large collection of documents directly . \n\t', '\n\t\t Then all co- occurrences can be counted in a single pass . \n\t', '\n\t\t Although this approach dramatically reduces the amount of data available , it does offer several advantages . \n\t', '\n\t\t \x95 Increased Query Options Search engine queries of the form phrase NEAR anchor may not produce all of the desired co- occurrence counts . \n\t', '\n\t\t For instance , one may wish to run queries that use stemmed words , hyphenated words , or punctuation marks . \n\t', '\n\t\t One may also wish to modify the definition of NEAR , or to count individual co-occurrences , rather than counting sites that contain at least one co-occurrence . \n\t', '\n\t\t \x95 Topic Matching Across the Internet as a whole , features may not exhibit the same correlation structure as they do within a specific domain . \n\t', '\n\t\t By restricting attention to documents within a domain , one may hope to avoid co- occurrences that are primarily relevant to other subjects . \n\t', '\n\t\t \x95 Reproducibility On a fixed corpus , counts of word occurrences produce consistent results . \n\t', '\n\t\t Due to the dynamic nature of the Internet , numbers may fluctuate . \n\t', '\n\t\t 3.3 Co-Occurrences and Derived Features The Naive Bayes coefficient estimate \x88~j may itself be interpreted as an intercept term plus a linear combination of features of the form log N(,,,j,ak) . \n\t', '\n\t\t Num . \n\t', '\n\t\t of Labeled Occurrences Correlation 1-5 0.022 6-10 0.082 11-25 0.113 26-50 0.183 51-75 0.283 76-100 0.316 Figure 1 : Correlation between Supervised and Unsupervised Coefficient Estimates N(j,pr.)/ i:si=^1IdiI N(j,exc.)/ i:si=1 IdiI ( 20 ) =log C1 + log N(j,exc.) \x97 log N(j,pr.) ( 21 ) We generalize this estimate as follows : for a collection of K different anchor words , we consider a general linear combination of logged co-occurrence counts . \n\t', ""\n\t\t \x88~j = ~K ' Yk log N(,,,j , ak ) ( 22 ) k=1 In the special case of a Naive Bayes model , ' Yk = 1 when the k-th anchor word ak conveys positive sentiment , \x971 when it conveys negative sentiment . \n\t"", ""\n\t\t Replacing the logit estimate in Equation 9 with an estimate of this form , the model becomes : = ' Y0 + ~K ' Yk ~p dj logN(,,,j,ak) k=1 j=1 ( 25 ) ( 26 ) This model has only K + 1 parameters : 'Y0,'Y1 , \x95 \x95 \x95 , ' YK . \n\t"", '\n\t\t These can be learned straightforwardly from labeled documents by a method such as logistic regression . \n\t', '\n\t\t Observe that a document receives a score for each anchor word ~ 1 dj log N(,,,j,ak) . \n\t', '\n\t\t Effectively , the predictor variables in this model are no longer counts of the original features dj . \n\t', ""\n\t\t Rather , they are logit(sId) = \x88~0 + ~p dj\x88~j ( 23 ) j=1 dj ' Yk log N( , , , j , ak ) ( 24 ) \x88~0 + ~p ~K j=1 k=1 \x88~j = log Positive Negative best awful brilliant bad excellent pathetic spectacular poor wonderful worst Figure 3 : Selected Anchor Words Unsupervised vs. . \n\t"", '\n\t\t Supervised Coefficients ^2.0 ^1.5^1.0 ^0.5 0.0 0.5 1.0 1.5 Traditional Naive Bayes Coefs . \n\t', '\n\t\t Figure 2 : Unsupervised versus Supervised Coefficient Estimates inner products between the entire feature vector d and the logged co-occurence vector N(w,aka . \n\t', '\n\t\t In this respect , the vector of logged co-occurrences is used to produce derived feature . \n\t', '\n\t\t 4 Data Analysis 4.1 Accuracy of Unsupervised Coefficients By means of a Perl script that uses the Lynx browser , Version 2.8.3rel.1 , we download AltaVista hit counts for queries of the form \x93target NEAR anchor.\x94 The initial list of targets consists of 44,321 word types extracted from the Pang corpus of 1400 labeled movie reviews . \n\t', '\n\t\t After preprocessing , this number is reduced to 28,629.3 In Figure 1 , we compare estimates produced by two Naive Bayes procedures . \n\t', '\n\t\t For each feature wd , we estimate ~d by using Turney\x92s procedure , and by fitting a traditional Naive Bayes model to the labeled documents . \n\t', '\n\t\t The traditional estimates are smoothed by assuming a Beta prior distribution that is equivalent to having four previous observations of wd in documents of each class . \n\t', '\n\t\t C2 4 + Ei:8i=1 did ( 27 ) 4 + Ei:8i=-1 did where C2 = 4p + EV.8i=1 IdiI(28) 4p + E=-1 Idi Here , did is used to indicate feature presence : ~1 if wd appears in di did = 0 otherwise ( 29 ) 3 We eliminate extremely rare words by requiring each target to co-occur at least once with each anchor . \n\t', '\n\t\t In addition , certain types , such as words containing hyphens , apostrophes , or other punctuation marks , do not appear to produce valid counts , so they are discarded . \n\t', '\n\t\t We choose this fitting procedure among several candidates because it performs well in classifying test documents . \n\t', '\n\t\t In Figure 1 , each entry in the right-hand column is the observed correlation between these two estimates over a subset of features . \n\t', '\n\t\t For features that occur in five documents or fewer , the correlation is very weak ( 0.022 ) . \n\t', '\n\t\t This is not surprising , as it is difficult to estimate a coefficient from such a small number of labeled examples . \n\t', '\n\t\t Correlations are stronger for more common features , but never strong . \n\t', '\n\t\t As a baseline for comparison , Naive Bayes coefficients can be estimated using a subset of their labeled occurrences . \n\t', '\n\t\t With two independent sets of 51-75 occurrences , Naive Bayes coefficient estimates had a correlation of 0.475 . \n\t', '\n\t\t Figure 2 is a scatterplot of the same coefficient estimates for word types that appear in 51 to 100 documents . \n\t', '\n\t\t The great majority of features do not have large coefficients , but even for the ones that do , there is not a tight correlation . \n\t', '\n\t\t 4.2 Additional Anchors We wish to learn how our model performance depends on the choice and number of anchor words . \n\t', '\n\t\t Selecting from WordNet synonym lists \n\t\t']",Positive
"['\n\t\t This produces a total of 25 different possible pairs for use in producing coefficient estimates . \n\t', '\n\t\t Figure 4 shows the classification performance of unsupervised procedures using the 1400 labeled Pang documents as test data . \n\t', '\n\t\t Coefficients \x88~d are estimated as described in Equation 22 . \n\t', '\n\t\t Several different experimental conditions are applied . \n\t', '\n\t\t The methods labeled \x94Count\x94 use the original un-normalized coefficients , while those labeled \x93Norm.\x94 have been normalized so that the number of co-occurrences with each anchor have identical variance . \n\t', '\n\t\t Results are shown when rare words ( with three or fewer occurrences in the labeled corpus ) are included and omitted . \n\t', '\n\t\t The methods \x93pair\x94 and \x9310\x94 describe whether all ten anchor coefficients are used at once , or just the ones that correspond to a single pair of \x88q1d \x88q-1d Method Feat . \n\t', '\n\t\t Misclass . \n\t', '\n\t\t St.Dev Count Pair >3 39.6 % 2.9 % Norm . \n\t', '\n\t\t Pair >3 38.4 % 3.0 % Count Pair all 37.4 % 3.1 % Norm . \n\t', '\n\t\t Pair all 37.3 % 3.0 % Count 10 > 3 36.4 % \x96 Norm . \n\t', '\n\t\t 10 > 3 35.4 % \x96 Count 10 all 34.6 % \x96 Norm . \n\t', '\n\t\t 10 all 34.1 % \x96 Figure 4 : Classification Error Rates for Different Unsupervised Approaches anchor words . \n\t', '\n\t\t For anchor pairs , the mean error across all 25 pairs is reported , along with its standard deviation . \n\t', '\n\t\t Patterns are consistent across the different conditions . \n\t', '\n\t\t A relatively large improvement comes from using all ten anchor words . \n\t', '\n\t\t Smaller benefits arise from including rare words and from normalizing model coefficients . \n\t', '\n\t\t Models that use the original pair of anchor words , excellent and poor , perform slightly better than the average pair . \n\t', '\n\t\t Whereas mean performance ranges from 37.3 % to 39.6 % , misclassification rates for this pair of anchors ranges from 37.4 % to 38.1 % . \n\t', '\n\t\t 4.3 A Smaller Unlabeled Corpus As described in Section 3.2 , there are several reasons to explore the use of a smaller unlabeled corpus , rather than the entire Internet . \n\t', '\n\t\t In our experiments , we use additional movie reviews as our documents . \n\t', '\n\t\t For this domain , Pang makes available 27,886 reviews.4 Because this corpus offers dramatically fewer instances of anchor words , we modify our estimation procedure . \n\t', '\n\t\t Rather than discarding words that rarely co-occur with anchors , we use the same feature set as before and regularize estimates by the same procedure used in the Naive Bayes procedure described earlier . \n\t', '\n\t\t Using all features , and ten anchor words with normalized scores , test error is 35.0 % . \n\t', '\n\t\t This suggests that comparable results can be attained while referring to a considerably smaller unlabeled corpus . \n\t', '\n\t\t Rather than requiring several days of downloads , the count of nearby co-occurrences was completed in under ten minutes . \n\t', '\n\t\t Because this procedure enables fast access to counts , we explore the possibility of dramatically enlarging our collection of anchor words . \n\t', '\n\t\t We col- 4 This corpus is freely available on the following website : Misclassification versus Sample Size Num of Labeled Documents Figure 5 : Misclassification with Labeled Documents . \n\t', '\n\t\t The solid curve represents a latent factor model with estimated coefficients . \n\t', '\n\t\t The dashed curve uses a Naive Bayes classifier . \n\t', '\n\t\t The two horizontal lines represent unsupervised estimates ; the upper one is for the original unsupervised classifier , and the lower is for the most successful unsupervised method . \n\t', '\n\t\t lect data for the complete set of WordNet synonyms for the words good , best , bad , boring , and dreadful . \n\t', '\n\t\t This yields a total of 83 anchor words , 35 positive and 48 negative . \n\t', '\n\t\t When all of these anchors are used in conjunction , test error increases to 38.3 % . \n\t', '\n\t\t One possible difficulty in using this automated procedure is that some synonyms for a word do not carry the same sentiment orientation . \n\t', '\n\t\t For instance , intense is listed as a synonym for bad , even though its presence in a movie review is a strongly positive indication.5 4.4 Methods with Supervision As demonstrated in Section 3.3 , each anchor word ak is associated with a coefficient yk . \n\t', '\n\t\t In unsupervised models , these coefficients are assumed to be known . \n\t', '\n\t\t However , when labeled documents are available , it may be advantageous to estimate them . \n\t', '\n\t\t Figure 5 compares the performance of a model with estimated coefficient vector y , as opposed to unsupervised models and a traditional supervised approach . \n\t', '\n\t\t When a moderate number of labeled documents are available , it offers a noticeable improvement . \n\t', '\n\t\t The supervised method used for reference in this case is the Naive Bayes model that is described in section 4.1 . \n\t', '\n\t\t Naive Bayes classification is of particular interest here because it converges faster to its asymptotic optimum than do discriminative methods \n\t\t']",Positive
"['\n\t\t Further , with 5In the labeled Pang corpus , intense appears in 38 positive 100 200 300 400 500 600 http://www.cs.cornell.edu/people/pabo/movie-rwviem-addbnYy 6 negative ones . \n\t', '\n\t\t a larger number of labeled documents , its performance on this corpus is comparable to that of Support Vector Machines and Maximum Entropy models \n\t\t']",Positive
"['\n\t\t The coefficient vector ry is estimated by regularized logistic regression . \n\t', '\n\t\t This method has been used in other text classification problems , as in \n\t\t']",Positive
"['\n\t\t In our case , the regularization6 is introduced in order to enforce the beliefs that : ry1 pz~ ry2 , if a1 , a2 synonyms ( 30 ) ry1 pz~ \x97ry2 , if a1 , a2 antonyms ( 31 ) For further information on regularized model fitting , see for instance , \n\t\t']",Positive
"['\n\t\t 5 Conclusion In business settings , there is growing interest in learning product reputations from the Internet . \n\t', '\n\t\t For such problems , it is often difficult or expensive to obtain labeled data . \n\t', '\n\t\t As a result , a change in modeling strategies is needed , towards approaches that require less supervision . \n\t', '\n\t\t In this paper we provide a framework for allowing human-provided information to be combined with unlabeled documents and labeled documents . \n\t', '\n\t\t We have found that this framework enables improvements over existing techniques , both in terms of the speed of model estimation and in classification accuracy . \n\t', '\n\t\t As a result , we believe that this is a promising new approach to problems of practical importance . \n\t', '\n\t\t References Kushal Dave , Steve Lawrence , and David M. Pennock . \n\t', '\n\t\t 2003. Mining the peanut gallery : Opinion extraction and semantic classification of product reviews . \n\t', '\n\t\t C. Fellbaum . \n\t', '\n\t\t 1998. Wordnet an electronic lexical database . \n\t', '\n\t\t T. Hastie , R. Tibshirani , and J. Friedman . \n\t', '\n\t\t 2001. The Elements of Statistical Learning : Data Mining , Inference , and Prediction . \n\t', '\n\t\t Springer-Verlag . \n\t', '\n\t\t Vasileios Hatzivassiloglou and Kathleen R. McKeown . \n\t', '\n\t\t 1997. Predicting the semantic orientation of adjectives . \n\t', '\n\t\t In Philip R. Cohen and Wolfgang Wahlster , editors , Proceedings of the Thirty-Fifth Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics , pages 174\x96181 , Somerset , New Jersey . \n\t', '\n\t\t Association for Computational Linguistics . \n\t', '\n\t\t 6By cross-validation , we choose the regularization term A = 1.5/sqrt(n) , where n is the number of labeled documents . \n\t', '\n\t\t Satoshi Morinaga , Kenji Yamanishi , Kenji Tateishi , and Toshikazu Fukushima . \n\t', '\n\t\t 2002. Mining product reputations on the web. Ng , A. Y. and Jordan , M. 2002 . \n\t', '\n\t\t On discriminative vs. generative classifiers : A comparison of logistic regression and naive bayes . \n\t', '\n\t\t Advances in Neural Information Processing Systems , 14 . \n\t', '\n\t\t Kamal Nigam , Andrew K. McCallum , Sebastian Thrun , and Tom M. Mitchell . \n\t', '\n\t\t 2000. Text classification from labeled and unlabeled documents using EM . \n\t', '\n\t\t Machine Learning , 39(2/3):103\x96134 . \n\t', '\n\t\t Bo Pang , Lillian Lee , and Shivakumar Vaithyanathan . \n\t', '\n\t\t 2002. Thumbs up ? \n\t', '\n\t\t sentiment classification using machine learning techniques . \n\t', '\n\t\t In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) . \n\t', '\n\t\t P.D. Turney and M.L. Littman . \n\t', '\n\t\t 2002. Unsupervised learning of semantic orientation from a hundredbillion-word corpus . \n\t', '\n\t\t Peter Turney . \n\t', '\n\t\t 2002. Thumbs up or thumbs down ? \n\t', '\n\t\t semantic orientation applied to unsupervised classification of reviews . \n\t', '\n\t\t In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics ( ACL\x9202 ) , pages 417\x96 424 , Philadelphia , Pennsylvania . \n\t', '\n\t\t Association for Computational Linguistics . \n\t', '\n\t\t Janyce Wiebe . \n\t', '\n\t\t 2000. Learning subjective adjectives from corpora . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 17th National Conference on Artificial Intelligence ( AAAI-2000 ) , Austin , Texas . \n\t', '\n\t\t Jian Zhang and Yiming Yang . \n\t', '\n\t\t 2003. \x94robustness of regularized linear classification methods in text categorization\x94 . \n\t', '\n\t\t In Proceedings of the 26th Annual International ACM SIGIR Conference ( SIGIR 2003 ) . \n\t', '\n\t\t A Sentimental Education : Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts Bo Pang and Lillian Lee Department of Computer Science Cornell University Ithaca , NY 14853-7501 1pabo,llee}@cs.cornell.edu Abstract Sentiment analysis seeks to identify the viewpoint(s) underlying a text span ; an example application is classifying a movie review as \x93thumbs up\x94 or \x93thumbs down\x94 . \n\t', '\n\t\t To determine this sentiment polarity , we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document . \n\t', '\n\t\t Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs ; this greatly facilitates incorporation of cross-sentence contextual constraints . \n\t', '\n\t\t 1 Introduction The computational treatment of opinion , sentiment , and subjectivity has recently attracted a great deal of attention ( see references ) , in part because of its potential applications . \n\t', '\n\t\t For instance , information- extraction and question-answering systems could flag statements and queries regarding opinions rather than facts \n\t\t']",Positive
"['\n\t\t Also , it has proven useful for companies , recommender systems , and editorial sites to create summaries of people\x92s experiences and opinions that consist of subjective expressions extracted from reviews ( as is commonly done in movie ads ) or even just a review\x92s polarity \x97 positive ( \x93thumbs up\x94 ) or negative ( \x93thumbs down\x94 ) . \n\t', '\n\t\t Document polarity classification poses a significant challenge to data-driven methods , resisting traditional text-categorization techniques \n\t\t']",Positive
"['\n\t\t Previous approaches focused on selecting indicative lexical features ( e.g. , the word \x93good\x94 ) , classifying a document according to the number of such features that occur anywhere within it . \n\t', '\n\t\t In contrast , we propose the following process : ( 1 ) label the sentences in the document as either subjective or objective , discarding the latter ; and then ( 2 ) apply a standard machine-learning classifier to the resulting extract . \n\t', '\n\t\t This can prevent the polarity classifier from considering irrelevant or even potentially misleading text : for example , although the sentence \x93The protagonist tries to protect her good name\x94 contains the word \x93good\x94 , it tells us nothing about the author\x92s opinion and in fact could well be embedded in a negative movie review . \n\t', '\n\t\t Also , as mentioned above , subjectivity extracts can be provided to users as a summary of the sentiment-oriented content of the document . \n\t', '\n\t\t Our results show that the subjectivity extracts we create accurately represent the sentiment information of the originating documents in a much more compact form : depending on choice of downstream polarity classifier , we can achieve highly statistically significant improvement ( from 82.8 % to 86.4 % ) or maintain the same level of performance for the polarity classification task while retaining only 60 % of the reviews\x92 words . \n\t', '\n\t\t Also , we explore extraction methods based on a minimum cut formulation , which provides an efficient , intuitive , and effective means for integrating inter-sentencelevel contextual information with traditional bag-ofwords features . \n\t', '\n\t\t 2 Method 2.1 Architecture One can consider document-level polarity classification to be just a special ( more difficult ) case of text categorization with sentiment- rather than topic-based categories . \n\t', '\n\t\t Hence , standard machine- learning classification techniques , such as support vector machines ( SVMs ) , can be applied to the entire documents themselves , as was done by Pang , Lee , and \n\t\t']",Positive
"['\n\t\t We refer to such classification techniques as default polarity classifiers . \n\t', '\n\t\t However , as noted above , we may be able to improve polarity classification by removing objective sentences ( such as plot summaries in a movie review ) . \n\t', '\n\t\t We therefore propose , as depicted in Figure 1 , to first employ a subjectivity detector that determines whether each sentence is subjective or not : discarding the objective ones creates an extract that should better represent a review\x92s subjective content to a default polarity classifier . \n\t', '\n\t\t Figure 1 : Polarity classification via subjectivity detection . \n\t', '\n\t\t To our knowledge , previous work has not integrated sentence-level subjectivity detection with document-level sentiment polarity . \n\t', '\n\t\t \n\t\t']",Positive
['\n\t\t The motivation behind the single- sentence selection method of \n\t\t'],Positive
"['\n\t\t 2.2 Context and Subjectivity Detection As with document-level polarity classification , we could perform subjectivity detection on individual sentences by applying a standard classification algorithm on each sentence in isolation . \n\t', '\n\t\t However , modeling proximity relationships between sentences would enable us to leverage coherence : text spans occurring near each other ( within discourse boundaries ) may share the same subjectivity status , other things being equal \n\t\t']",Positive
"['\n\t\t We would therefore like to supply our algorithms with pair-wise interaction information , e.g. , to specify that two particular sentences should ideally receive the same subjectivity label but not state which label this should be . \n\t', '\n\t\t Incorporating such information is somewhat unnatural for classifiers whose input consists simply of individual feature vectors , such as Naive Bayes or SVMs , precisely because such classifiers label each test item in isolation . \n\t', '\n\t\t One could define synthetic features or feature vectors to attempt to overcome this obstacle . \n\t', '\n\t\t However , we propose an alternative that avoids the need for such feature engineering : we use an efficient and intuitive graph-based formulation relying on finding minimum cuts . \n\t', '\n\t\t Our approach is inspired by \n\t\t']",Positive
"['\n\t\t 2.3 Cut-based classification Figure 2 shows a worked example of the concepts in this section . \n\t', '\n\t\t Suppose we have n items x1 , ... , xn to divide into two classes C1 and C2 , and we have access to two types of information : \x95 Individual scores indj ( xi ) : non-negative estimates of each xi\x92s preference for being in Cj based on just the features of xi alone ; and \x95 Association scores assoc(xi , xk ) : non-negative estimates of how important it is that xi and xk be in the same class.1 We would like to maximize each item\x92s \x93net happiness\x94 : its individual score for the class it is assigned to , minus its individual score for the other class . \n\t', '\n\t\t But , we also want to penalize putting tightly- associated items into different classes . \n\t', '\n\t\t Thus , after some algebra , we arrive at the following optimization problem : assign the xis to C1 and C2 so as to minimize the partition cost X ind2(x)+ X ind1 (x)+ X assoc(xi , xk ) . \n\t', '\n\t\t xEC1 xEC2 xi EC1 , xk EC2 The problem appears intractable , since there are 2n possible binary partitions of the xi\x92s . \n\t', '\n\t\t However , suppose we represent the situation in the following manner . \n\t', '\n\t\t Build an undirected graph G with vertices { v1 , ... , vn , s , t } ; the last two are , respectively , the source and sink . \n\t', '\n\t\t Add n edges ( s , vi ) , each with weight ind1 ( xi ) , and n edges ( vi , t ) , each with weight ind2(xi) . \n\t', '\n\t\t Finally , add ( 2 ) edges ( vi , vk ) , each with weight assoc(xi , xk ) . \n\t', ""\n\t\t Then , cuts in G are defined as follows : Definition 1 A cut ( 5 , T ) of G is a partition of its nodes into sets 5 = { s } U 5 ' and T = { t } U T ' , where s ^~ 5 ' , t ^~ T ' . \n\t"", '\n\t\t Its cost cost(5 , T ) is the sum of the weights of all edges crossing from 5 to T . \n\t', '\n\t\t A minimum cut of G is one of minimum cost . \n\t', '\n\t\t 1Asymmetry is allowed , but we used symmetric scores . \n\t', '\n\t\t subjective nsentence review sentence ? \n\t', '\n\t\t msentence extract ( m<=n ) positive or negative review ? \n\t', '\n\t\t s1 yes s2 no s1 s3 s4 no s4 yes +/ s n subjectivity extraction s ind1(M) [ \x955 ] M ind2(M) [ \x955 ] t ind1(Y) [ \x958 ] ind2(Y) [ \x952 ] ind1(N) [ \x951 ] ind2(N) [ \x959 ] assoc(Y,M) assoc(M,N) [ 1\x950 ] [ \x952 ] Y N assoc(Y,N) [ \x951 ] C1 Individual penalties Association penalties Cost { Y,M } .2+.5+.1 .1+.2 1.1 ( none ) .8+.5+.1 0 1.4 { Y,M,N } .2+.5+.9 0 1.6 { Y } .2+.5+.1 1.0+.1 1.9 { N } .8+.5+.9 .1+.2 2.5 { M } .8+.5+.1 1.0+.2 2.6 { Y,N } .2+.5+.9 1.0+.2 2.8 { M,N } .8+.5+.9 1.0+.1 3.3 Figure 2 : Graph for classifying three items . \n\t', '\n\t\t Brackets enclose example values ; here , the individual scores happen to be probabilities . \n\t', '\n\t\t Based on individual scores alone , we would put Y ( \x93yes\x94 ) in C1 , N ( \x93no\x94 ) in C2 , and be undecided about M ( \x93maybe\x94 ) . \n\t', '\n\t\t But the association scores favor cuts that put Y and M in the same class , as shown in the table . \n\t', '\n\t\t Thus , the minimum cut , indicated by the dashed line , places M together with Y in C1 . \n\t', '\n\t\t Observe that every cut corresponds to a partition of the items and has cost equal to the partition cost . \n\t', '\n\t\t Thus , our optimization problem reduces to finding minimum cuts . \n\t', '\n\t\t Practical advantages As we have noted , formulating our subjectivity-detection problem in terms of graphs allows us to model item-specific and pair- wise information independently . \n\t', '\n\t\t Note that this is a very flexible paradigm . \n\t', '\n\t\t For instance , it is perfectly legitimate to use knowledge-rich algorithms employing deep linguistic knowledge about sentiment indicators to derive the individual scores . \n\t', '\n\t\t And we could also simultaneously use knowledge- lean methods to assign the association scores . \n\t', '\n\t\t Interestingly , \n\t\t']",Positive
"['\n\t\t But a crucial advantage specific to the utilization of a minimum-cut-based approach is that we can use maximum -flow algorithms with polynomial asymptotic running times \x97 and near-linear running times in practice \x97 to exactly compute the minimum- cost cut(s) , despite the apparent intractability of the optimization problem \n\t\t']",Positive
"['\n\t\t 2Code available at http://www.avglab.com/andrew/soft.html . \n\t', '\n\t\t 3Graph-based approaches to general clustering problems are too numerous to mention here . \n\t', '\n\t\t 3 Evaluation Framework Our experiments involve classifying movie reviews as either positive or negative , an appealing task for several reasons . \n\t', '\n\t\t First , as mentioned in the introduction , providing polarity information about reviews is a useful service : witness the popularity of www.rottentomatoes.com . \n\t', '\n\t\t Second , movie reviews are apparently harder to classify than reviews of other products \n\t\t']",Positive
"['\n\t\t Third , the correct label can be extracted automatically from rating information ( e.g. , number of stars ) . \n\t', '\n\t\t Our data4 contains 1000 positive and 1000 negative reviews all written before 2002 , with a cap of 20 reviews per author ( 312 authors total ) per category . \n\t', '\n\t\t We refer to this corpus as the polarity dataset . \n\t', '\n\t\t Default polarity classifiers We tested support vector machines ( SVMs ) and Naive Bayes ( NB ) . \n\t', '\n\t\t Following \n\t\t']",Positive
"['\n\t\t ( For SVMs , the feature vectors are length-normalized ) . \n\t', '\n\t\t Each default document- level polarity classifier is trained and tested on the extracts formed by applying one of the sentence- level subjectivity detectors to reviews in the polarity dataset . \n\t', '\n\t\t Subjectivity dataset To train our detectors , we need a collection of labeled sentences . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t been so annotated.5 Fortunately , we were able to mine the Web to create a large , automatically- labeled sentence corpus6 . \n\t', '\n\t\t To gather subjective sentences ( or phrases ) , we collected 5000 movie- review snippets ( e.g. , \x93bold , imaginative , and impossible to resist\x94 ) from www.rottentomatoes.com . \n\t', '\n\t\t To obtain ( mostly ) objective data , we took 5000 sentences from plot summaries available from the Internet Movie Database ( www.imdb.com ) . \n\t', '\n\t\t We only selected sentences or snippets at least ten words long and drawn from reviews or plot summaries of movies released post-2001 , which prevents overlap with the polarity dataset . \n\t', '\n\t\t Subjectivity detectors As noted above , we can use our default polarity classifiers as \x93basic\x94 sentence- level subjectivity detectors ( after retraining on the subjectivity dataset ) to produce extracts of the original reviews . \n\t', '\n\t\t We also create a family of cut-based subjectivity detectors ; these take as input the set of sentences appearing in a single document and determine the subjectivity status of all the sentences simultaneously using per-item and pairwise relationship information . \n\t', '\n\t\t Specifically , for a given document , we use the construction in Section 2.2 to build a graph wherein the source s and sink t correspond to the class of subjective and objective sentences , respectively , and each internal node vi corresponds to the document\x92s ith sentence si . \n\t', '\n\t\t We can set the individual scores ind1(si) to Pr .b(si) and ind2(si) to 1 \x97 Pr ub(si) , as shown in Figure 3 , where Pr Ub(s) denotes Naive Bayes\x92 estimate of the probability that sentence s is subjective ; or , we can use the weights produced by the SVM classifier instead.7 If we set all the association scores to zero , then the minimum-cut classification of the sentences is the same as that of the basic subjectivity detector . \n\t', '\n\t\t Alternatively , we incorporate the degree of proximity between pairs of sentences , controlled by three parameters . \n\t', '\n\t\t The threshold T specifies the maximum distance two sentences can be separated by and still be considered proximal . \n\t', '\n\t\t The 5We therefore could not directly evaluate sentence- classification accuracy on the polarity dataset . \n\t', '\n\t\t 6Available at www.cs.cornell.edu/people/pabo/moviereview-data/ , sentence corpus version 1.0. 7We converted SVM output di , which is a signed distance ( negative=objective ) from the separating hyperplane , to non- negative numbers by def r1 di>2 ; ind1(si) = S ( 2 + di)/4 \x972 <_ di <_ 2 ; l 0 di < \x972 . \n\t', '\n\t\t and ind2(si) = 1 \x97 ind1(si) . \n\t', '\n\t\t Note that scaling is employed only for consistency ; the algorithm itself does not require probabilities for individual scores . \n\t', '\n\t\t non-increasing function f ( d ) specifies how the influence of proximal sentences decays with respect to distance d ; in our experiments , we tried f ( d ) = 1 , e1\x97d , and 1/d2 . \n\t', '\n\t\t The constant c controls the relative influence of the association scores : a larger c makes the minimum-cut algorithm more loath to put proximal sentences in different classes . \n\t', '\n\t\t With these in hand 8 , we set ( for j > i ) assoc(si , sj)=l def f f(j\x97i)\x95c if(j\x97i)<T ; 0 otherwise . \n\t', '\n\t\t 4 Experimental Results Below , we report average accuracies computed by ten-fold cross-validation over the polarity dataset . \n\t', '\n\t\t Section 4.1 examines our basic subjectivity extraction algorithms , which are based on individual- sentence predictions alone . \n\t', '\n\t\t Section 4.2 evaluates the more sophisticated form of subjectivity extraction that incorporates context information via the minimum-cut paradigm . \n\t', '\n\t\t As we will see , the use of subjectivity extracts can in the best case provide satisfying improvement in polarity classification , and otherwise can at least yield polarity-classification accuracies indistinguishable from employing the full review . \n\t', '\n\t\t At the same time , the extracts we create are both smaller on average than the original document and more effective as input to a default polarity classifier than the same-length counterparts produced by standard summarization tactics ( e.g. , first- or last-N sentences ) . \n\t', '\n\t\t We therefore conclude that subjectivity extraction produces effective summaries of document sentiment . \n\t', '\n\t\t 4.1 Basic subjectivity extraction As noted in Section 3 , both Naive Bayes and SVMs can be trained on our subjectivity dataset and then used as a basic subjectivity detector . \n\t', '\n\t\t The former has somewhat better average ten-fold cross-validation performance on the subjectivity dataset ( 92 % vs. 90 % ) , and so for space reasons , our initial discussions will focus on the results attained via NB subjectivity detection . \n\t', '\n\t\t Employing Naive Bayes as a subjectivity detector ( ExtractNB ) in conjunction with a Naive Bayes document-level polarity classifier achieves 86.4 % accuracy.9 This is a clear improvement over the 82.8 % that results when no extraction is applied 8 Parameter training is driven by optimizing the performance of the downstream polarity classifier rather than the detector itself because the subjectivity dataset\x92s sentences come from different reviews , and so are never proximal . \n\t', '\n\t\t 9This result and others are depicted in Figure 5 ; for now , consider only the y-axis in those plots . \n\t', '\n\t\t nsentence review Figure 3 : Graph-cut-based creation of subjective extracts . \n\t', '\n\t\t edge crossing the cut Pr(s1) v1 1\x97PrNB(s1) subsu ^^ v1 msentence extract ( m<=n ) construct graph ^ ~ s v2 v3 vn t compute ^^min . \n\t', '\n\t\t cut s t ^ ^extract v2 v3 vn create s1 s4 individual subjectivityprobability link proximity link s1 s2 s3 s4 sn ( Full review ) ; indeed , the difference is highly statistically significant ( p < 0.01 , paired t-test ) . \n\t', '\n\t\t With SVMs as the polarity classifier instead , the Full review performance rises to 87.15 % , but comparison via the paired t-test reveals that this is statistically indistinguishable from the 86.4 % that is achieved by running the SVM polarity classifier on ExtractNB input . \n\t', '\n\t\t ( More improvements to extraction performance are reported later in this section . \n\t', '\n\t\t ) These findings indicate10 that the extracts preserve ( and , in the NB polarity-classifier case , apparently clarify ) the sentiment information in the originating documents , and thus are good summaries from the polarity-classification point of view . \n\t', '\n\t\t Further support comes from a \x93flipping\x94 experiment : if we give as input to the default polarity classifier an extract consisting of the sentences labeled objective , accuracy drops dramatically to 71 % for NB and 67 % for SVMs . \n\t', '\n\t\t This confirms our hypothesis that sentences discarded by the subjectivity extraction process are indeed much less indicative of sentiment polarity . \n\t', '\n\t\t Moreover , the subjectivity extracts are much more compact than the original documents ( an important feature for a summary to have ) : they contain on average only about 60 % of the source reviews\x92 words . \n\t', '\n\t\t ( This word preservation rate is plotted along the x-axis in the graphs in Figure 5. ) This prompts us to study how much reduction of the original documents subjectivity detectors can perform and still accurately represent the texts\x92 sentiment information . \n\t', '\n\t\t We can create subjectivity extracts of varying lengths by taking just the N most subjective sentences11 from the originating review . \n\t', '\n\t\t As one base- 10Recall that direct evidence is not available because the polarity dataset\x92s sentences lack subjectivity labels . \n\t', '\n\t\t 11 These are the N sentences assigned the highest probability by the basic NB detector , regardless of whether their probabil line to compare against , we take the canonical summarization standard of extracting the first N sentences \x97 in general settings , authors often begin documents with an overview . \n\t', '\n\t\t We also consider the last N sentences : in many documents , concluding material may be a good summary , and www.rottentomatoes.com tends to select \x93snippets\x94 from the end of movie reviews \n\t\t']",Positive
"['\n\t\t Finally , as a sanity check , we include results from the N least subjective sentences according to Naive Bayes . \n\t', '\n\t\t Figure 4 shows the polarity classifier results as N ranges between 1 and 40 . \n\t', '\n\t\t Our first observation is that the NB detector provides very good \x93bang for the buck\x94 : with subjectivity extracts containing as few as 15 sentences , accuracy is quite close to what one gets if the entire review is used . \n\t', '\n\t\t In fact , for the NB polarity classifier , just using the 5 most subjective sentences is almost as informative as the Full review while containing on average only about 22 % of the source reviews\x92 words . \n\t', '\n\t\t Also , it so happens that at N = 30 , performance is actually slightly better than ( but statistically indistinguishable from ) Full review even when the SVM default polarity classifier is used ( 87.2 % vs. 87.15%).12 This suggests potentially effective extraction alternatives other than using a fixed probability threshold ( which resulted in the lower accuracy of 86.4 % reported above ) . \n\t', '\n\t\t Furthermore , we see in Figure 4 that the N mostsubjective-sentences method generally outperforms the other baseline summarization methods ( which perhaps suggests that sentiment summarization cannot be treated the same as topic-based summariza- ities exceed 50 % and so would actually be classified as subjective by Naive Bayes . \n\t', '\n\t\t For reviews with fewer than N sentences , the entire review will be returned . \n\t', '\n\t\t 12 Note that roughly half of the documents in the polarity dataset contain more than 30 sentences ( average=32.3 , standard deviation 15 ) . \n\t', '\n\t\t Accuracy for N-sentence abstracts ( def = NB ) Accuracy for N-sentence abstracts ( def = SVM ) 90 85 80 75 70 65 60 55 90 85 80 75 70 65 60 55 most subjective N sentences last N sentences first N sentences least subjective N sentences Full review most subjective N sentences last N sentences first N sentences least subjective N sentences Full review 1 5 10 15 20 25 30 35 40 N 1 5 10 15 20 25 30 35 40 N Figure 4 : Accuracies using N-sentence extracts for NB ( left ) and SVM ( right ) default polarity classifiers . \n\t', '\n\t\t Accuracy for subjective abstracts ( def = NB ) Accuracy for subjective abstracts ( def = SVM ) Full Review difference in accuracy ExtractSVM indicates statistically significant improvement in accuracy ExtractNB+Prox ExtractNB not statistically significant ExtractSVM+Prox 0.6 0.7 0.8 0.9 1 1.1 % of words extracted 87 86.5 86 85.5 84.5 84 83.5 83 85 ExtractNB ExtractSVM indicates statistically significant improvement in accuracy ExtractNB+Prox ExtractSVM+Prox difference in accuracy not statistically significant Full Review 87 86.5 86 85.5 84.5 84 83.5 83 85 0.6 0.7 0.8 0.9 1 1.1 % of words extracted Figure 5 : Word preservation rate vs. accuracy , NB ( left ) and SVMs ( right ) as default polarity classifiers . \n\t', '\n\t\t Also indicated are results for some statistical significance tests . \n\t', '\n\t\t tion , although this conjecture would need to be verified on other domains and data ) . \n\t', '\n\t\t It\x92s also interesting to observe how much better the last N sentences are than the first N sentences ; this may reflect a ( hardly surprising ) tendency for movie-review authors to place plot descriptions at the beginning rather than the end of the text and conclude with overtly opinionated statements . \n\t', '\n\t\t 4.2 Incorporating context information The previous section demonstrated the value of subjectivity detection . \n\t', '\n\t\t We now examine whether context information , particularly regarding sentence proximity , can further improve subjectivity extraction . \n\t', '\n\t\t As discussed in Section 2.2 and 3 , contextual constraints are easily incorporated via the minimum-cut formalism but are not natural inputs for standard Naive Bayes and SVMs . \n\t', '\n\t\t Figure 5 shows the effect of adding in proximity information . \n\t', '\n\t\t ExtractNB+Prox and ExtractSVM+Prox are the graph-based subjectivity detectors using Naive Bayes and SVMs , respectively , for the individual scores ; we depict the best performance achieved by a single setting of the three proximity-related edge-weight parameters over all ten data folds13 ( parameter selection was not a focus of the current work ) . \n\t', '\n\t\t The two comparisons we are most interested in are ExtractNB+Prox versus ExtractNB and ExtractSVM+Prox versus ExtractSVM . \n\t', '\n\t\t We see that the context-aware graph-based subjectivity detectors tend to create extracts that are more informative ( statistically significant so ( paired t-test ) for SVM subjectivity detectors only ) , although these extracts are longer than their context- blind counterparts . \n\t', '\n\t\t We note that the performance 13 Parameters are chosen from T E { 1 , 2 , 3 } , f ( d ) E { 1 , e1-d,1/d2 } , and c E [ 0 , 1 ] at intervals of 0.1. enhancements cannot be attributed entirely to the mere inclusion of more sentences regardless of whether they are subjective or not \x97 one counter- argument is that Full review yielded substantially worse results for the NB default polarity classifier\x97 and at any rate , the graph-derived extracts are still substantially more concise than the full texts . \n\t', '\n\t\t Now , while incorporating a bias for assigning nearby sentences to the same category into NB and SVM subjectivity detectors seems to require some non-obvious feature engineering , we also wish to investigate whether our graph-based paradigm makes better use of contextual constraints that can be ( more or less ) easily encoded into the input of standard classifiers . \n\t', '\n\t\t For illustrative purposes , we consider paragraph-boundary information , looking only at SVM subjectivity detection for simplicity\x92s sake . \n\t', '\n\t\t It seems intuitively plausible that paragraph boundaries ( an approximation to discourse boundaries ) loosen coherence constraints between nearby sentences . \n\t', '\n\t\t To capture this notion for minimum-cutbased classification , we can simply reduce the association scores for all pairs of sentences that occur in different paragraphs by multiplying them by a cross-paragraph-boundary weight w E [ 0 , 1 ] . \n\t', '\n\t\t For standard classifiers , we can employ the trick of having the detector treat paragraphs , rather than sentences , as the basic unit to be labeled . \n\t', '\n\t\t This enables the standard classifier to utilize coherence between sentences in the same paragraph ; on the other hand , it also ( probably unavoidably ) poses a hard constraint that all of a paragraph\x92s sentences get the same label , which increases noise sensitivity . \n\t', '\n\t\t 14 Our experiments reveal the graph-cut formulation to be the better approach : for both default polarity classifiers ( NB and SVM ) , some choice of parameters ( including w ) for ExtractSVM+Prox yields statistically significant improvement over its paragraph- unit non-graph counterpart ( NB : 86.4 % vs. 85.2 % ; SVM : 86.15 % vs. 85.45 % ) . \n\t', '\n\t\t 5 Conclusions We examined the relation between subjectivity detection and polarity classification , showing that subjectivity detection can compress reviews into much shorter extracts that still retain polarity information at a level comparable to that of the full review . \n\t', '\n\t\t In fact , for the Naive Bayes polarity classifier , the subjectivity extracts are shown to be more effective input than the originating document , which suggests 14 For example , in the data we used , boundaries may have been missed due to malformed html. that they are not only shorter , but also \x93cleaner\x94 representations of the intended polarity . \n\t', '\n\t\t We have also shown that employing the minimum-cut framework results in the development of efficient algorithms for sentiment analysis . \n\t', '\n\t\t Utilizing contextual information via this framework can lead to statistically significant improvement in polarity-classification accuracy . \n\t', '\n\t\t Directions for future research include developing parameter- selection techniques , incorporating other sources of contextual cues besides sentence proximity , and investigating other means for modeling such information . \n\t', '\n\t\t Acknowledgments We thank Eric Breck , Claire Cardie , Rich Caruana , Yejin Choi , Shimon Edelman , Thorsten Joachims , Jon Kleinberg , Oren Kurland , Art Munson , Vincent Ng , Fernando Pereira , Ves Stoyanov , Ramin Zabih , and the anonymous reviewers for helpful comments . \n\t', '\n\t\t This paper is based upon work supported in part by the National Science Foundation under grants ITR/IM IIS-0081334 and IIS-0329064 , a Cornell Graduate Fellowship in Cognitive Studies , and by an Alfred P. Sloan Research Fellowship . \n\t', '\n\t\t Any opinions , findings , and conclusions or recommendations expressed above are those of the authors and do not necessarily reflect the views of the National Science Foundation or Sloan Foundation . \n\t', '\n\t\t References Agrawal , Rakesh , Sridhar Rajagopalan , Ramakrishnan Srikant , and Yirong Xu . \n\t', '\n\t\t 2003. Mining newsgroups using networks arising from social behavior . \n\t', '\n\t\t In WWW , pages 529\x96535 . \n\t', '\n\t\t Ahuja , Ravindra , Thomas L. Magnanti , and James B. Orlin . \n\t', '\n\t\t 1993. Network Flows : Theory , Algorithms , and Applications . \n\t', '\n\t\t Prentice Hall . \n\t', '\n\t\t Beineke , Philip , Trevor Hastie , Christopher Manning , and Shivakumar Vaithyanathan . \n\t', '\n\t\t 2004. Exploring sentiment summarization . \n\t', '\n\t\t In AAAI Spring Symposium on Exploring Attitude and Affect in Text : Theories and Applications ( AAAI tech report SS-04-07 ) . \n\t', '\n\t\t Blum , Avrim and Shuchi Chawla. 2001 . \n\t', '\n\t\t Learning from labeled and unlabeled data using graph min- cuts . \n\t', '\n\t\t In Intl . \n\t', '\n\t\t Conf . \n\t', '\n\t\t on Machine Learning ( ICML ) , pages 19\x9626 . \n\t', '\n\t\t Boykov , Yuri , Olga Veksler , and Ramin Zabih . \n\t', '\n\t\t 1999. Fast approximate energy minimization via graph cuts . \n\t', '\n\t\t In Intl . \n\t', '\n\t\t Conf . \n\t', '\n\t\t on Computer Vision ( ICCV ) , pages 377\x96384 . \n\t', '\n\t\t Journal version in IEEE Trans . \n\t', '\n\t\t Pattern Analysis and Machine Intelligence ( PAMI ) 23(11):1222\x961239 , 2001 . \n\t', '\n\t\t Cardie , Claire , Janyce Wiebe , Theresa Wilson , and Diane Litman . \n\t', '\n\t\t 2003. Combining low-level and summary representations of opinions for multi- perspective question answering . \n\t', '\n\t\t In AAAI Spring Symposium on New Directions in Question Answering , pages 20\x9627 . \n\t', '\n\t\t Cormen , Thomas H. , Charles E. Leiserson , and Ronald L. Rivest . \n\t', '\n\t\t 1990. Introduction to Algorithms . \n\t', '\n\t\t MIT Press . \n\t', '\n\t\t Das , Sanjiv and Mike Chen . \n\t', '\n\t\t 2001. Yahoo ! \n\t', '\n\t\t for Amazon : Extracting market sentiment from stock message boards . \n\t', '\n\t\t In Asia Pacific Finance Association Annual Conf . \n\t', '\n\t\t ( APFA ) . \n\t', '\n\t\t Dave , Kushal , Steve Lawrence , and David M. Pennock . \n\t', '\n\t\t 2003. Mining the peanut gallery : Opinion extraction and semantic classification of product reviews . \n\t', '\n\t\t In WWW , pages 519\x96528 . \n\t', '\n\t\t Dini , Luca and Giampaolo Mazzini . \n\t', '\n\t\t 2002. Opinion classification through information extraction . \n\t', '\n\t\t In Intl . \n\t', '\n\t\t Conf . \n\t', '\n\t\t on Data Mining Methods and Databases for Engineering , Finance and Other Fields , pages 299\x96310 . \n\t', '\n\t\t Durbin , Stephen D. , J. Neal Richter , and Doug Warner . \n\t', '\n\t\t 2003. A system for affective rating of texts . \n\t', '\n\t\t In KDD Wksp. on Operational Text Classification Systems ( OTC-3 ) . \n\t', '\n\t\t Hatzivassiloglou , Vasileios and Kathleen McKeown . \n\t', '\n\t\t 1997. Predicting the semantic orientation of adjectives . \n\t', '\n\t\t In 35th ACL/8th EACL , pages 174\x96181 . \n\t', '\n\t\t Joachims , Thorsten . \n\t', '\n\t\t 2003. Transductive learning via spectral graph partitioning . \n\t', '\n\t\t In Intl . \n\t', '\n\t\t Conf . \n\t', '\n\t\t on Machine Learning ( ICML ) . \n\t', '\n\t\t Liu , Hugo , Henry Lieberman , and Ted Selker . \n\t', '\n\t\t 2003. A model of textual affect sensing using real-world knowledge . \n\t', '\n\t\t In Intelligent User Interfaces ( IUI ) , pages 125\x96132 . \n\t', '\n\t\t Montes-y-G´omez , Manuel , Aurelio L´opez-L´opez , and Alexander Gelbukh . \n\t', '\n\t\t 1999. Text mining as a social thermometer . \n\t', '\n\t\t In IJCAI Wksp. on Text Mining , pages 103\x96107 . \n\t', '\n\t\t Morinaga , Satoshi , Kenji Yamanishi , Kenji Tateishi , and Toshikazu Fukushima . \n\t', '\n\t\t 2002. Mining product reputations on the web. . \n\t', '\n\t\t In KDD , pages 341\x96 349 . \n\t', '\n\t\t Industry track . \n\t', '\n\t\t Pang , Bo , Lillian Lee , and Shivakumar Vaithyanathan . \n\t', '\n\t\t 2002. Thumbs up ? \n\t', '\n\t\t Sentiment classification using machine learning techniques . \n\t', '\n\t\t In EMNLP , pages 79\x9686 . \n\t', '\n\t\t Qu , Yan , James Shanahan , and Janyce Wiebe , editors . \n\t', '\n\t\t 2004. AAAI Spring Symposium on Exploring Attitude and Affect in Text : Theories and Applications . \n\t', '\n\t\t AAAI technical report SS-04-07 . \n\t', '\n\t\t Riloff , Ellen and Janyce Wiebe . \n\t', '\n\t\t 2003. Learning extraction patterns for subjective expressions . \n\t', '\n\t\t In EMNLP . \n\t', '\n\t\t Riloff , Ellen , Janyce Wiebe , and Theresa Wilson . \n\t', '\n\t\t 2003. Learning subjective nouns using extraction pattern bootstrapping . \n\t', '\n\t\t In Conf . \n\t', '\n\t\t on Natural Language Learning ( CoNLL ) , pages 25\x9632 . \n\t', '\n\t\t Subasic , Pero and Alison Huettner . \n\t', '\n\t\t 2001. Affect analysis of text using fuzzy semantic typing . \n\t', '\n\t\t IEEE Trans . \n\t', '\n\t\t Fuzzy Systems , 9(4):483\x96496 . \n\t', '\n\t\t Tong , Richard M. 2001 . \n\t', '\n\t\t An operational system for detecting and tracking opinions in on-line discussion . \n\t', '\n\t\t SIGIR Wksp. on Operational Text Classification . \n\t', '\n\t\t Turney , Peter . \n\t', '\n\t\t 2002. Thumbs up or thumbs down ? \n\t', '\n\t\t Semantic orientation applied to unsupervised classification of reviews . \n\t', '\n\t\t In ACL , pages 417\x96424 . \n\t', '\n\t\t Wiebe , Janyce M. 1994 . \n\t', '\n\t\t Tracking point of view in narrative . \n\t', '\n\t\t Computational Linguistics , 20(2):233\x96 287 . \n\t', '\n\t\t Yi , Jeonghee , Tetsuya Nasukawa , Razvan Bunescu , and Wayne Niblack . \n\t', '\n\t\t 2003 . \n\t', '\n\t\t Sentiment analyzer : Extracting sentiments about a given topic using natural language processing techniques . \n\t', '\n\t\t In IEEE Intl . \n\t', '\n\t\t Conf . \n\t', '\n\t\t on Data Mining ( ICDM ) . \n\t', '\n\t\t Yu , Hong and Vasileios Hatzivassiloglou . \n\t', '\n\t\t 2003. Towards answering opinion questions : Separating facts from opinions and identifying the polarity of opinion sentences . \n\t', '\n\t\t In EMNLP . \n\t', '\n\t\t Finding Predominant Word Senses in Untagged Text Diana McCarthy & Rob Koeling & Julie Weeds & John Carroll Department of Informatics , University of Sussex Brighton BN1 9QH , UK dianam,robk,juliewe,johnca @sussex.ac.uk Abstract In word sense disambiguation ( WSD ) , the heuristic of choosing the most common sense is extremely powerful because the distribution of the senses of a word is often skewed . \n\t', '\n\t\t The problem with using the predominant , or first sense heuristic , aside from the fact that it does not take surrounding context into account , is that it assumes some quantity of hand- tagged data . \n\t', '\n\t\t Whilst there are a few hand-tagged corpora available for some languages , one would expect the frequency distribution of the senses of words , particularly topical words , to depend on the genre and domain of the text under consideration . \n\t', '\n\t\t We present work on the use of a thesaurus acquired from raw textual corpora and the WordNet similarity package to find predominant noun senses automatically . \n\t', '\n\t\t The acquired predominant senses give a precision of 64 % on the nouns of the SENSEVAL2 English all-words task . \n\t', '\n\t\t This is a very promising result given that our method does not require any hand-tagged text , such as SemCor . \n\t', '\n\t\t Furthermore , we demonstrate that our method discovers appropriate predominant senses for words from two domain- specific corpora . \n\t', '\n\t\t 1 Introduction The first sense heuristic which is often used as a baseline for supervised WSD systems outperforms many of these systems which take surrounding context into account . \n\t', '\n\t\t This is shown by the results of the English all-words task in SENSEVAL-2 \n\t\t']",Positive
['\n\t\t The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor \n\t\t'],Positive
"['\n\t\t Senses that have not occurred in SemCor are ordered arbitrarily and after those senses of the word that have occurred . \n\t', '\n\t\t The figure distinguishes systems which make use of hand-tagged data ( using HTD ) such as SemCor , from those that do not ( without HTD ) . \n\t', '\n\t\t The high per- formance of the first sense baseline is due to the skewed frequency distribution of word senses . \n\t', '\n\t\t Even systems which show superior performance to this heuristic often make use of the heuristic where evidence from the context is not sufficient \n\t\t']",Positive
"['\n\t\t Whilst a first sense heuristic based on a sense-tagged corpus such as SemCor is clearly useful , there is a strong case for obtaining a first , or predominant , sense from untagged corpus data so that a WSD system can be tuned to the genre or domain at hand . \n\t', '\n\t\t SemCor comprises a relatively small sample of 250,000 words . \n\t', '\n\t\t There are words where the first sense in WordNet is counter-intuitive , because of the size of the corpus , and because where the frequency data does not indicate a first sense , the ordering is arbitrary . \n\t', '\n\t\t For example the first sense of tiger in WordNet is audacious person whereas one might expect that carnivorous animal is a more common usage . \n\t', '\n\t\t There are only a couple of instances of tiger within SemCor . \n\t', '\n\t\t Another example is embryo , which does not occur at all in SemCor and the first sense is listed as rudimentary plant rather than the anticipated fertilised egg meaning . \n\t', '\n\t\t We believe that an automatic means of finding a predominant sense would be useful for systems that use it as a means of backing-off \n\t\t']",Positive
"['\n\t\t More importantly , when working within a specific domain one would wish to tune the first sense heuristic to the domain at hand . \n\t', '\n\t\t The first sense of star in SemCor is celestial body , however , if one were disambiguating popular news celebrity would be preferred . \n\t', '\n\t\t Assuming that one had an accurate WSD system then one could obtain frequency counts for senses and rank them with these counts . \n\t', '\n\t\t However , the most accurate WSD systems are those which require manually sense tagged data in the first place , and their accuracy depends on the quantity of training examples \n\t\t']",Positive
"['\n\t\t We First Sense 0 20 40 60 80 100 precision "" using HTE "" "" without HTE "" "" First Sense "" Figure 1 : The first sense heuristic compared with the SENSEVAL-2 English all-words task results are therefore investigating a method of automatically ranking WordNet senses from raw text . \n\t', '\n\t\t Many researchers are developing thesauruses from automatically parsed data . \n\t', '\n\t\t In these each target word is entered with an ordered list of \x93nearest neighbours\x94 . \n\t', '\n\t\t The neighbours are words ordered in terms of the \x93distributional similarity\x94 that they have with the target . \n\t', '\n\t\t Distributional similarity is a measure indicating the degree that two words , a word and its neighbour , occur in similar contexts . \n\t', '\n\t\t From inspection , one can see that the ordered neighbours of such a thesaurus relate to the different senses of the target word . \n\t', '\n\t\t For example , the neighbours of star in a dependency-based thesaurus provided by Lin 1 has the ordered list of neighbours : superstar , player , teammate , actor early in the list , but one can also see words that are related to another sense of star e.g. galaxy , sun , world and planet further down the list . \n\t', '\n\t\t We expect that the quantity and similarity of the neighbours pertaining to different senses will reflect the dominance of the sense to which they pertain . \n\t', '\n\t\t This is because there will be more relational data for the more prevalent senses compared to the less frequent senses . \n\t', '\n\t\t In this paper we describe and evaluate a method for ranking senses of nouns to obtain the predominant sense of a word using the neighbours from automatically acquired thesauruses . \n\t', '\n\t\t The neighbours for a word in a thesaurus are words themselves , rather than senses . \n\t', '\n\t\t In order to associate the neighbours with senses we make use of another notion of similarity , \x93semantic similarity\x94 , which exists between senses , rather than words . \n\t', '\n\t\t We experiment with several WordNet Similarity measures \n\t\t']",Positive
"['\n\t\t We use WordNet as our sense inventory for this work . \n\t', '\n\t\t The paper is structured as follows . \n\t', '\n\t\t We discuss our method in the following section . \n\t', '\n\t\t Sections 3 and 4 concern experiments using predominant senses from the BNC evaluated against the data in SemCor and the SENSEVAL-2 English all-words task respectively . \n\t', '\n\t\t In section 5 we present results of the method on two domain specific sections of the Reuters corpus for a sample of words . \n\t', '\n\t\t We describe some related work in section 6 and conclude in section 7 . \n\t', '\n\t\t 2 Method In order to find the predominant sense of a target word we use a thesaurus acquired from automatically parsed text based on the method of \n\t\t']",Positive
"['\n\t\t This provides the nearest neighbours to each target word , along with the distributional similarity score between the target word and its neighbour . \n\t', '\n\t\t We then use the WordNet similarity package \n\t\t']",Positive
"['\n\t\t To find the first sense of a word ( ) we take each sense in turn and obtain a score reflecting the prevalence which is used for rank- ing . \n\t', '\n\t\t Let be the ordered set of the top scoring neighbours of from the thesaurus with associated distributional similarity scores ( 1 ) 2.1 Acquiring the Automatic Thesaurus The thesaurus was acquired using the method described by \n\t\t']",Positive
"['\n\t\t For input we used grammatical relation data extracted using an automatic where : 100 80 60 40 20 0 . \n\t', '\n\t\t Let be the set of senses of . \n\t', '\n\t\t For each sense of ( ) we obtain a rank- ing score by summing over the of each neighbour ( ) multiplied by a weight . \n\t', '\n\t\t This weight is the WordNet similarity score ( ) between the target sense ( ) and the sense of ( ) that maximises this score , divided by the sum of all such WordNet similarity scores for and . \n\t', '\n\t\t Thus we rank each sense using : parser \n\t\t']",Positive
"['\n\t\t For the experiments in sections 3 and 4 we used the 90 million words of written English from the BNC. . \n\t', '\n\t\t For each noun we considered the co-occurring verbs in the direct object and subject relation , the modifying nouns in noun-noun relations and the modifying adjectives in adjective-noun relations . \n\t', '\n\t\t We could easily extend the set of relations in the future . \n\t', '\n\t\t A noun , , is thus described by a set of co-occurrence triples and associated frequencies , where is a grammatical relation and is a possible co- occurrence with in that relation . \n\t', '\n\t\t For every pair of nouns , where each noun had a total frequency in the triple data of 10 or more , we computed their distributional similarity using the measure given by \n\t\t']",Positive
"['\n\t\t If is the set of co-occurrence types such that is positive then the simi- larity between two nouns , and , can be computed as : where : A thesaurus entry of size for a target noun is then defined as the most similar nouns to. 2.2 The WordNet Similarity Package We use the WordNet Similarity Package 0.05 and WordNet version 1.6.2 The WordNet Similarity package supports a range of WordNet similarity scores . \n\t', '\n\t\t We experimented using six of these to provide the in equation 1 above and obtained results well over our baseline , but because of space limitations give results for the two which perform the best . \n\t', '\n\t\t We briefly summarise the two measures here ; for a more detailed summary see \n\t\t']",Positive
"['\n\t\t The measures provide a similarity score between two WordNet senses ( and ) , these being synsets within WordNet . \n\t', '\n\t\t lesk \n\t\t']",Positive
"['\n\t\t It uses the glosses of semantically related ( according to Word- Net ) senses too . \n\t', '\n\t\t jcn \n\t\t']",Positive
"['\n\t\t Each 2We use this version of WordNet since it allows us to map information to WordNets of other languages more accurately . \n\t', '\n\t\t We are of course able to apply the method to other versions of WordNet . \n\t', '\n\t\t synset , is incremented with the frequency counts from the corpus of all words belonging to that synset , directly or via the hyponymy relation . \n\t', '\n\t\t The frequency data is used to calculate the \x93information content\x94 ( IC ) of a class . \n\t', '\n\t\t Jiang and Conrath specify a distance measure : , where the third class ( ) is the most informative , or most specific , superordinate synset of the two senses and . \n\t', '\n\t\t This is transformed from a distance measure in the WN-Similarity package by taking the reciprocal : 3 Experiment with SemCor In order to evaluate our method we use the data in SemCor as a gold-standard . \n\t', '\n\t\t This is not ideal since we expect that the sense frequency distributions within SemCor will differ from those in the BNC , from which we obtain our thesaurus . \n\t', '\n\t\t Nevertheless , since many systems performed well on the English all-words task for SENSEVAL-2 by using the frequency information in SemCor this is a reasonable approach for evaluation . \n\t', '\n\t\t We generated a thesaurus entry for all polysemous nouns which occurred in SemCor with a frequency 2 , and in the BNC with a frequency 10 in the grammatical relations listed in section 2.1 above . \n\t', '\n\t\t The jcn measure uses corpus data for the calculation of IC . \n\t', '\n\t\t We experimented with counts obtained from the BNC and the Brown corpus . \n\t', '\n\t\t The variation in counts had negligible affect on the results . \n\t', '\n\t\t 3 The experimental results reported here are obtained using IC counts from the BNC corpus . \n\t', '\n\t\t All the results shown here are those with the size of thesaurus entries ( ) set to 50.4 We calculate the accuracy of finding the predominant sense , when there is indeed one sense with a higher frequency than the others for this word in SemCor ( ) . \n\t', '\n\t\t We also calculate the WSD accuracy that would be obtained on SemCor , when using our first sense in all contexts ( ) . \n\t', '\n\t\t 3.1 Results The results in table 1 show the accuracy of the ranking with respect to SemCor over the entire set of 2595 polysemous nouns in SemCor with 3Using the default IC counts provided with the package did result in significantly higher results , but these default files are obtained from the sense-tagged data within SemCor itself so we discounted these results . \n\t', '\n\t\t 4We repeated the experiment with the BNC data for jcn using and however , the number of neighbours used gave only minimal changes to the results so we do not report them here . \n\t', '\n\t\t measure % % lesk 54 48 jcn 54 46 baseline 32 24 Table 1 : SemCor results the jcn and lesk WordNet similarity measures . \n\t', '\n\t\t The random baseline for choosing the predominant sense over all these words ( ) is 32 % . \n\t', '\n\t\t Both WordNet similarity measures beat this baseline . \n\t', '\n\t\t The random baseline for ( ) is 24 % . \n\t', '\n\t\t Again , the automatic ranking outperforms this by a large margin . \n\t', '\n\t\t The first sense in SemCor provides an upper- bound for this task of 67 % . \n\t', '\n\t\t Since both measures gave comparable results we restricted our remaining experiments to jcn because this gave good results for finding the predominant sense , and is much more efficient than lesk , given the precompilation of the IC files . \n\t', '\n\t\t 3.2 Discussion From manual analysis , there are cases where the acquired first sense disagrees with SemCor , yet is intuitively plausible . \n\t', '\n\t\t This is to be expected regardless of any inherent shortcomings of the ranking technique since the senses within SemCor will differ compared to those of the BNC. . \n\t', '\n\t\t For example , in WordNet the first listed sense ofpipe is tobacco pipe , and this is ranked joint first according to the Brown files in SemCor with the second sense tube made of metal or plastic used to carry water , oil or gas etc .... The automatic ranking from the BNC data lists the latter tube sense first . \n\t', '\n\t\t This seems quite reasonable given the nearest neighbours : tube , cable , wire , tank , hole , cylinder , fitting , tap , cistern , plate .... Since SemCor is derived from the Brown corpus , which predates the BNC by up to 30 years 5 and contains a higher proportion of fiction 6 , the high ranking for the tobacco pipe sense according to SemCor seems plausible . \n\t', '\n\t\t Another example where the ranking is intuitive , is soil . \n\t', '\n\t\t The first ranked sense according to SemCor is the filth , stain : state of being unclean sense whereas the automatic ranking lists dirt , ground , earth as the first sense , which is the second ranked 5The text in the Brown corpus was produced in 1961 , whereas the bulk of the written portion of the BNC contains texts produced between 1975 and 1993 . \n\t', '\n\t\t 66 out of the 15 Brown genres are fiction , including one specifically dedicated to detective fiction , whilst only 20 % of the BNC text represents imaginative writing , the remaining 80 % being classified as informative . \n\t', '\n\t\t sense according to SemCor . \n\t', '\n\t\t This seems intuitive given our expected relative usage of these senses in modern British English . \n\t', '\n\t\t Even given the difference in text type between SemCor and the BNC the results are encouraging , especially given that our results are for polysemous nouns . \n\t', '\n\t\t In the English all-words SENSEVAL-2 , 25 % of the noun data was monosemous . \n\t', '\n\t\t Thus , if we used the sense ranking as a heuristic for an \x93all nouns\x94 task we would expect to get precision in the region of 60 % . \n\t', '\n\t\t We test this below on the SENSEVAL-2 English all-words data . \n\t', '\n\t\t 4 Experiment on SENSEVAL-2 English all Words Data In order to see how well the automatically acquired predominant sense performs on a WSD task from which the WordNet sense ordering has not been taken , we use the SENSEVAL-2 all-words data \n\t\t']",Positive
"['\n\t\t 7 This is a hand-tagged test suite of 5,000 words of running text from three articles from the Penn Treebank II . \n\t', '\n\t\t We use an all- words task because the predominant senses will reflect the sense distributions of all nouns within the documents , rather than a lexical sample task , where the target words are manually determined and the results will depend on the skew of the words in the sample . \n\t', '\n\t\t We do not assume that the predominant sense is a method of WSD in itself . \n\t', '\n\t\t To disambiguate senses a system should take context into account . \n\t', '\n\t\t However , it is important to know the performance of this heuristic for any systems that use it . \n\t', '\n\t\t We generated a thesaurus entry for all polysemous nouns in WordNet as described in section 2.1 above . \n\t', '\n\t\t We obtained the predominant sense for each of these words and used these to label the instances in the noun data within the SENSEVAL-2 English all- words task . \n\t', '\n\t\t We give the results for this WSD task in table 2 . \n\t', '\n\t\t We compare results using the first sense listed in SemCor , and the first sense according to the SENSEVAL-2 English all-words test data itself . \n\t', '\n\t\t For the latter , we only take a first-sense where there is more than one occurrence of the noun in the test data and one sense has occurred more times than any of the others . \n\t', '\n\t\t We trivially labelled all monosemous items . \n\t', '\n\t\t Our automatically acquired predominant sense performs nearly as well as the first sense provided by SemCor , which is very encouraging given that 7In order to do this we use the mapping provided at http://www.lsi.upc.es/\x98nlp/tools/mapping.html ( Daud´e et al. , 2000 ) for obtaining the SENSEVAL-2 data in WordNet 1.6 . \n\t', '\n\t\t We discounted the few items for which there was no mapping . \n\t', '\n\t\t This amounted to only 3 % of the data . \n\t', '\n\t\t precision recall Automatic 64 63 SemCor 69 68 SENSEVAL-2 92 72 Table 2 : Evaluating predominant sense information on SENSEVAL-2 all-words data . \n\t', '\n\t\t our method only uses raw text , with no manual labelling . \n\t', '\n\t\t The performance of the predominant sense provided in the SENSEVAL-2 test data provides an upper bound for this task . \n\t', '\n\t\t The items that were not covered by our method were those with insufficient grammatical relations for the tuples employed . \n\t', '\n\t\t Two such words , today and one , each occurred 5 times in the test data . \n\t', '\n\t\t Extending the grammatical relations used for building the thesaurus should improve the coverage . \n\t', '\n\t\t There were a similar number of words that were not covered by a predominant sense in SemCor . \n\t', '\n\t\t For these one would need to obtain more sense-tagged text in order to use this heuristic . \n\t', '\n\t\t Our automatic ranking gave 67 % precision on these items . \n\t', '\n\t\t This demonstrates that our method of providing a first sense from raw text will help when sense-tagged data is not available . \n\t', '\n\t\t 5 Experiments with Domain Specific Corpora A major motivation for our work is to try to capture changes in ranking of senses for documents from different domains . \n\t', '\n\t\t In order to test this we applied our method to two specific sections of the Reuters corpus . \n\t', '\n\t\t We demonstrate that choosing texts from a particular domain has a significant influence on the sense ranking . \n\t', '\n\t\t We chose the domains of SPORTS and FINANCE since there is sufficient material for these domains in this publically available corpus . \n\t', '\n\t\t 5.1 Reuters Corpus The Reuters corpus \n\t\t']",Positive
"['\n\t\t Many of the articles are economy related , but several other topics are included too . \n\t', '\n\t\t We selected documents from the SPORTS domain ( topic code : GSPO ) and a limited number of documents from the FINANCE domain ( topic codes : ECAT and MCAT ) . \n\t', '\n\t\t The SPORTS corpus consists of 35317 documents ( about 9.1 million words ) . \n\t', '\n\t\t The FINANCE corpus consists of 117734 documents ( about 32.5 million words ) . \n\t', '\n\t\t We acquired thesauruses for these corpora using the procedure described in section 2.1 . \n\t', '\n\t\t 5.2 Two Experiments There is no existing sense-tagged data for these domains that we could use for evaluation . \n\t', '\n\t\t We therefore decided to select a limited number of words and to evaluate these words qualitatively . \n\t', '\n\t\t The words included in this experiment are not a random sample , since we anticipated different predominant senses in the SPORTS and FINANCE domains for these words . \n\t', '\n\t\t Additionally , we evaluated our method quantitatively using the Subject Field Codes ( SFC ) resource ( Magnini and Cavagli`a , 2000 ) which annotates WordNet synsets with domain labels . \n\t', '\n\t\t The SFC contains an economy label and a sports label . \n\t', '\n\t\t For this domain label experiment we selected all the words in WordNet that have at least one synset labelled economy and at least one synset labelled sports . \n\t', '\n\t\t The resulting set consisted of 38 words . \n\t', '\n\t\t We contrast the distribution of domain labels for these words in the 2 domain specific corpora . \n\t', '\n\t\t 5.3 Discussion The results for 10 of the words from the qualitative experiment are summarized in table 3 with the WordNet sense number for each word supplied alongside synonyms or hypernyms from WordNet for readability . \n\t', '\n\t\t The results are promising . \n\t', '\n\t\t Most words show the change in predominant sense ( PS ) that we anticipated . \n\t', '\n\t\t It is not always intuitively clear which of the senses to expect as predominant sense for either a particular domain or for the BNC , but the first senses of words like division and goal shift towards the more specific senses ( league and score respectively ) . \n\t', '\n\t\t Moreover , the chosen senses of the word tie proved to be a textbook example of the behaviour we expected . \n\t', '\n\t\t The word share is among the words whose predominant sense remained the same for all three corpora . \n\t', '\n\t\t We anticipated that the stock certificate sense would be chosen for the FINANCE domain , but this did not happen . \n\t', '\n\t\t However , that particular sense ended up higher in the ranking for the FINANCE domain . \n\t', '\n\t\t Figure 2 displays the results of the second experiment with the domain specific corpora . \n\t', '\n\t\t This figure shows the domain labels assigned to the predominant senses for the set of 38 words after ranking the words using the SPORTS and the FINANCE corpora . \n\t', '\n\t\t We see that both domains have a similarly high percentage of factotum ( domain independent ) labels , but as we would expect , the other peaks correspond to the economy label for the FINANCE corpus , and the sports label for the SPORTS corpus . \n\t', '\n\t\t Word PS BNC PS FINANCE PS SPORTS pass 1 ( accomplishment ) 14 ( attempt ) 15 ( throw ) share 2 ( portion , asset ) 2 2 division 4 ( admin . \n\t', '\n\t\t unit ) 4 6 ( league ) head 1 ( body part ) 4 ( leader ) 4 loss 2 ( transf . \n\t', '\n\t\t property ) 2 8 ( death , departure ) competition 2 ( contest , social event ) 3 ( rivalry ) 2 match 2 ( contest ) 7 ( equal , person ) 2 tie 1 ( neckwear ) 2 ( affiliation ) 3 ( draw ) strike 1 ( work stoppage ) 1 6 ( hit , success ) goal 1 ( end , mental object ) 1 2 ( score ) Table 3 : Domain specific results Figure 2 : Distribution of domain labels of predominant senses for 38 polysemous words ranked using the SPORTS and FINANCE corpus . \n\t', '\n\t\t 6 Related Work Most research in WSD concentrates on using contextual features , typically neighbouring words , to help determine the correct sense of a target word . \n\t', '\n\t\t In contrast , our work is aimed at discovering the predominant senses from raw text because the first sense heuristic is such a useful one , and because hand- tagged data is not always available . \n\t', '\n\t\t A major benefit of our work , rather than reliance on hand-tagged training data such as SemCor , is that this method permits us to produce predominant senses for the domain and text type required . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t Buitelaar and Sacaleanu have evaluated their method on identifying domain specific concepts using human judgements on 100 items . \n\t', '\n\t\t We have evaluated our method using publically available resources , both for balanced and domain spe cific text . \n\t', '\n\t\t Magnini and Cavagli`a ( 2000 ) have identified WordNet word senses with particular domains , and this has proven useful for high precision WSD \n\t\t']",Positive
"['\n\t\t Identification of these domain labels for word senses was semiautomatic and required a considerable amount of hand-labelling . \n\t', '\n\t\t Our approach is complementary to this . \n\t', '\n\t\t It only requires raw text from the given domain and because of this it can easily be applied to a new domain , or sense inventory , given sufficient text . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t They used syntactic evidence to find a prior distribution for verb classes , based on \n\t\t']",Positive
"['\n\t\t Lapata and Brew obtain their priors for verb classes directly from sub- categorisation evidence in a parsed corpus , whereas we use parsed data to find distributionally similar words ( nearest neighbours ) to the target word which reflect the different senses of the word and have associated distributional similarity scores which can be used for ranking the senses according to prevalence . \n\t', '\n\t\t There has been some related work on using automatic thesauruses for discovering word senses from corpora \n\t\t']",Positive
"['\n\t\t In this work the lists of neighbours are themselves clustered to bring out the various senses of the word . \n\t', '\n\t\t They evaluate using the lin measure described above in section 2.2 to determine the precision and recall of these discovered classes with respect to WordNet synsets . \n\t', '\n\t\t This method obtains precision of 61 % and recall 51 % . \n\t', '\n\t\t If WordNet sense distinctions are not ultimately required then discovering the senses directly from the neighbours list is useful because sense distinctions discovered are relevant to the corpus data and new senses can be found . \n\t', '\n\t\t In contrast , we use the neighbours lists and WordNet similarity measures to im- factotum biology politics law religion administr . \n\t', '\n\t\t 0.45 sport finance 0.4 0.35 0.3 0.25 0.2 commerce free_time physics mathematics sports play industry economy telecom . \n\t', '\n\t\t medicine 0.15 0.1 0.05 0 pose a prevalence ranking on the WordNet senses . \n\t', '\n\t\t We believe automatic ranking techniques such as ours will be useful for systems that rely on Word- Net , for example those that use it for lexical acquisition or WSD . \n\t', '\n\t\t It would be useful however to combine our method of finding predominant senses with one which can automatically find new senses within text and relate these to WordNet synsets , as \n\t\t']",Positive
"['\n\t\t We have restricted ourselves to nouns in this work , since this PoS is perhaps most affected by domain . \n\t', '\n\t\t We are currently investigating the performance of the first sense heuristic , and this method , for other PoS on SENSEVAL-3 data \n\t\t']",Positive
"['\n\t\t The lesk measure can be used when ranking adjectives , and adverbs as well as nouns and verbs ( which can also be ranked using jcn ) . \n\t', '\n\t\t Another major advantage that lesk has is that it is applicable to lexical resources which do not have the hierarchical structure that WordNet does , but do have definitions associated with word senses . \n\t', '\n\t\t 7 Conclusions We have devised a method that uses raw corpus data to automatically find a predominant sense for nouns in WordNet . \n\t', '\n\t\t We use an automatically acquired thesaurus and a WordNet Similarity measure . \n\t', '\n\t\t The automatically acquired predominant senses were evaluated against the hand-tagged resources SemCor and the SENSEVAL-2 English all-words task giving us a WSD precision of 64 % on an all-nouns task . \n\t', '\n\t\t This is just 5 % lower than results using the first sense in the manually labelled SemCor , and we obtain 67 % precision on polysemous nouns that are not in SemCor . \n\t', '\n\t\t In many cases the sense ranking provided in SemCor differs to that obtained automatically because we used the BNC to produce our thesaurus . \n\t', '\n\t\t Indeed , the merit of our technique is the very possibility of obtaining predominant senses from the data at hand . \n\t', '\n\t\t We have demonstrated the possibility of finding predominant senses in domain specific corpora on a sample of nouns . \n\t', '\n\t\t In the future , we will perform a large scale evaluation on domain specific corpora . \n\t', '\n\t\t In particular , we will use balanced and domain specific corpora to isolate words having very different neighbours , and therefore rankings , in the different corpora and to detect and target words for which there is a highly skewed sense distribution in these corpora . \n\t', '\n\t\t There is plenty of scope for further work . \n\t', '\n\t\t We want to investigate the effect of frequency and choice of distributional similarity measure \n\t\t']",Positive
"['\n\t\t Additionally , we need to determine whether senses which do not occur in a wide variety of grammatical contexts fare badly using distributional measures of similarity , and what can be done to combat this problem using relation specific thesauruses . \n\t', '\n\t\t Whilst we have used WordNet as our sense inventory , it would be possible to use this method with another inventory given a measure of semantic relatedness between the neighbours and the senses . \n\t', '\n\t\t The lesk measure for example , can be used with definitions in any standard machine readable dictionary . \n\t', '\n\t\t Acknowledgements We would like to thank Siddharth Patwardhan and Ted Pedersen for making the WN Similarity package publically available . \n\t', '\n\t\t This work was funded by EU-2001-34460 project MEANING : Developing Multilingual Web-scale Language Technologies , UK EPSRC project Robust Accurate Statistical Parsing ( RASP ) and a UK EPSRC studentship . \n\t', '\n\t\t References Satanjeev Banerjee and Ted Pedersen . \n\t', '\n\t\t 2002. An adapted Lesk algorithm for word sense disambiguation using WordNet . \n\t', '\n\t\t In Proceedings of the Third International Conference on Intelligent Text Processing and Computational Linguistics ( CICLing-02 ) , Mexico City . \n\t', '\n\t\t Edward Briscoe and John Carroll . \n\t', '\n\t\t 2002 . \n\t', '\n\t\t Robust accurate statistical annotation of general text . \n\t', '\n\t\t In Proceedings of the Third International Conference on Language Resources and Evaluation ( LREC ) , pages 1499\x961504 , Las Palmas , Canary Islands , Spain . \n\t', '\n\t\t Paul Buitelaar and Bogdan Sacaleanu . \n\t', '\n\t\t 2001. Ranking and selecting synsets by domain relevance . \n\t', '\n\t\t In Proceedings of WordNet and Other Lexical Resources : Applications , Extensions and Customizations , NAACL 2001 Workshop , Pittsburgh , PA . \n\t', '\n\t\t Massimiliano Ciaramita and Mark Johnson . \n\t', '\n\t\t 2003. Supersense tagging of unknown nouns in Word- Net . \n\t', '\n\t\t In Proceedings of the Conference on Empirical Methods in Natural Language Processing ( EMNLP 2003 ) . \n\t', '\n\t\t Scott Cotton , Phil Edmonds , Adam Kilgarriff , and Martha Palmer . \n\t', '\n\t\t 1998. SENSEVAL-2 . \n\t', '\n\t\t http://www.sle.sharp.co.uk/senseval2/ . \n\t', '\n\t\t Jordi Daud´e , Lluis Padr´o , and German Rigau . \n\t', '\n\t\t 2000. Mapping wordnets using structural information . \n\t', '\n\t\t In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics , Hong Kong . \n\t', '\n\t\t V´eronique Hoste , Anne Kool , and Walter Daelemans . \n\t', '\n\t\t 2001. Classifier optimization and combination in the English all words task . \n\t', '\n\t\t In Proceedings of the SENSEVAL-2 workshop , pages 84\x9686 . \n\t', '\n\t\t Jay Jiang and David Conrath . \n\t', '\n\t\t 1997. Semantic similarity based on corpus statistics and lexical taxonomy . \n\t', '\n\t\t In International Conference on Research in Computational Linguistics , Taiwan . \n\t', '\n\t\t Anna Korhonen . \n\t', '\n\t\t 2002. Semantically motivated subcategorization acquisition . \n\t', '\n\t\t In Proceedings of the ACL Workshop on Unsupervised Lexical Acquisition , Philadelphia , USA . \n\t', '\n\t\t Mirella Lapata and Chris Brew . \n\t', '\n\t\t 2004. Verb class disambiguation using informative priors . \n\t', '\n\t\t Computational Linguistics , 30(1):45\x9675 . \n\t', '\n\t\t Beth Levin . \n\t', '\n\t\t 1993. English Verb Classes and Alternations : a Preliminary Investigation . \n\t', '\n\t\t University of Chicago Press , Chicago and London . \n\t', '\n\t\t Dekang Lin . \n\t', '\n\t\t 1998. Automatic retrieval and clustering of similar words . \n\t', '\n\t\t In Proceedings of COLING-ACL 98 , Montreal , Canada . \n\t', '\n\t\t Bernardo Magnini and Gabriela Cavagli`a . \n\t', '\n\t\t 2000 . \n\t', '\n\t\t Integrating subject field codes into WordNet . \n\t', '\n\t\t In Proceedings ofLREC-2000 , Athens , Greece . \n\t', '\n\t\t Bernardo Magnini , Carlo Strapparava , Giovanni Pezzuli , and Alfio Gliozzo . \n\t', '\n\t\t 2001. Using domain information for word sense disambiguation . \n\t', '\n\t\t In Proceedings of the SENSEVAL-2 workshop , pages 111\x96114 . \n\t', '\n\t\t Diana McCarthy , Rob Koeling , Julie Weeds , and John Carroll . \n\t', '\n\t\t 2004. Using automatically acquired predominant senses for word sense disambiguation . \n\t', '\n\t\t In Proceedings of the ACL SENSEVAL-3 workshop . \n\t', '\n\t\t Diana McCarthy . \n\t', '\n\t\t 1997. Word sense disambiguation for acquisition of selectional preferences . \n\t', '\n\t\t In Proceedings of the ACL/EACL 97 Workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications , pages 52\x9661 . \n\t', '\n\t\t Paola Merlo and Matthias Leybold . \n\t', '\n\t\t 2001. Automatic distinction of arguments and modifiers : the case of prepositional phrases . \n\t', '\n\t\t In Proceedings of the Workshop on Computational Language Learning \n\t\t']",Positive
"['\n\t\t George A. Miller , Claudia Leacock , Randee Tengi , and Ross T Bunker . \n\t', '\n\t\t 1993. A semantic concordance . \n\t', '\n\t\t In Proceedings of the ARPA Workshop on Human Language Technology , pages 303\x96308 . \n\t', '\n\t\t Morgan Kaufman . \n\t', '\n\t\t Martha Palmer , Christiane Fellbaum , Scott Cotton , Lauren Delfs , and Hoa Trang Dang . \n\t', '\n\t\t 2001. English tasks : All-words and verb lexical sample . \n\t', '\n\t\t In Proceedings of the SENSEVAL-2 workshop , pages 21\x9624 . \n\t', '\n\t\t Patrick Pantel and Dekang Lin . \n\t', '\n\t\t 2002 . \n\t', '\n\t\t Discovering word senses from text . \n\t', '\n\t\t In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining , pages 613\x96619 , Edmonton , Canada . \n\t', '\n\t\t Siddharth Patwardhan and Ted Pedersen . \n\t', '\n\t\t 2003. The cpan wordnet::similarity package . \n\t', '\n\t\t http://search.cpan.org/author/SID/WordNetSimilarity-0.03/ . \n\t', '\n\t\t Siddharth Patwardhan , Satanjeev Banerj ee , and Ted Pedersen . \n\t', '\n\t\t 2003. Using measures of semantic relatedness for word sense disambiguation . \n\t', '\n\t\t In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics ( CICLing 2003 ) , Mexico City . \n\t', '\n\t\t Tony G. Rose , Mary Stevenson , and Miles Whitehead . \n\t', '\n\t\t 2002. The Reuters Corpus volume 1 - from yesterday\x92s news to tomorrow\x92s language resources . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of Third International Conference on Language Resources and Evaluation , Las Palmas de Gran Canaria . \n\t', '\n\t\t Julie Weeds , David Weir , and Diana McCarthy . \n\t', '\n\t\t 2004. Characterising measures of lexical distributional similarity . \n\t', '\n\t\t Yorick Wilks and Mark Stevenson . \n\t', '\n\t\t 1998. The grammar of sense : using part-of speech tags as a first step in semantic disambiguation . \n\t', '\n\t\t Natural Language Engineering , 4(2):135\x96143 . \n\t', '\n\t\t David Yarowsky and Radu Florian . \n\t', '\n\t\t 2002. Evaluating sense disambiguation performance across diverse parameter spaces . \n\t', '\n\t\t Natural Language Engineering , 8(4):293\x96310 . \n\t', '\n\t\t Unsupervised Sense Disambiguation Using Bilingual Probabilistic Models Indrajit Bhattacharya Dept. of Computer Science University of Maryland College Park , MD , USA indrajit@cs.umd.edu Lise Getoor Dept. of Computer Science University of Maryland College Park , MD , USA getoor@cs.umd.edu Yoshua Bengio Dept. IRO Universit´e de Montr´eal Montr´eal , Qu´ebec , Canada bengioy@IRO.UMontreal.CA Abstract We describe two probabilistic models for unsupervised word-sense disambiguation using parallel corpora . \n\t', '\n\t\t The first model , which we call the Sense model , builds on the work of \n\t\t']",Positive
"['\n\t\t The second model , which we call the Concept model , is a hierarchical model that uses a concept latent variable to relate different language specific sense labels . \n\t', '\n\t\t We show that both models improve performance on the word sense disambiguation task over previous unsupervised approaches , with the Concept model showing the largest improvement . \n\t', '\n\t\t Furthermore , in learning the Concept model , as a by-product , we learn a sense inventory for the parallel language . \n\t', '\n\t\t 1 Introduction Word sense disambiguation ( WSD ) has been a central question in the computational linguistics community since its inception . \n\t', '\n\t\t WSD is fundamental to natural language understanding and is a useful intermediate step for many other language processing tasks \n\t\t']",Positive
['\n\t\t Many recent approaches make use of ideas from statistical machine learning ; the availability of shared sense definitions ( e.g. WordNet \n\t\t'],Positive
['\n\t\t Supervised approaches which make use of a small hand-labeled training set \n\t\t'],Negative
"['\n\t\t In an effort to overcome the difficulty of finding sense-labeled training data , researchers have begun investigating unsupervised approaches to word- sense disambiguation . \n\t', '\n\t\t For example , the use of par allel corpora for sense tagging can help with word sense disambiguation \n\t\t']",Positive
"['\n\t\t As an illustration of sense disambiguation from translation data , when the English word bank is translated to Spanish as orilla , it is clear that we are referring to the shore sense of bank , rather than the financial institution sense . \n\t', '\n\t\t The main inspiration for our work is \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t They assume the same semantic hierarchy ( in particular , WordNet ) for both the languages and assign English words as well as their translations to WordNet synsets . \n\t', '\n\t\t Here we present two variants of the graphical model in \n\t\t']",Positive
"['\n\t\t We also present empirical word sense disambiguation results which demonstrate the gain brought by this probabilistic approach , even while only using the translated word to provide disambiguation information . \n\t', '\n\t\t Our first generative model , the Sense Model , groups semantically related words from the two languages into senses , and translations are generated by probabilistically choosing a sense and then words from the sense . \n\t', '\n\t\t We show that this improves on the results of \n\t\t']",Negative
"['\n\t\t Our next model , which we call the Concept Model , aims to improve on the above sense structure by modeling the senses of the two languages separately and relating senses from both languages through a higher-level , semantically less precise concept . \n\t', '\n\t\t The intuition here is that not all of the senses that are possible for a word will be relevant for a concept . \n\t', '\n\t\t In other words , the distribution over the senses of a word given a concept can be expected to have a lower entropy than the distribution over the senses of the word in the language as a whole . \n\t', '\n\t\t In this paper , we look at translation data as a re- source for identification of semantic concepts . \n\t', '\n\t\t Note that actual translated word pairs are not always good matches semantically , because the translation process is not on a word by word basis . \n\t', '\n\t\t This introduces a kind of noise in the translation , and an additional hidden variable to represent the shared meaning helps to take it into account . \n\t', '\n\t\t Improved performance over the Sense Model validates the use of concepts in modeling translations . \n\t', '\n\t\t An interesting by-product of the Concept Model is a semantic structure for the secondary language . \n\t', '\n\t\t This is automatically constructed using background knowledge of the structure for the primary language and the observed translation pairs . \n\t', '\n\t\t In the model , words sharing the same sense are synonyms while senses under the same concept are semantically related in the corpus . \n\t', '\n\t\t An investigation of the model trained over real data reveals that it can indeed group related words together . \n\t', '\n\t\t It may be noted that predicting senses from translations need not necessarily be an end result in itself . \n\t', '\n\t\t As we have already mentioned , lack of labeled data is a severe hindrance for supervised approaches to word sense disambiguation . \n\t', '\n\t\t At the same time , there is an abundance of bilingual documents and many more can potentially be mined from the web. . \n\t', '\n\t\t It should be possible using our approach to ( noisily ) assign sense tags to words in such documents , thus providing huge resources of labeled data for supervised approaches to make use of . \n\t', '\n\t\t For the rest of this paper , for simplicity we will refer to the primary language of the parallel document as English and to the secondary as Spanish . \n\t', '\n\t\t The paper is organized as follows . \n\t', '\n\t\t We begin by formally describing the models in Section 2 . \n\t', '\n\t\t We describe our approach for constructing the senses and concepts in Section 3 . \n\t', '\n\t\t Our algorithm for learning the model parameters is described in Section 4 . \n\t', '\n\t\t We present experimental results in Section 5 and our analysis in Section 6 . \n\t', '\n\t\t We conclude in Section 7 . \n\t', '\n\t\t 2 Probabilistic Models for Parallel Corpora We motivate the use of a probabilistic model by illustrating that disambiguation using translations is possible even when a word has a unique translation . \n\t', '\n\t\t For example , according to WordNet , the word prevention has two senses in English , which may be abbreviated as hindrance ( the act of hindering or obstruction ) and control ( by prevention , e.g. the control of a disease ) . \n\t', '\n\t\t It has a single translation in our corpus , that being prevenci´on . \n\t', '\n\t\t The first English sense , hindrance , also has other words like bar that occur in the corpus and all of these other words are observed to be translated in Spanish as the word obstrucci´on . \n\t', '\n\t\t In addition , none of these other words translate to prevenci´on . \n\t', '\n\t\t So it is not unreasonable to suppose that the intended sense for prevention when translated as prevenci´on is different from that of bar . \n\t', '\n\t\t Therefore , the intended sense is most likely to be control . \n\t', '\n\t\t At the very heart of the reasoning is probabilistic analysis and independence assumptions . \n\t', '\n\t\t We are assuming that senses and words have certain occurrence probabilities and that the choice of the word can be made independently once the sense has been decided . \n\t', '\n\t\t This is the flavor that we look to add to modeling parallel documents for sense disambiguation . \n\t', '\n\t\t We formally describe the two generative models that use these ideas in Subsections 2.2 and 2.3 . \n\t', '\n\t\t Figure 1 : Graphical Representations of the a ) Sense Model and the b ) Concept Model 2.1 Notation Throughout , we use uppercase letters to denote random variables and lowercase letters to denote specific instances of the random variables . \n\t', '\n\t\t A translation pair is ( , ) where the subscript and indicate the primary language ( English ) and the secondary language ( Spanish ) . \n\t', '\n\t\t and . \n\t', '\n\t\t We use the shorthand for by ideas in \n\t\t']",Positive
"['\n\t\t In other words , the set of sense labels for the words in the two languages is the same and may be collapsed into one set of senses that is responsible for both English and Spanish words and the single latent variable in the model is the sense label for both words and . \n\t', '\n\t\t We also make the assumption that the words in both languages are conditionally independent given the sense label . \n\t', '\n\t\t The generative parameters for the model are the prior a ) Sense Model b ) Concept Model concept sense word We Ws T Te Ts We C Ws. 2.2 The Sense Model The Sense Model makes the assumption , inspired probability of each sense and the conditional probabilities and of each word and in the two languages given the sense . \n\t', '\n\t\t The generation of a translation pair by this model may be viewed as a two-step process that first selects a sense according to the priors on the senses and then selects a word from each language using the conditional probabilities for that sense . \n\t', '\n\t\t This may be imagined as a factoring of the joint distribution : . \n\t', '\n\t\t Note that in the absence of labeled training data , two of the random variables and are observed , while the sense variable is not . \n\t', '\n\t\t However , we can derive the possible values for our sense labels from WordNet , which gives us the possible senses for each English word . \n\t', '\n\t\t The Sense model is shown in Figure 1(a) . \n\t', '\n\t\t 2.3 The Concept Model The assumption of a one-to-one association between sense labels made in the Sense Model may be too simplistic to hold for arbitrary languages . \n\t', '\n\t\t In particular , it does not take into account that translation is from sentence to sentence ( with a shared meaning ) , while the data we are modeling are aligned single-word translations , in which the in- tended meaning of does not always match per- fectly with the intended meaning of . \n\t', '\n\t\t Generally , a set of related senses in one language may be translated by one of related senses in the other . \n\t', '\n\t\t This many-to-many mapping is captured in our alternative model using a second level hidden variable called a concept . \n\t', '\n\t\t Thus we have three hidden variables in the Concept Model \x97 the English sense , the Spanish sense and the concept , where . \n\t', '\n\t\t We make the assumption that the senses and are independent of each other given the shared concept . \n\t', '\n\t\t The generative parameters in the model are the prior probabilities over the concepts , the conditional probabilities and for the English and Spanish senses given the concept , and the conditional probabilities and for the words and in each language given their senses . \n\t', '\n\t\t We can now imagine the generative process of a translation pair by the Concept Model as first selecting a concept according to the priors , then a sense for each language given the concept , and finally a word for each sense using the conditional probabilities of the words . \n\t', '\n\t\t As in \n\t\t']",Positive
"['\n\t\t The Concept model is shown in Figure 1(b) . \n\t', '\n\t\t 3 Constructing the Senses and Concepts Building the structure of the model is crucial for our task . \n\t', '\n\t\t Choosing the dimensionality of the hidden variables by selecting the number of senses and concepts , as well as taking advantage of prior knowledge to impose constraints , are very important aspects of building the structure . \n\t', '\n\t\t If certain words are not possible for a given sense , or certain senses are not possible for a given concept , their corresponding parameters should be 0 . \n\t', '\n\t\t For instance , for all words that do not belong to a sense , the corresponding parameter would be permanently set to 0 . \n\t', '\n\t\t Only the remaining parameters need to be modeled explicitly . \n\t', '\n\t\t While model selection is an extremely difficult problem in general , an important and interesting option is the use of world knowledge . \n\t', '\n\t\t Semantic hierarchies for some languages have been built . \n\t', '\n\t\t We should be able to make use of these known taxonomies in constructing our model . \n\t', '\n\t\t We make heavy use of the WordNet ontology to assign structure to both our models , as we discuss in the following subsections . \n\t', '\n\t\t There are two major tasks in building the structure \x97 determining the possible sense labels for each word , both English and Spanish , and constructing the concepts , which involves choosing the number of concepts and the probable senses for each concept . \n\t', '\n\t\t 3.1 Building the Sense Model Each word in WordNet can belong to multiple synsets in the hierarchy , which are its possible senses . \n\t', '\n\t\t In both of our models , we directly use the WordNet senses as the English sense labels . \n\t', '\n\t\t All WordNet senses for which a word has been observed in the corpus form our set of English sense labels . \n\t', '\n\t\t The Sense Model holds that the sense labels for the two domains are the same . \n\t', '\n\t\t So we must use the same WordNet labels for the Spanish words as well . \n\t', '\n\t\t We include a Spanish word for a sense if is the translation of any English word in . \n\t', '\n\t\t 3.2 Building the Concept Model Unlike the Sense Model , the Concept Model does not constrain the Spanish senses to be the same as the English ones . \n\t', '\n\t\t So the two major tasks in building the Concept Model are constructing the Spanish senses and then clustering the English and Spanish senses to build the concepts. , and c20 c6118 tel te2 bar prevention obstruccio\x92n prevencio\x92n te2 tel tsl ts2 prevention bar prevencio\x92n obstruccio\x92n Sense Model Concept Model Figure 2 : The Sense and Concept models for prevention , bar , prevenci´on and obstrucci´on For each Spanish word , we have its set of English translations . \n\t', '\n\t\t One possibility is to group Spanish words looking at their translations . \n\t', '\n\t\t However , a more robust approach is to consider the relevant English senses for . \n\t', '\n\t\t Each English translation for has its set of English sense labels drawn from WordNet . \n\t', '\n\t\t So the relevant English sense labels for may be defined as . \n\t', '\n\t\t We call this the English sense map or for . \n\t', '\n\t\t We use the s to define the Spanish senses . \n\t', '\n\t\t We may imagine each Spanish word to come from one or more Spanish senses . \n\t', '\n\t\t If each word has a single sense , then we add a Spanish sense for each and all Spanish words that share that belong to that sense . \n\t', '\n\t\t Otherwise , the s have to be split into frequently occurring subgroups . \n\t', '\n\t\t Frequently co-occurring subsets of s can define more refined Spanish senses . \n\t', '\n\t\t We identify these subsets by looking at pairs of s and computing their intersections . \n\t', '\n\t\t An intersection is considered to be a Spanish sense if it occurs for a significant number of pairs of s . \n\t', '\n\t\t We consider both ways of building Spanish senses . \n\t', '\n\t\t In either case , a constructed Spanish sense comes with its rele- vant set of English senses , which we denote as . \n\t', '\n\t\t Once we have the Spanish senses , we cluster them to form concepts . \n\t', '\n\t\t We use the corresponding to each Spanish sense to define a measure of similarity for a pair of Spanish senses . \n\t', '\n\t\t There are many options to choose from here . \n\t', '\n\t\t We use a simple measure that counts the number of common items in the two s.1 The similarity measure is now used to cluster the Spanish senses . \n\t', '\n\t\t Since this measure is not transitive , it does not directly define equivalence classes over . \n\t', '\n\t\t Instead , we get a similarity graph where the vertices are the Spanish senses and we add an edge between two senses if their similarity is above a threshold . \n\t', '\n\t\t We now pick each connected component from this graph as a cluster of similar Spanish senses . \n\t', ""\n\t\t ' Another option would be to use a measure of similarity for English senses , proposed in \n\t\t""]",Negative
"['\n\t\t Our initial results with this measure were not favorable . \n\t', '\n\t\t Now we build the concepts from the Spanish sense clusters . \n\t', '\n\t\t We recall that a concept is defined by a set of English senses and a set of Spanish senses that are related . \n\t', '\n\t\t Each cluster represents a concept . \n\t', '\n\t\t A particular concept is formed by the set of Spanish senses in the cluster and the English senses relevant for them . \n\t', '\n\t\t The relevant English senses for any Span- ish sense is given by its . \n\t', '\n\t\t Therefore , the union of the s of all the Spanish senses in the cluster forms the set of English senses for each concept . \n\t', '\n\t\t 4 Learning the Model Parameters Once the model is built , we use the popular EM algorithm \n\t\t']",Positive
"['\n\t\t The algorithm repeatedly iterates over two steps . \n\t', '\n\t\t The first step maximizes the expected log-likelihood of the joint probability of the observed data with the current parameter settings . \n\t', '\n\t\t The next step then re- estimates the values of the parameters of the model . \n\t', '\n\t\t Below we summarize the re-estimation steps for each model . \n\t', '\n\t\t 4.1 EM for the Sense Model follows similarly . \n\t', '\n\t\t 4.2 EM for the Concept Model follow similarly . \n\t', '\n\t\t 4.3 Initialization of Model Probabilities Since the EM algorithm performs gradient ascent as it iteratively improves the log-likelihood , it is prone to getting caught in local maxima , and selection of the initial conditions is crucial for the learning procedure . \n\t', '\n\t\t Instead of opting for a uniform or random initialization of the probabilities , we make use of prior knowledge about the English words and senses available from WordNet . \n\t', '\n\t\t Word- Net provides occurrence frequencies for each synset in the SemCor Corpus that may be normalized to derive probabilities for each English sense . \n\t', '\n\t\t For the Sense Model , these probabilities form the initial priors over the senses , while all English ( and Spanish ) words belonging to a sense are initially assumed to be equally likely . \n\t', '\n\t\t However , initialization of the Concept Model using the same knowledge is trickier . \n\t', '\n\t\t We would like each English sense to have . \n\t', '\n\t\t But the fact that each sense belongs to multiple concepts and the constraint makes the solution non-trivial . \n\t', '\n\t\t Instead , we settle for a compromise . \n\t', '\n\t\t We set and . \n\t', '\n\t\t Subsequent normalization takes care of the sum constraints . \n\t', '\n\t\t For a Spanish sense , we set . \n\t', '\n\t\t Once we have the Spanish sense probabilities , we follow the same procedure for setting for each concept . \n\t', '\n\t\t All the Spanish and English words for a sense are set to be equally likely , as in the Sense Model . \n\t', '\n\t\t It turned out in our experiments on real data that this initialization makes a significant difference in model performance . \n\t', '\n\t\t 5 Experimental Evaluation Both the models are generative probabilistic models learned from parallel corpora and are expected to fit the training and subsequent test data . \n\t', '\n\t\t A good fit should be reflected in good prediction accuracy over a test set . \n\t', '\n\t\t The prediction task of interest is the sense of an English word when its translation is provided . \n\t', '\n\t\t We estimate the prediction accuracy and recall of our models on Senseval data.2 In addition , the Concept Model learns a sense structure for the Spanish 2Accuracy is the ratio of the number of correct predictions and the number of attempted predictions . \n\t', '\n\t\t Recall is the ratio of the number of correct predictions and the size of the test set . \n\t', '\n\t\t language . \n\t', '\n\t\t While it is hard to objectively evaluate the quality of such a structure , we present some interesting concepts that are learned as an indication of the potential of our approach . \n\t', '\n\t\t 5.1 Evaluation with Senseval Data In our experiments with real data , we make use of the parallel corpora constructed by \n\t\t']",Positive
"['\n\t\t We chose to work on these corpora in order to permit a direct comparison with their results . \n\t', '\n\t\t The sense-tagged portion of the English corpus is comprised of the English \x93all- words\x94 section of the SENSEVAL-2 test data . \n\t', '\n\t\t The remainder of this corpus is constructed by adding the Brown Corpus , the SENSEVAL-1 corpus , the SENSEVAL-2 English Lexical Sample test , trial and training corpora and the Wall Street Journal sections 18-24 from the Penn Treebank . \n\t', '\n\t\t This English corpus is translated into Spanish using two commercially available MT systems : Globalink Pro 6.4 and Systran Professional Premium . \n\t', '\n\t\t The GIZA++ implementation of the IBM statistical MT models was used to derive the most-likely word-level alignments , and these define the English/Spanish word co-occurrences . \n\t', '\n\t\t To take into account variability of translation , we combine the translations from the two systems for each English word , following in the footsteps of \n\t\t']",Positive
"['\n\t\t For our experiments , we focus only on nouns , of which there are 875 occurrences in our tagged data . \n\t', '\n\t\t The sense tags for the English domain are derived from the WordNet 1.7 inventory . \n\t', '\n\t\t After pruning stopwords , we end up with 16,186 English words , 31,862 Spanish words and 2,385,574 instances of 41,850 distinct translation pairs . \n\t', '\n\t\t The English words come from 20,361 WordNet senses . \n\t', '\n\t\t Table 1 : Comparison with Diab\x92s Model Model Accuracy Recall Parameters Diab 0.618 0.572 - Sense M. 0.624 0.616 154,947 Concept M. 0.672 0.651 120,268 As can be seen from the following table , both our models clearly outperform \n\t\t']",Negative
"['\n\t\t The comparison is restricted to the same subset of the test data . \n\t', '\n\t\t For our best results , the Sense Model has 20,361 senses , while the Concept Model has 20,361 English senses , 11,961 Spanish senses and 7,366 concepts . \n\t', '\n\t\t The Concept Model results are for the version that allows multiple senses for a Spanish word . \n\t', '\n\t\t Results for the and Figure 3 : Comparison with Senseval2 Systems single-sense model are similar . \n\t', '\n\t\t In Figure 3 , we compare the prediction accuracy and recall against those of the 21 Senseval-2 English All Words participants and that of \n\t\t']",Negative
"['\n\t\t It can be seen that our models outperform all the unsupervised approaches in recall and many supervised ones as well . \n\t', '\n\t\t No unsupervised approach is better in both accuracy and recall . \n\t', '\n\t\t It needs to be kept in mind that we take into account only bilingual data for our predictions , and not monolingual features like context of the word as most other WSD approaches do . \n\t', '\n\t\t 5.2 Semantic Grouping of Spanish Senses Table 2 shows some interesting examples of different Spanish senses for discovered concepts.3 The context of most concepts , like the ones shown , can be easily understood . \n\t', '\n\t\t For example , the first concept is about government actions and the second deals with murder and accidental deaths . \n\t', '\n\t\t The penultimate concept is interesting because it deals with different kinds of association and involves three different senses containing the word conexi´on . \n\t', '\n\t\t The other words in two of these senses suggest that they are about union and relation respectively . \n\t', '\n\t\t The third probably involves the link sense of connection . \n\t', '\n\t\t Conciseness of the concepts depends on the similarity threshold that is selected . \n\t', '\n\t\t Some may bring together loosely-related topics , which can be separated by a higher threshold . \n\t', '\n\t\t 6 Model Analysis In this section , we back up our experimental results with an in-depth analysis of the performance of our two models . \n\t', '\n\t\t Our Sense Model was motivated by \n\t\t']",Positive
"['\n\t\t This is because the machine translation system used to create the Spanish document left certain words untranslated . \n\t', '\n\t\t different . \n\t', '\n\t\t The most important distinction is that the Sense Model is a probabilistic generative model for parallel corpora , where interaction between different words stemming from the same sense comes into play , even if the words are not related through translations , and this interdependence of the senses through common words plays a role in sense disambiguation . \n\t', '\n\t\t We started off with our discussions on semantic ambiguity with the intuition that identification of semantic concepts in the corpus that relate multiple senses should help disambiguate senses . \n\t', '\n\t\t The Sense Model falls short of this target since it only brings together a single sense from each language . \n\t', '\n\t\t We will now revisit the motivating example from Section 2 and see how concepts help in disambiguation by grouping multiple related senses together . \n\t', '\n\t\t For the Sense Model , since it is the only word that can generate . \n\t', '\n\t\t However , this difference is compensated for by the higher prior probability , which is strengthened by both the translation pairs . \n\t', '\n\t\t Since the probability of joint occurrence is given by the product for any sense , the model does not develop a clear preference for any of the two senses . \n\t', '\n\t\t The critical difference in the Concept Model can be appreciated directly from the corresponding joint probability . \n\t', '\n\t\t In our example , since bar , obstrucci´on can be generated only through concept , is the only English sense conditional boosted by it . \n\t', '\n\t\t prevention , prevenci´on is generated through a different concept , where the higher condi- tional gradually strengthens one of the possible instantiations for it , and the other one becomes increasingly unlikely as the iterations progress . \n\t', '\n\t\t The inference is that only one sense of prevention is possible in the context of the parallel corpus . \n\t', '\n\t\t The key factor in this disambiguation was that two senses of prevention separated out in two different concepts . \n\t', '\n\t\t The other significant difference between the models is in the constraints on the parameters and the effect that they have on sense disambiguation . \n\t', '\n\t\t In the Sense Model , , while in the Con- cept Model , separately for each concept . \n\t', '\n\t\t Now for two relevant senses for an English word , a slight difference in their priors will tend to get ironed out when normalized over the en- 0.8 0.7 0.6 0.5 unsup . \n\t', '\n\t\t sup . \n\t', '\n\t\t diab concept model sense model 0.4 0.3 0.2 0.1 0 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Accuracy , where is the relevant concept in the model . \n\t', '\n\t\t The preference for a particular instantiation in the model is dependent not on the prior over a sense , but on the sense conditional Table 2 : Example Spanish Senses in a Concept . \n\t', '\n\t\t For each concept , each row is a separate sense . \n\t', '\n\t\t Dictionary senses of Spanish words are provided in English within parenthesis where necessary . \n\t', '\n\t\t actos accidente accidentes supremas muertes(deaths) decisi´on decisiones casualty gobernando gobernante matar(to kill ) matanzas(slaughter) muertes-le gubernamentales slaying gobernaci´on gobierno-proporciona derramamiento-de-sangre ( spilling-of-blood ) cachiporra(bludgeon) obligar(force) obligando(forcing) asesinato(murder) asesinatos prohibir prohibiendo prohibitivo prohibitiva gubernamental gobiernos linterna-el´ectrica linterna(lantern) mania craze faros-autom´ovil(headlight) culto(cult) cultos proto-senility delirio delirium linternas-portuarias(harbor-light) rabias(fury) rabia farfulla(do hastily ) antorcha(torch) antorchas antorchas-pino-nudo oportunidad oportunidades diferenciaci´on ocasi´on ocasiones distinci´on distinciones riesgo(risk) riesgos peligro(danger) destino sino(fate) especializaci´on fortuna suerte(fate) maestria ( mastery ) probabilidad probabilidades peculiaridades particularidades peculiaridades-inglesas especialidad especialidades diablo(devil) diablos dickens modelo parang´on heller ideal ideales lucifer satan satan´as santo(saint) santos san idol idols idolo deslumbra(dazzle) cromo(chromium) meteoro meteoros meteor meteoros-blue dios god dioses meteorito meteoritos pedregosos(rocky) divinidad divinity inmortal(immortal) inmortales teologia teolog deidad deity deidades variaci´on variaciones minutos minuto discordancia desacuerdo(discord) discordancias desviaci´on(deviation) desviaciones desviaciones-normales discrepancia discrepancias fugaces(fieeting) variaci´on diferencia disensi´on momento momentos un-momento minutos momentos momento segundos instante momento pesta\x98neo(blink) gui\x98na(wink) pesta\x98nean adhesi´on adherencia ataduras(tying) enlace(connection) ataduras pasillo(corridor) atadura ataduras aisle conexi´on conexiones pasarela(footbridge) conexi´on une(to unite ) hall vestibulos relaci´on conexi´on pasaje(passage) implicaci´on ( complicity ) envolvimiento callej´on(alley) callejas-ciegas ( blind alley ) callejones-ocultos tire set of senses for the corpus . \n\t', '\n\t\t In contrast , if these two senses belong to the same concept in the Concept Model , the difference in the sense conditionals will be highlighted since the normalization occurs over a very small set of senses \x97 the senses for only that concept , which in the best possible scenario will contain only the two contending senses , as in concept of our example . \n\t', '\n\t\t As can be seen from Table 1 , the Concept Model not only outperforms the Sense Model , it does so with significantly fewer parameters . \n\t', '\n\t\t This may be counter-intuitive since Concept Model involves an extra concept variable . \n\t', '\n\t\t However , the dissociation of Spanish and English senses can significantly reduce the parameter space . \n\t', '\n\t\t Imagine two Spanish words that are associated with ten English senses and ac cordingly each of them has a probability for belonging to each of these ten senses . \n\t', '\n\t\t Aided with a concept variable , it is possible to model the same relationship by creating a separate Spanish sense that contains these two words and relating this Spanish sense with the ten English senses through a concept variable . \n\t', '\n\t\t Thus these words now need to belong to only one sense as opposed to ten . \n\t', '\n\t\t Of course , now there are new transition probabilities for each of the eleven senses from the new concept node . \n\t', '\n\t\t The exact reduction in the parameter space will depend on the frequent subsets discovered for the s of the Spanish words . \n\t', '\n\t\t Longer and more frequent subsets will lead to larger reductions . \n\t', '\n\t\t It must also be borne in mind that this reduction comes with the independence assumptions made in the Concept Model . \n\t', '\n\t\t 7 Conclusions and Future Work We have presented two novel probabilistic models for unsupervised word sense disambiguation using parallel corpora and have shown that both models outperform existing unsupervised approaches . \n\t', '\n\t\t In addition , we have shown that our second model , the Concept model , can be used to learn a sense inventory for the secondary language . \n\t', '\n\t\t An advantage of the probabilistic models is that they can easily incorporate additional information , such as context information . \n\t', '\n\t\t In future work , we plan to investigate the use of additional monolingual context . \n\t', '\n\t\t We would also like to perform additional validation of the learned secondary language sense inventory . \n\t', '\n\t\t 8 Acknowledgments The authors would like to thank Mona Diab and Philip Resnik for many helpful discussions and insightful comments for improving the paper and also for making their data available for our experiments . \n\t', '\n\t\t This study was supported by NSF Grant 0308030 . \n\t', '\n\t\t References E. Agirre , J. Atserias , L. Padr , and G. Rigau . \n\t', '\n\t\t 2000. Combining supervised and unsupervised lexical knowledge methods for word sense disambiguation . \n\t', '\n\t\t In Computers and the Humanities , Special Double Issue on SensEval . \n\t', '\n\t\t Eds . \n\t', '\n\t\t Martha Palmer and Adam Kilgarriff . \n\t', '\n\t\t 34:1,2. Yoshua Bengio and Christopher Kermorvant . \n\t', '\n\t\t 2003. Extracting hidden sense probabilities from bitexts . \n\t', '\n\t\t Technical report , TR 1231 , Departement d\x92 informatique et recherche operationnelle , Universite de Montreal . \n\t', '\n\t\t Peter F. Brown , Stephen Della Pietra , Vincent J. Della Pietra , and Robert L. Mercer . \n\t', '\n\t\t 1991. Word-sense disambiguation using statistical methods . \n\t', '\n\t\t In Meeting of the Association for Computational Linguistics , pages 264\x96270 . \n\t', '\n\t\t Rebecca Bruce and Janyce Wiebe . \n\t', '\n\t\t 1994. A new approach to sense identification . \n\t', '\n\t\t In ARPA Workshop on Human Language Technology . \n\t', '\n\t\t Ido Dagan and Alon Itai . \n\t', '\n\t\t 1994. Word sense disambiguation using a second language monolingual corpus . \n\t', '\n\t\t Computational Linguistics , 20(4):563\x96 596 . \n\t', '\n\t\t Ido Dagan . \n\t', '\n\t\t 1991. Lexical disambiguation : Sources of information and their statistical realization . \n\t', '\n\t\t In Meeting of the Association for Computational Linguistics , pages 341\x96342 . \n\t', '\n\t\t A.P. Dempster , N.M. Laird , and D.B. Rubin . \n\t', '\n\t\t 1977. Maximum likelihood from incomplete data via the EM algorithm . \n\t', '\n\t\t Journal of the Royal Statistical Society , B 39:1\x9638 . \n\t', '\n\t\t Mona Diab and Philip Resnik . \n\t', '\n\t\t 2002. An unsupervised method for word sense tagging using parallel corpora . \n\t', '\n\t\t In Proceedings of the 40th Anniversary Meeting of the Association for Computational Linguistics ( ACL-02 ) . \n\t', '\n\t\t Mona Diab . \n\t', '\n\t\t 2003. Word Sense Disambiguation Within a Multilingual Framework . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , University of Maryland , College Park . \n\t', '\n\t\t Christiane Fellbaum . \n\t', '\n\t\t 1998. WordNet : An Electronic Lexical Database . \n\t', '\n\t\t MIT Press . \n\t', '\n\t\t Nancy Ide and Jean Veronis . \n\t', '\n\t\t 1998. Word sense disambiguation : The state of the art . \n\t', '\n\t\t Computational Linguistics , 28(1):1\x9640 . \n\t', '\n\t\t Nancy Ide . \n\t', '\n\t\t 2000. Cross-lingual sense determination : Can it work ? \n\t', '\n\t\t In Computers and the Humanities : Special Issue on Senseval , 34:147-152 . \n\t', '\n\t\t Adam Kilgarrif and Joseph Rosenzweig . \n\t', '\n\t\t 2000. Framework and results for english senseval . \n\t', '\n\t\t Computers and the Humanities , 34(1):15\x9648 . \n\t', '\n\t\t Dekang Lin . \n\t', '\n\t\t 2000. Word sense disambiguation with a similarity based smoothed library . \n\t', '\n\t\t In Computers and the Humanities : Special Issue on Senseval , 34:147-152 . \n\t', '\n\t\t K. C. Litkowski . \n\t', '\n\t\t 2000. Senseval : The cl research experience . \n\t', '\n\t\t In Computers and the Humanities , 34(1-2) , pp. 153-8 . \n\t', '\n\t\t Philip Resnik and David Yarowsky . \n\t', '\n\t\t 1999 . \n\t', '\n\t\t Distinguishing systems and distinguishing senses : new evaluation methods for word sense disambiguation . \n\t', '\n\t\t Natural Language Engineering , 5(2) . \n\t', '\n\t\t Philip Resnik . \n\t', '\n\t\t 1995. Using information content to evaluate semantic similarity in a taxonomy . \n\t', '\n\t\t In Proceedings of the International Joint Conference on Artificial Intelligence , pages 448\x96453 . \n\t', '\n\t\t Philip Resnik . \n\t', '\n\t\t 1997. Selectional preference and sense disambiguation . \n\t', '\n\t\t In Proceedings of ACL Siglex Workshop on Tagging Text with Lexical Semantics , Why , What and How ? \n\t', '\n\t\t , Washington , April 4-5 . \n\t', '\n\t\t David Yarowsky . \n\t', '\n\t\t 1992. Word-sense disambiguation using statistical models of Roget\x92s categories trained on large corpora . \n\t', '\n\t\t In Proceedings of COLING-92 , pages 454\x96460 , Nantes , France , July . \n\t', '\n\t\t David Yarowsky . \n\t', '\n\t\t 1993. One sense per collocation . \n\t', '\n\t\t In Proceedings , ARPA Human Language Technology Workshop , Princeton . \n\t', '\n\t\t David Yarowsky . \n\t', '\n\t\t 1995. Unsupervised word sense disambiguation rivaling supervised methods . \n\t', '\n\t\t In Meeting of the Association for Computational Linguistics , pages 189\x96196 . \n\t', '\n\t\t Chinese Verb Sense Discrimination Using an EM Clustering Model with Rich Linguistic Features ~inying Chen , Martha Palmer Department of Computer and Information Science University of Pennsylvania Philadelphia , PA , 19104 {jinying,mpalmer}@linc.cis.upenn.edu Abstract This paper discusses the application of the Expectation-Maximization ( EM ) clustering algorithm to the task of Chinese verb sense discrimination . \n\t', '\n\t\t The model utilized rich linguistic features that capture predicate- argument structure information of the target verbs . \n\t', '\n\t\t A semantic taxonomy for Chinese nouns , which was built semi-automatically based on two electronic Chinese semantic dictionaries , was used to provide semantic features for the model . \n\t', '\n\t\t Purity and normalized mutual information were used to evaluate the clustering performance on 12 Chinese verbs . \n\t', '\n\t\t The experimental results show that the EM clustering model can learn sense or sense group distinctions for most of the verbs successfully . \n\t', '\n\t\t We further enhanced the model with certain fine-grained semantic categories called lexical sets . \n\t', ""\n\t\t Our results indicate that these lexical sets improve the model 's performance for the three most challenging verbs chosen from the first set of experiments . \n\t"", '\n\t\t 1 Introduction Highly ambiguous words may lead to irrelevant document retrieval and inaccurate lexical choice in machine translation \n\t\t']",Positive
"['\n\t\t This paper addresses WSD in Chinese through developing an Expectation-Maximization ( EM ) clustering model to learn Chinese verb sense distinctions . \n\t', '\n\t\t The major goal is to do sense discrimination rather than sense labeling , similar to \n\t\t']",Positive
"['\n\t\t The basic idea is to divide instances of a word into several clusters that have no sense labels . \n\t', '\n\t\t The instances in the same cluster are regarded as having the same meaning . \n\t', '\n\t\t Word sense discrimination can be applied to document retrieval and similar tasks in information access , and to facilitating the building of large annotated corpora . \n\t', '\n\t\t In addition , since the clustering model can be trained on large unannotated corpora and evaluated on a relatively small sense-tagged corpus , it can be used to find indicative features for sense distinctions through exploring huge amount of available unannotated text data . \n\t', '\n\t\t The EM clustering algorithm \n\t\t']",Positive
"['\n\t\t In our task , we equipped the EM clustering model with rich linguistic features that capture the predicate-argument structure information of verbs and restricted the feature set for each verb using knowledge from dictionaries . \n\t', '\n\t\t We also semiautomatically built a semantic taxonomy for Chinese nouns based on two Chinese electronic semantic dictionaries , the Hownet dictionary1 and the Rocling dictionary.2 The 7 top-level categories of this taxonomy were used as semantic features for the model . \n\t', '\n\t\t Since external knowledge is used to obtain the semantic features and guide feature selection , the model is not completely unsupervised from this perspective ; however , it does not make use of any annotated training data . \n\t', '\n\t\t Two external quality measures , purity and normalized mutual information ( NMI ) ( Strehl . \n\t', ""\n\t\t 2002 ) , were used to evaluate the model 's performance on 12 Chinese verbs . \n\t"", '\n\t\t The experimental results show that rich linguistic features and the semantic taxonomy are both very useful in sense discrimination . \n\t', '\n\t\t The model generally performs well in learning sense group distinctions for difficult , highly polysemous verbs and sense distinctions for other verbs . \n\t', '\n\t\t Enhanced by certain fine-grained semantic categories called lexical sets \n\t\t']",Positive
"['\n\t\t 2 A Chinese electronic dictionary liscenced from The Association for Computational Linguistics and Chinese Language Processing ( ACLCLP ) , Nankang , Taipei , Taiwan . \n\t', '\n\t\t performance improved in a preliminary experiment for the three most difficult verbs chosen from the first set of experiments . \n\t', '\n\t\t The paper is organized as follows : we briefly introduce the EM clustering model in Section 2 and describe the features used by the model in Section 3 . \n\t', '\n\t\t In Section 4 , we introduce a semantic taxonomy for Chinese nouns , which is built semiautomatically for our task but can also be used in other NLP tasks such as co-reference resolution and relation detection in information extraction . \n\t', '\n\t\t We report our experimental results in Section 5 and conclude our discussion in Section 6 . \n\t', '\n\t\t 2 EM Clustering Model The basic idea of our EM clustering approach is similar to the probabilistic model of co-occurrence described in detail in \n\t\t']",Positive
"['\n\t\t In our model , we treat a set of features { f1,f2,...,fm } , which are extracted from the parsed sentences that contain a target verb , as observed variables . \n\t', '\n\t\t These variables are assumed to be independent given a hidden variable c , the sense of the target verb . \n\t', '\n\t\t Therefore the joint probability of the observed variables ( features ) for each verb instance , i.e. , each parsed sentence containing the target verb , is defined in equation ( 1 ) , m A 1,f2,...,fm)=^p(0^pV 10 (1)c i=1 In the maximization step ( M-step ) , p(c) and p(fi I c ) are re-computed by maximizing the log- likelihood of all the observed data which is calculated by using p(c I f\x84 f2 , ... , fm ) estimated in the E-step . \n\t', '\n\t\t The E-step and M-step are repeated for a fixed number of rounds , which is set to 20 in our experiments,4 or till the amount of change of p(c) and p(fi I c ) is under the threshold 0.001 . \n\t', '\n\t\t When doing classification , for each verb instance , the model calculates the same conditional probability as in equation ( 3 ) and assigns the instance to the cluster with the maximal p(c~ f1,f2 , ... , fm ) . \n\t', '\n\t\t 3 Features Used in the Model The EM clustering model uses a set of linguistic features to capture the predicate-argument structure information of the target verbs . \n\t', '\n\t\t These features are usually more indicative of verb sense distinctions than simple features such as words next to the target verb or their POS tags . \n\t', '\n\t\t For example , the Chinese verb "" ff{ ~ chu1 "" has a sense of produce , the distinction between this sense and the verb \'s other senses , such as happen and go out , largely depends on the semantic category of the verb \'s direct object . \n\t', ""\n\t\t Typical examples are shown in ( 1 ) , The fi 's are discrete-valued features that can take multiple values . \n\t"", ""\n\t\t A typical feature used in our model is shown in ( 2 ) , 0 iff the target verb has no sentential complement 1 iff the target verb has a nonfinite ( 2 ) sentential complement 2 iff the target verb has a finite sentential complement At the beginning of training ( i.e. , clustering ) , the model 's parameters p(c) and p(fi ~I c ) are randomly initialized.3 Then , the probability of c conditioned on the observed features is computed in the expectation step ( E-step ) , using equation ( 3 ) , Ac f , f2 , ... , fm ) 3 In our experiments , for verbs with more than 3 senses , syntactic and semantic restrictions derived from dictionary entries are used to constrain the random initialization . \n\t"", '\n\t\t ( 1 ) a. Wrl/their R/county ff{/produce * /banana "" Their county produces bananas . \n\t', '\n\t\t "" b. Wrl/their R/county ff{/happen )Q/big ~/event _T /ASP "" A big event happened in their county . \n\t', '\n\t\t "" c. Wrl/their R/county ff{/go out 1/door a /right away A/be 111 /mountain "" In their county , you can see mountains as soon as you step out of the doors . \n\t', '\n\t\t "" The verb has the sense produce in ( 1a ) and its object should be something producible , such as "" ~~/banana "" . \n\t', '\n\t\t While in ( 1b ) , with the sense happen , the verb typically takes an event or event- like object , such as "" )Q~/big event "" , "" #/accident "" or "" MM/problem "" etc. . \n\t', '\n\t\t In ( 1c ) , the verb \'s object "" 1/door "" is closely related to location , consistent with the sense go out . \n\t', '\n\t\t In contrast , simple lexical or POS tag features sometimes fail to capture such information , which can be seen clearly in ( 2 ) , 4 In our experiments , we set 20 as the maximal number of rounds after trying different numbers of rounds ( 20 , 40 , 60 , 80 , 100 ) in a preliminary experiment . \n\t', '\n\t\t f , i fi l m p(c) p(f 10 i 1 m ( 3 ) ^p(c)^ p ( f 10 ci= 1 ^ ( 2 ) a. -T,-*/last year HI/produce fF /banana 3000 ~~ / kilogram "" 3000 kilograms of bananas were produced last year . \n\t', '\n\t\t "" "" The verb \'s object "" fF /banana "" , which is next to the verb in ( 2a ) , is far away from the verb in ( 2b ) . \n\t', '\n\t\t For ( 2b ) , a classifier only looking at the adjacent positions of the target verb tends to be misled by the NP right after the verb , i.e. , "" MM/Hainan "" , which is a Province in China and a typical object of the verb with the sense go out . \n\t', '\n\t\t Five types of features are used in our model : 1 . \n\t', '\n\t\t Semantic category of the subject of the target verb 2 . \n\t', '\n\t\t Semantic category of the object of the target verb 3 . \n\t', '\n\t\t Transitivity of the target verb 4 . \n\t', '\n\t\t Whether the target verb takes a sentential complement and which type of sentential complement ( finite or nonfinite ) it takes 5 . \n\t', '\n\t\t Whether the target verb occurs in a verb compound We obtain the values for the first two types of features ( 1 ) and ( 2 ) from a semantic taxonomy for Chinese nouns , which we will introduce in detail in the next section . \n\t', '\n\t\t In our implementation , the model uses different features for different verbs . \n\t', '\n\t\t The criteria for feature selection are from the electronic CETA dictionary file 5 and a hard copy English-Chinese dictionary , The Warmth Modern Chinese-English Dictionary.6 For example , the verb "" HI ~chu1 "" never takes sentential complements , thus the fourth type of feature is not used for it . \n\t', '\n\t\t It could be supposed that we can still have a uniform model , i.e. , a model using the same set of features for all the target verbs , and just let the EM clustering algorithm find useful features for different verbs automatically . \n\t', '\n\t\t The problem here is that unsupervised learning models ( i.e. , models trained on unlabeled data ) are more likely to be affected by noisy data than supervised ones . \n\t', '\n\t\t Since all the features used in our model are extracted from automatically parsed sentences that inevitably have preprocessing errors such as segmentation , POS tagging and parsing errors , using verb-specific sets of features can alleviate the problem caused by noisy data to some extent . \n\t', '\n\t\t For example , if the model already knows 5 Licensed from the Department of Defense 6 The Warmth Modern Chinese-English Dictionary , Wang-Wen Books Ltd , 1997. that a verb like "" HI~chu1 "" can never take sentential complements ( i.e. , it does not use the fourth type of feature for that verb ) , it will not be misled by erroneous parsing information saying that the verb takes sentential complements in certain sentences . \n\t', '\n\t\t Since the corresponding feature is not included , the noisy data is filtered out . \n\t', '\n\t\t In our EM clustering model , all the features selected for a target verb are treated in the same way , as described in Section 2 . \n\t', '\n\t\t 4 A Semantic Taxonomy Built Semiautomatically Examples in ( 1 ) have shown that the semantic category of the object of a verb sometimes is crucial in distinguishing certain Chinese verb senses . \n\t', '\n\t\t And our previous work on information extraction in Chinese \n\t\t']",Positive
"['\n\t\t We have two Chinese electronic semantic dictionaries : the Hownet dictionary , which assigns 26,106 nouns to 346 semantic categories , and the Rocling dictionary , which assigns 4,474 nouns to 110 semantic categories.7 A preliminary experimental result suggests that these semantic categories might be too fine-grained for the EM clustering model ( see Section 5.2 for greater details ) . \n\t', '\n\t\t An analysis of the sense distinctions of several Chinese verbs also suggests that more general categories on top of the Hownet and Rocling categories could still be informative and most importantly , could enable the model to generate meaningful clusters more easily . \n\t', '\n\t\t We therefore built a three-level semantic taxonomy based on the two semantic dictionaries using both automatic methods and manual effort . \n\t', '\n\t\t The taxonomy was built in three steps . \n\t', '\n\t\t First , a simple mapping algorithm was used to map semantic categories defined in Hownet and Rocling into 27 top-level WordNet categories.8 The Hownet or Rocling semantic categories have English glosses . \n\t', '\n\t\t For each category gloss , the algorithm looks through the hypernyms of its first sense in WordNet and chooses the first WordNet top-level category it finds . \n\t', '\n\t\t 7 Hownet assigns multiple entries ( could be different semantic categories ) to polysemous words . \n\t', '\n\t\t The Rocling dictionary we used only assigns one entry ( i.e. , one semantic category ) to each noun . \n\t', '\n\t\t 8 The 27 categories contain 25 unique beginners for noun source files in WordNet , as defined in \n\t\t']",Positive
"['\n\t\t b. -5/in order to HI/produce ~~/Hainan AO/best N/DE fF /banana "" In order to produce the best bananas in Hainan , Figure 1 . \n\t', '\n\t\t Part of the 3-level Semantic Taxonomy for Chinese Nouns ( other top-level nodes are Time , Human , Animal and State ) Top level Intermediate level Hownet/Rocling categories Event Entity Plant Artifact Document Food Money drinks , edible , meals , vegetable , ... \n\t', '\n\t\t Location\x97Part Location Group ...... institution , army , corporation , ... chase , cut , pass , split , cheat , ... process , BecomeLess , StateChange , disappear , .... Location Natural Phenomena Happening Activity ...... Process The mapping obtained from step 1 needs further modification for two reasons . \n\t', '\n\t\t First , the glosses of Hownet or Rocling semantic categories usually have multiple senses in WordNet . \n\t', '\n\t\t Sometimes , the first sense in WordNet for a category gloss is not its intended meaning in Hownet or Rocling . \n\t', '\n\t\t In this case , the simple algorithm cannot get the correct mapping . \n\t', '\n\t\t Second , Hownet and Rocling sometimes use adjectives or non-words as category glosses , such as animate and LandYehicle etc. , which have no WordNet nominal hypernyms at all . \n\t', '\n\t\t However , those adjectives or non-words usually have straightforward meanings and can be easily reassigned to an appropriate WordNet category . \n\t', '\n\t\t Although not accurate , the automatic mapping in step 1 provides a basic framework or skeleton for the semantic taxonomy we want to build and makes subsequent work easier . \n\t', '\n\t\t In step 2 , hand correction , we found that we could make judgments and necessary adjustments on about 80 % of the mappings by only looking at the category glosses used by Hownet or Rocling , such as livestock , money , building and so on . \n\t', '\n\t\t For the other 20 % , we could make quick decisions by looking them up in an electronic table we created . \n\t', '\n\t\t For each Hownet or Rocling category , our table lists all the nouns assigned to it by the two dictionaries . \n\t', '\n\t\t We merged two WordNet categories into others and subdivided three categories that seemed more coarse-grained than others into 2~5 subcategories . \n\t', '\n\t\t Step 2 took three days and 35 intermediate-level categories were generated . \n\t', '\n\t\t In step 3 , we manually clustered the 35 intermediate-level categories into 7 top-level semantic categories . \n\t', '\n\t\t Figure 1 shows part of the taxonomy . \n\t', '\n\t\t The EM clustering model uses the 7 top-level categories to define the first two types of features that were introduced in Section 3 . \n\t', '\n\t\t For example , the value of a feature fk is 1 if and only if the object NP of the target verb belongs to the semantic category Event and is otherwise 0 . \n\t', '\n\t\t 5 Clustering Experiments Since we need labeled data to evaluate the clustering performance but have limited sense- tagged corpora , we applied the clustering model to 12 Chinese verbs in our experiments . \n\t', '\n\t\t The verbs are chosen from 28 annotated verbs in Penn Chinese Treebank so that they have at least two verb meanings in the corpus and for each of them , the number of instances for a single verb sense does not exceed 90 % of the total number of instances . \n\t', '\n\t\t In our task , we generally do not include senses for other parts of speech of the selected words , such as noun , preposition , conjunction and particle etc. , since the parser we used has a very high accuracy in distinguishing different parts of speech of these words ( >98 % for most of them ) . \n\t', '\n\t\t However , we do include senses for conjunctional and/or prepositional usage of two words , "" 101dao4 "" and "" Awei4 "" , since our parser cannot distinguish the verb usage from the conjunctional or prepositional usage for the two words very well . \n\t', '\n\t\t Five verbs , the first five listed in Table 1 , are both highly polysemous and difficult for a supervised word sense classifier \n\t\t']",Positive
"['\n\t\t 9 In our experiments , we manually grouped the verb senses for the five verbs . \n\t', ""\n\t\t The criteria for the grouping are similar to Palmer et al. 's ( to appear ) work on English verbs , which considers both sense coherence and predicate-argument structure distinctions . \n\t"", '\n\t\t Figure 2 gives an example of 9 In the supervised task , their accuracies are lower than 85 % , and four of them are even lower than the baselines . \n\t', '\n\t\t Senses for "" ^~dao4 "" Sense groups for "" ^~dao4 "" 1. to go to , leave for 2. to come 3. to arrive 4. to reach a particular stage , condition , or level 5. marker for completion of activities ( after a verb ) 6. marker for direction of activities ( after a verb ) 7. to reach a time point 8. up to , until ( prepositional usage ) 9. up to , until , ( from ... ) to ... ( conjunctional usage ) 1 , 2 5 3 4,7,8,9 6 Figure 2. Sense groups for the Chinese verb "" YgJdao4 "" the definition of sense groups . \n\t', ""\n\t\t The manually defined sense groups are used to evaluate the model 's performance on the five verbs . \n\t"", ""\n\t\t The model was trained on an unannotated corpus , People 's Daily News ( PDN ) , and tested on the manually sense-tagged Chinese Treebank ( with some additional sense-tagged PDN data).10 We parsed the training and test data using a Maximum Entropy parser and extracted the features from the parsed data automatically . \n\t"", '\n\t\t The number of clusters used by the model is set to the number of the defined senses or sense groups of each target verb . \n\t', '\n\t\t For each verb , we ran the EM clustering algorithm ten times . \n\t', '\n\t\t Table 2 shows the average performance and the standard deviation for each verb . \n\t', '\n\t\t Table 1 summarizes the data used in the experiments , where we also give the normalized sense perplexity11 of each verb in the test data . \n\t', '\n\t\t 5.1 Evaluation Methods We use two external quality measures , purity and normalized mutual information ( NMI ) ( Strehl . \n\t', '\n\t\t 2002 ) to evaluate the clustering performance . \n\t', '\n\t\t Assuming a verb has l senses , the clustering model assigns n instances of the verb into k clusters , ni is the size of the ith cluster , nj is the number of instances hand-tagged with the jth sense , and n ; is the number of instances with the jth sense in the ith cluster , purity is defined in equation ( 4 ) : 10 The sense-tagged PDN data we used here are the same as in \n\t\t']",Positive
"['\n\t\t 11 It is calculated as the entropy of the sense distribution of a verb in the test data divided by the largest possible entropy , i.e. , log2 ( the number of senses of the verb in the test data ) . \n\t', '\n\t\t It can be interpreted as classification accuracy when for each cluster we treat the majority of instances that have the same sense as correctly classified . \n\t', '\n\t\t The baseline purity is calculated by treating all instances for a target verb in a single cluster . \n\t', '\n\t\t The purity measure is very intuitive . \n\t', '\n\t\t In our case , since the number of clusters is preset to the number of senses , purity for verbs with two senses is equal to classification accuracy defined in supervised WSD . \n\t', '\n\t\t However , for verbs with more than 2 senses , purity is less informative in that a clustering model could achieve high purity by making the instances of 2 or 3 dominant senses the majority instances of all the clusters . \n\t', '\n\t\t Mutual information ( MI ) is more theoretically well-founded than purity . \n\t', '\n\t\t Treating the verb sense and the cluster as random variables S and C , the MI between them is defined in equation ( 5 ) : MI(S,C) = p(s,c ^ ni logni n j n ni n MI(S,C) characterizes the reduction in uncertainty of one random variable S ( or C ) due to knowing the other variable C ( or S ) . \n\t', '\n\t\t A single cluster with all instances for a target verb has a zero MI . \n\t', '\n\t\t Random clustering also has a zero MI in the limit . \n\t', '\n\t\t In our experiments , we used [0,1]- normalized mutual information ( NMI ) ( Strehl . \n\t', '\n\t\t 2002 ) . \n\t', '\n\t\t A shortcoming of this measure , however , is that the best possible clustering ( upper bound ) evaluates to less than 1 , unless classes are balanced . \n\t', '\n\t\t Unfortunately , unbalanced sense distribution is the usual case in WSD tasks , which makes NMI itself hard to interpret . \n\t', '\n\t\t Therefore , in addition to NMI , we also give its upper bound ( upper-NMI ) and the ratio of NMI and its upper bound ( NMI-ratio ) for each verb , as shown in columns 6 to 8 in Table 2 . \n\t', ""\n\t\t 1 max nij ( 4 ) n i=1 k purity j s,c l k ^^ j =1 i=1 ) log Xs , c ) p(s)p(c) ( 5 ) Verbl Pinyin Sample senses of the verb , ` f ~chu1 go out /produce Yq Jdao4 come /reach AH ~jian4 see /show ~ ~xiang3 think/suppose O Jyao4 Should/intend to */~biao3shi4 Indicate /express R~~fa1xian4 discover /realize R&~fa1zhan3 develop /grow ' k~~hui1fu4 resume /restore iA ~shuo1 say /express by written words R,&Jtou2ru4 to input /plunge into j ~wei2~4 to be /in order to # Senses in test data # Sense groups in test data Sense perplexity # Clusters # Training instances # Test instances 16 7 0.68 8 399 157 9 5 0.72 6 1838 186 8 5 0.68 6 117 82 6 4 0.64 6 94 228 8 4 0.65 7 2781 185 2 0.93 2 666 97 2 0.76 2 319 27 3 0.69 3 458 130 4 0.83 4 107 125 7 0.40 7 2692 307 2 1.00 2 136 23 6 0.82 6 547 463 Table 1 . \n\t"", '\n\t\t A summary of the training and test data used in the experiments Verb Sense Baseline Purity Std. Dev . \n\t', '\n\t\t of NMI Upper- NMI- Std. Dev . \n\t', ""\n\t\t of perplexity Purity ( % ) ( % ) purity ( % ) NMI ratio ( % ) NMI ratio ( % ) , ` f 0.68 52.87 63.31 1.59 0.2954 0.6831 43.24 1.76 Yq 0.72 40.32 90.48 1.08 0.4802 0.7200 75.65 0.00 AH 0.68 58.54 72.20 1.61 0.1526 0.6806 22.41 0.66 ~ 0.64 68.42 79.39 3.74 0.2366 0.6354 37.24 8.22 O 0.65 69.19 69.62 0.34 0.0108 0.6550 1.65 0.78 */ 0.93 64.95 98.04 1.49 0.8670 0.9345 92.77 0.00 RJR 0.76 77.78 97.04 3.87 0.7161 0.7642 93.71 13.26 R& 0.69 53.13 90.77 0.24 0.4482 0.6918 64.79 2.26 ' kA 0.83 45.97 65.32 0.00 0.1288 0.8234 15.64 0.00 iA 0.40 80.13 93.00 0.58 0.3013 0.3958 76.13 4.07 ~,& 1.00 52.17 95.65 0.00 0.7827 0.9986 78.38 0.00 j 0.82 32.61 75.12 0.43 0.4213 0.8213 51.30 2.07 Average 0.73 58.01 82.50 1.12 0.4088 0.7336 54.41 3.31 Table 2 . \n\t"", '\n\t\t The performance of the EM clustering model on 12 Chinese verbs measured by purity and normalized mutual information ( NMI ) 5.2 Experimental Results Table 2 summarizes the experimental results for the 12 Chinese verbs . \n\t', '\n\t\t As we see , the EM clustering model performs well on most of them , except the verb ""O~yao4"".12 The NMI measure NMI-ratio turns out to be more stringent than purity . \n\t', '\n\t\t A high purity does not necessarily mean a high NMI-ratio . \n\t', '\n\t\t Although intuitively , NMI-ratio should be related to sense perplexity and purity , it is hard to formalize the relationships between them from the results . \n\t', ""\n\t\t In fact , the NMI-ratio for a particular verb is eventually determined by its concrete sense distribution in the test data and the model 's clustering behavior for that verb . \n\t"", '\n\t\t For example , the verbs "" , ` f ~chu1 "" and "" AH~jian4 "" have the same sense perplexity and "" AH~jian4 "" has a higher purity than "" , ` f~chu1 "" ( 72.20 % vs. 63.31 % ) , but the NMI- ratio for "" AH~jian4 "" is much lower than "" , ` f ~chu1 "" ( 22.41 % vs. 43.24 % ) . \n\t', '\n\t\t An analysis of the 12 For all the verbs except "" Olyao4 "" , the model \'s purities outperformed the baseline purities significantly ( p<0.05 , and p<0.001 for 8 of them ) . \n\t', '\n\t\t classification results for "" AH~jian4 "" shows that the clustering model made the instances of the verb \'s most dominant sense the majority instances of three clusters ( of total 5 clusters ) , which is penalized heavily by the NMI measure . \n\t', '\n\t\t Rich linguistic features turn out to be very effective in learning Chinese verb sense distinctions . \n\t', '\n\t\t Except for the two verbs , "" R~~fa1xian4 "" and "" */~biao3shi4 "" , the sense distinctions of which can usually be made only by syntactic alternations,13 features such as semantic features or combinations of semantic features and syntactic alternations are very beneficial and sometimes even necessary for learning sense distinctions of other verbs . \n\t', '\n\t\t For example , the verb "" AH~jian4 "" has one sense see , in which the verb typically takes a Human subject and a sentential complement , while in another sense show , the verb typically takes an Entity subject and a State object . \n\t', '\n\t\t An inspection of the classification results shows 13 For example , the verb "" R~~fa1xian4 "" takes an object in one sense discover and a sentential complement in the other sense realize . \n\t', '\n\t\t that the EM clustering model has indeed learned such combinatory patterns from the training data . \n\t', '\n\t\t The experimental results also indicate that the semantic taxonomy we built is beneficial for the task . \n\t', '\n\t\t For example , the verb "" RA~tou1ru4 "" has two senses , input and plunge into . \n\t', '\n\t\t It typically takes an Event object for the second sense but not for the first one . \n\t', '\n\t\t A single feature obtained from our semantic taxonomy , which tests whether the verb takes an Event object , captures this property neatly ( achieves purity 95.65 % and NMI-ratio 78.38 % when using 2 clusters ) . \n\t', '\n\t\t Without the taxonomy , the top-level category Event is split into many fine- grained Hownet or Rocling categories , which makes it very difficult for the EM clustering model to learn sense distinctions for this verb . \n\t', '\n\t\t In fact , in a preliminary experiment only using the Hownet and Rocling categories , the model had the same purity as the baseline ( 52.17 % ) and a low NMI-ratio ( 4.22 % ) when using 2 clusters . \n\t', '\n\t\t The purity improved when using more clusters ( 70.43 % with 4 clusters and 76.09 % with 6 ) , but it was still much lower than the purity achieved by using the semantic taxonomy and the NMI-ratio dropped further ( 1.19 % and 1.20 % for the two cases ) . \n\t', '\n\t\t By looking at the classification results , we identified three major types of errors . \n\t', '\n\t\t First , preprocessing errors create noisy data for the model . \n\t', '\n\t\t Second , certain sense distinctions depend heavily on global contextual information ( cross- sentence information ) that is not captured by our model . \n\t', '\n\t\t This problem is especially serious for the verb "" -W~yao4 "" . \n\t', '\n\t\t For example , without global contextual information , the verb can have at least three meanings want , need or should in the same clause , as shown in ( 3 ) . \n\t', ""\n\t\t ( 3 ) 4th/he -W/want/need/should \x971~1_'/at once Ik7/finish reading 7Z*/this 4/book . \n\t"", '\n\t\t "" He wants to/needs to/should finish reading this book at once . \n\t', '\n\t\t "" Third , a target verb sometimes has specific types of NP arguments or co-occurs with specific types of verbs in verb compounds in certain senses . \n\t', '\n\t\t Such information is crucial for distinguishing these senses from others , but is not captured by the general semantic taxonomy used here . \n\t', '\n\t\t We did further experiments to investigate how much improvement the model could gain by capturing such information , as discussed in Section 5.3 . \n\t', '\n\t\t 5.3 Experiments with Lexical Sets As discussed by Patrick \n\t\t']",Positive
"['\n\t\t For example , in our case , the verb "" \' NV_~hui1fu4 "" has a sense recover in which its direct object should be something that can be recovered naturally . \n\t', ""\n\t\t A typical set of object NPs of the verb for this particular sense is partially listed in ( 4 ) , ( 4 ) Lexical set for naturally recoverable things J#JJ/physical strength , &14/body , tl /health , ~JJ/mental energy , of JJ/hearing , O /feeling , iE'hZJJ/memory , ...... } Most words in this lexical set belong to the Hownet category attribute and the top-level category State in our taxonomy . \n\t"", '\n\t\t However , even the lower-level category attribute still contains many other words irrelevant to the lexical set , some of which are even typical objects of the verb for two other senses , resume and regain , such as "" MZ/diplomatic relations "" in "" \' NV_/resume MZ/diplomatic relations "" and "" 12;-4/reputation "" in "" \' NV_/regain12;-4/reputation "" . \n\t', '\n\t\t Therefore , a lexical set like ( 4 ) is necessary for distinguishing the recover sense from other senses of the verb . \n\t', '\n\t\t It has been argued that the extensional definition of lexical sets can only be done using corpus evidence and it cannot be done fully automatically \n\t\t']",Positive
"['\n\t\t In our experiments , we use a bootstrapping approach to obtain five lexical sets semi-automatically for three verbs "" , ` f ~chu1 "" , "" JM~jian4 "" and "" \' NV_~hui1fu4 "" that have both low purity and low NMI-ratio in the first set of experiments . \n\t', '\n\t\t 14 We first extracted candidates for the lexical sets from the training data . \n\t', '\n\t\t For example , we extracted all the direct objects of the verb "" \' NV_~hui1fu4 "" and all the verbs that combined with the verb "" , ` f ~chu1 "" to form verb compounds from the automatically parsed training data . \n\t', '\n\t\t From the candidates , we manually selected words to form five initial seed sets , each of which contains no more than ten words . \n\t', '\n\t\t A simple algorithm was used to search for all the words that have the same detailed Hownet semantic definitions ( semantic category plus certain supplementary information ) as the seed words . \n\t', '\n\t\t We did not use Rocling because its semantic definitions are so general that a seed word tends to extend to a huge set of irrelevant words . \n\t', '\n\t\t Highly relevant words were manually selected from all the words found by the searching algorithm and added to the initial seed sets . \n\t', '\n\t\t The enlarged sets were used as lexical sets . \n\t', '\n\t\t The enhanced model first uses the lexical sets to obtain the semantic category of the NP arguments 14 We did not include "" -Wlyao4 "" , since its meaning rarely depends on local predicate-argument structure information . \n\t', '\n\t\t of the three verbs . \n\t', '\n\t\t Only when the search fails does the model resort to the general semantic taxonomy . \n\t', '\n\t\t The model also uses the lexical sets to determine the types of the compound verbs that contain the target verb "" , ` f ~chu1 "" and uses them as new features . \n\t', ""\n\t\t Table 3 shows the model 's performance on the three verbs with or without using lexical sets . \n\t"", '\n\t\t As we see , lexical sets improves the model \'s performance on all of them , especially on the verb "" , ` f ~chu1 "" . \n\t', '\n\t\t Although the results are still preliminary , they nevertheless provide us hints of how much a WSD model for Chinese verbs could gain from lexical sets . \n\t', ""\n\t\t Verb w/o lexical sets ( % ) with lexical sets ( % ) Purity NMI-ratio Purity NMI-ratio , ` f 63.61 43.24 76.50 52.81 h 72.20 22.41 77.56 34.63 ' kA 65.32 15.64 69.03 19.71 Table 3 . \n\t"", '\n\t\t Clustering performance with and without lexical sets for three Chinese verbs 6 Conclusion We have shown that an EM clustering model that uses rich linguistic features and a general semantic taxonomy for Chinese nouns generally performs well in learning sense distinctions for 12 Chinese verbs . \n\t', ""\n\t\t In addition , using lexical sets improves the model 's performance on three of the most challenging verbs . \n\t"", '\n\t\t Future work is to extend our coverage and to apply the semantic taxonomy and the same types of features to supervised WSD in Chinese . \n\t', '\n\t\t Since the experimental results suggest that a general semantic taxonomy and more constrained lexical sets are both beneficial for WSD tasks , we will develop automatic methods to build large-scale semantic taxonomies and lexical sets for Chinese , which reduce human effort as much as possible but still ensure high quality of the obtained taxonomies or lexical sets . \n\t', '\n\t\t 7 Acknowledgements This work has been supported by an ITIC supplement to a National Science Foundation Grant , NSF-ITR-EIA-0205448 . \n\t', '\n\t\t Any opinions , findings , and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation . \n\t', '\n\t\t References Jinying Chen , Nianwen Xue and Martha Palmer . \n\t', '\n\t\t 2004. Using a Smoothing Maximum Entropy Model for Chinese Nominal Entity Tagging . \n\t', '\n\t\t In Proceedings of the 1st Int. . \n\t', '\n\t\t Joint Conference on Natural Language Processing . \n\t', '\n\t\t Hainan Island , China . \n\t', '\n\t\t Hoa Trang Dang , Ching-yi Chia , Martha Palmer , and Fu-Dong Chiou . \n\t', '\n\t\t 2002. Simple Features for Chinese Word Sense Disambiguation . \n\t', '\n\t\t In Proceedings of COLING-2002 Nineteenth Int. Conference on Computational Linguistics , Taipei , Aug.24~Sept.1 . \n\t', '\n\t\t Christiane Fellbaum . \n\t', '\n\t\t 1998. WordNet - an Electronic Lexical Database . \n\t', '\n\t\t The MIT Press , Cambridge , Massachusetts , London . \n\t', '\n\t\t Daniel Gildea and Daniel Jurafsky . \n\t', '\n\t\t 2002. Automatic Labeling of Semantic Roles . \n\t', '\n\t\t Computational Linguistics , 28(3) : 245-288 , 2002 . \n\t', '\n\t\t Patrick Hanks . \n\t', '\n\t\t 1996. Contextual dependencies and lexical sets . \n\t', '\n\t\t The Int. Journal of Corpus Linguistics , 1:1 . \n\t', '\n\t\t Patrick Hanks . \n\t', '\n\t\t 1997. Lexical sets : relevance and probability . \n\t', '\n\t\t in B. Lewandowska-Tomaszczyk and M. Thelen ( eds . \n\t', '\n\t\t ) Translation and Meaning , Part 4 , School of Translation and Interpreting , Maastricht , The Netherlands . \n\t', '\n\t\t Thomas Hofmann and Puzicha Jan. 1998 . \n\t', '\n\t\t Statistical models for co-occurrence data , MIT Artificial Intelligence Lab. , Technical Report AIM-1625 . \n\t', '\n\t\t Adam Kilgarriff and Martha Palmer . \n\t', '\n\t\t 2000. Introduction to the sepcial issue on SENSEVAL . \n\t', '\n\t\t Computers and the Humanities , 34(1-2) : 15-48 . \n\t', '\n\t\t Martha Palmer , Hoa Trang Dang , and Christiane Fellbaum . \n\t', '\n\t\t To appear . \n\t', '\n\t\t Making fine-grained and coarse-grained sense distinctions , both manually and automatically . \n\t', '\n\t\t Natural Language Engineering . \n\t', '\n\t\t Mats Rooth , Stefan Riezler , Detlef Prescher , Glenn Carroll , and Franz Beil . \n\t', '\n\t\t 1998. EM-based clustering for NLP applications . \n\t', '\n\t\t AIMS Report 4(3).Institut fiir Maschinelle Sprachverarbeitung . \n\t', '\n\t\t Sabine Schulte im Walde . \n\t', '\n\t\t 2000. Clustering verbs semantically according to their alternation behaviour . \n\t', '\n\t\t In Proceedings of the 18th Int. Conference on Computational Linguistics , 747- 753 . \n\t', '\n\t\t Hinrich Schutze . \n\t', '\n\t\t 1998. Automatic Word Sense Discrimination . \n\t', '\n\t\t Computational Linguistics , 24 ( 1 ) : 97-124 . \n\t', '\n\t\t Alexander Strehl . \n\t', '\n\t\t 2002. Relationship-based Clustering and Cluster Ensembles for High- dimensional Data Mining . \n\t', '\n\t\t Dissertation . \n\t', '\n\t\t The University of Texas at Austin . \n\t', '\n\t\t http://www.lans . \n\t', '\n\t\t ece.utexas.edu/~strehl/diss/ . \n\t', '\n\t\t Relieving The Data Acquisition Bottleneck In Word Sense Disambiguation Mona Diab Linguistics Department Stanford University mdiab@stanford.edu Abstract Supervised learning methods for WSD yield better performance than unsupervised methods . \n\t', '\n\t\t Yet the availability of clean training data for the former is still a severe challenge . \n\t', '\n\t\t In this paper , we present an unsupervised bootstrapping approach for WSD which exploits huge amounts of automatically generated noisy data for training within a supervised learning framework . \n\t', '\n\t\t The method is evaluated using the 29 nouns in the English Lexical Sample task of SENSEVAL2 . \n\t', '\n\t\t Our algorithm does as well as supervised algorithms on 31 % of this test set , which is an improvement of 11 % ( absolute ) over state-of-the-art bootstrapping WSD algorithms . \n\t', '\n\t\t We identify seven different factors that impact the performance of our system . \n\t', '\n\t\t 1 Introduction Supervised Word Sense Disambiguation ( WSD ) systems perform better than unsupervised systems . \n\t', '\n\t\t But lack of training data is a severe bottleneck for supervised systems due to the extensive labor and cost involved . \n\t', '\n\t\t Indeed , one of the main goals of the SENSEVAL exercises is to create large amounts of sense-annotated data for supervised systems ( Kilgarriff&Rosenzweig , 2000 ) . \n\t', '\n\t\t The problem is even more challenging for languages which possess scarce computer readable knowledge resources . \n\t', '\n\t\t In this paper , we investigate the role of large amounts of noisily sense annotated data obtained using an unsupervised approach in relieving the data acquisition bottleneck for the WSD task . \n\t', '\n\t\t We bootstrap a supervised learning WSD system with an unsupervised seed set . \n\t', '\n\t\t We use the sense annotated data produced by Diab\x92s unsupervised system SALAAM ( Diab&Resnik , 2002 ; Diab , 2003 ) . \n\t', '\n\t\t SALAAM is a WSD system that exploits parallel corpora for sense disambiguation of words in running text . \n\t', '\n\t\t To date , SALAAM yields the best scores for an unsupervised system on the SENSEVAL2 English All-Words task \n\t\t']",Positive
"['\n\t\t SALAAM is an appealing approach as it provides automatically sense annotated data in two languages simultaneously , thereby providing a multilingual framework for solving the data acquisition problem . \n\t', '\n\t\t For instance , SALAAM has been used to bootstrap the WSD process for Arabic as illustrated in \n\t\t']",Positive
"['\n\t\t In a supervised learning setting , WSD is cast as a classification problem , where a predefined set of sense tags constitutes the classes . \n\t', '\n\t\t The ambiguous words in text are assigned one or more of these classes by a machine learning algorithm based on some extracted features . \n\t', '\n\t\t This algorithm learns parameters from explicit associations between the class and the features , or combination of features , that characterize it . \n\t', '\n\t\t Therefore , such systems are very sensitive to the training data , and those data are , generally , assumed to be as clean as possible . \n\t', '\n\t\t In this paper , we question that assumption . \n\t', '\n\t\t Can large amounts of noisily annotated data used in training be useful within such a learning paradigm for WSD ? \n\t', '\n\t\t What is the nature of the quality-quantity trade-off in addressing this problem ? \n\t', '\n\t\t 2 Related Work To our knowledge , the earliest study of bootstrapping a WSD system with noisy data is by Gale et . \n\t', '\n\t\t al. , \n\t\t']",Positive
"['\n\t\t Their investigation was limited in scale to six data items with two senses each and a bounded number of examples per test item . \n\t', '\n\t\t Two more recent investigations are by Yarowsky , \n\t\t']",Positive
"['\n\t\t Each of the studies , in turn , addresses the issue of data quantity while maintaining good quality training examples . \n\t', '\n\t\t Both investigations present algorithms for bootstrapping supervised WSD systems using clean data based on a dictionary or an ontological resource . \n\t', '\n\t\t The general idea is to start with a clean initial seed and iteratively increase the seed size to cover more data . \n\t', '\n\t\t Yarowsky starts with a few tagged instances to train a decision list approach . \n\t', '\n\t\t The initial seed is manually tagged with the correct senses based on entries in Roget\x92 s Thesaurus . \n\t', '\n\t\t The approach yields very successful results \x97 95 % \x97 on a handful of data items . \n\t', '\n\t\t Mihalcea , on the other hand , bases the bootstrapping approach on a generation algorithm , G e n C o r ( Mihalcea&Moldovan , 1999 ) . \n\t', '\n\t\t GenCor creates seeds from monosemous words in WordNet , Semcor data , sense tagged examples from the glosses of polysemous words in WordNet , and other hand tagged data if available . \n\t', '\n\t\t This initial seed set is used for querying the Web for more examples and the retrieved contexts are added to the seed corpus . \n\t', '\n\t\t The words in the contexts of the seed words retrieved are then disambiguated . \n\t', '\n\t\t The disambiguated contexts are then used for querying the Web for yet more examples , and so on . \n\t', '\n\t\t It is an iterative algorithm that incrementally generates large amounts of sense tagged data . \n\t', '\n\t\t The words found are restricted to either part of noun compounds or internal arguments of verbs . \n\t', '\n\t\t Mihalcea\x92s supervised learning system is an instance-based-learning algorithm . \n\t', '\n\t\t In the study , Mihalcea compares results yielded by the supervised learning system trained on the automatically generated data , GenCor , against the same system trained on manually annotated data . \n\t', '\n\t\t She reports successful results on six of the data items tested . \n\t', '\n\t\t 3 Empirical Layout Similar to Mihalcea\x92s approach , we compare results obtained by a supervised WSD system for English using manually sense annotated training examples against results obtained by the same WSD system trained on SALAAM sense tagged examples . \n\t', '\n\t\t The test data is the same , namely , the SENSEVAL 2 English Lexical Sample test set . \n\t', '\n\t\t The supervised WSD system chosen here is the University of Maryland System for SENSEVAL 2 Tagging ( ) \n\t\t']",Positive
"['\n\t\t 3.1 The learning approach adopted by is based on Support Vector Machines ( S VM ) . \n\t', '\n\t\t uses SVM- by Joachims \n\t\t']",Positive
"['\n\t\t All the positive examples for a sense are considered the nega- tive examples of , where .\n\t\t']",Positive
"['\n\t\t The features used for are mainly con- textual features with weight values associated with each feature . \n\t', '\n\t\t The features are space delimited units , 1http://www.ai.cs.uni.dortmund.de/svmlight . \n\t', '\n\t\t tokens , extracted from the immediate context of the target word . \n\t', '\n\t\t Three types of features are extracted : Wide Context Features : All the tokens in the paragraph where the target word occurs . \n\t', '\n\t\t Narrow Context features : The tokens that collocate in the surrounding context , to the left and right , with the target word within a fixed window size of . \n\t', '\n\t\t Grammatical Features : Syntactic tuples such as verb-obj , subj-verb , etc. extracted from the context of the target word using a dependency parser , M IN I PAR \n\t\t']",Positive
"['\n\t\t Each feature extracted is associated with a weight value . \n\t', '\n\t\t The weight calculation is a variant on the Inverse Document Frequency ( IDF ) measure in Information Retrieval . \n\t', '\n\t\t The weighting , in this case , is an Inverse Category Frequency ( ICF ) measure where each token is weighted by the inverse of its frequency of occurrence in the specified context of the target word . \n\t', '\n\t\t 3.1.1 Manually Annotated Training Data The manually-annotated training data is the SENSEVAL2 Lexical Sample training data for the English task , ( SV2LS Train).2 This training data corpus comprises 44856 lines and 917740 tokens . \n\t', '\n\t\t There is a close affinity between the test data and the manually annotated training data . \n\t', '\n\t\t The Pearson correlation between the sense distributions for the test data and the manually annotated training data , per test item , ranges between .3 3.2 SALAAM SALAAM exploits parallel corpora for sense annotation . \n\t', '\n\t\t The key intuition behind SALAAM is that when words in one language , L1 , are translated into the same word in a second language , L2 , then those L 1 words are semantically similar . \n\t', '\n\t\t For example , when the English \x97 L 1\x97 words bank , brokerage , mortgage-lender translate into the French \x97 L2 \x97 word banque in a parallel corpus , where bank is polysemous , SALAAM discovers that the intended sense for bank is the financial institution sense , not the geological formation sense , based on the fact that it is grouped with brokerage and mortgage-lender . \n\t', '\n\t\t SALAAM\x92s algorithm is as follows : SALAAM expects a word aligned parallel corpus as input ; 2 http://www.senseval.org 3 The correlation is measured between two frequency distributions . \n\t', '\n\t\t Throughout this paper , we opt for using the parametric Pearson correlation rather than KL distance in order to test statistical significance . \n\t', '\n\t\t L 1 words that translate into the same L2 word are grouped into clusters ; SALAAM identifies the appropriate senses for the words in those clusters based on the words senses\x92 proximity in WordNet . \n\t', '\n\t\t The word sense proximity is measured in information theoretic terms based on an algorithm by Resnik \n\t\t']",Positive
"['\n\t\t Simultaneously , SALAAM projects the propagated sense tags for L1 words onto their L2 corresponding translations . \n\t', '\n\t\t 3.2.1 Automatically Generated SALAAM Training Data Three sets of SALAAM tagged training corpora are created : SV2LS TR : English SENSEVAL2 Lexical Sample trial and training corpora with no manual annotations . \n\t', '\n\t\t It comprises 61879 lines and 1084064 tokens . \n\t', '\n\t\t MT : The English Brown Corpus , SENSEVAL1 ( trial , training and test corpora ) , Wall Street Journal corpus , and SENSEVAL 2 All Words corpus . \n\t', '\n\t\t All of which comprise 151762 lines and 37945517 tokens . \n\t', '\n\t\t HT : UN English corpus which comprises 71672 lines of 1734001 tokens The SALAAM-tagged corpora are rendered in a format similar to that of the manually annotated training data . \n\t', '\n\t\t The automatic sense tagging for MT and SV2LS TR training data is based on using SALAAM with machine translated parallel corpora . \n\t', '\n\t\t The HT training corpus is automatically sense tagged based on using SALAAM with the English- Spanish UN naturally occurring parallel corpus . \n\t', '\n\t\t 3.3 Experimental Conditions Experimental conditions are created based on three of SALAAM\x92s tagging factors , Corpus , Language and Threshold : Corpus : There are 4 different combinations for the training corpora : MT+SV2LS TR ; MT+HT+SV2LS TR ; HT+SV2LS TR ; or SV2LS TR alone . \n\t', '\n\t\t Language : The context language of the parallel corpus used by SALAAM to obtain the sense tags for the English training corpus . \n\t', '\n\t\t There are three options : French ( FR ) , Spanish ( SP ) , or , Merged languages ( ML ) , where the results are obtained by merging the English output of FR and SP . \n\t', '\n\t\t Threshold : Sense selection criterion , in SALAAM , is set to either MAX ( M ) or THRESH ( T ) . \n\t', '\n\t\t These factors result in 39 conditions.4 3.4 Test Data The test data are the 29 noun test items for the SENSEVAL 2 English Lexical Sample task , ( SV2LSTest ) . \n\t', '\n\t\t The data is tagged with the WordNet 1.7pre \n\t\t']",Positive
"['\n\t\t The average perplexity for the test items is 3.47 ( see Section 5.3 ) , the average number of senses is 7.93 , and the total number of contexts for all senses of all test items is 1773 . \n\t', '\n\t\t 4 Evaluation In this evaluation , is the system trained with SALAAM-tagged data and is the system trained with manually annotated data . \n\t', '\n\t\t Since we don\x92t expect to outperform human tagging , the results yielded by , are the upper bound for the purposes of this study . \n\t', '\n\t\t It is important to note that is always trained with SV2LS TR as part of the training set in order to guarantee genre congruence between the training and test sets.The scores are calculated using scorer2.5 The average precision score over all the items for is 65.3 % at 100 % Coverage . \n\t', '\n\t\t 4.1 Metrics We report the results using two metrics , the harmonic mean of precision and recall , ( ) score , and the Performance Ratio ( PR ) , which we define as the ratio between two precision scores on the same test data where precision is rendered using scorer2 . \n\t', '\n\t\t PR is measured as follows : ( 1 ) 4Originally , there are 48 conditions , 9 of which are excluded due to extreme sparseness in training contexts . \n\t', '\n\t\t 5From http://www.senseval.org , all scorer2 results are reported in fine-grain mode . \n\t', '\n\t\t 4.2 Results Table 1 shows the scores for the upper bound .is the condition in that yields the highest overall score over all noun items . \n\t', '\n\t\t the max- imum score achievable , if we know which condition yields the best performance per test item , therefore it is an oracle condition.6 Since our approach is unsupervised , we also report the results of other unsupervised systems on this test set . \n\t', '\n\t\t Accordingly , the last seven row entries in Table 1 present state-of-the-art SENSEVAL2 unsupervised systems performance on this test set.7 System 65.3 36.02 45.1 ITRI 45 UNED-LS-U 40.1 CLRes 29.3 IIT2 ( R ) 24.4 IIT1 ( R ) 23.9 I I T 2 23.2 IIT1 22 Table 1 : scores on SV2LS Test for , , , and state-of-the-art unsupervised systems participating in the SENSEVAL2 English Lexical Sample task . \n\t', '\n\t\t . \n\t', '\n\t\t is the third in the unsupervised methods . \n\t', '\n\t\t It is worth noting that the average score across the 39 conditions is , and the lowest is . \n\t', '\n\t\t The five best conditions for , that yield the highest average across all test items , use the HT corpus in the training data , four of which are the result of merged languages in SALAAM indicating that evidence from different languages simultaneously is desirable . \n\t', '\n\t\t is the maximum potential among all unsupervised approaches if the best of all the conditions are combined . \n\t', '\n\t\t One of our goals is to automatically determine which condition or set of conditions yield the best results for each test item . \n\t', '\n\t\t Of central interest in this paper is the performance ratio ( PR ) for the individual nouns . \n\t', '\n\t\t Table 6The different conditions are considered independent taggers and there is no interaction across target nouns 7http://www.senseval . \n\t', '\n\t\t org 2 illustrates the PR of the different nouns yielded by and sorted in de- scending order by PR scores . \n\t', '\n\t\t A PR indicates an equivalent performance between and . \n\t', '\n\t\t The highest PR values are highlighted in bold . \n\t', '\n\t\t Nouns #Ss UMH % UMSb UMSm detention 4 65.6 1.00 1.05 chair 7 83.3 1.02 1.02 bum 4 85 0.14 1.00 dyke 2 89.3 1.00 1.00 fatigue 6 80.5 1.00 1.00 hearth 3 75 1.00 1.00 spade 6 75 1.00 1.00 stress 6 50 0.05 1.00 yew 3 78.6 1.00 1.00 art 17 47.9 0.98 0.98 child 7 58.7 0.93 0.97 material 16 55.9 0.81 0.92 church 6 73.4 0.75 0.77 mouth 10 55.9 0 0.73 authority 9 62 0.60 0.70 post 12 57.6 0.66 0.66 nation 4 78.4 0.34 0.59 feeling 5 56.9 0.33 0.59 restraint 8 60 0.2 0.56 channel 7 62 0.52 0.52 facility 5 54.4 0.32 0.51 circuit 13 62.7 0.44 0.44 nature 7 45.7 0.43 0.43 bar 19 60.9 0.20 0.30 grip 6 58.8 0.27 0.27 sense 8 39.6 0.24 0.24 lady 8 72.7 0.09 0.16 day 16 62.5 0.06 0.08 holiday 6 86.7 0.08 0.08 Table 2 : The number of senses per item , in column #Ss , precision performance per item as indicated in column UMH , PR scores for in column UMSb and in column UMSm on SV2LS Test , 31 % of the test items , ( 9 nouns yield PR scores ) , do as well as . \n\t', '\n\t\t This is an improve- ment of 11 % absolute over state-of-the-art bootstrapping WSD algorithm yielded by Mihalcea \n\t\t']",Negative
"['\n\t\t Mihalcea reports high PR scores for six test items only : art , chair , channel , church , detention , nation . \n\t', '\n\t\t It is worth highlighting that her bootstrapping approach is partially supervised since All of the unsupervised methods including and are significantly below the supervised method , yields PR scores for the top 12 test items listed in Table 2 . \n\t', '\n\t\t Our algorithm does as well as supervised algorithm , , on 41.6 % of this test set . \n\t', '\n\t\t In it depends mainly on hand labelled data as a seed for the training data . \n\t', '\n\t\t Interestingly , two nouns , detention and chair , yield better performance than , as in- dicated by the PRs and , respectively . \n\t', '\n\t\t This is attributed to the fact that SALAAM produces a lot more correctly annotated training data for these two words than that provided in the manually annotated training data for . \n\t', '\n\t\t Some nouns yield very poor PR values mainly due to the lack of training contexts , which is the case for mouth in , for example . \n\t', '\n\t\t Or lack of coverage of all the senses in the test data such as for bar and day , or simply errors in the annotation of the SALAAM-tagged training data . \n\t', '\n\t\t If we were to include only nouns that achieve ac- ceptable PR scores of \x97 the first 16 nouns in Table 2 for \x97 the overall potential precision of is significantly increased to 63.8 % and the overall precision of is increased to 68.4%.8 These results support the idea that we could replace hand tagging with SALAAM\x92s unsupervised tagging if we did so for those items that yield an acceptable PR score . \n\t', '\n\t\t But the question remains : How do we predict which training/test items will yield acceptable PR scores ? \n\t', '\n\t\t 5 Factors Affecting Performance Ratio In an attempt to address this question , we analyze several different factors for their impact on the performance of quanitified as PR . \n\t', '\n\t\t In order to effectively alleviate the sense annotation acquisition bottleneck , it is crucial to predict which items would be reliably annotated automatically using . \n\t', '\n\t\t Accordingly , in the rest of this paper , we explore 7 different factors by examining the yielded PR values in 5.1 Number of Senses The test items that possess many senses , such as art ( 17 senses ) , material ( 16 senses ) , mouth ( 10 senses ) and post ( 12 senses ) , exhibit PRs of 0.98 , 0.92 , 0.73 and 0.66 , respectively . \n\t', '\n\t\t Overall , the correlation between number of senses per noun and its PR score is an insignificant $ A PR of is considered acceptable since achieves an overall score of in the WSD task . \n\t', '\n\t\t ber of training examples available to for each noun in the training data . \n\t', '\n\t\t The correlation between the number of training examples and PR is insignificant at , . \n\t', '\n\t\t More interestingly , however , spade , with only 5 training examples , yields a PR score of . \n\t', '\n\t\t This contrasts with nation , which has more than 4200 training examples , but yields a low PR score of . \n\t', '\n\t\t Accordingly , the number of training examples alone does not seem to have a direct impact on PR . \n\t', '\n\t\t 5.3 Sense Perplexity This factor is a characteristic of the training data . \n\t', '\n\t\t Perplexity is . \n\t', '\n\t\t Entropy is measured as follows : ( 2 ) where is a sense for a polysemous noun and is the set of all its senses . \n\t', '\n\t\t Entropy is a measure of confusability in the senses\x92 contexts distributions ; when the distribution is relatively uniform , entropy is high . \n\t', '\n\t\t A skew in the senses\x92 contexts distributions indicates low entropy , and accordingly , low perplexity . \n\t', '\n\t\t The lowest possible perplexity is , corresponding to entropy . \n\t', '\n\t\t A low sense perplexity is desirable since it facilitates the discrimination of senses by the learner , therefore leading to better classification . \n\t', '\n\t\t In the SALAAM- tagged training data , for example , bar has the highest perplexity value of over its 19 senses , while day , with 16 senses , has a much lower perplexity of . \n\t', '\n\t\t Surprisingly , we observe nouns with high perplexity such as bum ( sense perplexity value of ) achieving PR scores of . \n\t', '\n\t\t While nouns with relatively low perplexity values such as grip ( sense perplexity of ) yields a low PR score of . \n\t', '\n\t\t Moreover , nouns with the same perplexity and similar number of senses yield very different PR scores . \n\t', '\n\t\t For example , examining holiday and child , both have the same perplexity of and the number of senses is close , with 6 and 7 senses , respectively , however , the PR scores are very different ; holiday yields a PR of , and child achieves a PR of . \n\t', '\n\t\t Furthermore , nature and art have the same perplexity of ; art has 17 senses while nature has 7 senses only , nonetheless , art yields a much higher PR score of ( ) compared to a PR of for nature . \n\t', '\n\t\t These observations are further solidified by the insignificant correlation of , between sense perplexity and PR . \n\t', '\n\t\t At first blush , one is inclined to hypothesize that ,. , . \n\t', '\n\t\t Though it is a weak negative correlation , it does suggest that when the number of senses increases , PR tends to decrease . \n\t', '\n\t\t 5.2 Number of Training Examples This is a characteristic of the training data . \n\t', '\n\t\t We examine the correlation between the PR and the num- the combination of low perplexity associated with a large number of senses \x97 as an indication of high skew in the distribution \x97 is a good indicator of high PR , but reviewing the data , this hypothesis is dispelled by day which has 16 senses and a sense perplexity of , yet yields a low PR score of . \n\t', '\n\t\t 5.4 Semantic Translation Entropy Semantic translation entropy ( STE ) \n\t\t']",Positive
"['\n\t\t STE measures the amount of translational variation for an L1 word in L2 , in a parallel corpus . \n\t', '\n\t\t STE is a variant on the entropy measure . \n\t', '\n\t\t STE is expressed as follows : ( 3 ) where is a translation in the set of possible translations in L2 ; and is L 1 word . \n\t', '\n\t\t The probability of a translation is calculated directly from the alignments of the test nouns and their corresponding translations via the maximum likelihood estimate . \n\t', '\n\t\t Variation in translation is beneficial for SALAAM tagging , therefore , high STE is a desirable feature . \n\t', '\n\t\t Correlation between the automatic tagging precision and STE is expected to be high if SALAAM has good quality translations and good quality alignments . \n\t', '\n\t\t However , this correlation is a low . \n\t', '\n\t\t Consequently , we observe a low correlation between STE and PR , . \n\t', '\n\t\t Examining the data , the nouns bum , detention , dyke , stress , and yew exhibit both high STE and high PR ; Moreover , there are several nouns that exhibit low STE and low PR . \n\t', '\n\t\t But the intriguing items are those that are inconsistent . \n\t', '\n\t\t For instance , child and holiday : child has an STE of and comprises 7 senses at a low sense perplexity of , yet yields a high PR of . \n\t', '\n\t\t As mentioned earlier , low STE indicates lack of translational variation . \n\t', '\n\t\t In this specific experimental condition , child is translated as enfant , enfantile , ni\x98no , ni\x98no -peque\x98no , which are words that preserve ambiguity in both French and Spanish . \n\t', '\n\t\t On the other hand , holiday has a relatively high STE value of , yet results in the lowest PR of . \n\t', '\n\t\t Consequently , we conclude that STE alone is not a good direct indicator of PR . \n\t', '\n\t\t 5.5 Perplexity Difference Perplexity difference ( PerpDiff ) is a measure of the absolute difference in sense perplexity between the test data items and the training data items . \n\t', '\n\t\t For the manually annotated training data items , the overall correlation between the perplexity measures is a sig- nificant which contrasts to a low over- all correlation of between the SALAAM- tagged training data items and the test data items . \n\t', '\n\t\t Across the nouns in this study , the correlation between PerpDiff and PR is . \n\t', '\n\t\t It is advantageous to be as similar as possible to the training data to guarantee good classification results within a supervised framework , therefore a low PerpDiff is desirable . \n\t', '\n\t\t We observe cases with a low PerpDiff such as holiday ( PerpDiff of ) , yet the PR is a low . \n\t', '\n\t\t On the other hand , items such as art have a relatively high PerpDiff of , but achieves a high PR of . \n\t', '\n\t\t Accordingly , PerpDiff alone is not a good indicator of PR . \n\t', '\n\t\t 5.6 Sense Distributional Correlation Sense Distributional Correlation ( SDC ) results from comparing the sense distributions of the test data items with those of SALAAM-tagged training data items . \n\t', '\n\t\t It is worth noting that the correlation between the SDC of manually annotated training data and that of the test data ranges from . \n\t', '\n\t\t A strong significant correlation of high SDC values , and , respectively , in , but they score lower PR values than detention which has a comparatively lower SDC value of . \n\t', '\n\t\t The fact that both circuit and post have many senses , 13 and 12 , respectively , while detention has 4 senses only is noteworthy . \n\t', '\n\t\t detention has a higher STE and lower sense perplexity than either of them however . \n\t', '\n\t\t Overall , the data suggests that SDC is a very good direct indicator of PR . \n\t', '\n\t\t 5.7 Sense Context Confusability A situation of sense context confusability ( SCC ) arises when two senses of a noun are very similar and are highly uniformly represented in the training examples . \n\t', '\n\t\t This is an artifact of the fine granularity of senses in WordNet 1.7pre . \n\t', '\n\t\t Highly similar senses typically lead to similar usages , therefore similar contexts , which in a learning framework detract from the learning algorithm\x92s discriminatory power . \n\t', '\n\t\t Upon examining the 29 polysemous nouns in the training and test sets , we observe that a significant number of the words have similar senses according , , between SDC and PR exists for SALAAM-tagged training data and the test data . \n\t', '\n\t\t Overall , nouns that yield high PR have high SDC values . \n\t', '\n\t\t However , there are some in- stances where this strong correlation is not exhib- ited . \n\t', '\n\t\t For example , circuit and post have relatively to a manual grouping provided by Palmer , in 2002.9 For example , senses 2 and 3 of nature , meaning trait and quality , respectively , are considered similar by the manual grouping . \n\t', '\n\t\t The manual grouping does not provide total coverage of all the noun senses in this test set . \n\t', '\n\t\t For instance , it only considers the homonymic senses 1 , 2 and 3 of spade , yet , in the current test set , spade has 6 senses , due to the existence of sub senses . \n\t', '\n\t\t 26 of the 29 test items exhibit multiple groupings based on the manual grouping . \n\t', '\n\t\t Only three nouns , detention , dyke , spade do not have any sense groupings . \n\t', '\n\t\t They all , in turn , achieve high PR scores of . \n\t', '\n\t\t There are several nouns that have relatively high SDC values yet their performance ratios are low such as post , nation , channel and circuit . \n\t', '\n\t\t For in- stance , nation has a very high SDC value of , a low sense perplexity of \x97 relatively close to the sense perplexity of the test data \x97 a suffi- cient number of contexts ( 4350 ) , yet it yields a PR of . \n\t', '\n\t\t According to the manual sense grouping , senses 1 and 3 are similar , and indeed , upon inspection of the context distributions , we find the bulk of the senses\x92 instance examples in the SALAAM- tagged training data for the condition that yields this PR in are annotated with either sense 1 or sense 3 , thereby creating confusable contexts for the learning algorithm . \n\t', '\n\t\t All the cases of nouns that achieve high PR and possess sense groups do not have any SCC in the training data which strongly suggests that SCC is an important factor to consider when predicting the PR of a system . \n\t', '\n\t\t 5.8 Discussion We conclude from the above exploration that SDC and SCC affect PR scores directly . \n\t', '\n\t\t PerpDiff , STE , and Sense Perplexity , number of senses and number of contexts seem to have no noticeable direct impact on the PR . \n\t', '\n\t\t Based on this observation , we calculate the SDC values for all the training data used in our experimental conditions for the 29 test items . \n\t', '\n\t\t Table 3 illustrates the items with the highest SDC values , in descending order , as yielded from any of the SALAAM conditions . \n\t', '\n\t\t We use an empirical cut-off value of for SDC . \n\t', '\n\t\t The SCC values are reported as a boolean Y/N value , where a Y indicates the presence of a sense confusable context . \n\t', '\n\t\t As shown a high SDC can serve as a means of auto- 9http://www.senseval.org/sense-groups . \n\t', '\n\t\t The manual sense grouping comprises 400 polysemous nouns including the 29 nouns in this evaluation . \n\t', '\n\t\t Noun SDC SCC PR dyke 1 N 1.00 bum 1 N 1.00 fatigue 1 N 1.00 hearth 1 N 1.00 yew 1 N 1.00 chair 0.99 N 1.02 child 0.99 N 0.95 detention 0.98 N 1.0 spade 0.97 N 1.00 mouth 0.96 Y 0.73 nation 0.96 N 0.59 material 0.92 N 0.92 post 0.90 Y 0.63 authority 0.86 Y 0.70 art 0.83 N 0.98 church 0.80 N 0.77 circuit 0.79 N 0.44 stress 0.77 N 1.00 Table 3 : Highest SDC values for the test items associated with their respective SCC and PR values . \n\t', '\n\t\t 11 matically predicting a high PR , but it is not sufficient . \n\t', '\n\t\t If we eliminate the items where an SCC exists , namely , mouth , post , and authority , we are still left with nation and circuit , where both yield very low PR scores . \n\t', '\n\t\t nation has the desirable low Per- pDiff of . \n\t', '\n\t\t The sense annotation tagging pre- cision of the in this condition which yields the highest SDC \x97 Spanish UN data with the for training \x97 is a low and a low STE value of . \n\t', '\n\t\t This is due to the fact that both French and Spanish preserve ambiguity in similar ways to English which does not make it a good target word for disambiguation within the SALAAM framework , given these two languages as sources of evidence . \n\t', '\n\t\t Accordingly , in this case , STE coupled with the noisy tagging could have resulted in the low PR . \n\t', '\n\t\t However , for circuit , the STE value for its respective condition is a high , but we observe a relatively high PerpDiff of compared to the PerpDiff of for the manually annotated data . \n\t', '\n\t\t Therefore , a combination of high SDC and nonexistent SCC can reliably predict good PR . \n\t', '\n\t\t But the other factors still have a role to play in order to achieve accurate prediction . \n\t', '\n\t\t It is worth emphasizing that two of the identified factors are dependent on the test data in this study , SDC and PerpDiff . \n\t', '\n\t\t One solution to this problem is to estimate SDC and PerpDiff using a held out data set that is hand tagged . \n\t', '\n\t\t Such a held out data set would be considerably smaller than the required size of a manually tagged training data for a classical supervised WSD system . \n\t', '\n\t\t Hence , SALAAM- tagged training data offers a viable solution to the annotation acquisition bottleneck . \n\t', '\n\t\t 6 Conclusion and Future Directions In this paper , we applied an unsupervised approach within a learning framework for the sense annotation of large amounts of data . \n\t', '\n\t\t The ultimate goal of is to alleviate the data labelling bottleneck by means of a trade-off between quality and quantity of the training data . \n\t', '\n\t\t is competitive with state-of-the-art unsupervised systems evaluated on the same test set from SENSEVAL2 . \n\t', '\n\t\t Moreover , it yields superior results to those obtained by the only comparable bootstrapping approach when tested on the same data set . \n\t', '\n\t\t Moreover , we explore , in depth , different factors that directly and indirectly affect the performance of quantified as a performance ratio , PR . \n\t', '\n\t\t Sense Distribution Correlation ( SDC ) and Sense Context Confusability ( SCC ) have the highest direct impact on performance ratio , PR . \n\t', '\n\t\t However , evidence suggests that probably a confluence of all the different factors leads to the best prediction of an acceptable PR value . \n\t', '\n\t\t An investigation into the feasibility of combining these different factors with the different attributes of the experimental conditions for SALAAM to automatically predict when the noisy training data can reliably replace manually annotated data is a matter of future work . \n\t', '\n\t\t 7 Acknowledgements I would like to thank Philip Resnik for his guidance and insights that contributed tremendously to this paper . \n\t', '\n\t\t Also I would like to acknowledge Daniel Jurafsky and Kadri Hacioglu for their helpful comments . \n\t', '\n\t\t I would like to thank the three anonymous reviewers for their detailed reviews . \n\t', '\n\t\t This work has been supported , in part , by NSF Award #IIS0325646 . \n\t', '\n\t\t References Erin L. Allwein , Robert E. Schapire , and Yoram Singer . \n\t', '\n\t\t 2000. Reducing multiclass to binary : A unifying approach for margin classifiers . \n\t', '\n\t\t Journal of Machine Learning Research , 1:113-141 . \n\t', '\n\t\t Clara Cabezas , Philip Resnik , and Jessica Stevens . \n\t', '\n\t\t 2002 . \n\t', '\n\t\t Supervised Sense Tagging using Support Vector Machines . \n\t', '\n\t\t Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems ( SENSEVAL-2 ) . \n\t', '\n\t\t Toulouse , France . \n\t', '\n\t\t Scott Cotton , Phil Edmonds , Adam Kilgarriff , and Martha Palmer , ed . \n\t', '\n\t\t 2001. SENSEVAL-2 : Second International Workshop on Evaluating Word Sense Disambiguation Systems . \n\t', '\n\t\t ACL SIGLEX , Toulouse , France . \n\t', '\n\t\t Mona Diab . \n\t', '\n\t\t 2004. An Unsupervised Approach for Bootstrapping Arabic Word Sense Tagging . \n\t', '\n\t\t Proceedings of Arabic Based Script Languages , COLING 2004 . \n\t', '\n\t\t Geneva , Switzerland . \n\t', '\n\t\t Mona Diab and Philip Resnik . \n\t', '\n\t\t 2002. An Unsupervised Method for Word Sense Tagging Using Parallel Corpora . \n\t', '\n\t\t Proceedings of 40th meeting of ACL . \n\t', '\n\t\t Pennsylvania , USA . \n\t', '\n\t\t Mona Diab . \n\t', '\n\t\t 2003. Word Sense Disambiguation Within a Multilingual Framework . \n\t', '\n\t\t PhD Thesis . \n\t', '\n\t\t University of Maryland College Park , USA . \n\t', '\n\t\t Christiane Fellbaum . \n\t', '\n\t\t 1998. WordNet : An Electronic Lexical Database . \n\t', '\n\t\t MIT Press . \n\t', '\n\t\t William A. Gale and Kenneth W. Church and David Yarowsky . \n\t', '\n\t\t 1992. Using Bilingual Materials to Develop Word Sense Disambiguation Methods . \n\t', '\n\t\t Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation . \n\t', '\n\t\t Montr´eal , Canada . \n\t', '\n\t\t Thorsten Joachims . \n\t', '\n\t\t 1998. Text Categorization with Support Vector Machines : Learning with Many Relevant Features . \n\t', '\n\t\t Proceedings of the European Conference on Machine Learning . \n\t', '\n\t\t Springer . \n\t', '\n\t\t A. Kilgarriff and J. Rosenzweig . \n\t', '\n\t\t 2000. Framework and Resultsfor English SENSE VAL . \n\t', '\n\t\t Journal of Computers and the Humanities . \n\t', '\n\t\t pages 15\x9748 , 34 . \n\t', '\n\t\t Dekang Lin . \n\t', '\n\t\t 1998. Dependency-Based Evaluation of MINIPAR . \n\t', '\n\t\t Proceedings of the Workshop on the Evaluation of Parsing Systems , First International Conference on Language Resources and Evaluation . \n\t', '\n\t\t Granada , Spain . \n\t', '\n\t\t Dan I. Melamed . \n\t', '\n\t\t 1997. Measuring Semantic Entropy . \n\t', '\n\t\t ACL SIGLEX , Washington , DC . \n\t', '\n\t\t Rada Mihalcea and Dan Moldovan . \n\t', '\n\t\t 1999. A methodfor Word Sense Disambiguation of unrestricted text . \n\t', '\n\t\t Proceedings of the 37th Annual Meeting of ACL . \n\t', '\n\t\t Maryland , USA . \n\t', '\n\t\t Rada Mihalcea . \n\t', '\n\t\t 2002. Bootstrapping Large sense tagged corpora . \n\t', '\n\t\t Proceedings of the 3rd International Conference on Languages Resources and Evaluations ( LREC ) . \n\t', '\n\t\t Las Palmas , Canary Islands , Spain . \n\t', '\n\t\t Philip Resnik . \n\t', '\n\t\t 1999. Semantic Similarity in a Taxonomy : An Information-Based Measure and its Application to Problems of Ambiguity in Natural Language . \n\t', '\n\t\t Journal Artificial Intelligence Research . \n\t', '\n\t\t ( 11 ) p. 95- 130. David Yarowsky . \n\t', '\n\t\t 1995. Unsupervised Word Sense Disambiguation Rivaling Supervised Methods . \n\t', '\n\t\t Proceedings of the 33rd Annual Meeting of ACL . \n\t', '\n\t\t Cambridge , MA . \n\t', '\n\t\t Enriching the Output of a Parser Using Memory-Based Learning Valentin Jijkoun and Maarten de Rijke Informatics Institute , University of Amsterdam jijkoun , mdr @science.uva.nl Abstract We describe a method for enriching the output of a parser with information available in a corpus . \n\t', '\n\t\t The method is based on graph rewriting using memory- based learning , applied to dependency structures . \n\t', '\n\t\t This general framework allows us to accurately recover both grammatical and semantic information as well as non-local dependencies . \n\t', '\n\t\t It also facilitates dependency-based evaluation of phrase structure parsers . \n\t', '\n\t\t Our method is largely independent of the choice of parser and corpus , and shows state of the art performance . \n\t', '\n\t\t 1 Introduction We describe a method to automatically enrich the output of parsers with information that is present in existing treebanks but usually not produced by the parsers themselves . \n\t', '\n\t\t Our motivation is two-fold . \n\t', '\n\t\t First and most important , for applications requiring information extraction or semantic interpretation of text , it is desirable to have parsers produce grammatically and semantically rich output . \n\t', '\n\t\t Second , to facilitate dependency-based comparison and evaluation of different parsers , their outputs may need to be transformed into specific rich dependency formalisms . \n\t', '\n\t\t The method allows us to automatically transform the output of a parser into structures as they are annotated in a dependency treebank . \n\t', '\n\t\t For a phrase structure parser , we first convert the produced phrase structures into dependency graphs in a straightforward way , and then apply a sequence of graph transformations : changing dependency labels , adding new nodes , and adding new dependencies . \n\t', '\n\t\t A memory-based learner trained on a dependency corpus is used to detect which modifications should be performed . \n\t', '\n\t\t For a dependency corpus derived from the Penn Treebank and the parsers we considered , these transformations correspond to adding Penn functional tags ( e.g. , -SBJ , -TMP , -LOC ) , empty nodes ( e.g. , NP PRO ) and non-local dependencies ( controlled traces , WH extraction , etc. ) . \n\t', '\n\t\t For these specific sub-tasks our method achieves state of the art performance . \n\t', '\n\t\t The evaluation of the transformed output of the parsers of \n\t\t']",Positive
"['\n\t\t The paper is organized as follows . \n\t', '\n\t\t After providing some background and motivation in Section 2 , we give the general overview of our method in Section 3 . \n\t', '\n\t\t In Sections 4 through 8 , we describe all stages of the transformation process , providing evaluation results and comparing our methods to earlier work . \n\t', '\n\t\t We discuss the results in Section 9 . \n\t', '\n\t\t 2 Background and Motivation State of the art statistical parsers , e.g. , parsers trained on the Penn Treebank , produce syntactic parse trees with bare phrase labels , such as NP , PP , S , although the training corpora are usually much richer and often contain additional grammatical and semantic information ( distinguishing various modifiers , complements , subjects , objects , etc. ) , including non-local dependencies , i.e. , relations between phrases not adjacent in the parse tree . \n\t', '\n\t\t While this information may be explicitly annotated in a treebank , it is rarely used or delivered by parsers.l The reason is that bringing in more information of this type usually makes the underlying parsing model more complicated : more parameters need to be estimated and independence assumptions may no longer hold . \n\t', '\n\t\t \n\t\t']",Positive
"[""\n\t\t Currently , there are no parsers trained on the Penn Treebank that use the structure of the treebank in full and that are thus ' Some notable exceptions are the CCG parser described in \n\t\t""]",Positive
"['\n\t\t capable of producing syntactic structures containing all or nearly all of the information annotated in the corpus . \n\t', '\n\t\t In recent years there has been a growing interest in getting more information from parsers than just bare phrase trees . \n\t', '\n\t\t \n\t\t']",Positive
['\n\t\t Pattern-matching approaches were used in \n\t\t'],Positive
"['\n\t\t Furthermore , experiments described in \n\t\t']",Positive
"['\n\t\t 3 An Overview of the Method In this section we give a high-level overview of our method for transforming a parser\x92s output and describe the different steps of the process . \n\t', '\n\t\t In the experiments we used the parsers described in \n\t\t']",Positive
"['\n\t\t For Collins\x92 parser the text was first POS-tagged using Ratnaparkhi\x92s maximum enthropy tagger . \n\t', '\n\t\t The training phase of the method consists in learning which transformations need to be applied to the output of a parser to make it as similar to the treebank data as possible . \n\t', '\n\t\t As a preliminary step ( Step 0 ) , we convert the WSJ2 to a dependency corpus without losing the annotated information ( functional tags , empty nodes , non-local dependencies ) . \n\t', '\n\t\t The same conversion is applied to the output of the parsers we consider . \n\t', '\n\t\t The details of the conversion process are described in Section 4 below . \n\t', '\n\t\t The training then proceeds by comparing graphs derived from a parser\x92s output with the graphs from the dependency corpus , detecting various mismatches , such as incorrect arc labels and missing nodes or arcs . \n\t', '\n\t\t Then the following steps are taken to fix the mismatches : Step 1 : changing arc labels Step 2 : adding new nodes Step 3 : adding new arcs Obviously , other modifications are possible , such as deleting arcs or moving arcs from one node to another . \n\t', '\n\t\t We leave these for future work , though , and focus on the three transformations mentioned above . \n\t', '\n\t\t The dependency corpus was split into training ( WSJ sections 02\x9621 ) , development ( sections 00\x96 2Thoughout the paper WSJ refers to the Penn Treebank II Wall Street Journal corpus . \n\t', '\n\t\t 01 ) and test ( section 23 ) corpora . \n\t', '\n\t\t For each of the steps 1 , 2 and 3 we proceed as follows : 1. compare the training corpus to the output of the parser on the strings of the corpus , after applying the transformations of the previous steps 2. identify possible beneficial transformations ( which arc labels need to be changed or where new nodes or arcs need to be added ) 3. train a memory-based classifier to predict possible transformations given their context ( i.e. , information about the local structure of the dependency graph around possible application sites ) . \n\t', '\n\t\t While the definitions of the context and application site and the graph modifications are different for the three steps , the general structure of the method remains the same at each stage . \n\t', '\n\t\t Sections 6 , 7 and 8 describe the steps in detail . \n\t', '\n\t\t In the application phase of the method , we proceed similarly . \n\t', '\n\t\t First , the output of the parser is converted to dependency graphs , and then the learners trained during the steps 1 , 2 and 3 are applied in sequence to perform the graph transformations . \n\t', '\n\t\t Apart from the conversion from phrase structures to dependency graphs and the extraction of some linguistic features for the learning , our method does not use any information about the details of the tree- bank annotation or the parser\x92s output : it works with arbitrary labelled directed graphs . \n\t', '\n\t\t 4 Step 0 : From Constituents to Dependencies To convert phrase trees to dependency structures , we followed the commonly used scheme \n\t\t']",Positive
"['\n\t\t The conversion routine,3 described below , is applied both to the original WSJ structures and the output of the parsers , though the former provides more information ( e.g. , traces ) which is used by the conversion routine if available . \n\t', '\n\t\t First , for the treebank data , all traces are resolved and corresponding empty nodes are replaced with links to target constituents , so that syntactic trees become directed acyclic graphs . \n\t', '\n\t\t Second , for each constituent we detect its head daughters ( more than one in the case of conjunction ) and identify lexical heads . \n\t', '\n\t\t Then , for each constituent we output new dependencies between its lexical head and the lexical heads of its non-head daughters . \n\t', '\n\t\t The label of every new dependency is the constituent\x92s phrase 3Our converter is available at http : //www. science . \n\t', '\n\t\t uva.nl/\x98jijkoun/software . \n\t', '\n\t\t *^1 ( a ) to seek NP seats S planned this month NP^SBJ VP S NP^SBJ^1 directors NP^TMP VP S NP directors VP NP planned S VP to seek NP seats ( b ) this month planned Figure 1 : Example of ( a ) the Penn Treebank WSJ annotation , ( b ) the output of Charniak\x92s parser , and the results of the conversion to dependency structures of ( c ) the Penn tree and of ( d ) the parser\x92s output planned SINP VPIS directors SINP VPITO VPINP month seek seats NPIDT to this S I NP-SBJ VPIS directors seek VPINP seats month VPITO NPIDT to this S I NP-TMP ( d ) ( c ) S I NP-SBJ label , stripped of all functional tags and coindexing marks , conjoined with the label of the non-head daughter , with its functional tags but without coindexing marks . \n\t', '\n\t\t Figure 1 shows an example of the original Penn annotation ( a ) , the output of Charniak\x92s parser ( b ) and the results of our conversion of these trees to dependency structures ( c and d ) . \n\t', '\n\t\t The interpretation of the dependency labels is straightforward : e.g. , the label S NP-TMP corresponds to a sentence ( S ) being modified by a temporal noun phrase ( NP-TMP ) . \n\t', '\n\t\t The core of the conversion routine is the selection of head daughters of the constituents . \n\t', '\n\t\t Following \n\t\t']",Negative
"['\n\t\t The most notable extension is our handling of conjunctions , which are often left relatively flat in WSJ and , as a result , in a parser\x92s output : we used simple pattern-based heuristics to detect conjuncts and mark all conjuncts as heads of a conjunction . \n\t', '\n\t\t After the conversion , every resulting dependency structure is modified deterministically : auxiliary verbs ( be , do , have ) become dependents of corresponding main verbs ( similar to modal verbs , which are handled by the head table ) ; to fix a WSJ inconsistency , we move the -LGS tag ( indicating logical subject of passive in a by-phrase ) from the PP to its child NP . \n\t', '\n\t\t 5 Dependency-based Evaluation of Parsers After the original WSJ structures and the parsers\x92 outputs have been converted to dependency structures , we evaluate the performance of the parsers against the dependency corpus . \n\t', '\n\t\t We use the standard precision/recall measures over sets of dependencies ( excluding punctuation marks , as usual ) and evaluate Collins\x92 and Charniak\x92s parsers on WSJ section 23 in three settings : on unlabelled dependencies ; on labelled dependencies with only bare labels ( all functional tags discarded ) ; on labelled dependencies with functional tags . \n\t', '\n\t\t Notice that since neither Collins\x92 nor Charniak\x92s parser outputs WSJ functional labels , all dependencies with functional labels in the gold parse will be judged incorrect in the third setting . \n\t', '\n\t\t The evaluation results are shown in Table 1 , in the row \x93step 0\x94.4 As explained above , the low numbers for the dependency evaluation with functional tags are expected , because the two parsers were not intended to produce functional labels . \n\t', '\n\t\t Interestingly , the ranking of the two parsers is different for the dependency-based evaluation than for PARSEVAL : Charniak\x92s parser obtains a higher PARSEVAL score than Collins\x92 ( 89.0 % vs. 88.2 % ) , 4For meaningful comparison , the Collins\x92 tags -A and -g are removed in this evaluation . \n\t', '\n\t\t Evaluation Parser unlabelled P labelled f with func. tags P R f P R f R after conversion Charniak 89.9 83.9 86.8 85.9 80.1 82.9 68.0 63.5 65.7 ( step 0 , Section 4 ) Collins 90.4 83.7 87.0 86.7 80.3 83.4 68.4 63.4 65.8 after relabelling Charniak 89.9 83.9 86.8 86.3 80.5 83.3 83.8 78.2 80.9 ( step 1 , Section 6 ) Collins 90.4 83.7 87.0 87.0 80.6 83.7 84.6 78.4 81.4 after adding nodes Charniak 90.1 85.4 87.7 86.5 82.0 84.2 84.1 79.8 81.9 ( step 2 , Section 7 ) Collins 90.6 85.3 87.9 87.2 82.1 84.6 84.9 79.9 82.3 after adding arcs Charniak 90.0 89.7 89.8 86.5 86.2 86.4 84.2 83.9 84.0 ( step 3 , Section 8 ) Collins 90.4 89.4 89.9 87.1 86.2 86.6 84.9 83.9 84.4 Table 1 : Dependency-based evaluation of the parsers after different transformation steps but slightly lower f-score on dependencies without functional tags ( 82.9 % vs. 83.4 % ) . \n\t', '\n\t\t To summarize the evaluation scores at this stage , both parsers perform with f-score around 87 % on unlabelled dependencies . \n\t', '\n\t\t When evaluating on bare dependency labels ( i.e. , disregarding functional tags ) the performance drops to 83 % . \n\t', '\n\t\t The new errors that appear when taking labels into account come from different sources : incorrect POS tags ( NN vs. VBG ) , different degrees of flatness of analyses in gold and test parses ( JJ vs. ADJP , or CD vs. QP ) and inconsistencies in the Penn annotation ( VP vs. RRC ) . \n\t', '\n\t\t Finally , the performance goes down to around 66 % when taking into account functional tags , which are not produced by the parsers at all . \n\t', '\n\t\t 6 Step 1 : Changing Dependency Labels Intuitively , it seems that the 66 % performance on labels with functional tags is an underestimation , because much of the missing information is easily recoverable . \n\t', '\n\t\t E.g. , one can think of simple heuristics to distinguish subject NPs , temporal PPs , etc. , thus introducing functional labels and improving the scores . \n\t', '\n\t\t Developing such heuristics would be a very time consuming and ad hoc process : e.g. , Collins\x92 -A and -g tags may give useful clues for this labelling , but they are not available in the output of other parsers . \n\t', '\n\t\t As an alternative to hard- coded heuristics , \n\t\t']",Positive
"['\n\t\t On the Penn Treebank , they trained a statistical model that , given a constituent in a parsed sentence and its context ( parent , grandparent , head words thereof etc. ) , predicted the functional label , possibly empty . \n\t', '\n\t\t The method gave impressive performance , with 98.64 % accuracy on all constituents and 87.28 % f-score for non-empty functional labels , when applied to constituents correctly identified by Charniak\x92s parser . \n\t', '\n\t\t If we extrapolate these re- sults to labelled PARSEVAL with functional labels , the method would give around 87.8 % performance ( 98.64 % of the \x93usual\x94 89 % ) for Charniak\x92s parser . \n\t', '\n\t\t Adding functional labels can be viewed as a relabelling task : we need to change the labels produced by a parser . \n\t', '\n\t\t We considered this more general task , and used a different approach , taking dependency graphs as input . \n\t', '\n\t\t We first parsed the training part of our dependency tree- bank ( sections 02\x9621 ) and identified possible relabellings by comparing dependencies output by a parser to dependencies from the treebank . \n\t', '\n\t\t E.g. , for Collins\x92 parser the most frequent relabellings were SNP SNP-SBJ , PP NP-A PP NP , VP NP-A VP NP , S NP-A S NP-SBJ and VP PP VP PP-CLR . \n\t', '\n\t\t In total , around 30 % of all the parser\x92s dependencies had different labels in the treebank . \n\t', '\n\t\t We then learned a mapping from the parser\x92s labels to those in the dependency corpus , using TiMBL , a memory-based classifier \n\t\t']",Positive
"['\n\t\t The features used for the relabelling were similar to those used by Blaheta and Charniak , but redefined for dependency structures . \n\t', '\n\t\t For each dependency we included : the head ( ) and dependent ( ) , their POS tags ; the leftmost dependent of and its POS ; the head of ( ) , its POS and the label of the dependency the closest left and right siblings of ( dependents of ) and their POS tags ; the label of the dependency ( ) as derived from the parser\x92s output . \n\t', '\n\t\t When included in feature vectors , all dependency labels were split at \x91 \x92 , e.g. , the label S NP-A resulted in two features : S and NP-A . \n\t', '\n\t\t Testing was done as follows . \n\t', '\n\t\t The test corpus ( section 23 ) was also parsed , and for each dependency a feature vector was formed and given to ; TiMBL to correct the dependency label . \n\t', '\n\t\t After this transformation the outputs of the parsers were evaluated , as before , on dependencies in the three settings . \n\t', '\n\t\t The results of the evaluation are shown in Table 1 ( the row marked \x93step 1\x94 ) . \n\t', '\n\t\t Let us take a closer look at the evaluation results . \n\t', '\n\t\t Obviously , relabelling does not change the unlabelled scores . \n\t', '\n\t\t The 1 % improvement for evaluation on bare labels suggests that our approach is capable not only of adding functional tags , but can also correct the parser\x92s phrase labels and partof-speech tags : for Collins\x92 parser the most frequent correct changes not involving functional labels were NP NN NP JJ and NP JJ NP VBN , fixing POS tagging errors . \n\t', '\n\t\t A very substantial increase of the labelled score ( from 66 % to 81 % ) , which is only 6 % lower than unlabelled score , clearly indicates that , although the parsers do not produce functional labels , this information is to a large extent implicitly present in trees and can be recovered . \n\t', '\n\t\t 6.1 Comparison to Earlier Work One effect of the relabelling procedure described above is the recovery of Penn functional tags . \n\t', '\n\t\t Thus , it is informative to compare our results with those reported in \n\t\t']",Positive
"['\n\t\t Blaheta and Charniak measured tagging accuracy and precision/recall for functional tag identification only for constituents correctly identified by the parser ( i.e. , having the correct span and nonterminal label ) . \n\t', '\n\t\t Since our method uses the dependency formalism , to make a meaningful comparison we need to model the notion of a constituent being correctly found by a parser . \n\t', '\n\t\t For a word we say that the constituent corresponding to its maximal projection is correctly identified if there exists , the head of , and for the dependency the right part of its label ( e.g. , NP-SBJ for S NP-SBJ ) is a nonterminal ( i.e. , not a POS tag ) and matches the right part of the label in the gold dependency structure , after stripping functional tags . \n\t', '\n\t\t Thus , the constituent\x92s label and headword should be correct , but not necessarily the span . \n\t', '\n\t\t Moreover , 2.5 % of all constituents with functional labels ( 246 out of 9928 in section 23 ) are not maximal projections . \n\t', '\n\t\t Since our method ignores functional tags of such constituents ( these tags disappear after the conversion of phrase structures to dependency graphs ) , we consider them as errors , i.e. , reducing our recall value . \n\t', '\n\t\t Below , the tagging accuracy , precision and recall are evaluated on constituents correctly identified by Charniak\x92s parser for section 23 . \n\t', '\n\t\t Method Accuracy P R f Blaheta This paper 98.6 87.2 87.4 87.3 94.7 90.2 86.9 88.5 The difference in the accuracy is due to two reasons . \n\t', '\n\t\t First , because of the different definition of a correctly identified constituent in the parser\x92s output , we apply our method to a greater portion of all labels produced by the parser ( 95 % vs. 89 % reported in \n\t\t']",Positive
"['\n\t\t This might make the task for out system more difficult . \n\t', '\n\t\t And second , whereas 22 % of all constituents in section 23 have a functional tag , 36 % of the maximal projections have one . \n\t', '\n\t\t Since we apply our method only to labels of maximal projections , this means that our accuracy baseline ( i.e. , never assign any tag ) is lower . \n\t', '\n\t\t 7 Step 2 : Adding Missing Nodes As the row labelled \x93step 1\x94 in Table 1 indicates , for both parsers the recall is relatively low ( 6 % lower than the precision ) : while the WSJ trees , and hence the derived dependency structures , contain non-local dependencies and empty nodes , the parsers simply do not provide this information . \n\t', '\n\t\t To make up for this , we considered two further tranformations of the output of the parsers : adding new nodes ( corresponding to empty nodes in WSJ ) , and adding new labelled arcs . \n\t', '\n\t\t This section describes the former modification and Section 8 the latter . \n\t', '\n\t\t As described in Section 4 , when converting WSJ trees to dependency structures , traces are resolved , their empty nodes removed and new dependencies introduced . \n\t', '\n\t\t Of the remaining empty nodes ( i.e. , non-traces ) , the most frequent in WSJ are : NP PRO , empty units , empty complementizers , empty relative pronouns . \n\t', '\n\t\t To add missing empty nodes to dependency graphs , we compared the output of the parsers on the strings of the training corpus after steps 0 and 1 ( conversion to dependencies and relabelling ) to the structures in the corpus itself . \n\t', '\n\t\t We trained a classifier which , for every word in the parser\x92s output , had to decide whether an empty node should be added as a new dependent of the word , and what its symbol ( \x91*\x92 , \x91*U*\x92 or \x910\x92 in WSJ ) , POS tag ( always -NONE- in WSJ ) and the label of the new dependency ( e.g. , \x91S NP-SBJ\x92 for NP PRO and \x91VP SBAR\x92 for empty complementizers ) should be . \n\t', '\n\t\t This decision is conditioned on the word itself and its context . \n\t', '\n\t\t The features used were : the word and its POS tag , whether the word has any subject and object dependents , and whether it is the head of a finite verb group ; the same information for the word\x92s head ( if any ) and also the label of the corresponding dependency ; the same information for the rightmost and leftmost dependents of the word ( if exist ) along with their dependency labels . \n\t', '\n\t\t In total , we extracted 23 symbolic features for every word in the corpus . \n\t', '\n\t\t TiMBL was trained on sections 02\x9621 and applied to the output of the parsers ( after steps 0 and 1 ) on the test corpus ( section 23 ) , producing a list of empty nodes to be inserted in the dependency graphs . \n\t', '\n\t\t After insertion of the empty nodes , the resulting structures were evaluated against section 23 of the gold dependency treebank . \n\t', '\n\t\t The results are shown in Table 1 ( the row \x93step 2\x94 ) . \n\t', '\n\t\t For both parsers the insertion of empty nodes improves the recall by 1.5 % , resulting in a 1 % increase of the f-score . \n\t', '\n\t\t 7.1 Comparison to Earlier Work A procedure for empty node recovery was first described in \n\t\t']",Positive
"['\n\t\t Since our method works with dependency structures , not phrase trees , we adopt a different but comparable criterion : an empty node should be attached as a dependent to the correct word , and with the correct dependency label . \n\t', '\n\t\t Unlike the first metric , our correctness criterion also requires that possible attachment ambiguities are resolved correctly ( e.g. , as in the number of reports 0 they sent , where the empty relative pronoun may be attached either to number or to reports ) . \n\t', '\n\t\t For this task , the best published results ( using Johnson\x92s metric ) were reported by \n\t\t']",Positive
"['\n\t\t Below we give the comparison to our method . \n\t', '\n\t\t Notice that this evaluation does not include traces ( i.e. , empty elements with antecedents ) : recovery of traces is described in Section 8 . \n\t', '\n\t\t This paper Dienes&Dubey P R f P R f PRO-NP 73.1 63.89 68.1 68.7 70.4 69.5 COMP-SBAR 82.6 83.1 82.8 93.8 78.6 85.5 COMP-VRHNP 65.3 40.0 49.6 67.2 38.3 48.8 UNIT 95.4 91.8 93.6 99.1 92.5 95.7 For comparison we use the notation of Dienes and Dubey : PRO-NP for uncontrolled PROs ( nodes \x91*\x92 in the WSJ ) , COMP-SBAR for empty complementizers ( nodes \x910\x92 with dependency label VP SBAR ) , COMP-WHNP for empty relative pronouns ( nodes \x910\x92 with dependency label X SBAR , where X VP ) and UNIT for empty units ( nodes \x91*U*\x92 ) . \n\t', '\n\t\t It is interesting to see that for empty nodes except for UNIT both methods have their advantages , showing better precision or better recall . \n\t', '\n\t\t Yet shallow tagging clearly performs better for UNIT . \n\t', '\n\t\t 8 Step 3 : Adding Missing Dependencies We now get to the third and final step of our transformation method : adding missing arcs to dependency graphs . \n\t', '\n\t\t The parsers we considered do not explicitly provide information about non-local dependencies ( control , WH-extraction ) present in the treebank . \n\t', '\n\t\t Moreover , newly inserted empty nodes ( step 2 , Section 7 ) might also need more links to the rest of a sentence ( e.g. , the inserted empty complementizers ) . \n\t', '\n\t\t In this section we describe the insertion of missing dependencies . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t He proposed a pattern-matching algorithm : first , from the training corpus the patterns that license non- local dependencies are extracted , and then these patterns are detected in unseen trees , dependencies being added when matches are found . \n\t', '\n\t\t Building on these ideas , \n\t\t']",Positive
"['\n\t\t We extended Jijkoun\x92s approach by providing the classifier with lexical information and using richer patterns with labels containing the Penn functional tags and empty nodes , detected at steps 1 and 2 . \n\t', '\n\t\t First , we compared the output of the parsers on the strings of the training corpus after steps 0 , 1 and 2 to the dependency structures in the training corpus . \n\t', '\n\t\t For every dependency that is missing in the parser\x92s output , we find the shortest undirected path in the dependency graph connecting the head and the dependent . \n\t', '\n\t\t These paths , connected sequences of labelled dependencies , define the set of possible patterns . \n\t', '\n\t\t For our experiments we only considered patterns occuring more than 100 times in the training corpus . \n\t', '\n\t\t E.g. , for Collins\x92 parser , 67 different patterns were found . \n\t', '\n\t\t Next , from the parsers\x92 output on the strings of the training corpus , we extracted all occurrences of the patterns , along with information about the nodes involved . \n\t', '\n\t\t For every node in an occurrence of a pattern we extracted the following features : the word and its POS tag ; whether the word has subject and object dependents ; whether the word is the head of a finite verb cluster . \n\t', '\n\t\t Type We then trained TiMBL to predict the label of the missing dependency ( or \x91none\x92 ) , given an occurrence of a pattern and the features of all the nodes involved . \n\t', '\n\t\t We trained a separate classifier for each pattern . \n\t', '\n\t\t For evaluation purposes we extracted all occurrences of the patterns and the features of their nodes from the parsers\x92 outputs for section 23 after steps 0 , 1 and 2 and used TiMBL to predict and insert new dependencies . \n\t', '\n\t\t Then we compared the resulting dependency structures to the gold corpus . \n\t', '\n\t\t The results are shown in Table 1 ( the row \x93step 3\x94 ) . \n\t', '\n\t\t As expected , adding missing dependencies substantially improves the recall ( by 4 % for both parsers ) and allows both parsers to achieve an 84 % f-score on dependencies with functional tags ( 90 % on unlabelled dependencies ) . \n\t', '\n\t\t The unlabelled f-score 89.9 % for Collins\x92 parser is close to the 90.9 % reported in \n\t\t']",Positive
"['\n\t\t Since as many as 5 % of all dependencies in WSJ involve traces or empty nodes , the results in Table 1 are encouraging . \n\t', '\n\t\t 8.1 Comparison to Earlier Work Recently , several methods for the recovery of non- local dependencies have been described in the literature . \n\t', '\n\t\t \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
['\n\t\t Their method achieves a considerable improvement over the results reported in \n\t\t'],Negative
"['\n\t\t To compare our results to Dienes and Dubey\x92s , we carried out the transformation steps 0\x963 described above , with a single modification : when adding missing dependencies ( step 3 ) , we only considered patterns that introduce non- local dependencies ( i.e. , traces : we kept the information whether a dependency is a trace when converting WSJ to a dependency corpus ) . \n\t', '\n\t\t As before , a dependency is correctly found if its head , dependent , and label are correct . \n\t', '\n\t\t For traces , this corresponds to the evaluation using the head-based antecedent representation described in \n\t\t']",Positive
"['\n\t\t To make the results comparable to other methods , we strip functional tags from the dependency labels before label comparison . \n\t', '\n\t\t Below are the overall precision , recall , and f-score for our method and the scores reported in \n\t\t']",Positive
"['\n\t\t Method P R f Dienes and Dubey This paper 81.5 68.7 74.6 82.8 67.8 74.6 Interestingly , the overall performance of our post- processing method is very similar to that of the pre- and in-processing methods of \n\t\t']",Positive
"['\n\t\t Hence , for most cases , traces and empty nodes can be reliably identified using only local information provided by a parser , using the parser itself as a black box . \n\t', '\n\t\t This is important , since making parsers aware of non-local relations need not improve the overall performance : \n\t\t']",Positive
"['\n\t\t 9 Discussion The experiments described in the previous sections indicate that although statistical parsers do not explicitly output some information available in the corpus they were trained on ( grammatical and semantic tags , empty nodes , non-local dependencies ) , this information can be recovered with reasonably high accuracy , using pattern matching and machine learning methods . \n\t', '\n\t\t For our task , using dependency structures rather than phrase trees has several advantages . \n\t', '\n\t\t First , after converting both the treebank trees and parsers\x92 outputs to graphs with head\x96modifier relations , our method needs very little information about the linguistic nature of the data , and thus is largely corpus- and parser-independent . \n\t', '\n\t\t Indeed , after the conversion , the only linguistically informed operation is the straightforward extraction of features indicating the presence of subject and object dependents , and finiteness of verb groups . \n\t', '\n\t\t Second , using a dependency formalism facilitates a very straightforward evaluation of the systems that produce structures more complex than trees . \n\t', '\n\t\t It is not clear whether the PARSEVAL evaluation can be easily extended to take non-local relations into account ( see \n\t\t']",Positive
"['\n\t\t Finally , the independence from the details of the parser and the corpus suggests that our method can be applied to systems based on other formalisms , e.g. , \n\t\t']",Positive
"['\n\t\t Furthermore , with the fine-grained set of dependency labels that our system provides , it is possible to map the resulting structures to other dependency formalisms , either automatically in case annotated corpora exist , or with a manually developed set of rules . \n\t', '\n\t\t Our preliminary experiments with Collins\x92 parser and the corpus annotated with grammatical relations \n\t\t']",Positive
['\n\t\t This is very close to the performance reported by \n\t\t'],Positive
"['\n\t\t Despite the high-dimensional feature spaces , the large number of lexical features , and the lack of independence between features , we achieved high accuracy using a memory-based learner . \n\t', '\n\t\t TiMBL performed well on tasks where structured , more complicated and task-specific statistical models have been used previously \n\t\t']",Negative
"['\n\t\t For all subtasks we used the same settings for TiMBL : simple feature overlap measure , 5 nearest neighbours with majority voting . \n\t', '\n\t\t During further experiments with our method on different corpora , we found that quite different settings led to a better performance . \n\t', '\n\t\t It is clear that more careful and systematic parameter tuning and the analysis of the contribution of different features have to be addressed . \n\t', '\n\t\t Finally , our method is not restricted to syntactic structures . \n\t', '\n\t\t It has been successfully applied to the identification of semantic relations \n\t\t']",Positive
"['\n\t\t For this task , we viewed semantic relations ( e.g. , Speaker , Topic , Addressee ) as dependencies between a predicate and its arguments . \n\t', '\n\t\t Adding such semantic relations to syntactic dependency graphs was simply an additional graph transformation step . \n\t', '\n\t\t 10 Conclusions We presented a method to automatically enrich the output of a parser with information that is not provided by the parser itself , but is available in a tree- bank . \n\t', '\n\t\t Using the method with two state of the art statistical parsers and the Penn Treebank allowed us to recover functional tags ( grammatical and semantic ) , empty nodes and traces . \n\t', '\n\t\t Thus , we are able to provide virtually all information available in the corpus , without modifying the parser , viewing it , indeed , as a black box . \n\t', '\n\t\t Our method allows us to perform a meaningful dependency-based comparison of phrase structure parsers . \n\t', '\n\t\t The evaluation on a dependency corpus derived from the Penn Treebank showed that , after our post-processing , two state of the art statistical parsers achieve 84 % accuracy on a fine-grained set of dependency labels . \n\t', '\n\t\t Finally , our method for enriching the output of a parser is , to a large extent , independent of a specific parser and corpus , and can be used with other syntactic and semantic resources . \n\t', '\n\t\t 11 Acknowledgements We are grateful to David Ahn and Stefan Schlobach and to the anonymous referees for their useful suggestions . \n\t', '\n\t\t This research was supported by grants from the Netherlands Organization for Scientific Research ( NWO ) under project numbers 220- 80-001 , 365-20-005 , 612.069.006 , 612.000.106 , 612.000.207 and 612.066.302 . \n\t', '\n\t\t References David Ahn , Sisay Fissaha , Valentin Jijkoun , and Maarten de Rijke . \n\t', '\n\t\t 2004. The University of Amsterdam at Senseval-3 : semantic roles and logic forms . \n\t', '\n\t\t In Proceedings of the ACL-2004 Workshop on Evaluation of Systems for the Semantic Analysis of Text . \n\t', '\n\t\t Don Blaheta and Eugene Charniak . \n\t', '\n\t\t 2000 . \n\t', '\n\t\t Assigning function tags to parsed text . \n\t', '\n\t\t In Proceedings of the 1st Meeting ofNAACL , pages 234\x96240 . \n\t', '\n\t\t John Carroll , Guido Minnen , and Ted Briscoe . \n\t', '\n\t\t 2003. Parser evaluation using a grammatical relation annotation scheme . \n\t', '\n\t\t In Anne Abeill´e , editor , Building and Using Parsed Corpora , pages 299\x96316 . \n\t', '\n\t\t Kluwer . \n\t', '\n\t\t Eugene Charniak . \n\t', '\n\t\t 2000. A maximum-entropy-inspired parser . \n\t', '\n\t\t In Proceedings of the 1 st Meeting of NAACL , pages 132\x96139 . \n\t', '\n\t\t Michael Collins . \n\t', '\n\t\t 1999. Head-Driven Statistical Models for Natural Language Parsing . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , University of Pennsylvania . \n\t', '\n\t\t Walter Daelemans , Jakub Zavrel , Ko van der Sloot , and Antal van den Bosch , 2003 . \n\t', '\n\t\t TiMBL : Tilburg Memory Based Learner , version 5. 0 , Reference Guide . \n\t', '\n\t\t ILK Technical Report 03-10 . \n\t', '\n\t\t Available from http://ilk.kub.nl/downloads/pub/papers/ilk0310.ps.gz . \n\t', '\n\t\t P´eter Dienes and Amit Dubey . \n\t', '\n\t\t 2003. Antecedent recovery : Experiments with a trace tagger . \n\t', '\n\t\t In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing , pages 33\x9640 . \n\t', '\n\t\t Julia Hockenmaier . \n\t', '\n\t\t 2003. Parsing with generative models of predicate-argument structure . \n\t', '\n\t\t In Proceedings of the 41st Meeting ofACL , pages 359\x96366 . \n\t', '\n\t\t Valentin Jijkoun . \n\t', '\n\t\t 2003. Finding non-local dependencies : Beyond pattern matching . \n\t', '\n\t\t In Proceedings of the ACL-2003 Student Research Workshop , pages 37\x9643 . \n\t', '\n\t\t Mark Johnson . \n\t', '\n\t\t 2002. A simple pattern-matching algorithm for recovering empty nodes and their antecedents . \n\t', '\n\t\t In Proceedings of the 40th meeting ofACL , pages 136\x96143 . \n\t', '\n\t\t Dan Klein and Christopher D. Manning . \n\t', '\n\t\t 2003 . \n\t', '\n\t\t Accurate unlexicalized parsing . \n\t', '\n\t\t In Proceedings of the 41st Meeting ofACL , pages 423\x96430 . \n\t', '\n\t\t Long-Distance Dependency Resolution in Automatically Acquired Wide-Coverage PCFG-Based LFG Approximations Aoife Cahill , Michael Burke , Ruth O\x92Donovan , Josef van Genabith , Andy Way National Centre for Language Technology and School of Computing , Dublin City University , Dublin , Ireland {acahill,mburke,rodonovan,josef,away}@computing.dcu.ie Abstract This paper shows how finite approximations of long distance dependency ( LDD ) resolution can be obtained automatically for wide-coverage , robust , probabilistic Lexical-Functional Grammar ( LFG ) resources acquired from treebanks . \n\t', '\n\t\t We extract LFG subcategorisation frames and paths linking LDD reentrancies from f-structures generated automatically for the Penn-II treebank trees and use them in an LDD resolution algorithm to parse new text . \n\t', '\n\t\t Unlike \n\t\t']",Negative
['\n\t\t Currently our best automatically induced grammars achieve 80.97 % f-score for f- structures parsing section 23 of the WSJ part of the Penn-II treebank and evaluating against the DCU 1051 and 80.24 % against the PARC 700 Dependency Bank \n\t\t'],Positive
"['\n\t\t 1 Introduction The determination of syntactic structure is an important step in natural language processing as syntactic structure strongly determines semantic interpretation in the form of predicate-argument structure , dependency relations or logical form . \n\t', '\n\t\t For a substantial number of linguistic phenomena such as topicalisation , wh-movement in relative clauses and interrogative sentences , however , there is an important difference between the location of the ( surface ) realisation of linguistic material and the location where this material should be interpreted semantically . \n\t', '\n\t\t Resolution of such long-distance dependencies ( LDDs ) is therefore crucial in the determination of accurate predicate-argument struc- 1Manually constructed f-structures for 105 randomly selected trees from Section 23 of the WSJ section of the Penn-II Treebank ture , deep dependency relations and the construction of proper meaning representations such as logical forms \n\t\t']",Positive
"['\n\t\t Modern unification/constraint-based grammars such as LFG or HPSG capture deep linguistic information including LDDs , predicate-argument structure , or logical form . \n\t', '\n\t\t Manually scaling rich unification grammars to naturally occurring free text , however , is extremely time-consuming , expensive and requires considerable linguistic and computational expertise . \n\t', '\n\t\t Few hand-crafted , deep unification grammars have in fact achieved the coverage and robustness required to parse a corpus of say the size and complexity of the Penn treebank : \n\t\t']",Positive
"['\n\t\t The last 20 years have seen continuously increasing efforts in the construction of parse-annotated corpora . \n\t', '\n\t\t Substantial treebanks2 are now available for many languages ( including English , Japanese , Chinese , German , French , Czech , Turkish ) , others are currently under construction ( Arabic , Bulgarian ) or near completion ( Spanish , Catalan ) . \n\t', '\n\t\t Treebanks have been enormously influential in the development of robust , state-of-the-art parsing technology : grammars ( or grammatical information ) automatically extracted from treebank resources provide the backbone of many state-of-the-art probabilistic parsing approaches \n\t\t']",Positive
"['\n\t\t Such approaches are attractive as they achieve robustness , coverage and performance while incurring very low grammar development cost . \n\t', '\n\t\t However , with few notable exceptions ( e.g. Collins\x92 Model 3 , \n\t\t']",Positive
"['\n\t\t The grammars used by such systems are sometimes re- 2 Or dependency banks . \n\t', '\n\t\t ferred to as \x93half\x94 ( or \x93shallow\x94 ) grammars \n\t\t']",Positive
['\n\t\t Recently \n\t\t'],Positive
"['\n\t\t Many second generation treebanks provide a certain amount of deep syntactic or dependency information ( e.g. in the form of Penn-II functional tags and traces ) supporting the computation of representations of deep linguistic information . \n\t', '\n\t\t Exploiting this information \n\t\t']",Positive
"['\n\t\t From the f-structure annotated treebank they automatically extract wide-coverage , robust , PCFG-based LFG approximations that parse new text into trees and f-structure representations . \n\t', '\n\t\t The LFG approximations of \n\t\t']",Negative
"['\n\t\t In this paper we show how finite approximations of long distance dependency resolution can be obtained automatically for wide-coverage , robust , probabilistic LFG resources automatically acquired from treebanks . \n\t', '\n\t\t We extract LFG subcategorisation frames and paths linking LDD reentrancies from f-structures generated automatically for the Penn- II treebank trees and use them in an LDD resolution algorithm to parse new text . \n\t', '\n\t\t Unlike \n\t\t']",Positive
"['\n\t\t Currently we achieve fstructure/dependency f-scores of 80.24 and 80.97 for parsing section 23 of the WSJ part of the Penn- II treebank , evaluating against the PARC 700 and DCU 105 respectively . \n\t', '\n\t\t The paper is structured as follows : we give a brief introduction to LFG . \n\t', '\n\t\t We outline the automatic f-structure annotation algorithm , PCFG-based LFG grammar approximations and parsing architectures of \n\t\t']",Positive
"['\n\t\t We present our subcategorisation frame extraction and introduce the treebankbased acquisition of finite approximations of LFG functional uncertainty equations in terms of LDD paths . \n\t', '\n\t\t We present the f-structure LDD resolution algorithm , provide results and extensive evaluation . \n\t', '\n\t\t We compare our method with previous work . \n\t', '\n\t\t Finally , we conclude . \n\t', '\n\t\t 2 Lexical Functional Grammar ( LFG ) Lexical-Functional Grammar \n\t\t']",Positive
"['\n\t\t C(onstituent)-structure represents the grouping of words and phrases into larger constituents and is realised in terms of a CF- PSG grammar . \n\t', '\n\t\t F(unctional)-structure represents abstract syntactic functions such as SUBJ(ect) , OBJ(ect) , OBL(ique) , closed and open clausal COMP/XCOMP(lement) , ADJ(unct) , APP(osition) etc. and is implemented in terms of recursive feature structures ( attribute-value matrices ) . \n\t', '\n\t\t C-structure captures surface grammatical configurations , f- structure encodes abstract syntactic information approximating to predicate-argument/dependency structure or simple logical form ( van Genabith and Crouch , 1996 ) . \n\t', '\n\t\t C- and f-structures are related in terms of functional annotations ( constraints , attribute-value equations ) on c-structure rules ( cf. . \n\t', '\n\t\t Figure 1 ) . \n\t', '\n\t\t S \x97 . \n\t', '\n\t\t NP VP ?SUBJ=f . \n\t', '\n\t\t ?=f . \n\t', '\n\t\t VP \x97 . \n\t', '\n\t\t V NP ? \n\t', '\n\t\t ?=f . \n\t', '\n\t\t IOBJ=\x97 NP \x97 . \n\t', '\n\t\t U.N V \x97 . \n\t', '\n\t\t signs ?PRED=U.N . \n\t', '\n\t\t ?PRED=sign Figure 1 : Simple LFG C- and F-Structure Uparrows point to the f-structure associated with the mother node , downarrows to that of the local node . \n\t', '\n\t\t The equations are collected with arrows instantiated to unique tree node identifiers , and a constraint solver generates an f-structure . \n\t', '\n\t\t 3 Automatic F-Structure Annotation The Penn-II treebank employs CFG trees with additional \x93functional\x94 node annotations ( such as -LOC , -TMP , -SBJ , -LGS , ... ) as well as traces and coindexation ( to indicate LDDs ) as basic data structures . \n\t', '\n\t\t The f-structure annotation algorithm of ( Cahill et 3LFGs may also involve morphological and semantic levels of representation . \n\t', '\n\t\t S NP VP U.N. V NP signs treaty ~SUBJ LPRED U.N. ] PRED srrign J OBJ LPRED treaty ] al. , 2002 ) exploits configurational , categorial , Penn- II \x93functional\x94 , local head and trace information to annotate nodes with LFG feature-structure equations . \n\t', '\n\t\t A slightly adapted version of \n\t\t']",Positive
"['\n\t\t This partitions local subtrees of depth one ( corresponding to CFG rules ) into left and right contexts ( relative to head ) . \n\t', '\n\t\t The annotation algorithm is modular with four components ( Figure 2 ) : left-right ( L-R ) annotation principles ( e.g. leftmost NP to right of V head of VP type rule is likely to be an object etc. ) ; coordination annotation principles ( separating these out simplifies other components of the algorithm ) ; traces ( translates traces and coindexation in trees into corresponding reentrancies in f-structure ( 1 in Figure 3 ) ) ; catch all and clean-up . \n\t', '\n\t\t Lexical information is provided via macros for POS tag classes . \n\t', '\n\t\t L/R Context ==> Coordination ==> Traces ==> Catch-All Figure 2 : Annotation Algorithm The f-structure annotations are passed to a constraint solver to produce f-structures . \n\t', '\n\t\t Annotation is evaluated in terms of coverage and quality , summarised in Table 1 . \n\t', '\n\t\t Coverage is near complete with 99.82 % of the 48K Penn-II sentences receiving a single , connected f-structure . \n\t', '\n\t\t Annotation quality is measured in terms of precision and recall ( P&R ) against the DCU 105 . \n\t', '\n\t\t The algorithm achieves an F-score of 96.57 % for full f-structures and 94.3 % for preds-only f-structures .4 ~~SUBJ [ PRED U.N.1 TOPIC PRED sign J ~r OBJ [ PRED treaty SUBJ ( SPEC the 1 LPRED headline J ~ PRED say COMP 1 Figure 3 : Penn-II style tree with LDD trace and corresponding reentrancy in f-structure 4Full f-structures measure all attribute-value pairs including\x93minor\x94 features such as person , number etc. . \n\t', '\n\t\t The stricter preds-only captures only paths ending in PRED:VALUE . \n\t', '\n\t\t # frags # sent percent 0 85 0.176 1 48337 99.820 2 2 0.004 Table 1 : F-structure annotation results for DCU 105 4 PCFG-Based LFG Approximations Based on these resources \n\t\t']",Positive
"['\n\t\t Both generate PCFG-based approximations of LFG grammars . \n\t', '\n\t\t In the pipeline architecture a standard PCFG is extracted from the \x93raw\x94 treebank to parse unseen text . \n\t', '\n\t\t The resulting parse-trees are then annotated by the automatic f-structure annotation algorithm and resolved into f-structures . \n\t', '\n\t\t In the integrated architecture the treebank is first annotated with f-structure equations . \n\t', '\n\t\t An annotated PCFG is then extracted where each non-terminal symbol in the grammar has been augmented with LFG f-equations : NP[TOBJ=~] --+ DT[TSPEC=1,] NN[T=~] . \n\t', '\n\t\t Nodes followed by annotations are treated as a monadic category for grammar extraction and parsing . \n\t', '\n\t\t Post-parsing , equations are collected from parse trees and resolved into f-structures . \n\t', '\n\t\t Both architectures parse raw text into \x93proto\x94 f- structures with LDDs unresolved resulting in incomplete argument structures as in Figure 4. ~~SUBJ [ PRED U.N.1 TOPIC PRED sign J ~ ~IS OBJ [ PRED treaty SUBJ PPEC the 1 FRED headline J PRED say Figure 4 : Shallow-Parser Output with Unresolved LDD and Incomplete Argument Structure ( cf. . \n\t', '\n\t\t Figure 3 ) 5 LDDs and LFG FU-Equations Theoretically , LDDs can span unbounded amounts of intervening linguistic material as in [ U.N. signs treaty ] 1 the paper claimed ... a source said [ ] 1 . \n\t', '\n\t\t In LFG , LDDs are resolved at the f-structure level , obviating the need for empty productions and traces S V N Det S-TPC- 1 NP VP signs treaty U.N. V NP NP VP the headline said T- 1 S NP VP NP V VP N Det U.N. said headline the NP V signs treaty 1 1 all preds P 96.52 94.45 R 96.63 94.16 in trees \n\t\t']",Positive
"['\n\t\t FUs are regular expressions specifying paths in f-structure between a source ( where linguistic material is encountered ) and a target ( where linguistic material is interpreted semantically ) . \n\t', '\n\t\t To account for the fronted sentential constituents in Figures 3 and 4 , an FU equation of the form T TOPIC = T COMP* COMP would be required . \n\t', '\n\t\t The equation states that the value of the TOPIC attribute is token identical with the value of the final COMP argument along a path through the immediately enclosing f-structure along zero or more COMP attributes . \n\t', '\n\t\t This FU equation is annotated to the topicalised sentential constituent in the relevant CFG rules as follows S S NP VP TTOPIC=~ TSUBJ=J T=~ TTOPIC=TCOMP*COMP and generates the LDD-resolved proper f-structure in Figure 3 for the traceless tree in Figure 4 , as required . \n\t', '\n\t\t In addition to FU equations , subcategorisation information is a crucial ingredient in LFG\x92s account of LDDs . \n\t', '\n\t\t As an example , for a topicalised constituent to be resolved as the argument of a local predicate as specified by the FU equation , the local predicate must ( i ) subcategorise for the argument in question and ( ii ) the argument in question must not be already filled . \n\t', '\n\t\t Subcategorisation requirements are provided lexically in terms of semantic forms ( subcat lists ) and coherence and completeness conditions ( all GFs specified must be present , and no others may be present ) on f-structure representations . \n\t', '\n\t\t Semantic forms specify which grammatical functions ( GFs ) a predicate requires locally . \n\t', '\n\t\t For our example in Figures 3 and 4 , the relevant lexical entries are : V said TPRED=say(T SUBJ , T COMP ) V signs TPRED=sign(T SUBJ , T OBJ ) FU equations and subcategorisation requirements together ensure that LDDs can only be resolved at suitable f-structure locations . \n\t', '\n\t\t 6 Acquiring Lexical and LDD Resources In order to model the LFG account of LDD resolution we require subcat frames ( i.e. semantic forms ) and LDD resolution paths through f-structure . \n\t', '\n\t\t Traditionally , such resources were handcoded . \n\t', '\n\t\t Here we show how they can be acquired from f-structure annotated treebank resources . \n\t', '\n\t\t LFG distinguishes between governable ( arguments ) and nongovernable ( adjuncts ) grammatical functions ( GFs ) . \n\t', '\n\t\t If the automatic f-structure annotation algorithm outlined in Section 3 generates high quality f-structures , reliable semantic forms can be extracted ( reverse-engineered ) : for each f-structure generated , for each level of embedding we determine the local PRED value and collect the governable , i.e. subcategorisable grammatical functions present at that level of embedding . \n\t', '\n\t\t For the proper f-structure in Figure 3 we obtain sign( [ subj , obj ] ) and s a y ( [ s ub j , c omp ] ) . \n\t', '\n\t\t We extract frames from the full WSJ section of the Penn-II Treebank with 48K trees . \n\t', '\n\t\t Unlike many other approaches , our extraction process does not predefine frames , fully reflects LDDs in the source data-structures ( cf. . \n\t', '\n\t\t Figure 3 ) , discriminates between active and passive frames , computes GF , GF:CFG category pair- as well as CFG category-based subcategorisation frames and associates conditional probabilities with frames . \n\t', '\n\t\t Given a lemma l and an argument list s , the probability of s given l is estimated as : count(l , s ) P(s~l) := ~~ 1 count(l , si ) Table 2 summarises the results . \n\t', '\n\t\t We extract 3586 verb lemmas and 10969 unique verbal semantic form types ( lemma followed by non-empty argument list ) . \n\t', '\n\t\t Including prepositions associated with the subcategorised OBLs and particles , this number goes up to 14348 . \n\t', '\n\t\t The number of unique frame types ( without lemma ) is 38 without specific prepositions and particles , 577 with . \n\t', '\n\t\t F-structure annotations allow us to distinguish passive and active frames . \n\t', '\n\t\t Table 3 shows the most frequent semantic forms for accept . \n\t', '\n\t\t Passive frames are marked p . \n\t', '\n\t\t We carried out a comprehensive evaluation of the automatically acquired verbal semantic forms against the COMLEX Resource \n\t\t']",Positive
"['\n\t\t We report on the evaluation of GF-based frames for the full frames with complete prepositional and particle infomation . \n\t', '\n\t\t We use relative conditional probability thresholds ( 1 % and 5 % ) to filter the selection of semantic forms ( Table 4 ) . \n\t', '\n\t\t ( O\x92Donovan et al. , 2004 ) provide a more detailed description of the extraction and evaluation of semantic forms . \n\t', '\n\t\t Without Prep/Part With Prep/Part Lemmas 3586 3586 Sem . \n\t', '\n\t\t Forms 10969 14348 Frame Types 38 577 Active Frame Types 38 548 Passive Frame Types 21 177 Table 2 : Verb Results Semantic Form Occurrences Prob . \n\t', '\n\t\t accept([obj,subj]) 122 0.813 accept([subj],p) 9 0.060 accept([comp,subj]) 5 0.033 accept([subj,obl:as],p) 3 0.020 accept([obj,subj,obl:as]) 3 0.020 accept([obj,subj,obl:from]) 3 0.020 accept ( [ subj ] ) 2 0.013 accept([obj,subj,obl:at]) 1 0.007 accept([obj,subj,obl:for]) 1 0.007 accept([obj,subj,xcomp]) 1 0.007 Table 3 : Semantic forms for the verb accept . \n\t', '\n\t\t wh-less TOPIC-REL # wh-less TOPIC-REL # subj 5692 adjunct 1314 xcomp:adjunct 610 obj 364 xcomp:obj 291 xcomp:xcomp:adjunct 96 comp:subj 76 xcomp:subj 67 Table 5 : Most frequent wh-less TOPIC-REL paths 02\x9621 23 23/(02\x9621) TOPIC 26 7 2 FOCUS 13 4 0 TOPIC-REL 60 22 1 Table 6 : Number of path types extracted Threshold 1 % Threshold 5 % P R F-Score P R F-Score Exp . \n\t', '\n\t\t 73.7 % 22.1 % 34.0 % 78.0 % 18.3 % 29.6 % Table 4 : COMLEX Comparison We further acquire finite approximations of FU- equations . \n\t', '\n\t\t by extracting paths between co-indexed material occurring in the automatically generated f- structures from sections 02-21 of the Penn-II tree- bank . \n\t', '\n\t\t We extract 26 unique TOPIC , 60 TOPIC-REL and 13 FOCUS path types ( with a total of 14,911 token occurrences ) , each with an associated probability . \n\t', '\n\t\t We distinguish between two types of TOPICREL paths , those that occur in wh-less constructions , and all other types ( c.f Table 5 ) . \n\t', '\n\t\t Given a path p and an LDD type t ( either TOPIC , TOPIC-REL or FOCUS ) , the probability of p given t is estimated as : _ count(t , p ) P(p|t) : ~i= , count(t , pi ) In order to get a first measure of how well the approximation models the data , we compute the path types in section 23 not covered by those extracted from 02-21 : 23/(02-21) . \n\t', '\n\t\t There are 3 such path types ( Table 6 ) , each occuring exactly once . \n\t', '\n\t\t Given that the total number of path tokens in section 23 is 949 , the finite approximation extracted from 02-23 covers 99.69 % of all LDD paths in section 23 . \n\t', '\n\t\t 7 Resolving LDDs in F-Structure Given a set of semantic forms s with probabilities P(sll) ( where l is a lemma ) , a set of paths p with P(plt) ( where t is either TOPIC , TOPIC-REL or FOCUS ) and an f-structure f , the core of the algorithm to resolve LDDs recursively traverses f to : find TOPIC |TOPIC-REL|FOCUS:g pair ; retrieve TOPIC |TOPIC-REL|FOCUS paths ; for each path p with GF , : ... : GFn : GF , traverse f along GF , : ... : GFn to sub-f-structure h ; retrieve local PRED:l ; add GF:g to h iff ^ GF is not present at h ^ h together with GF is locally complete and coherent with respect to a semantic form s for l rank resolution by P(s|l) × P(p|t) The algorithm supports multiple , interacting TOPIC , TOPIC-REL and FOCUS LDDs . \n\t', '\n\t\t We use P(sll) x P(pl t ) to rank a solution , depending on how likely the PRED takes semantic frame s , and how likely the TOPIC , FOCUS or TOPIC-REL is resolved using path p . \n\t', '\n\t\t The algorithm also supports resolution of LDDs where no overt linguistic material introduces a source TOPIC-REL function ( e.g. in reduced relative clause constructions ) . \n\t', '\n\t\t We distinguish between passive and active constructions , using the relevant semantic frame type when resolving LDDs . \n\t', '\n\t\t 8 Experiments and Evaluation We ran experiments with grammars in both the pipeline and the integrated parsing architectures . \n\t', '\n\t\t The first grammar is a basic PCFG , while A-PCFG includes the f-structure annotations . \n\t', '\n\t\t We apply a parent transformation to each grammar \n\t\t']",Positive
"['\n\t\t We train on sections 02-21 ( grammar , lexical extraction and LDD paths ) of the Penn-II Treebank and test on section 23 . \n\t', '\n\t\t The only pre-processing of the trees that we do is to remove empty nodes , and remove all Penn- II functional tags in the integrated model . \n\t', '\n\t\t We evaluate the parse trees using evalb . \n\t', '\n\t\t Following \n\t\t']",Positive
"['\n\t\t Using their software we evaluate the f-structure parser output against : 1 . \n\t', '\n\t\t The DCU 105 \n\t\t']",Positive
"['\n\t\t The full 2,416 f-structures automatically generated by the f-structure annotation algorithm for the original Penn-II trees , in a CCG-style \n\t\t']",Positive
"['\n\t\t F-Score 75.83 80.80 79.17 81.32 Unlab . \n\t', '\n\t\t F-Score 78.28 82.70 81.49 83.28 DCU 105 F-Strs All GFs F-Score ( before LDD resolution ) 79.82 79.24 81.12 81.20 All GFs F-Score ( after LDD resolution ) 83.79 84.59 86.30 87.04 Preds only F-Score ( before LDD resolution ) 70.00 71.57 73.45 74.61 Preds only F-Score ( after LDD resolution ) 73.78 77.43 78.76 80.97 2416 F-Strs All GFs F-Score ( before LDD resolution ) 81.98 81.49 83.32 82.78 All GFs F-Score ( after LDD resolution ) 84.16 84.37 86.45 86.00 Preds only F-Score ( before LDD resolution ) 72.00 73.23 75.22 75.10 Preds only F-Score ( after LDD resolution ) 74.07 76.12 78.36 78.40 PARC 700 Dependency Bank Subset of GFs following \n\t\t']",Positive
['\n\t\t A subset of 560 dependency structures of the PARC 700 Dependency Bank following \n\t\t'],Positive
"['\n\t\t The parent- transformed grammars perform best in both architectures . \n\t', '\n\t\t In all cases , there is a marked improvement ( 2.07-6.36 % ) in the f-structures after LDD resolution . \n\t', '\n\t\t We achieve between 73.78 % and 80.97 % preds-only and 83.79 % to 87.04 % all GFs f-score , depending on gold-standard . \n\t', '\n\t\t We achieve between 77.68 % and 80.24 % against the PARC 700 following the experiments in \n\t\t']",Positive
"['\n\t\t For details on how we map the f-structures produced by our parsers to a format similar to that of the PARC 700 Dependency Bank , see \n\t\t']",Positive
"['\n\t\t Table 8 shows the evaluation result broken down by individual GF ( preds-only ) for the integrated model PA-PCFG against the DCU 105 . \n\t', '\n\t\t In order to measure how many of the LDD reentrancies in the gold-standard f-structures are captured correctly by our parsers , we developed evaluation software for f-structure LDD reentrancies ( similar to Johnson\x92s ( 2002 ) evaluation to capture traces and their antecedents in trees ) . \n\t', '\n\t\t Table 9 shows the results with the integrated model achieving more than 76 % correct LDD reentrancies . \n\t', '\n\t\t 9 Related Work \n\t\t']",Negative
"['\n\t\t Johnson\x92s ( 2002 ) work is closest to ours in spirit . \n\t', '\n\t\t Like our approach he provides a finite approximation of LDDs . \n\t', '\n\t\t Unlike our approach , however , he works with tree fragments in a post- processing approach to add empty nodes and their DEP . \n\t', '\n\t\t PRECISION RECALL F-SCORE adjunct 717/903 = 79 717/947 = 76 78 app 14/15 = 93 14/19 = 74 82 comp 35/43 = 81 35/65 = 54 65 coord 109/143 = 76 109/161 = 68 72 det 253/264 = 96 253/269 = 94 95 focus 1/1 = 100 1/1 = 100 100 obj 387/445 = 87 387/461= 84 85 obj2 0/1= 0 0/2 = 0 0 obl 27/56 = 48 27/61 = 44 46 obl2 1/3 = 33 1/2 = 50 40 obl ag 5/11 = 45 5/12 = 42 43 poss 69/73 = 95 69/81= 85 90 quant 40/55 = 73 40/52 = 77 75 relmod 26/38 = 68 26/50 = 52 59 subj 330/361 = 91 330/414 = 80 85 topic 12/12 = 100 12/13 = 92 96 topicrel 35/42 = 83 35/52 = 67 74 xcomp 139/160 = 87 139/146 = 95 91 OVERALL 83.78 78.35 80.97 Table 8 : Preds-only results of PA-PCFG against the DCU 105 antecedents to parse trees , while we present an approach to LDD resolution on the level of f-structure . \n\t', '\n\t\t It seems that the f-structure-based approach is more abstract ( 99 LDD path types against approximately 9,000 tree-fragment types in \n\t\t']",Negative
"['\n\t\t In contrast to Johnson\x92s approach , our LDD resolution algorithm is not biased . \n\t', '\n\t\t It computes all possible complete resolutions and order- ranks them using LDD path and subcat frame probabilities . \n\t', '\n\t\t It is difficult to provide a satisfactory comparison between the two methods , but we have carried out an experiment that compares them at the f-structure level . \n\t', '\n\t\t We take the output of Charniak\x92s PCFG Pipeline A-PCFG Integrated P-PCFG PA-PCFG TOPIC Precision ( 11/14 ) ( 12/13 ) ( 12/13 ) ( 12/12 ) Recall ( 11/13 ) ( 12/13 ) ( 12/13 ) ( 12/13 ) F-Score 0.81 0.92 0.92 0.96 FOCUS Precision ( 0/1 ) ( 0/1 ) ( 0/1 ) ( 0/1 ) Recall ( 0/1 ) ( 0/1 ) ( 0/1 ) ( 0/1 ) F-Score 0 0 0 0 TOPIC-REL Precision ( 20/34 ) ( 27/36 ) ( 34/42 ) ( 34/42 ) Recall ( 20/52 ) ( 27/52 ) ( 34/52 ) ( 34/52 ) F-Score 0.47 0.613 0.72 0.72 OVERALL 0.54 0.67 0.75 0.76 Table 9 : LDD Evaluation on the DCU 105 Charniak -LDD res . \n\t', '\n\t\t +LDD res . \n\t', '\n\t\t \n\t\t']",Negative
['\n\t\t Using the software described in \n\t\t'],Positive
"['\n\t\t The results are given in Table 10 . \n\t', '\n\t\t Our method of resolving LDDs at f-structure level results in a preds-only f-score of 80.97 % . \n\t', '\n\t\t Using \n\t\t']",Negative
['\n\t\t \n\t\t'],Negative
"['\n\t\t Some of these involve extensive cleanup of the underlying Penn-II treebank resource prior to grammar extraction . \n\t', '\n\t\t In contrast , in our approach we leave the treebank as is and only add ( but never correct ) annotations . \n\t', '\n\t\t Earlier HPSG work \n\t\t']",Negative
"['\n\t\t In contrast , we acquire our resources from treebanks and achieve substantially wider coverage . \n\t', '\n\t\t Our approach provides wide-coverage , robust , and \x96 with the addition of LDD resolution \x96 \x93deep\x94 or \x93full\x94 , PCFG-based LFG approximations . \n\t', '\n\t\t Crucially , we do not claim to provide fully adequate statistical models . \n\t', '\n\t\t It is well known \n\t\t']",Positive
"['\n\t\t This case , however , is surprisingly rare for our grammars : only 0.0018 % ( 85 out of 48424 ) of the original Penn-II trees ( without FRAGs ) fail to produce an f-structure due to inconsistent annotations ( Table 1 ) , and for parsing section 23 with the integrated model ( A-PCFG ) , only 9 sentences do not receive a parse because no f-structure can be generated for the highest ranked tree ( 0.4 % ) . \n\t', '\n\t\t Parsing with the pipeline model , all sentences receive one complete f-structure . \n\t', '\n\t\t Research on adequate probability models for unification grammars is important . \n\t', '\n\t\t \n\t\t']",Negative
"['\n\t\t They achieve coverage of 50.2 % on section 23 , as against 99 % in our approach . \n\t', '\n\t\t \n\t\t']",Negative
"['\n\t\t They achieve 79 % coverage ( full parse ) and 21 % fragement/skimmed parses . \n\t', '\n\t\t By the same measure , full parse coverage is around 99 % for our automatically acquired PCFG-based LFG approximations . \n\t', '\n\t\t Against the PARC 700 , the hand-crafted LFG grammar reported in \n\t\t']",Negative
"['\n\t\t For the same experiment , our best automatically-induced grammar achieves an f-score of 80.24 % . \n\t', '\n\t\t 10 Conclusions We presented and extensively evaluated a finite approximation of LDD resolution in automatically constructed , wide-coverage , robust , PCFGbased LFG approximations , effectively turning the \x93half \x94(or \x93shallow\x94)-grammars presented in \n\t\t']",Positive
"['\n\t\t In our approach , LDDs are resolved in f-structure , not trees . \n\t', '\n\t\t The method achieves a preds-only f-score of 80.97 % for f-structures with the PA-PCFG in the integrated architecture against the DCU 105 and 78.4 % against the 2,416 automatically generated f-structures for the original Penn-II treebank trees . \n\t', '\n\t\t Evaluating against the PARC 700 Dependency Bank , the P-PCFG achieves an f-score of 80.24 % , an overall improvement of approximately 0.6 % on the result reported for the best hand-crafted grammars in \n\t\t']",Negative
"['\n\t\t Acknowledgements This research was funded by Enterprise Ireland Basic Research Grant SC/2001/186 and IRCSET . \n\t', '\n\t\t References S. Abney . \n\t', '\n\t\t 1997. Stochastic attribute-value grammars . \n\t', '\n\t\t Computational Linguistics , 23(4):597\x96 618 . \n\t', '\n\t\t M. Burke , A. Cahill , R. O\x92Donovan , J. van Genabith , and A. Way 2004 . \n\t', '\n\t\t The Evaluation of an Automatic Annotation Algorithm against the PARC 700 Dependency Bank . \n\t', '\n\t\t In Proceedings of the Ninth International Conference on LFG , Christchurch , New Zealand ( to appear ) . \n\t', '\n\t\t A. Cahill , M. McCarthy , J. van Genabith , and A. Way . \n\t', '\n\t\t 2002. Parsing with PCFGs and Automatic F-Structure Annotation . \n\t', '\n\t\t In Miriam Butt and Tracy Holloway King , editors , Proceedings of the Seventh International Conference on LFG , pages 76\x9695 . \n\t', '\n\t\t CSLI Publications , Stanford , CA . \n\t', '\n\t\t E. Charniak . \n\t', '\n\t\t 1996. Tree-Bank Grammars . \n\t', '\n\t\t In AAAI/IAAI , Vol. 2 , pages 1031\x961036 . \n\t', '\n\t\t E. Charniak . \n\t', '\n\t\t 1999. A Maximum-Entropy-Inspired Parser . \n\t', '\n\t\t Technical Report CS-99-12 , Brown University , Providence , RI . \n\t', '\n\t\t M. Collins . \n\t', '\n\t\t 1999. Head-Driven Statistical Models for Natural Language Parsing . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , University of Pennsylvania , Philadelphia , PA . \n\t', '\n\t\t M. Dalrymple . \n\t', '\n\t\t 2001. Lexical-Functional Gram- mar . \n\t', '\n\t\t San Diego , CA ; London Academic Press . \n\t', '\n\t\t J. Hockenmaier . \n\t', '\n\t\t 2003. Parsing with Generative models of Predicate-Argument Structure . \n\t', '\n\t\t In Proceedings of the 41st Annual Conference of the Association for Computational Linguistics , pages 359\x96366 , Sapporo , Japan . \n\t', '\n\t\t M. Johnson . \n\t', '\n\t\t 1999. PCFG models of linguistic tree representations . \n\t', '\n\t\t Computational Linguistics , 24(4):613\x96632 . \n\t', '\n\t\t M. Johnson . \n\t', '\n\t\t 2002. A simple pattern-matching algorithm for recovering empty nodes and their antecedents . \n\t', '\n\t\t In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , pages 136\x96143 , Philadelphia , PA . \n\t', '\n\t\t R. Kaplan and J. Bresnan . \n\t', '\n\t\t 1982. Lexical Functional Grammar , a Formal System for Grammatical Representation . \n\t', '\n\t\t In The Mental Representation of Grammatical Relations , pages 173\x96281 . \n\t', '\n\t\t MIT Press , Cambridge , MA . \n\t', '\n\t\t R. Kaplan , S. Riezler , T. H. King , J. T. Maxwell , A. Vasserman , and R. Crouch . \n\t', '\n\t\t 2004. Speed and accuracy in shallow and deep stochastic parsing . \n\t', '\n\t\t In Proceedings of the Human Language Technology Conference and the 4th Annual Meeting of the North American Chapter of the Association for Computational Linguistics , pages 97\x96 104 , Boston , MA . \n\t', '\n\t\t T.H. King , R. Crouch , S. Riezler , M. Dalrymple , and R. Kaplan . \n\t', '\n\t\t 2003. The PARC700 dependency bank . \n\t', '\n\t\t In Proceedings of the EACL03 : 4th International Workshop on Linguistically Interpreted Corpora ( LINC-03 ) , pages 1\x968 , Budapest . \n\t', '\n\t\t D. Klein and C. Manning . \n\t', '\n\t\t 2003 . \n\t', '\n\t\t Accurate Unlexicalized Parsing . \n\t', '\n\t\t In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics ( ACL\x9202 ) , pages 423\x96430 , Sapporo , Japan . \n\t', '\n\t\t C. Macleod , A. Meyers , and R. Grishman . \n\t', '\n\t\t 1994. The COMLEX Syntax Project : The First Year . \n\t', '\n\t\t In Proceedings of the ARPA Workshop on Human Language Technology , pages 669-703 , Princeton , NJ . \n\t', '\n\t\t D. Magerman . \n\t', '\n\t\t 1994. Natural Language Parsing as Statistical Pattern Recognition . \n\t', '\n\t\t PhD thesis , Stanford University , CA . \n\t', '\n\t\t M. Marcus , G. Kim , M.A. Marcinkiewicz , R. MacIntyre , A. Bies , M. Ferguson , K. Katz , and B. Schasberger . \n\t', '\n\t\t 1994. The Penn Treebank : Annotating Predicate Argument Structure . \n\t', '\n\t\t In Proceedings of the ARPA Workshop on Human Language Technology , pages 110\x96115 , Princeton , NJ . \n\t', '\n\t\t Y. Miyao , T. Ninomiya , and J. Tsujii . \n\t', '\n\t\t 2003. Probabilistic modeling of argument structures including non-local dependencies . \n\t', '\n\t\t In Proceedings of the Conference on Recent Advances in Natural Language Processing ( RANLP ) , pages 285\x96291 , Borovets , Bulgaria . \n\t', '\n\t\t R. O\x92Donovan , M. Burke , A. Cahill , J. van Genabith , and A. Way . \n\t', '\n\t\t 2004. Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II Treebank . \n\t', '\n\t\t In Proceedings of the 42nd Annual Conference of the Association for Computational Linguistics ( ACL-04 ) , Barcelona . \n\t', '\n\t\t S. Riezler , T.H. King , R. Kaplan , R. Crouch , J. T. Maxwell III , and M. Johnson . \n\t', '\n\t\t 2002. Parsing the Wall Street Journal using a Lexical- Functional Grammar and Discriminative Estimation Techniques . \n\t', '\n\t\t In Proceedings of the 40th Annual Conference of the Association for Computational Linguistics ( ACL-02 ) , pages 271\x96278 , Philadelphia , PA . \n\t', '\n\t\t Y. Tateisi , K. Torisawa , Y. Miyao , and J. Tsujii . \n\t', '\n\t\t 1998 . \n\t', '\n\t\t Translating the XTAG English Grammar to HPSG . \n\t', '\n\t\t In 4th International Workshop on Tree Adjoining Grammars and Related Frameworks , Philadelphia , PA , pages 172\x96175 . \n\t', '\n\t\t J. van Genabith and R. Crouch . \n\t', '\n\t\t 1996. Direct and Underspecified Interpretations of LFG f- Structures . \n\t', '\n\t\t In Proceedings of the 16th International Conference on Computational Linguistics ( COLING ) , pages 262\x96267 , Copenhagen . \n\t', '\n\t\t Deep dependencies from context-free statistical parsers : correcting the surface dependency approximation Roger Levy Christopher D. Manning Department of Linguistics Departments of Computer Science and Linguistics Stanford University Stanford University rog@stanford.edu manning@cs.stanford.edu Abstract We present a linguistically-motivated algorithm for reconstructing nonlocal dependency in broad-coverage context-free parse trees derived from treebanks . \n\t', '\n\t\t We use an algorithm based on loglinear classifiers to augment and reshape context-free trees so as to reintroduce underlying nonlocal dependencies lost in the context-free approximation . \n\t', '\n\t\t We find that our algorithm compares favorably with prior work on English using an existing evaluation metric , and also introduce and argue for a new dependency-based evaluation metric . \n\t', '\n\t\t By this new evaluation metric our algorithm achieves 60 % error reduction on gold-standard input trees and 5 % error reduction on state-of- the-art machine-parsed input trees , when compared with the best previous work . \n\t', '\n\t\t We also present the first results on non- local dependency reconstruction for a language other than English , comparing performance on English and German . \n\t', '\n\t\t Our new evaluation metric quantitatively corroborates the intuition that in a language with freer word order , the surface dependencies in context-free parse trees are a poorer approximation to underlying dependency structure . \n\t', '\n\t\t 1 Introduction While parsers are been used for other purposes , the primary motivation for syntactic parsing is as an aid to semantic interpretation , in pursuit of broader goals of natural language understanding . \n\t', '\n\t\t Proponents of traditional \x91deep\x92 or \x91precise\x92 approaches to syntax , such as GB , CCG , HPSG , LFG , or TAG , have argued that sophisticated grammatical formalisms are essential to resolving various hidden relationships such as the source phrase of moved whphrases in questions and relativizations , or the controller of clauses without an overt subject . \n\t', '\n\t\t Knowledge of these hidden relationships is in turn essential to semantic interpretation of the kind practiced in the semantic parsing \n\t\t']",Positive
"['\n\t\t However , work in statistical parsing has for the most part put these needs aside , being content to recover surface context-free ( CF ) phrase structure trees . \n\t', '\n\t\t This perhaps reflects the fact that context-free phrase structure grammar ( CFG ) is in some sense at the the heart of the majority of both formal and computational syntactic research . \n\t', '\n\t\t Although , upon introducing it , \n\t\t']",Positive
"['\n\t\t CFGs seem adequate to weakly generate almost all common natural language structures , and also facilitate a transparent predicate-argument and/or semantic interpretation for the more basic ones \n\t\t']",Positive
"['\n\t\t Nevertheless , despite their success in providing surface phrase structure analyses , if statistical parsers and the representations they produce do not provide a useful stepping stone to recovering the hidden relationships , they will ultimately come to be seen as a dead end , and work will necessarily return to using richer formalisms . \n\t', '\n\t\t In this paper we attempt to establish to what degree current statistical parsers are a useful step in analysis by examining the performance of further statistical classifiers on non-local dependency recovery from CF parse trees . \n\t', '\n\t\t The natural isomorphism from CF trees to dependency trees induces only local dependencies , derived from the head- sister relation in a CF local tree . \n\t', '\n\t\t However , if the output of a context-free parser can be algorithmically augmented to accurately identify and incorporate nonlocal dependencies , then we can say that the context-free parsing model is a safe approximation to the true task of dependency reconstruction . \n\t', '\n\t\t We investigate the safeness of this approximation , devising an algorithm to reconstruct non-local dependencies from context-free parse trees using log- linear classifiers , tested on treebanks of not only English but also German , a language with much freer word order and correspondingly more discontinuity than English . \n\t', '\n\t\t This algorithm can be used as an intermediate step between the surface output trees of modern statistical parsers and semantic interpretation systems for a variety of tasks.1 1Many linguistic and technical intricacies are involved in the interpretation and use of non-local annotation structure found in treebanks . \n\t', '\n\t\t A more complete exposition of the work presented here can be found in \n\t\t']",Positive
"['\n\t\t Figure 1 : Example of empty and nonlocal annotations from the Penn Treebank of English , including null complementizers ( 0 ) , relativization ( *T*-1 ) , rightextraposition ( *ICH*-2 ) , and syntactic control ( *-3 ) . \n\t', '\n\t\t 1.1 Previous Work Previous work on nonlocal dependency has focused entirely on English , despite the disparity in type and frequency of various non-local dependency constructions for varying languages \n\t\t']",Positive
"['\n\t\t Collins (1999)\x92s Model 3 investigated GPSG-style trace threading for resolving nonlocal relative pronoun dependencies . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t Dienes and Dubey ( 2003a,b ) and \n\t\t']",Positive
"['\n\t\t Traditional LFG parsing , in both non-stochastic \n\t\t']",Positive
['\n\t\t 2 Datasets The datasets used for this study consist of the Wall Street Journal section of the Penn Treebank of English ( WSJ ) and the context-free version of the NEGRA ( version 2 ) corpus of German \n\t\t'],Positive
"['\n\t\t Full-size experiments on WSJ described in Section 4 used the standard sections 2-21 for training , 24 for development , and trees whose yield is under 100 words from section 23 for testing . \n\t', '\n\t\t Experiments described in Section 4.3 used the same development and test sets but files 200-959 of WSJ as a smaller training set ; for NEGRA we followed \n\t\t']",Positive
"['\n\t\t Consistent with prior work and with common practice in statistical parsing , we stripped categories of all functional tags prior to training and testing ( though in several cases this seems to have been a limiting move ; see Section 5 ) . \n\t', '\n\t\t Nonlocal dependency annotation in Penn Tree- banks can be divided into three major types : unindexed empty elements , dislocations , and control . \n\t', '\n\t\t The first type consists primarily of null complementizers , as exemplified in Figure 1 by the null relative pronoun 0 ( c.f. aspects that it sees ) , and do not participate in ( though they may mediate ) nonlocal dependency . \n\t', '\n\t\t The second type consists of a dislocated element coindexed with an origin site of semantic interpretation , as in the association in Figure 1 of WHNP-1 with the direct object position of sees ( a relativization ) , and the association of S2 with the ADJP quick ( a right dislocation ) . \n\t', '\n\t\t This type encompasses the classic cases of nonlocal dependency : topicalization , relativization , wh- movement , and right dislocation , as well as expletives and other instances of non-canonical argument positioning . \n\t', '\n\t\t The third type involves control loci in syntactic argument positions , sometimes coindexed with overt controllers , as in the association of the NP Farmers with the empty subject position of the S2 node . \n\t', '\n\t\t ( An example of a control locus with no controller would be [ S NP-* [ VP Eating ice cream ] ] is fun . \n\t', '\n\t\t ) Controllers are to be interpreted as syntactic ( and possibly semantic ) arguments both in their overt position and in the position of loci they control . \n\t', '\n\t\t This type encompasses raising , control , passivization , and unexpressed subjects of to- infinitive and gerund verbs , among other constructions.2 NEGRA\x92s original annotation is as dependency trees with phrasal nodes , crossing branches , and no empty elements . \n\t', '\n\t\t However , the distribution includes a context-free version produced algorithmically by recursively remapping discontinuous parts of nodes upward into higher phrases and marking their sites of origin .3 The resulting \x93traces\x94 correspond roughly to a subclass of the second class of Penn Treebank empties discussed above , and include wh- movement , topicalization , right extrapositions from NP , expletives , and scrambling of sub- 2 Four of the annotation errors in WSJ lead to uninterpretable dislocation and sharing patterns , including failure to annotate dislocations corresponding to marked origin sites , and mislabelings of control loci as origin sites of dislocation that lead to cyclic dislocations ( which are explicitly prohibited in WSJ annotation guidelines ) . \n\t', '\n\t\t We corrected these errors manually before model testing and training . \n\t', '\n\t\t 3For a detailed description of the algorithm for creating the context-free version of NEGRA , see \n\t\t']",Positive
"['\n\t\t to NP-3 NNP Farmers JJ quick S *ICH*-2 NP NN yesterday S-2 NP *-3 VP VP VBD was ADJP VP TO S VP VBZ *T*-1 point VB PRT RP out DT the NP NN problems NP SBAR WHNP-1 0 NP PRP it NP sees S. . \n\t', '\n\t\t \x93The RMV will not begin to be formed for a long time.\x94 Figure 2 : Nonlocal dependencies via right-extraposition ( *T1* ) and topicalization ( *T2* ) in the NEGRA corpus of German , before ( top ) and after ( bottom ) transformation to context-free form . \n\t', '\n\t\t Dashed lines show where nodes go as a result of remapping into context-free form . \n\t', '\n\t\t jects after other complements . \n\t', '\n\t\t The positioning of NEGRA\x92s \x93traces\x94 inside the mother node is completely algorithmic ; a dislocated constituent C has its trace at the edge of the original mother closest to C\x92s overt position . \n\t', '\n\t\t Given a context-free NEGRA tree shorn of its trace/antecedent notation , however , it is far from trivial to determine which nodes are dislocated , and where they come from . \n\t', '\n\t\t Figure 2 shows an annotated sentence from the NEGRA corpus with discontinuities due to right extraposition ( *T1* ) and topicalization ( *T2* ) , before and after transformation into context-free form with traces . \n\t', '\n\t\t 3 Algorithm Corresponding to the three types of empty-element annotation found in the Penn Treebank , our algorithm divides the process of CF tree enhancement into three phases . \n\t', '\n\t\t Each phase involves the identification of a certain subset of tree nodes to be operated on , followed by the application of the appropriate operation to the node . \n\t', '\n\t\t Operations may involve the insertion of a category at some position among a node\x92s daughters ; the marking of certain nodes as dislocated ; or the relocation of dislocated nodes to other positions within the tree . \n\t', '\n\t\t The content and ordering of phases is consistent with the syntactic theory upon which treebank annotation is based . \n\t', '\n\t\t For example , WSJ annotates relative clauses lacking overt relative pronouns , such as the SBAR in Figure 1 , with a trace in the relativization site whose antecedent is an empty relative pronoun . \n\t', '\n\t\t This requires that empty relative pronoun insertion precede dislocated element identification . \n\t', '\n\t\t Likewise , dislocated elements can serve as controllers of control loci , based on their originating site , so it is sensible to return dislocated nodes to their originating sites before identifying control loci and their controllers . \n\t', '\n\t\t For WSJ , the three phases are : 1. ( a ) Determine nodes at which to insert null COMPlementizers4 ( IDENTNULL ) ( b ) For each COMP insertion node , determine position of each insertion and insert COMP ( INSERTNULL ) 2 . \n\t', '\n\t\t ( a ) Classify each tree node as +/- DISLOCATED ( IDENTMOVED ) ( b ) For each DISLOCATED node , choose an ORIGIN node ( RELOCMOVED ) ( c ) For each pair ( DISLOCATED,origin ) , choose a position of insertion and insert dislocated ( INSERTRELOC ) 3 . \n\t', '\n\t\t ( a ) Classify each node as +/- control LOCUS ( IDENTLOCUS ) ( b ) For each LOCUS , determine position of insertion and insert LOCUS ( INSERTLOCUS ) ( c ) For each LOCUS , determine CONTROLLER ( if any ) ( FINDCONTROLLER ) Note in particular that phase 2 involves the classification of overt tree nodes as dislocated , followed by the identification of an origin site ( annotated in the treebank as an empty node ) for each dislocated element ; whereas phase 3 involves the identification of ( empty ) control loci first , and of controllers later . \n\t', '\n\t\t This approach contrasts with \n\t\t']",Positive
"['\n\t\t Our motivation is that it should generally be easier to determine whether an overt element is dislocated than whether a given position is the origin of some yet unknown dislocated element ( particularly in the absence of a sophisticated model of argument expression ) ; but control loci are highly predictable from local context , such as the subjectless non-finite S in Figure 1\x92s S-2.5 Indeed this difference seems to be implicit in the non- local feature templates used by Dienes and Dubey ( 2003a,b ) in their empty element tagger , in particular lookback for wh- words preceding a candidate verb . \n\t', '\n\t\t As described in Section 2 , NEGRA\x92s nonlocal annotation schema is much simpler , involving no 4The WSJ contains a number of SBARs headed by empty complementizers with trace S\x92s . \n\t', '\n\t\t These SBARs are introduced in our algorithm as projections of identified empty complementizers as daughters of non-SBAR categories . \n\t', '\n\t\t 5Additionally , whereas dislocated nodes are always overt , control loci may be controlled by other ( null ) control loci , meaning that identifying controllers before control loci would still entail looking for nulls . \n\t', '\n\t\t S VAFIN VP $ , $ . \n\t', '\n\t\t AP wird PP VVPP . \n\t', '\n\t\t ADV NP ADJD PROAV begonnen , VP Erst ADJA NN sp¨ater damit NP VZ lange Zeit ART NE PTKZU VVINF den RMV zu schaffen lange long Zeit time damit with it den the RMV RMV zu to schaffen form ADV AP-2 NP VAFIN wird will S VVPP ADJD VP $ , , NP VP-1 VZ $ . \n\t', '\n\t\t . \n\t', '\n\t\t *T2* PP NE ART NN PTKZU PROAV VVINF ADJA *T1 * begonnen be begun Erst not until sp¨ater later IDENTMOVED S Expletive dislocation NP(it/there) VP S/SBAR Figure 3 : Different classifiers\x92 specialized tree-matching fragments and their purposes uncoindexed empties or control loci . \n\t', '\n\t\t Correspondingly , our NEGRA algorithm includes only phase 2 of the WSJ algorithm , step ( c ) of which is trivial for NEGRA due to the deterministic positioning of trace insertion in the treebank . \n\t', '\n\t\t In each case we use a loglinear model for node classification , with a combination of quadratic regularization and thresholding by individual feature count to prevent overfitting . \n\t', ""\n\t\t In the second and third parts of phases 2 and 3 , when determining an originating site or controller for a given node N , or an insertion position for a node N ' in N , we use a competition-based setting , using a binary classification ( yes/no for association with N ) on each node in the tree , and during testing choosing the node with the highest score for positive association with N.6 All other phases of classification involve independent decisions at each node . \n\t"", '\n\t\t In phase 3 , we include a special zero node to indicate a control locus with no antecedent . \n\t', '\n\t\t 3.1 Feature templates Each subphase of our dependency reconstruction algorithm involves the training of a separate model and the development of a separate feature set . \n\t', '\n\t\t We found that it was important to include both a variety of general feature templates and a number of manually designed , specialized features to resolve specific problems observed for individual classifiers . \n\t', '\n\t\t We developed all feature templates exclusively on the training and development sets specified in Section 2 . \n\t', '\n\t\t Table 1 shows which general feature templates we used in each classifier . \n\t', '\n\t\t The features are 6The choice of a unique origin site makes our algorithm unable to deal with right-node raising or parasitic gaps . \n\t', '\n\t\t Cases of right-node raising could be automatically transformed into single-origin dislocations by making use of a theory of coordination such as \n\t\t']",Positive
"['\n\t\t Both phenomena are low-frequency , however , and we ignore them here . \n\t', '\n\t\t Feature type TAG ® / / HD / CAT x MCAT / CATxMCATxGCAT / / / CATxHDxMCATxMHD ® CATxTAGxMCATxMTAG ® CAT x TAG / / CAT x HD ® (FIRST/LAST)CAT / / (L/RSIS)CAT / / DPOS x CAT / PATH / / CAT x RCAT / TAG x RCAT / CAT x TAG x RCAT / CAT x RCAT x DPO S / HDxRHD ® CAT xHDxRHD / CATxDCAT / / / / MHDxHD ® # Special 9 0 11 0 0 12 0 3 Table 1 : Shared feature templates . \n\t', '\n\t\t See text for template descriptions . \n\t', '\n\t\t # Special is the number of special templates used for the classifier . \n\t', '\n\t\t ® denotes that all subsets of the template conjunction were included . \n\t', '\n\t\t coded as follows . \n\t', '\n\t\t The prefixes { 0,M,G,D,R } indicate that the feature value is calculated with respect to the node in question , its mother , grandmother , daughter , or relative node respectively.7 { CAT,POS,TAG,WORD } stand for syntactic category , position ( of daughter ) in mother , head tag , and head word respectively . \n\t', '\n\t\t For example , when determining whether an infinitival VP is extraposed , such as S-2 in Figure 1 , the plausibility of the VP head being a deep dependent of the head verb is captured with the MHD x HD template . \n\t', '\n\t\t (FIRST/LAST)CAT and (L/RSIS)CAT are templates used for choosing the position to insert insert relocated nodes , respectively recording whether a node of a given category is the first/last daughter , and the syntactic category of a node\x92s left/right sisters . \n\t', '\n\t\t PATH is the syntactic path between relative and base node , defined as the list of the syntactic categories on the ( inclusive ) node path linking the relative node to the node in question , paired with whether the step on the path was upward or downward . \n\t', '\n\t\t For example , in Figure 2 the syntactic path from VP-1 to PP is [ T-VP,TS , ~-VP , ~-PP ] . \n\t', '\n\t\t This is a crucial feature for the relativized classifiers RELOCATEMOVED and FIND- CONTROLLER ; in an abstract sense it mediates the gap-threading information incorporated into GPSG- 7The relative node is DISLOCATED in RELOCMOVED and LOCUS in FINDCONTROLLER . \n\t', '\n\t\t IDENTLOCUS S VP h VP-internal context to determine null subjecthood INSERTNULLS S VP Possible null com- plementizer ( records syntactic path from every S in sentence ) PCF P AoP JoP D G AoG JoG 91.2 87.6 90.5 90.0 88.3 95.7 99.4 98.5 91.6 89.9 91.4 91.2 89.4 97.9 99.8 99.6 93.3 83.4 91.2 89.9 89.2 89.0 98.0 96.0 91.2 87.3 90.2 89.6 88.0 95.2 99.0 97.7 73.1 72.8 72.9 72.8 72.5 99.7 99.6 98.8 94.4 66.7 89.3 84.9 85.0 72.6 99.4 94.1 70.1 69.7 69.5 69.7 67.7 99.4 99.4 99.7 Gold trees Parser output Jn Pres Jn DD Pres 62.4 75.3 55.6 ( 69.5 ) 61.1 85.1 67.6 80.0 ( 82.0 ) 63.3 89.3 99.6 77.1 ( 48.8 ) 87.0 74.8 74.7 71.0 73.8 71.0 90 93.3 87 84.5 83.6 NP-* WH-t 0 SBAR S-t Overall NP S VP ADJP SBAR ADVP Table 2 : Comparison with previous work using Johnson\x92s PARSEVAL metric . \n\t', '\n\t\t Jn is \n\t\t']",Positive
['\n\t\t style \n\t\t'],Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t We expressed specialized hand-coded feature templates as tree-matching patterns that capture a fragment of the content of the pattern in the feature value . \n\t', '\n\t\t Representative examples appear in Figure 3 . \n\t', '\n\t\t The italicized node is the node for which a given feature is recorded ; underscores indicate variables that can match any category ; and the angle-bracketed parts of the tree fragment , together with an index for the pattern , determine the feature value.8 4 Evaluation 4.1 Comparison with previous work Our algorithm\x92s performance can be compared with the work of \n\t\t']",Positive
"['\n\t\t Valid comparisons exist for the insertion of uncoindexed empty nodes ( COMP and ARB-SUBJ ) , identification of control and raising loci ( CONTROLLOCUS ) , and pairings of dislocated and controller/raised nodes with their origins ( DISLOC,CONTROLLER ) . \n\t', '\n\t\t In Table 2 we present comparative results , using the PARSEVAL-based evaluation metric introduced by \n\t\t']",Positive
"['\n\t\t Additionally , our algorithm does not distinguish the syntactic cate- Table 3 : Typed dependency F1 performance when composed with statistical parser . \n\t', '\n\t\t PCF is parser output evaluated by context-free ( shallow ) dependencies ; all others are evaluated on deep dependencies . \n\t', '\n\t\t P is parser , G is string-to-context-free-gold-tree mapping , A is present remapping algorithm , J is Johnson 2002 , D is the COMBINED model of Dienes 2003 . \n\t', '\n\t\t the parse tree . \n\t', '\n\t\t In Figure 1 , for example , WHNP1 could be erroneously remapped to the right edge of any S or VP node in the sentence without resulting in error according to this metric . \n\t', '\n\t\t We therefore abandon this metric in further evaluations as it is not clear whether it adequately approximates performance in predicate-argument structure recovery . \n\t', '\n\t\t 11 4.2 Composition with a context-free parser If we think of a statistical parser as a function from strings to CF trees , and the nonlocal dependency recovery algorithm A presented in this paper as a function from trees to trees , we can naturally compose our algorithm with a parser P to form a function A o P from strings to trees whose dependency interpretation is , hopefully , an improvement over the trees from P . \n\t', '\n\t\t To test this idea quantitatively we evaluate performance with respect to recovery of typed dependency relations between words . \n\t', '\n\t\t A dependency relation , commonly employed for evaluation in the statistical parsing literature , is defined at a node N of a lexicalized parse tree as a pair ( wi , wj ) where wi is the lexical head of N and wj is the lexical head of some non-head daughter of N. Dependency relations may further be typed according to information at or near the relevant tree node ; \n\t\t']",Positive
"['\n\t\t We present here dependency evaluations where the gold-standard dependency set is defined by the remapped tree , typed gory of null insertions , whereas previous work has ; as a result , the null complementizer class 0 and W H-t dislocation class are aggregates of classes used in previous work . \n\t', '\n\t\t 11\n\t\t']",Positive
"['\n\t\t This figure is difficult to compare directly with other figures in this section ; a tree search indicates that non-infinitival subjects make up at most 85.4 % of the WHNP dislocations in WSJ . \n\t', '\n\t\t WSJ(full) WSJ(sm) NEGRA Performance on gold trees Performance on parsed trees ID Rel Combo ID Combo P R F1 Acc P R F1 P R F1 P R F1 92.0 82.9 87.2 95.0 89.6 80.1 84.6 34.5 47.6 40.0 17.8 24.3 20.5 92.3 79.5 85.5 93.3 90.4 77.2 83.2 38.0 47.3 42.1 19.7 24.3 21.7 73.9 64.6 69.0 85.1 63.3 55.4 59.1 48.3 39.7 43.6 20.9 17.2 18.9 Table 4 : Cross-linguistic comparison of dislocated node identification and remapping . \n\t', '\n\t\t ID is correct identification of nodes as +/\x96 dislocated ; Rel is relocation of node to correct mother given gold-standard data on which nodes are dislocated ( only applicable for gold trees ) ; Combo is both correct identification and remapping . \n\t', '\n\t\t by syntactic category of the mother node . \n\t', '\n\t\t 12 In Figure 1 , for example , to would be an ADJP dependent of quick rather than a VP dependent of was ; and Farmers would be an S dependent both of to in to point out ... and of was . \n\t', '\n\t\t We use the head-finding rules of \n\t\t']",Positive
"['\n\t\t To further compare the results of our algorithm with previous work , we obtained the output trees produced by \n\t\t']",Positive
"['\n\t\t Table 3 shows the results of this evaluation . \n\t', '\n\t\t For comparison , we include shallow dependency accuracy for Charniak\x92s parser under PCF . \n\t', '\n\t\t 4.3 Cross-linguistic comparison In order to compare the results of nonlocal dependency reconstruction between languages , we must identify equivalence classes of nonlocal dependency annotation between treebanks . \n\t', '\n\t\t NEGRA\x92s nonlocal dependency annotation is quite different from WSJ , as described in Section 2 , ignoring controlled and arbitrary unexpressed subjects . \n\t', '\n\t\t The natural basis of comparison is therefore the set of all nonlocal NEGRA annotations against all WSJ dislocations , excluding relativizations ( defined simply as dislocated wh- constituents under SBAR).13 Table 4 shows the performance comparison between WSJ and NEGRA of IDENTDISLOC and RELOCMOVED , on sentences of 40 tokens or less . \n\t', '\n\t\t For this evaluation metric we use syntactic category and left & right edges of ( 1 ) dislocated nodes ( ID ) ; and ( 2 ) originating mother node to which dislocated node is mapped ( Rel ) . \n\t', '\n\t\t Combo requires both ( 1 ) and ( 2 ) to be correct . \n\t', '\n\t\t NEGRA is smaller than WSJ ( ^350,000 words vs. 1 million ) , so for fair 12 Unfortunately , 46 WSJ dislocation annotations in this test- set involve dislocated nodes dominating their origin sites . \n\t', '\n\t\t It is not entirely clear how to interpret the intended semantics of these examples , so we ignore them in evaluation . \n\t', '\n\t\t 13 The interpretation of comparative results must be modulated by the fact that more total time was spent on feature engineering for WSJ than for NEGRA , and the first author , who engineered the NEGRA feature set , is not a native speaker of German . \n\t', '\n\t\t comparison we tested WSJ using the smaller training set described in Section 2 , comparable in size to NEGRA\x92s . \n\t', '\n\t\t Since the positioning of traces within NEGRA nodes is trivial , we evaluate remapping and combination performances requiring only proper selection of the originating mother node ; thus we carry the algorithm out on both treebanks through step ( 2b ) . \n\t', '\n\t\t This is adequate for purposes of our typed dependency evaluation in Section 4.2 , since typed dependencies do not depend on positional information . \n\t', '\n\t\t State-of-the-art statistical parsing is far better on WSJ \n\t\t']",Positive
"['\n\t\t Table 5 compares the testset performance of algorithms on the two treebanks on the typed dependency measure introduced in Section 4.2 . \n\t', '\n\t\t 14 5 Discussion The WSJ results shown in Tables 2 and 3 suggest that discriminative models incorporating both non- local and local lexical and syntactic information can achieve good results on the task of non-local dependency identification . \n\t', '\n\t\t On the PARSEVAL metric , our algorithm performed particularly well on null complementizer and control locus insertion , and on S node relocation . \n\t', '\n\t\t In particular , Johnson noted that the proper insertion of control loci was a difficult issue involving lexical as well as structural sensitivity . \n\t', '\n\t\t We found the loglinear paradigm a good one in which to model this feature combination ; when run in isolation on gold-standard development trees , our model reached 96.4 % F 1 on control locus insertion , reducing error over the Johnson model\x92s 89.3 % 14 Many head-dependent relations in NEGRA are explicitly marked , but for those that are not we used a Collins (1999)- style head-finding algorithm independently developed for German PCFG parsing . \n\t', '\n\t\t PCF P AoP G AoG 76.3 75.4 75.7 98.7 99.7 76.3 75.4 75.7 98.7 99.6 62.0 59.3 61.0 90.9 93.6 Table 5 : Typed dependency F1 performance when composed with statistical parser . \n\t', '\n\t\t Remapped dependencies involve only non-relativization dislocations and exclude control loci . \n\t', '\n\t\t by nearly two-thirds . \n\t', '\n\t\t The performance of our algorithm is also evident in the substantial contribution to typed dependency accuracy seen in Table 3 . \n\t', '\n\t\t For gold-standard input trees , our algorithm reduces error by over 80 % from the surface-dependency baseline , and over 60 % compared with Johnson\x92s results . \n\t', '\n\t\t For parsed input trees , our algorithm reduces dependency error by 23 % over the baseline , and by 5 % compared with Johnson\x92s results . \n\t', '\n\t\t Note that the dependency figures of Dienes lag behind even the parsed results for Johnson\x92s model ; this may well be due to the fact that Dienes built his model as an extension of \n\t\t']",Negative
"['\n\t\t Manual investigation of errors on English gold- standard data revealed two major issues that suggest further potential for improvement in performance without further increase in algorithmic complexity or training set size . \n\t', '\n\t\t First , we noted that annotation inconsistency accounted for a large number of errors , particularly false positives . \n\t', '\n\t\t VPs from which an S has been extracted ( [ SShut up , ] he [ VP said t ] ) are inconsistently given an empty SBAR daughter , suggesting the cross-model low-70\x92s performance on null SBAR insertion models ( see Table 2 ) may be a ceiling . \n\t', '\n\t\t Control loci were often under-annotated ; the first five development-set false positive control loci we checked were all due to annotation error . \n\t', '\n\t\t And why-WHADVPs under SBAR , which are always dislocations , were not so annotated 20 % of the time . \n\t', '\n\t\t Second , both control locus insertion and dislocated NP remapping must be sensitive to the presence of argument NPs under classified nodes . \n\t', '\n\t\t But temporal NPs , indistinguishable by gross category , also appear under such nodes , creating a major confound . \n\t', '\n\t\t We used customized features to compensate to some extent , but temporal annotation already exists in WSJ and could be used . \n\t', '\n\t\t We note that \n\t\t']",Positive
"['\n\t\t As can be seen in Table 3 , the absolute improvement in dependency recovery is smaller for both our and Johnson\x92s postprocessing algorithms when applied to parsed input trees than when applied to gold-standard input trees . \n\t', '\n\t\t It seems that this degradation is not primarily due to noise in parse tree out puts reducing recall of nonlocal dependency identification : precision/recall splits were largely the same between gold and parsed data , and manual inspection revealed that incorrect nonlocal dependency choices often arose from syntactically reasonable yet incorrect input from the parser . \n\t', '\n\t\t For example , the gold-standard parse right-wing whites ... will [ VP step up [ NP their threats [ S [ VP * to take matters into their own hands ] ] ] ] has an unindexed control locus because Treebank annotation specifies that infinitival VPs inside NPs are not assigned controllers . \n\t', '\n\t\t Charniak\x92s parser , however , attaches the infinitival VP into the higher step up ... \n\t', '\n\t\t VP . \n\t', '\n\t\t Infinitival VPs inside VPs generally do receive controllers for their null subjects , and our algorithm accordingly yet mistakenly assigns right-wing-whites as the antecedent . \n\t', '\n\t\t The English/German comparison shown in Tables 4 and 5 is suggestive , but caution is necessary in its interpretation due to the fact that differences in both language structure and treebank annotation may be involved . \n\t', '\n\t\t Results in the G column of Table 5 , showing the accuracy of the context-free dependency approximation from gold-standard parse trees , quantitatively corroborates the intuition that nonlocal dependency is more prominent in German than in English . \n\t', '\n\t\t Manual investigation of errors made on German gold-standard data revealed two major sources of error beyond sparsity . \n\t', '\n\t\t The first was a widespread ambiguity of S and VP nodes within S and VP nodes ; many true dislocations of all sorts are expressed at the S and VP levels in CFG parse trees , such as VP- 1 of Figure 2 , but many adverbial and subordinate phrases of S or VP category are genuine dependents of the main clausal verb . \n\t', '\n\t\t We were able to find a number of features to distinguish some cases , such as the presence of certain unambiguous relative- clause introducing complementizers beginning an S node , but much ambiguity remained . \n\t', '\n\t\t The second was the ambiguity that some matrix S-initial NPs are actually dependents of the VP head ( in these cases , NEGRA annotates the finite verb as the head of S and the non-finite verb as the head of VP ) . \n\t', '\n\t\t This is not necessarily a genuine discontinuity per se , but rather corresponds to identification of the subject NP in a clause . \n\t', '\n\t\t Obviously , having access to reliable case marking would improve performance in this area ; such information is in fact included in NEGRA\x92s morphological annotation , another argument for the utility of involving enhanced annotation in CF parsing . \n\t', '\n\t\t As can be seen in the right half of Table 4 , performance falls off considerably on vanilla PCFG- WSJ(full) WSJ(sm) NEGRA parsed data . \n\t', '\n\t\t This fall-off seems more dramatic than that seen in Sections 4.1 and 4.2 , no doubt partly due to the poorer performance of the vanilla PCFG , but likely also because only non-relativization dislocations are considered in Section 4.3 . \n\t', '\n\t\t These dislocations often require non-local information ( such as identity of surface lexical governor ) for identification and are thus especially susceptible to degradation in parsed data . \n\t', '\n\t\t Nevertheless , seemingly dismal performance here still provided a strong boost to typed dependency evaluation of parsed data , as seen in A o P of Table 5 . \n\t', '\n\t\t We suspect this indicates that dislocated terminals are being usefully identified and mapped back to their proper governors , even if the syntactic projections of these terminals and governors are not being correctly identified by the parser . \n\t', '\n\t\t 6 Further Work Against the background of CFG as the standard approximation of dependency structure for broad- coverage parsing , there are essentially three options for the recovery of nonlocal dependency . \n\t', '\n\t\t The first option is to postprocess CF parse trees , which we have closely investigated in this paper . \n\t', '\n\t\t The second is to incorporate nonlocal dependency information into the category structure of CF trees . \n\t', '\n\t\t This was the approach taken by Dienes and Dubey ( 2003a,b ) and \n\t\t']",Positive
"['\n\t\t The third would be to incorporate nonlocal dependency information into the edge structure parse trees , allowing discontinuous constituency to be explicitly represented in the parse chart . \n\t', '\n\t\t This approach was tentatively investigated by \n\t\t']",Positive
"['\n\t\t As the syntactic diversity of languages for which treebanks are available grows , it will become increasingly important to compare these three approaches . \n\t', '\n\t\t 7 Acknowledgements This work has benefited from feedback from Dan Jurafsky and three anonymous reviewers , and from presentation at the Institute of Cognitive Science , University of Colorado at Boulder . \n\t', '\n\t\t The authors are also grateful to Dan Klein and Jenny Finkel for use of maximum-entropy software they wrote . \n\t', '\n\t\t This work was supported in part by the Advanced Research and Development Activity (ARDA)\x92s Advanced Question Answering for Intelligence ( AQUAINT ) Program . \n\t', '\n\t\t References Charniak , E. ( 2000 ) . \n\t', '\n\t\t A Maximum-Entropy-inspired parser . \n\t', '\n\t\t In Proceedings ofNAACL . \n\t', '\n\t\t Chomsky , N. ( 1956 ) . \n\t', '\n\t\t Three models for the description of language . \n\t', '\n\t\t IRE Transactions on Information Theory , 2(3):113\x96 124 . \n\t', '\n\t\t Collins , M. ( 1999 ) . \n\t', '\n\t\t Head-Driven Statistical Models forNatural Language Parsing . \n\t', '\n\t\t PhD thesis , University of Pennsylvania . \n\t', '\n\t\t Dienes , P. ( 2003 ) . \n\t', '\n\t\t Statistical Parsing with Non-local Depen- dencies . \n\t', '\n\t\t PhD thesis , Saarland University . \n\t', '\n\t\t Dienes , P. and Dubey , A. ( 2003a ) . \n\t', '\n\t\t Antecedent recovery : Ex- periments with a trace tagger . \n\t', '\n\t\t In Proceedings ofEMNLP . \n\t', '\n\t\t Dienes , P. and Dubey , A. ( 2003b ) . \n\t', '\n\t\t Deep processing by com- bining shallow methods . \n\t', '\n\t\t In Proceedings ofACL . \n\t', '\n\t\t Dubey , A. and Keller , F. ( 2003 ) . \n\t', '\n\t\t Parsing German with sister- head dependencies . \n\t', '\n\t\t In Proceedings ofACL . \n\t', '\n\t\t Gazdar , G. , Klein , E. , Pullum , G. , and Sag , I. ( 1985 ) . \n\t', '\n\t\t Generalized Phrase Structure Grammar . \n\t', '\n\t\t Harvard . \n\t', '\n\t\t Gildea , D. and Jurafsky , D. ( 2002 ) . \n\t', '\n\t\t Automatic labeling of se- mantic roles . \n\t', '\n\t\t Computational Linguistics , 28(3):245\x96288 . \n\t', '\n\t\t Hockenmaier , J. ( 2003 ) . \n\t', '\n\t\t Data and models for Statistical Parsing with Combinatory Categorial Grammar . \n\t', '\n\t\t PhD thesis , University of Edinburgh . \n\t', '\n\t\t Johnson , M. ( 2002 ) . \n\t', '\n\t\t A simple pattern-matching algorithm for recovering empty nodes and their antecedents . \n\t', '\n\t\t In Proceedings ofACL , volume 40 . \n\t', '\n\t\t Kaplan , R. , Riezler , S. , King , T. H. , Maxwell , J. T. , Vasserman , A. , and Crouch , R. ( 2004 ) . \n\t', '\n\t\t Speed and accuracy in shallow and deep stochastic parsing . \n\t', '\n\t\t In Proceedings ofNAACL . \n\t', '\n\t\t Kaplan , R. M. and Maxwell , J. T. ( 1993 ) . \n\t', '\n\t\t The interface between phrasal and functional constraints . \n\t', '\n\t\t Computational Linguistics , 19(4):571\x96590 . \n\t', '\n\t\t Klein , D. and Manning , C. D. ( 2003 ) . \n\t', '\n\t\t Accurate unlexicalized parsing . \n\t', '\n\t\t In Proceedings ofACL . \n\t', '\n\t\t Kruijff , G.-J. ( 2002 ) . \n\t', '\n\t\t Learning linearization rules from treebanks . \n\t', '\n\t\t Invited talk at the Formal Grammar\x9202/COLOGNET-ELSNET Symposium . \n\t', '\n\t\t Levy , R. ( 2004 ) . \n\t', '\n\t\t Probabilistic Models of Syntactic Discontinuity . \n\t', '\n\t\t PhD thesis , Stanford University . \n\t', '\n\t\t In progress . \n\t', '\n\t\t Maxwell , J. T. and Manning , C. D. ( 1996 ) . \n\t', '\n\t\t A theory of non- constituent coordination based on finite-state rules . \n\t', '\n\t\t In Butt , M. and King , T. H. , editors , Proceedings ofLFG . \n\t', '\n\t\t Pasca , M. and Harabagiu , S. M. ( 2001 ) . \n\t', '\n\t\t High performance question/answering . \n\t', '\n\t\t In Proceedings of SIGIR . \n\t', '\n\t\t Plaehn , O. ( 2000 ) . \n\t', '\n\t\t Computing the most probable parse for a discontinuous phrase structure grammar . \n\t', '\n\t\t In Proceedings of IWPT , Trento , Italy . \n\t', '\n\t\t Riezler , S. , King , T. H. , Kaplan , R. M. , Crouch , R. S. , Maxwell , J. T. , and Johnson , M. ( 2002 ) . \n\t', '\n\t\t Parsing the Wall Street Journal using a Lexical-Functional Grammar and discriminative estimation techniques . \n\t', '\n\t\t In Proceedings ofACL , pages 271\x96 278 . \n\t', '\n\t\t Skut , W. , Brants , T. , Krenn , B. , and Uszkoreit , H. ( 1997a ) . \n\t', '\n\t\t Annotating unrestricted German text . \n\t', '\n\t\t In Fachtagung der Sektion Computerlinguistik der Deutschen Gesellschaft fr Sprachwissenschaft , Heidelberg , Germany . \n\t', '\n\t\t Skut , W. , Krenn , B. , Brants , T. , and Uszkoreit , H. ( 1997b ) . \n\t', '\n\t\t An annotation scheme for free word order languages . \n\t', '\n\t\t In Proceedings ofANLP . \n\t', '\n\t\t A Study on Convolution Kernels for Shallow Semantic Parsing Alessandro Moschitti University of Texas at Dallas Human Language Technology Research Institute Richardson , TX 75083-0688 , USA alessandro.moschitti@utdallas.edu Abstract In this paper we have designed and experimented novel convolution kernels for automatic classification of predicate arguments . \n\t', '\n\t\t Their main property is the ability to process structured representations . \n\t', '\n\t\t Support Vector Machines ( SVMs ) , using a combination of such kernels and the flat feature kernel , classify Prop- Bank predicate arguments with accuracy higher than the current argument classification state- of-the-art . \n\t', '\n\t\t Additionally , experiments on FrameNet data have shown that SVMs are appealing for the classification of semantic roles even if the proposed kernels do not produce any improvement . \n\t', '\n\t\t 1 Introduction Several linguistic theories , e.g. \n\t\t']",Positive
"['\n\t\t Hence , to deal with natural language semantics , the learning algorithm should be able to represent and process structured data . \n\t', '\n\t\t The classical solution adopted for such tasks is to convert syntax structures into flat feature representations which are suitable for a given learning model . \n\t', '\n\t\t The main drawback is that structures may not be properly represented by flat features . \n\t', '\n\t\t In particular , these problems affect the processing of predicate argument structures annotated in PropBank \n\t\t']",Negative
"['\n\t\t Figure 1 shows an example of a predicate annotation in PropBank for the sentence : "" Paul gives a lecture in Rome "" . \n\t', '\n\t\t A predicate may be a verb or a noun or an adjective and most of the time Arg 0 is the logical subject , Arg 1 is the logical object and ArgM may indicate locations , as in our example . \n\t', '\n\t\t FrameNet also describes predicate/argument structures but for this purpose it uses richer semantic structures called frames . \n\t', '\n\t\t These latter are schematic representations of situations involving various participants , properties and roles in which a word may be typically used . \n\t', '\n\t\t Frame elements or semantic roles are arguments of predicates called target words . \n\t', '\n\t\t In FrameNet , the argument names are local to a particular frame . \n\t', '\n\t\t Figure 1 : A predicate argument structure in a parse-tree representation . \n\t', '\n\t\t Several machine learning approaches for argument identification and classification have been developed \n\t\t']",Positive
"['\n\t\t Their common characteristic is the adoption of feature spaces that model predicate-argument structures in a flat representation . \n\t', '\n\t\t On the contrary , convolution kernels aim to capture structural information in term of sub-structures , providing a viable alternative to flat features . \n\t', '\n\t\t In this paper , we select portions of syntactic trees , which include predicate/argument salient sub-structures , to define convolution kernels for the task of predicate argument classification . \n\t', '\n\t\t In particular , our kernels aim to ( a ) represent the relation between predicate and one of its arguments and ( b ) to capture the overall argument structure of the target predicate . \n\t', '\n\t\t Additionally , we define novel kernels as combinations of the above two with the polynomial kernel of standard flat features . \n\t', '\n\t\t Experiments on Support Vector Machines using the above kernels show an improvement N VP V Paul PP NP Arg . \n\t', '\n\t\t 0 gives Predicate IN N D N Arg . \n\t', '\n\t\t 1 Arg . \n\t', '\n\t\t M S in a lecture Rome of the state-of-the-art for PropBank argument classification . \n\t', '\n\t\t On the contrary , FrameNet semantic parsing seems to not take advantage of the structural information provided by our kernels . \n\t', '\n\t\t The remainder of this paper is organized as follows : Section 2 defines the Predicate Argument Extraction problem and the standard solution to solve it . \n\t', '\n\t\t In Section 3 we present our kernels whereas in Section 4 we show comparative results among SVMs using standard features and the proposed kernels . \n\t', '\n\t\t Finally , Section 5 summarizes the conclusions . \n\t', '\n\t\t 2 Predicate Argument Extraction : a standard approach Given a sentence in natural language and the target predicates , all arguments have to be recognized . \n\t', '\n\t\t This problem can be divided into two subtasks : ( a ) the detection of the argument boundaries , i.e. all its compounding words and ( b ) the classification of the argument type , e.g. Arg0 or ArgM in PropBank or Agent and Goal in FrameNet . \n\t', '\n\t\t The standard approach to learn both detection and classification of predicate arguments is summarized by the following steps : 1 . \n\t', '\n\t\t Given a sentence from the training-set generate a full syntactic parse-tree ; 2. let P and A be the set of predicates and the set of parse-tree nodes ( i.e. the potential arguments ) , respectively ; 3. for each pair <p , a> E P x A : \x95 extract the feature representation set , Fp,a ; \x95 if the subtree rooted in a covers exactly the words of one argument of p , put Fp,a in T+ ( positive examples ) , otherwise put it in T\x97 ( negative examples ) . \n\t', '\n\t\t For example , in Figure 1 , for each combination of the predicate give with the nodes N , S , VP , V , NP , PP , D or IN the instances F\x94give\x94,a are generated . \n\t', '\n\t\t In case the node a exactly covers Paul , a lecture or in Rome , it will be a positive instance otherwise it will be a negative one , e.g. F\x94give\x94,\x94 IN\x94 . \n\t', '\n\t\t To learn the argument classifiers the T+ set can be re-organized as positive T +argi and negative T\x97argi examples for each argument i . \n\t', '\n\t\t In this way , an individual ONE-vs-ALL classifier for each argument i can be trained . \n\t', '\n\t\t We adopted this solution as it is simple and effective \n\t\t']",Positive
"['\n\t\t In the classification phase , given a sentence of the test-set , all its Fp a are generated and classified by each individ- argument associated with the maximum value among the scores provided by the SVMs , i.e. argmaxiES Ci , where S is the target set of arguments . \n\t', '\n\t\t - Phrase Type : This feature indicates the syntactic type of the phrase labeled as a predicate argument , e.g. NP for Arg1 . \n\t', '\n\t\t - Parse Tree Path : This feature contains the path in the parse tree between the predicate and the argument phrase , expressed as a sequence of nonterminal labels linked by direction ( up or down ) symbols , e.g. V I VP J NP for Arg1 . \n\t', '\n\t\t - Position : Indicates if the constituent , i.e. the potential argument , appears before or after the predicate in the sentence , e.g. after for Arg1 and before for Arg0 . \n\t', '\n\t\t - Voice : This feature distinguishes between active or passive voice for the predicate phrase , e.g. active for every argument . \n\t', '\n\t\t - Head Word : This feature contains the headword of the evaluated phrase . \n\t', '\n\t\t Case and morphological information are preserved , e.g. lecture for Arg1 . \n\t', '\n\t\t - Governing Category indicates if an NP is dominated by a sentence phrase or by a verb phrase , e.g. the NP associated with Arg1 is dominated by a VP . \n\t', '\n\t\t ual classifier . \n\t', '\n\t\t As a final decision , we select the - Predicate Word : This feature consists of two components : ( 1 ) the word itself , e.g. gives for all arguments ; and ( 2 ) the lemma which represents the verb normalized to lower case and infinitive form , e.g. give for all arguments . \n\t', '\n\t\t Table 1 : Standard features extracted from the parse-tree in Figure 1. 2.1 Standard feature space The discovery of relevant features is , as usual , a complex task , nevertheless , there is a common consensus on the basic features that should be adopted . \n\t', '\n\t\t These standard features , firstly proposed in \n\t\t']",Positive
"['\n\t\t Table 1 presents the standard features and exemplifies how they are extracted from the parse tree in Figure 1 . \n\t', '\n\t\t For example , the Parse Tree Path feature represents the path in the parse-tree between a predicate node and one of its argument nodes . \n\t', '\n\t\t It is expressed as a sequence of nonterminal labels linked by direction symbols ( up or down ) , e.g. in Figure 1 , VTVPINP is the path between the predicate to give and the argument 1 , a lecture . \n\t', '\n\t\t Two pairs <p1 , a1> and <p2 , a2> have two different Path features even if the paths differ only for a node in the parse-tree . \n\t', '\n\t\t This pre- Figure 2 : Structured features for Arg0 , Arg1 and ArgM . \n\t', '\n\t\t jj a talk in N a ) S Fdeliver , Arg0 b ) S Fdeliver , Arg1 c ) S 0 Arg . \n\t', '\n\t\t N Paul V delivers D N NP VP IN NP PP Paul N delivers V D N IN NP a Arg . \n\t', '\n\t\t 1 NP VP talk in PP jj N Paul N delivers V D N a talk NP VP PP IN NP in jj N formal style Fdeliver , ArgM style formal style formal Arg . \n\t', '\n\t\t M vents the learning algorithm to generalize well on unseen data . \n\t', '\n\t\t In order to address this problem , the next section describes a novel kernel space for predicate argument classification . \n\t', '\n\t\t 2.2 Support Vector Machine approach Given a vector space in Rn and a set of positive and negative points , SVMs classify vectors according to a separating hyperplane , H(~x) = w~ x x~+ b= 0 , where w~ E Rn and b E Rare learned by applying the Structural Risk Minimization principle \n\t\t']",Positive
"['\n\t\t To apply the SVM algorithm to Predicate Argument Classification , we need a function O : F \x97 , Rn to map our features space F = { f1 , .. , f|F| } and our predicate/argument pair representation,( Fp a =(O1 Fz , into Rn , such that : Fz \x97 O(Fz) = ( Fz ) , .. , On ( Fz ) ) From the kernel theory we have that : H(x) = ( E aixi ) \x95 x + b = E ai~xi\x95~x+b= i=1..l i=1..l = aiO(Fi) \x95 O(Fz) + b. i=1..l where , Fi Vi E { 1 , .. , l } are the training instances and the product K(Fi , Fz ) =<O(Fi) \x95 O(Fz)> is the kernel function associated with the mapping O . \n\t', '\n\t\t The simplest mapping that we can apply is O(Fz) = z~ = ( z1 , ... , zn ) where zi = 1 if fi E Fz otherwise zi = 0 , i.e. the characteristic vector of the set Fz with respect to F . \n\t', '\n\t\t If we choose as a kernel function the scalar product we obtain the linear kernel KL(Fx , Fz ) =x~\x95~z . \n\t', '\n\t\t Another function which is the current state- of-the-art of predicate argument classification is the polynomial kernel : Kp(Fx , Fz ) = ( c+~x \x95~z)d , where c is a constant and d is the degree of the polynom . \n\t', '\n\t\t 3 Convolution Kernels for Semantic Parsing We propose two different convolution kernels associated with two different predicate argu- ment sub-structures : the first includes the target predicate with one of its arguments . \n\t', '\n\t\t We will show that it contains almost all the standard feature information . \n\t', '\n\t\t The second relates to the sub-categorization frame of verbs . \n\t', '\n\t\t In this case , the kernel function aims to cluster together verbal predicates which have the same syntactic realizations . \n\t', '\n\t\t This provides the classification algorithm with important clues about the possible set of arguments suited for the target syntactic structure . \n\t', '\n\t\t 3.1 Predicate/Argument Feature ( PAF ) We consider the predicate argument structures annotated in PropBank or FrameNet as our semantic space . \n\t', '\n\t\t The smallest sub-structure which includes one predicate with only one of its arguments defines our structural feature . \n\t', '\n\t\t For example , Figure 2 illustrates the parse-tree of the sentence "" Paul delivers a talk in formal style "" . \n\t', '\n\t\t The circled substructures in ( a ) , ( b ) and ( c ) are our semantic objects associated with the three arguments of the verb to deliver , i.e. <deliver , Arg0> , <deliver , Arg1 > and <deliver , ArgM> . \n\t', '\n\t\t Note that each predicate/argument pair is associated with only one structure , i.e. Fp,a contain only one of the circled sub-trees . \n\t', '\n\t\t Other important properties are the followings : ( 1 ) The overall semantic feature space F contains sub-structures composed of syntactic information embodied by parse-tree dependencies and semantic information under the form of predicate/argument annotation . \n\t', '\n\t\t ( 2 ) This solution is efficient as we have to classify as many nodes as the number of predicate arguments . \n\t', '\n\t\t ( 3 ) A constituent cannot be part of two different arguments of the target predicate , i.e. there is no overlapping between the words of two arguments . \n\t', '\n\t\t Thus , two semantic structures Fp1,a1 and Fp2,a21 , associated with two different ar- 1Fp,a was defined as the set of features of the object <p , a> . \n\t', ""\n\t\t Since in our representations we have only one Figure 4 : All 17 valid fragments of the semantic structure associated with Arg 1 of Figure 2. F~ = ~f'1 , .. , f ' IF'I~ and ( 2 ) from F~ to RIF'I . \n\t"", '\n\t\t An example of features in F~ is given in Figure 4 where the whole set of fragments , F~deliver,Ary1 , of the argument structure Fdeliver,Ary1 , is shown ( see also Figure 2 ) . \n\t', '\n\t\t It is worth noting that the allowed sub-trees contain the entire ( not partial ) production rules . \n\t', '\n\t\t For instance , the sub-tree [ NP [ D a ] ] is excluded from the set of the Figure 4 since only a part of the production NP \x97* D N is used in its generation . \n\t', '\n\t\t However , this constraint does not apply to the production VP \x97* V NP PP along with the fragment [ VP [ V NP ] ] as the subtree [ VP [ PP [ ... ] ] ] is not considered part of the semantic structure . \n\t', '\n\t\t Thus , in step 1 , an argument structure Fp,a is mapped in a fragment set F~p,a . \n\t', ""\n\t\t In step 2 , this latter is mapped into x~ = ( x1,..,xIF'I ) E RIF'I , where xi is equal to the number of times that f'i occurs in F~p,a2 . \n\t"", '\n\t\t In order to evaluate K(O(Fx) , O(Fz)) without evaluating the feature vector x~ and z~ we define the indicator function Ii ( n ) = 1 if the substructure i is rooted at node n and 0 otherwise . \n\t', '\n\t\t It follows that Oi(Fx) = ~nENx Ii(n) , where Nx is the set of the Fx\x92s nodes . \n\t', ""\n\t\t Therefore , the kernel can be written as : IF'1 K(O(Fx) , ~(Fz)) = E ( E Ii(nx))( E Ii(nz)) i=1 nxENx nz ENz E= E E Ii (nx)Ii ( nz ) nxENx nz ENz i where Nx and Nz are the nodes in Fx and Fz , respectively . \n\t"", '\n\t\t In \n\t\t']",Positive
"['\n\t\t Fflush Fbuckle S NPArg1 VP ( flush ) Arg1 ( buckle ) Arg0 ( flush and buckle ) PRP VP CC VP He and VBD NP buckled PRP $ NN VBD NP flushed DT NN the pan his belt Figure 3 : Sub-Categorization Features for two predicate argument structures . \n\t', '\n\t\t guments , cannot be included one in the other . \n\t', '\n\t\t This property is important because a convolution kernel would not be effective to distinguish between an object and its sub-parts . \n\t', '\n\t\t 3.2 Sub-Categorization Feature ( SCF ) The above object space aims to capture all the information between a predicate and one of its arguments . \n\t', '\n\t\t Its main drawback is that important structural information related to inter- argument dependencies is neglected . \n\t', '\n\t\t In order to solve this problem we define the Sub- Categorization Feature ( SCF ) . \n\t', '\n\t\t This is the sub- parse tree which includes the sub-categorization frame of the target verbal predicate . \n\t', '\n\t\t For example , Figure 3 shows the parse tree of the sentence "" He flushed the pan and buckled his belt "" . \n\t', '\n\t\t The solid line describes the SCF of the predicate flush , i.e. Fflush whereas the dashed line tailors the SCF of the predicate buckle , i.e. Fbuckle . \n\t', '\n\t\t Note that SCFs are features for predicates , ( i.e. they describe predicates ) whereas PAF characterizes predicate/argument pairs . \n\t', '\n\t\t Once semantic representations are defined , we need to design a kernel function to estimate the similarity between our objects . \n\t', '\n\t\t As suggested in Section 2 we can map them into vectors in Rn and evaluate implicitly the scalar product among them . \n\t', '\n\t\t 3.3 Predicate/Argument structure Kernel ( PAK ) Given the semantic objects defined in the previous section , we design a convolution kernel in a way similar to the parse-tree kernel proposed in \n\t\t']",Positive
"['\n\t\t We divide our mapping 0 in two steps : ( 1 ) from the semantic structure space F ( i.e. PAF or SCF objects ) to the set of all their possible sub-structures element in Fv,a with an abuse of notation we use it to indicate the objects themselves . \n\t', '\n\t\t Predicate 1 Predicate 2 VP VP VP V D N D N D N NP NP NP V V a talk a talk a talk VP D N delivers VP VP VP VP VP VP NP NP NP D N a talk a talk a D N talk D N D N D N a talk V NP V D N V delivers NP D N NP NP V NP V NP V NP V delivers D N delivers D N delivers NP V delivers ( 2 ) if the productions at n , , and nz are the same , and n , , and nz are pre-terminals then 0(n , , , nz ) = 1 ; ( 3 ) if the productions at n , , and nz are the same , and n , , and nz are not pre-terminals then nc(nx) ~(n , , , nz ) = H ( 1 + ~(ch(n , , , j ) , ch(nz , j ) ) ) , j=1 where nc(n,,) is the number of the children of n , , and ch(n , i ) is the i-th child of the node n . \n\t', '\n\t\t Note that as the productions are the same ch(n , , , i ) = ch(nz , i ) . \n\t', '\n\t\t This kind of kernel has the drawback of assigning more weight to larger structures while the argument type does not strictly depend on the size of the argument \n\t\t']",Positive
"['\n\t\t To overcome this problem we can scale the relative importance of the tree fragments using a parameter A for the cases ( 2 ) and ( 3 ) , i.e. ~(n , , , nz ) = A and ~(n , , , nz ) = A jl ; c(i , ) ( 1 + ~(ch(n , , , j ) , ch(nz , j ) ) ) respectively . \n\t', '\n\t\t It is worth noting that even if the above equations define a kernel function similar to the one proposed in \n\t\t']",Positive
"['\n\t\t For example , Figure 4 shows that structures such as [ VP [ V ] [ NP ] ] , [ VP [ V delivers ] [ NP ] ] and [ VP [ V ] [ NP [ DT ] [ N ] ] ] are valid features , but these fragments ( and many others ) are not generated by a complete production , i.e. VP \x97* V NP PP . \n\t', '\n\t\t As a consequence they would not be included in the parse-tree kernel of the sentence . \n\t', '\n\t\t 3.4 Comparison with Standard Features In this section we compare standard features with the kernel based representation in order to derive useful indications for their use : First , PAK estimates a similarity between two argument structures ( i.e. , PAF or SCF ) by counting the number of sub-structures that are in common . \n\t', '\n\t\t As an example , the similarity between the two structures in Figure 2 , F\x94delivers\x94,Arg0 and F\x94delivers\x94,Arg1 , is equal to 1 since they have in common only the [ V delivers ] substructure . \n\t', '\n\t\t Such low value depends on the fact that different arguments tend to appear in different structures . \n\t', '\n\t\t On the contrary , if two structures differ only for a few nodes ( especially terminals or near terminal nodes ) the similarity remains quite high . \n\t', '\n\t\t For example , if we change the tense of the verb to deliver ( Figure 2 ) in delivered , the [ VP [ V delivers ] [ NP ] ] subtree will be transformed in [ VP [ VBD delivered ] [ NP ] ] , where the NP is unchanged . \n\t', '\n\t\t Thus , the similarity with the previous structure will be quite high as : ( 1 ) the NP with all sub-parts will be matched and ( 2 ) the small difference will not highly affect the kernel norm and consequently the final score . \n\t', '\n\t\t The above property also holds for the SCF structures . \n\t', '\n\t\t For example , in Figure 3 , KPAK ( 0(Ffl.sh) , 0(Fb.ckle)) is quite high as the two verbs have the same syntactic realization of their arguments . \n\t', '\n\t\t In general , flat features do not possess this conservative property . \n\t', '\n\t\t For example , the Parse Tree Path is very sensible to small changes of parse-trees , e.g. two predicates , expressed in different tenses , generate two different Path features . \n\t', '\n\t\t Second , some information contained in the standard features is embedded in PAF : Phrase Type , Predicate Word and Head Word explicitly appear as structure fragments . \n\t', '\n\t\t For example , in Figure 4 are shown fragments like [ NP [ DT ] [ N ] ] or [ NP [ DT a ] [ N talk ] ] which explicitly encode the Phrase Type feature NP for the Arg 1 in Figure 2.b. . \n\t', '\n\t\t The Predicate Word is represented by the fragment [ V delivers ] and the Head Word is encoded in [ N talk ] . \n\t', '\n\t\t The same is not true for SCF since it does not contain information about a specific argument . \n\t', '\n\t\t SCF , in fact , aims to characterize the predicate with respect to the overall argument structures rather than a specific pair <p , a> . \n\t', '\n\t\t Third , Governing Category , Position and Voice features are not explicitly contained in both PAF and SCF . \n\t', '\n\t\t Nevertheless , SCF may allow the learning algorithm to detect the active/passive form of verbs . \n\t', '\n\t\t Finally , from the above observations follows that the PAF representation may be used with PAK to classify arguments . \n\t', '\n\t\t On the contrary , SCF lacks important information , thus , alone it may be used only to classify verbs in syntactic categories . \n\t', '\n\t\t This suggests that SCF should be used in conjunction with standard features to boost their classification performance . \n\t', '\n\t\t 4 The Experiments The aim of our experiments are twofold : On the one hand , we study if the PAF representation produces an accuracy higher than standard features . \n\t', '\n\t\t On the other hand , we study if SCF can be used to classify verbs according to their syntactic realization . \n\t', '\n\t\t Both the above aims can be carried out by combining PAF and SCF with the standard features . \n\t', '\n\t\t For this purpose we adopted two ways to combine kernels3 : ( 1 ) K = K1 · K2 and ( 2 ) K = -yK1 + K2 . \n\t', '\n\t\t The resulting set of kernels used in the experiments is the following : \x95 Kpd is the polynomial kernel with degree d over the standard features . \n\t', '\n\t\t \x95 KPAF is obtained by using PAK function over the PAF structures . \n\t', '\n\t\t \x95~v K KPAF+P = lIKPAFI+IKp d dI,i.e. the sum be- tween the normalized4 PAF-based kernel and the normalized polynomial kernel . \n\t', '\n\t\t KPAF\x95Kpd \x95KPAF.P =IKPAF I \x95 I Kpd I , i.e. the normalized product between the PAF-based kernel and the polynomial kernel . \n\t', '\n\t\t \x95K KSCF KSCF+P=-yIKSCFI + IK dI,i.e. the summa- tion between the normalized SCF-based kernel and the normalized polynomial kernel . \n\t', '\n\t\t KSCF\x95Kpd KSCFI\x95IKpdI , i.e. the normal- I ized product between SCF-based kernel and the polynomial kernel . \n\t', '\n\t\t 4.1 Corpora set-up The above kernels were experimented over two corpora : PropBank ( www.cis.upenn.edu/\x97ace ) along with Penn TreeBank5 2 \n\t\t']",Positive
"['\n\t\t PropBank contains about 53,700 sentences and a fixed split between training and testing which has been used in other researches e.g. , \n\t\t']",Positive
"['\n\t\t In this split , Sections from 02 to 21 are used for training , section 23 for testing and sections 1 and 22 as developing set . \n\t', '\n\t\t We considered all PropBank arguments6 from Arg0 to Arg9 , ArgA and ArgM for a total of 122,774 and 7,359 arguments in training and testing respectively . \n\t', '\n\t\t It is worth noting that in the experiments we used the gold standard parsing from Penn TreeBank , thus our kernel structures are derived with high precision . \n\t', '\n\t\t For the FrameNet corpus ( www. icsi . \n\t', '\n\t\t berkeley 3It can be proven that the resulting kernels still satisfy Mercer\x92s conditions \n\t\t']",Positive
"['\n\t\t 4To normalize a kernel K(-x , z- ) we can divide it by IK(-x , -x ) - K(-z , z- ) . \n\t', '\n\t\t 5We point out that we removed from Penn TreeBank the function tags like SBJ and TMP as parsers usually are not able to provide this information . \n\t', '\n\t\t 6We noted that only Arg0 to Arg4 and ArgM contain enough training/testing data to affect the overall performance . \n\t', '\n\t\t .edu/\x97framenet ) we extracted all 24,558 sen tences from the 40 frames of Senseval 3 task ( www. senseval . \n\t', '\n\t\t org ) for the Automatic Labeling of Semantic Roles . \n\t', '\n\t\t We considered 18 of the most frequent roles and we mapped together those having the same name . \n\t', '\n\t\t Only verbs are se- lected to be predicates in our evaluations . \n\t', '\n\t\t More- over , as it does not exist a fixed split between training and testing , we selected randomly 30 % of sentences for testing and 70 % for training . \n\t', '\n\t\t Additionally , 30 % of training was used as a validation-set . \n\t', '\n\t\t The sentences were processed us- ing Collins\x92 parser \n\t\t']",Positive
['\n\t\t 4.2 Classification set-up The classifier evaluations were carried out using the SVM-light software \n\t\t'],Positive
"['\n\t\t To process PAF and SCF , we implemented our own kernels and we used them inside SVM-light . \n\t', '\n\t\t The classification performances were evaluated using the f1 measure7 for single arguments and the accuracy for the final multi-class classifier . \n\t', '\n\t\t This latter choice allows us to compare the results with previous literature works , e.g. \n\t\t']",Positive
"['\n\t\t For the evaluation of SVMs , we used the default regularization parameter ( e.g. , C = 1 for normalized kernels ) and we tried a few cost- factor values ( i.e. , j E { 0.1,1 , 2 , 3 , 4 , 5 } ) to adjust the rate between Precision and Recall . \n\t', '\n\t\t We chose parameters by evaluating SVM using Kp3 kernel over the validation-set . \n\t', '\n\t\t Both A ( see Section 3.3 ) and -y parameters were evaluated in a similar way by maximizing the performance of SVM using KPAF and -y KSCF IKSCFI +IKPdIrespec- tively . \n\t', '\n\t\t These parameters were adopted also for all the other kernels . \n\t', '\n\t\t 4.3 Kernel evaluations To study the impact of our structural kernels we firstly derived the maximal accuracy reachable with standard features along with polynomial kernels . \n\t', '\n\t\t The multi-class accuracies , for Prop- Bank and FrameNet using Kpd with d = 1 , .. , 5 , are shown in Figure 5 . \n\t', '\n\t\t We note that ( a ) the highest performance is reached for d = 3 , ( b ) for PropBank our maximal accuracy ( 90.5 % ) 7 f 1 assigns equal importance to Precision P and Recall R i.e. f1 2P\x95R = P+R . \n\t', '\n\t\t \x95KSCF\x95P = is substantially equal to the SVM performance ( 88 % ) obtained in \n\t\t']",Positive
"['\n\t\t This different outcome is due to a different task ( we classify different roles ) and a different classification algorithm . \n\t', '\n\t\t Moreover , we did not use the Frame information which is very important $. 1 2 d 3 4 5 Figure 5 : Multi-classifier accuracy according to different degrees of the polynomial kernel . \n\t', '\n\t\t It is worth noting that the difference between linear and polynomial kernel is about 3-4 percent points for both PropBank and FrameNet . \n\t', '\n\t\t This remarkable difference can be easily explained by considering the meaning of standard features . \n\t', '\n\t\t For example , let us restrict the classification function CArg0 to the two features Voice and Position . \n\t', '\n\t\t Without loss of generality we can assume : ( a ) Voice=1 if active and 0 if passive , and ( b ) Position=1 when the argument is after the predicate and 0 otherwise . \n\t', '\n\t\t To simplify the example , we also assume that if an argument precedes the target predicate it is a subject , otherwise it is an objects . \n\t', '\n\t\t It follows that a constituent is Arg0 , i.e. CArg0 = 1 , if only one feature at a time is 1 , otherwise it is not an Arg0 , i.e. CArg0 = 0 . \n\t', '\n\t\t In other words , CArg0 = Position XOR Voice , which is the classical example of a non-linear separable function that becomes separable in a superlinear space \n\t\t']",Positive
"['\n\t\t After it was established that the best kernel for standard features is Kp3 , we carried out all the other experiments using it in the kernel combinations . \n\t', '\n\t\t Table 2 and 3 show the single class ( f1 measure ) as well as multi-class classifier ( accuracy ) performance for PropBank and FrameNet respectively . \n\t', '\n\t\t Each column of the two tables refers to a different kernel defined in the 8Preliminary experiments indicate that SVMs can reach 90 % by using the frame feature . \n\t', '\n\t\t 9Indeed , this is true in most part of the cases . \n\t', '\n\t\t previous section . \n\t', '\n\t\t The overall meaning is discussed in the following points : First , PAF alone has good performance , since in PropBank evaluation it outperforms the linear kernel ( Kp1 ) , 88.7 % vs. 86.7 % whereas in FrameNet , it shows a similar performance 79.5 % vs. 82.1 % ( compare tables with Figure 5 ) . \n\t', '\n\t\t This suggests that PAF generates the same information as the standard features in a linear space . \n\t', '\n\t\t However , when a degree greater than 1 is used for standard features , PAF is outperformed10 . \n\t', '\n\t\t Args P3 PAF PAF+P PAF\x95P SCF+P SCF\x95P Arg0 90.8 88.3 90.6 90.5 94.6 94.7 Arg1 91.1 87.4 89.9 91.2 92.9 94.1 Arg2 80.0 68.5 77.5 74.7 77.4 82.0 Arg3 57.9 56.5 55.6 49.7 56.2 56.4 Arg4 70.5 68.7 71.2 62.7 69.6 71.1 ArgM 95.4 94.1 96.2 96.2 96.1 96.3 Acc . \n\t', '\n\t\t 90.5 88.7 90.2 90.4 92.4 93.2 Table 2 : Evaluation of Kernels on PropBank . \n\t', '\n\t\t Roles P3 PAF PAF+P PAF\x95P SCF+P SCF\x95P agent 92.0 88.5 91.7 91.3 93.1 93.9 cause 59.7 16.1 41.6 27.7 42.6 57.3 degree 74.9 68.6 71.4 57.8 68.5 60.9 depict . \n\t', '\n\t\t 52.6 29.7 51.0 28.6 46.8 37.6 durat . \n\t', '\n\t\t 45.8 52.1 40.9 29.0 31.8 41.8 goal 85.9 78.6 85.3 82.8 84.0 85.3 instr . \n\t', '\n\t\t 67.9 46.8 62.8 55.8 59.6 64.1 mann. 81.0 81.9 81.2 78.6 77.8 77.8 Acc . \n\t', '\n\t\t 85.2 79.5 84.6 81.6 83.8 84.2 18 roles Table 3 : Evaluation of Kernels on FrameNet semantic roles . \n\t', '\n\t\t Second , SCF improves the polynomial kernel ( d = 3 ) , i.e. the current state-of-the-art , of about 3 percent points on PropBank ( column SCF-P ) . \n\t', '\n\t\t This suggests that ( a ) PAK can mea- sure the similarity between two SCF structures and ( b ) the sub-categorization information provides effective clues about the expected argument type . \n\t', '\n\t\t The interesting consequence is that SCF together with PAK seems suitable to automatically cluster different verbs that have the same syntactic realization . \n\t', '\n\t\t We note also that to fully exploit the SCF information it is necessary to use a kernel product ( K1 \x95 K2 ) combination rather than the sum ( K1 + K2 ) , e.g. column SCF+P . \n\t', '\n\t\t Finally , the FrameNet results are completely different . \n\t', '\n\t\t No kernel combinations with both PAF and SCF produce an improvement . \n\t', '\n\t\t On 10Unfortunately the use of a polynomial kernel on top the tree fragments to generate the XOR functions seems not successful . \n\t', '\n\t\t 0.91 0.9 0.89 0.88 0.87 0.86 0.85 0.84 0.83 0.82 FrameNet PropBank the contrary , the performance decreases , suggesting that the classifier is confused by this syntactic information . \n\t', '\n\t\t The main reason for the different outcomes is that PropBank arguments are different from semantic roles as they are an intermediate level between syntax and semantic , i.e. they are nearer to grammatical functions . \n\t', '\n\t\t In fact , in PropBank arguments are annotated consistently with syntactic alternations ( see the Annotation guidelines for Prop- Bank at www.cis.upenn.edu/\x97ace ) . \n\t', '\n\t\t On the contrary FrameNet roles represent the final semantic product and they are assigned according to semantic considerations rather than syntactic aspects . \n\t', '\n\t\t For example , Cause and Agent semantic roles have identical syntactic realizations . \n\t', '\n\t\t This prevents SCF to distinguish between them . \n\t', '\n\t\t Another minor reason may be the use of automatic parse-trees to extract PAF and SCF , even if preliminary experiments on automatic semantic shallow parsing of PropBank have shown no important differences versus semantic parsing which adopts Gold Standard parse-trees . \n\t', '\n\t\t 5 Conclusions In this paper , we have experimented with SVMs using the two novel convolution kernels PAF and SCF which are designed for the semantic structures derived from PropBank and FrameNet corpora . \n\t', '\n\t\t Moreover , we have combined them with the polynomial kernel of standard features . \n\t', '\n\t\t The results have shown that : First , SVMs using the above kernels are appealing for semantically parsing both corpora . \n\t', '\n\t\t Second , PAF and SCF can be used to improve automatic classification of PropBank arguments as they provide clues about the predicate argument structure of the target verb . \n\t', '\n\t\t For example , SCF improves ( a ) the classification state-of-the- art ( i.e. the polynomial kernel ) of about 3 percent points and ( b ) the best literature result of about 5 percent points . \n\t', '\n\t\t Third , additional work is needed to design kernels suitable to learn the deep semantic contained in FrameNet as it seems not sensible to both PAF and SCF information . \n\t', '\n\t\t Finally , an analysis of SVMs using polynomial kernels over standard features has explained why they largely outperform linear classifiers based-on standard features . \n\t', '\n\t\t In the future we plan to design other structures and combine them with SCF , PAF and standard features . \n\t', '\n\t\t In this vision the learning will be carried out on a set of structural features instead of a set of flat features . \n\t', '\n\t\t Other studies may relate to the use of SCF to generate verb clusters . \n\t', '\n\t\t Acknowledgments This research has been sponsored by the ARDA AQUAINT program . \n\t', '\n\t\t In addition , I would like to thank Professor Sanda Harabagiu for her advice , Adrian Cosmin Bejan for implementing the feature extractor and Paul Mor^arescu for processing the FrameNet data . \n\t', '\n\t\t Many thanks to the anonymous reviewers for their invaluable suggestions . \n\t', '\n\t\t References Michael Collins and Nigel Duffy . \n\t', '\n\t\t 2002. New ranking algorithms for parsing and tagging : Kernels over discrete structures , and the voted perceptron . \n\t', '\n\t\t In proceeding of ACL-02 . \n\t', '\n\t\t Michael Collins . \n\t', '\n\t\t 1997. Three generative , lexicalized models for statistical parsing . \n\t', '\n\t\t In proceedings of the ACL-97 , pages 16\x9623 , Somerset , New Jersey . \n\t', '\n\t\t Nello Cristianini and John Shawe-Taylor . \n\t', '\n\t\t 2000. An introduction to Support Vector Machines . \n\t', '\n\t\t Cam- bridge University Press . \n\t', '\n\t\t Charles J. Fillmore . \n\t', '\n\t\t 1982. Frame semantics . \n\t', '\n\t\t In Lin- guistics in the Morning Calm , pages 111\x96137 . \n\t', '\n\t\t Daniel Gildea and Daniel Jurasfky . \n\t', '\n\t\t 2002. Auto- matic labeling of semantic roles . \n\t', '\n\t\t Computational Linguistic . \n\t', '\n\t\t Daniel Gildea and Martha Palmer . \n\t', '\n\t\t 2002. The necessity of parsing for predicate argument recognition . \n\t', '\n\t\t In proceedings of ACL-02 , Philadelphia , PA . \n\t', '\n\t\t R. Jackendoff . \n\t', '\n\t\t 1990. Semantic Structures , Current Studies in Linguistics series . \n\t', '\n\t\t Cambridge , Massachusetts : The MIT Press . \n\t', '\n\t\t T. Joachims . \n\t', '\n\t\t 1999. Making large-scale SVM learning practical . \n\t', '\n\t\t In Advances in Kernel Methods - Support Vector Learning . \n\t', '\n\t\t Paul Kingsbury and Martha Palmer . \n\t', '\n\t\t 2002. From treebank to propbank . \n\t', '\n\t\t In proceedings of LREC02 , Las Palmas , Spain . \n\t', '\n\t\t M. P. Marcus , B. Santorini , and M. A. Marcinkiewicz . \n\t', '\n\t\t 1993. Building a large annotated corpus of english : The penn treebank . \n\t', '\n\t\t Computational Linguistics . \n\t', '\n\t\t Alessandro Moschitti and Cosmin Adrian Bejan . \n\t', '\n\t\t 2004. A semantic kernel for predicate argument classification . \n\t', '\n\t\t In proceedings of CoNLL-04 , Boston , USA . \n\t', '\n\t\t Kadri Hacioglu , Sameer Pradhan , Wayne Ward , James H. Martin , and Daniel Jurafsky . \n\t', '\n\t\t 2003. Shallow Semantic Parsing Using Support Vector Machines . \n\t', '\n\t\t TR-CSLR-2003-03 , University of Colorado . \n\t', '\n\t\t Mihai Surdeanu , Sanda M. Harabagiu , John Williams , and John Aarseth . \n\t', '\n\t\t 2003. Using predicate-argument structures for information extraction . \n\t', '\n\t\t In proceedings of ACL-03 , Sapporo , Japan . \n\t', '\n\t\t V. Vapnik . \n\t', '\n\t\t 1995. The Nature of Statistical Learning Theory . \n\t', '\n\t\t Springer-Verlag New York , Inc. \n\t', '\n\t\t Combining Acoustic and Pragmatic Features to Predict Recognition Performance in Spoken Dialogue Systems Malte Gabsdil Department of Computational Linguistics Saarland University Germany gabsdil@coli.uni-sb.de Oliver Lemon School of Informatics Edinburgh University Scotland olemon@inf.ed.ac.uk Abstract We use machine learners trained on a combination of acoustic confidence and pragmatic plausibility features computed from dialogue context to predict the accuracy of incoming n-best recognition hypotheses to a spoken dialogue system . \n\t', '\n\t\t Our best results show a 25 % weighted f-score improvement over a baseline system that implements a \x93grammar-switching\x94 approach to context-sensitive speech recognition . \n\t', '\n\t\t 1 Introduction A crucial problem in the design of spoken dialogue systems is to decide for incoming recognition hypotheses whether a system should accept ( consider correctly recognized ) , reject ( assume misrecognition ) , or ignore ( classify as noise or speech not directed to the system ) them . \n\t', '\n\t\t In addition , a more sophisticated dialogue system might decide whether to clarify or confirm certain hypotheses . \n\t', '\n\t\t Obviously , incorrect decisions at this point can have serious negative effects on system usability and user satisfaction . \n\t', '\n\t\t On the one hand , accepting misrecognized hypotheses leads to misunderstandings and unintended system behaviors which are usually difficult to recover from . \n\t', '\n\t\t On the other hand , users might get frustrated with a system that behaves too cautiously and rejects or ignores too many utterances . \n\t', '\n\t\t Thus an important feature in dialogue system engineering is the tradeoff between avoiding task failure ( due to misrecognitions ) and promoting overall dialogue efficiency , flow , and naturalness . \n\t', '\n\t\t In this paper , we investigate the use of machine learners trained on a combination of acoustic confidence and pragmatic plausibility features ( i.e. computed from dialogue context ) to predict the quality of incoming n-best recognition hypotheses to a spoken dialogue system . \n\t', '\n\t\t These predictions are then used to select a \x93best\x94 hypothesis and to decide on appropriate system reactions . \n\t', '\n\t\t We evaluate this approach in comparison with a baseline system that combines fixed recognition confidence rejection thresholds with dialogue-state dependent recognition grammars \n\t\t']",Positive
"['\n\t\t The paper is organized as follows . \n\t', '\n\t\t After a short relation to previous work , Section 3 introduces the WITAS multimodal dialogue system , which we use to collect data ( Section 4 ) and to derive baseline results ( Section 5 ) . \n\t', '\n\t\t Section 6 describes our learning experiments for classifying and selecting from n- best recognition hypotheses and Section 7 reports our results . \n\t', '\n\t\t 2 Relation to Previous Work \n\t\t']",Positive
"['\n\t\t In our experiments , we also use recognizer confidence scores and a limited number of acoustic- prosodic features ( e.g. amplitude in the speech signal ) for hypothesis classification . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t Our work is related to these experiments in that we also combine confidence scores and higher- level features for classification . \n\t', '\n\t\t However , both \n\t\t']",Positive
"['\n\t\t We go a step further in that we classify n-best hypotheses and then select among the alternatives . \n\t', '\n\t\t We also explore the use of more dialogue and task-oriented features ( e.g. the dialogue move type of a recognition hypothesis ) for classification . \n\t', '\n\t\t The main difference between our approach and work on hypothesis reordering ( e.g. \n\t\t']",Negative
"['\n\t\t Furthermore , our approach is more generally applica- ble than preceding research , since we frame our methodology in the Information State Update ( ISU ) approach to dialogue management \n\t\t']",Positive
['\n\t\t 3 The WITAS Dialogue System The WITAS dialogue system \n\t\t'],Positive
"['\n\t\t The human operator is provided with a GUI \x96 an interactive ( i.e. mouse click- able ) map \x96 and specifies mission goals using natural language commands spoken into a headset , or by using combinations of GUI actions and spoken commands . \n\t', '\n\t\t The simulated UAV can carry out different activities such as flying to locations , following vehicles , and delivering objects . \n\t', '\n\t\t The dialogue system uses the Nuance 8.0 speech recognizer with language models compiled from a grammar ( written using the Gemini system \n\t\t']",Positive
['\n\t\t 3.1 WITAS Information States The WITAS dialogue system is part of a larger family of systems that implement the Information State Update ( ISU ) approach to dialogue management \n\t\t'],Positive
"['\n\t\t The ISU approach has been used to formalize different theories of dialogue and forms the basis of several dialogue system implementations in domains such as route planning , home automation , and tutorial dialogue . \n\t', '\n\t\t The ISU approach is a particularly useful testbed for our technique because it collects information relevant to dialogue context in a central data structure from which it can be easily extracted . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t Here , we briefly introduce parts of the IS which are needed to understand the system\x92s basic workings , and from which we will extract dialogue-level and task-level information for our learning experiments : \x95 Dialogue Move Tree ( DMT ) : a tree-structure , in which each subtree of the root node represents a \x93thread\x94 in the conversation , and where each node in a subtree represents an utterance made either by the system or the user . \n\t', '\n\t\t 1 \x95 Active Node List ( ANL ) : a list that records all \x93active\x94 nodes in the DMT ; active nodes indi- 1A tree is used in order to overcome the limitations of stack- based processing , see \n\t\t']",Positive
"['\n\t\t cate conversational contributions that are still in some sense open , and to which new utterances can attach . \n\t', '\n\t\t \x95 Activity Tree ( AT ) : a tree-structure representing the current , past , and planned activities that the back-end system ( in this case a UAV ) performs . \n\t', '\n\t\t \x95 Salience List ( SL ) : a list of NPs introduced in the current dialogue ordered by recency . \n\t', '\n\t\t \x95 Modality Buffer ( MB ) : a temporary store that registers click events on the GUI . \n\t', '\n\t\t The DMT and AT are the core components of Information States . \n\t', '\n\t\t The SL and MB are subsidiary data-structures needed for interpreting and generating anaphoric expressions and definite NPs . \n\t', '\n\t\t Finally , the ANL plays a crucial role in integrating new user utterances into the DMT . \n\t', '\n\t\t 4 Data Collection For our experiments , we use data collected in a small user study with the grammar-switching version of the WITAS dialogue system \n\t\t']",Positive
"['\n\t\t In this study , six subjects from Edinburgh University ( 4 male , 2 female ) had to solve five simple tasks with the system , resulting in 30 complete dialogues . \n\t', '\n\t\t The subjects\x92 utterances were recorded as 8kHz 16bit waveform files and all aspects of the Information State transitions during the interactions were logged as html files . \n\t', '\n\t\t Altogether , 303 utterances were recorded in the user study ( ^ 10 user utterances/dialogue ) . \n\t', '\n\t\t 4.1 Labeling We transcribed all user utterances and parsed the transcriptions offline using WITAS\x92 natural language understanding component in order to get a gold-standard labeling of the data . \n\t', '\n\t\t Each utterance was labeled as either in-grammar or out-ofgrammar ( oog ) , depending on whether its transcription could be parsed or not , or as crosstalk : a special marker that indicated that the input was not directed to the system ( e.g. noise , laughter , self-talk , the system accidentally recording itself ) . \n\t', '\n\t\t For all in-grammar utterances we stored their interpretations ( quasi-logical forms ) as computed by WITAS\x92 parser . \n\t', '\n\t\t Since the parser uses a domain-specific semantic grammar designed for this particular application , each in-grammar utterance had an interpretation that is \x93correct\x94 with respect to the WITAS application . \n\t', '\n\t\t 4.2 Simplifying Assumptions The evaluations in the following sections make two simplifying assumptions . \n\t', '\n\t\t First , we consider a user utterance correctly recognized only if the logical form of the transcription is the same as the logical form of the recognition hypothesis . \n\t', '\n\t\t This assumption can be too strong because the system might react appropriately even if the logical forms are not literally the same . \n\t', '\n\t\t Second , if a transcribed utterance is out-of-grammar , we assume that the system cannot react appropriately . \n\t', '\n\t\t Again , this assumption might be too strong because the recognizer can accidentally map an utterance to a logical form that is equivalent to the one intended by the user . \n\t', '\n\t\t 5 The Baseline System The baseline for our experiments is the behavior of the WITAS dialogue system that was used to collect the experimental data ( using dialogue context as a predictor of language models for speech recognition , see below ) . \n\t', '\n\t\t We chose this baseline because it has been shown to perform significantly better than an earlier version of the system that always used the same ( i.e. full ) grammar for recognition \n\t\t']",Negative
"['\n\t\t We evaluate the performance of the baseline by analyzing the dialogue logs from the user study . \n\t', '\n\t\t With this information , it is possible to decide how the system reacted to each user utterance . \n\t', '\n\t\t We distinguish between the following three cases : 1. accept : the system accepted the recognition hypothesis of a user utterance as correct . \n\t', '\n\t\t 2. reject : the system rejected the recognition hypothesis of a user utterance given a fixed confidence rejection threshold . \n\t', '\n\t\t 3. ignore : the system did not react to a user utterance at all . \n\t', '\n\t\t These three classes map naturally to the gold- standard labels of the transcribed user utterances : the system should accept in-grammar utterances , reject out-of-grammar input , and ignore crosstalk . \n\t', '\n\t\t 5.1 Context-sensitive Speech Recognition In the the WITAS dialogue system , the \x93grammar- switching\x94 approach to context-sensitive speech recognition \n\t\t']",Positive
"['\n\t\t At any point in the dialogue , there is a \x93most active node\x94 at the top of the ANL . \n\t', '\n\t\t The dialogue move type of this node defines the name of a language model that is used for recognizing the next user utterance . \n\t', '\n\t\t For instance , if the most active node is a system yes-no-question then the appropriate language model is defined by a small context-free grammar covering phrases such as \x93yes\x94 , \x93that\x92s right\x94 , \x93okay\x94 , \x93negative\x94 , \x93maybe\x94 , and so on . \n\t', '\n\t\t The WITAS dialogue system with context- sensitive speech recognition showed significantly better recognition rates than a previous version of the system that used the full grammar for recognition at all times ( \n\t\t']",Negative
"['\n\t\t Note however that an inherent danger with grammar-switching is that the system may have wrong expectations and thus might activate a language model which is not appropriate for the user\x92s next utterance , leading to misrecognitions or incorrect rejections . \n\t', '\n\t\t 5.2 Results Table 1 summarizes the evaluation of the baseline system . \n\t', '\n\t\t System behavior accept reject ignore 154/22 8 4 45 43 4 12 9 2 Accuracy : 65.68 % Weighted f-score : 61.81 % Table 1 : WITAS dialogue system baseline results Table 1 should be read as follows : looking at the first row , in 154 cases the system understood and accepted the correct logical form of an in-grammar utterance by the user . \n\t', '\n\t\t In 22 cases , the system accepted a logical form that differed from the one for the transcribed utterance.2 In 8 cases , the system rejected an in-grammar utterance and in 4 cases it did not react to an in-grammar utterance at all . \n\t', '\n\t\t The second row of Table 1 shows that the system accepted 45 , rejected 43 , and ignored 4 user utterances whose transcriptions were out-of-grammar and could not be parsed . \n\t', '\n\t\t Finally , the third row of the table shows that the baseline system accepted 12 utterances that were not addressed to it , rejected 9 , and ignored 2 . \n\t', '\n\t\t Table 1 shows that a major problem with the baseline system is that it accepts too many user utterances . \n\t', '\n\t\t In particular , the baseline system accepts the wrong interpretation for 22 in-grammar utterances , 45 utterances which it should have rejected as outof-grammar , and 12 utterances which it should have 2For the computation of accuracy and weighted f-scores , these were counted as wrongly accepted out-of-grammar utterances . \n\t', '\n\t\t User utterance in-grammar out-of-grammar crosstalk ignored . \n\t', '\n\t\t All of these cases will generally lead to unintended actions by the system . \n\t', '\n\t\t 6 Classifying and Selecting N-best Recognition Hypotheses We aim at improving over the baseline results by considering the n-best recognition hypotheses for each user utterance . \n\t', '\n\t\t Our methodology consists of two steps : i ) we automatically classify the n-best recognition hypotheses for an utterance as either correctly or incorrectly recognized and ii ) we use a simple selection procedure to choose the \x93best\x94 hypothesis based on this classification . \n\t', '\n\t\t In order to get multiple recognition hypotheses for all utterances in the experimental data , we re-ran the speech recognizer with the full recognition grammar and 10- best output and processed the results offline with WITAS\x92 parser , obtaining a logical form for each recognition hypothesis ( every hypothesis has a logical form since language models are compiled from the parsing grammar ) . \n\t', '\n\t\t 6.1 Hypothesis Labeling We labeled all hypotheses with one of the following four classes , based on the manual transcriptions of the experimental data : in-grammar , oog ( WER < 50 ) , oog ( WER > 50 ) , or crosstalk . \n\t', '\n\t\t The in-grammar and crosstalk classes correspond to those described for the baseline . \n\t', '\n\t\t However , we decided to divide up the out-of-grammar class into the two classes oog ( WER < 50 ) and oog ( WER > 50 ) to get a more fine- grained classification . \n\t', '\n\t\t In order to assign hypotheses to the two oog classes , we compute the word error rate ( WER ) between recognition hypotheses and the transcription of corresponding user utterances . \n\t', '\n\t\t If the WER is < 50 % , we label the hypothesis as oog ( WER < 50 ) , otherwise as oog ( WER > 50 ) . \n\t', '\n\t\t We also annotate all misrecognized hypotheses of in-grammar utterances with their respective WER scores . \n\t', '\n\t\t The motivation behind splitting the out-ofgrammar class into two subclasses and for annotating misrecognized in-grammar hypotheses with their WER scores is that we want to distinguish between different \x93degrees\x94 of misrecognition that can be used by the dialogue system to decide whether it should initiate clarification instead of rejection.3 We use a threshold ( 50 % ) on a hypothesis\x92 WER as an indicator for whether hypotheses should be 3The WITAS dialogue system currently does not support this type of clarification dialogue ; the WER annotations are therefore only of theoretical interest . \n\t', '\n\t\t However , an extended system could easily use this information to decide when clarification should be initiated . \n\t', '\n\t\t clarified or rejected . \n\t', '\n\t\t This is adopted from \n\t\t']",Positive
"['\n\t\t The WER threshold can be set differently according to the needs of an application . \n\t', '\n\t\t However , one would ideally set a threshold directly on CA scores for this labeling , but these are currently not available for our data . \n\t', '\n\t\t We also introduce the distinction between out-ofgrammar ( WER < 50 ) and out-of-grammar ( WER > 50 ) in the gold standard for the classification of ( whole ) user utterances . \n\t', '\n\t\t We split the out-ofgrammar class into two sub-classes depending on whether the 10-best recognition results include at least one hypothesis with a WER < 50 compared to the corresponding transcription . \n\t', '\n\t\t Thus , if there is a recognition hypothesis which is close to the transcription , an utterance is labeled as oog ( WER < 50 ) . \n\t', '\n\t\t In order to relate these classes to different system behaviors , we define that utterances labeled as oog ( WER < 50 ) should be clarified and utterances labeled as oog ( WER > 50 ) should be rejected by the system . \n\t', '\n\t\t The same is done for all in-grammar utterances for which only misrecognized hypotheses are available . \n\t', '\n\t\t 6.2 Classification : Feature Groups We represent recognition hypotheses as 20- dimensional feature vectors for automatic classification . \n\t', '\n\t\t The feature vectors combine recognizer confidence scores , low-level acoustic information , information from WITAS system Information States , and domain knowledge about the different tasks in the scenario . \n\t', '\n\t\t The following list gives an overview of all features ( described in more detail below ) . \n\t', '\n\t\t 1. Recognition ( 6 ) : nbestRank , hypothe- sisLength , confidence , confidenceZScore , confidence-StandardDeviation , minWordConfidence 2 . \n\t', '\n\t\t Utterance ( 3 ) : minAmp , meanAmp , RMS-amp 3 . \n\t', '\n\t\t Dialogue ( 9 ) : currentDM , currentCommand , mostActiveNode , DMBigramFrequency , qaMatch , aqMatch , #unresolvedNPs , #unresolvedPronouns , #uniqueIndefinites 4 . \n\t', '\n\t\t Task ( 2 ) : taskConflict , #taskConstraintConflict All features are extracted automatically from the output of the speech recognizer , utterance waveforms , IS logs , and a small library of plan operators describing the actions the UAV can perform . \n\t', '\n\t\t The recognition ( REC ) feature group includes the position of a hypothesis in the n-best list ( nbestRank ) , its length in words ( hypothesisLength ) , and five features representing the recognizer\x92s confidence assessment . \n\t', '\n\t\t Similar features have been used in the literature ( e.g. \n\t\t']",Positive
"['\n\t\t The min Word- Confidence and standard deviation/zScore features are computed from individual word confidences in the recognition output . \n\t', '\n\t\t We expect them to help the machine learners decide between the different WER classes ( e.g. a high overall confidence score can sometimes be misleading ) . \n\t', '\n\t\t The utterance ( UTT ) feature group reflects information about the amplitude in the speech signal ( all features are extracted with the UNIX sox utility ) . \n\t', '\n\t\t The motivation for including the amplitude features is that they might be useful for detecting crosstalk utterances which are not directly spoken into the headset microphone ( e.g. the system accidentally recognizing itself ) . \n\t', '\n\t\t The dialogue features ( DIAL ) represent information derived from Information States and can be coarsely divided into two sub-groups . \n\t', '\n\t\t The first group includes features representing general coherence constraints on the dialogue : the dialogue move types of the current utterance ( currentDM ) and of the most active node in the ANL ( mostActiveNode ) , the command type of the current utterance ( currentCommand , if it is a command , null otherwise ) , statistics on which move types typically follow each other ( DMBigramFrequency ) , and two features ( qaMatch and aqMatch ) that explicitly encode whether the current and the previous utterance form a valid question answer pair ( e.g. yn-question followed by yn-answer ) . \n\t', '\n\t\t The second group includes features that indicate how many definite NPs and pronouns cannot be resolved in the current Information State ( #unresolvedNP , #unresolvedPronouns , e.g. \x93the car\x94 if no car was mentioned before ) and a feature indicating the number of indefinite NPs that can be uniquely resolved in the Information State ( #uniqueIndefinites , e.g. \x93a tower\x94 where there is only one tower in the domain ) . \n\t', '\n\t\t We include these features because ( short ) determiners are often confused by speech recognizers . \n\t', '\n\t\t In the WITAS scenario , a misrecognized determiner/demonstrative pronoun can lead to confusing system behavior ( e.g. a wrongly recognized \x93there\x94 will cause the system to ask \x93Where is that?\x94 ) . \n\t', '\n\t\t Finally , the task features ( TASK ) reflect conflicting instructions in the domain . \n\t', '\n\t\t The feature taskConflict indicates a conflict if the current dialogue move type is a command and that command already appears as an active task in the AT . \n\t', '\n\t\t #taskConstraintConflict counts the number of conflicts that arise between the currently active tasks in the AT and the hypothesis . \n\t', '\n\t\t For example , if the UAV is already fly ing somewhere the preconditions of the action operator for take off ( altitude = 0 ) conflict with those for fly ( altitude =~ 0 ) , so that \x93take off\x94 would be an unlikely command in this context . \n\t', '\n\t\t 6.3 Learners and Selection Procedure We use the memory based learner TiMBL \n\t\t']",Positive
"['\n\t\t We chose these two learners because they implement different learning strategies , are well established , fast , freely available , and easy to use . \n\t', '\n\t\t In a second step , we decide which ( if any ) of the classified hypotheses we actually want to pick as the best result and how the user utterance should be classified as a whole . \n\t', '\n\t\t This task is decided by the following selection procedure ( see Figure 1 ) which implements a preference ordering accept > clarify > reject > ignore.4 1 . \n\t', '\n\t\t Scan the list of classified n-best recognition hypotheses top-down . \n\t', '\n\t\t Return the first result that is classified as accept and classify the utterance as accept . \n\t', '\n\t\t 2. If 1. fails , scan the list of classified n-best recognition hypotheses top-down . \n\t', '\n\t\t Return the first result that is classified as clarify and classify the utterance as clarify . \n\t', '\n\t\t 3. If 2. fails , count the number of rejects and ignores in the classified recognition hypotheses . \n\t', '\n\t\t If the number of rejects is larger or equal than the number of ignores classify the utterance as reject . \n\t', '\n\t\t 4. Else classify the utterance as ignore . \n\t', '\n\t\t Figure 1 : Selection procedure This procedure is applied to choose from the classified n-best hypotheses for an utterance , independent of the particular machine learner , in all of the following experiments . \n\t', '\n\t\t Since we have a limited amount experimental data in this study ( 10 hypotheses for each of the 303 user utterances ) , we use a \x93leave-one-out\x94 crossvalidation setup for classification . \n\t', '\n\t\t This means that we classify the 10-best hypotheses for a particular utterance based on the 10-best hypotheses of all 302 other utterances and repeat this 303 times . \n\t', '\n\t\t 4Note that in a dialogue application one would not always need to classify all n-best hypotheses in order to select a result but could stop as soon as a hypothesis is classified as correct , which can save processing time . \n\t', '\n\t\t 7 Results and Evaluation The middle part of Table 2 shows the classification results for TiMBL and RIPPER when run with default parameter settings ( the other results are included for comparison ) . \n\t', '\n\t\t The individual rows show the performance when different combinations of feature groups are used for training . \n\t', '\n\t\t The results for the three-way classification are included for comparison with the baseline system and are obtained by combining the two classes clarify and reject . \n\t', '\n\t\t Note that we do not evaluate the performance of the learners for classifying the individual recognition hypotheses but the classification of ( whole ) user utterances ( i.e. including the selection procedure to choose from the classified hypotheses ) . \n\t', '\n\t\t The results show that both learners profit from the addition of more features concerning dialogue context and task context for classifying user speech input appropriately . \n\t', '\n\t\t The only exception from this trend is a slight performance decrease when task features are added in the four-way classification for RIPPER . \n\t', '\n\t\t Note that both learners already outperform the baseline results even when only recognition features are considered . \n\t', '\n\t\t The most striking result is the performance gain for TiMBL ( almost 10 % ) when we include the dialogue features . \n\t', '\n\t\t As soon as dialogue features are included , TiMBL also performs slightly better than RIPPER . \n\t', '\n\t\t Note that the introduction of ( limited ) task features , in addition to the DIAL and UTT features , did not have dramatic impact in this study . \n\t', '\n\t\t One aim for future work is to define and analyze the influence of further task related features for classification . \n\t', '\n\t\t 7.1 Optimizing TiMBL Parameters In all of the above experiments we ran the machine learners with their default parameter settings . \n\t', '\n\t\t However , recent research \n\t\t']",Positive
"['\n\t\t We therefore selected 40 possible parameter combinations for TiMBL ( varying the number of nearest neighbors , feature weighting , and class voting weights ) and nested a parameter optimization step into the \x93leave-oneout\x94 evaluation paradigm ( cf. . \n\t', '\n\t\t Figure 2).5 Note that our optimization method is not as sophisticated as the \x93Iterative Deepening\x94 approach 5We only optimized parameters for TiMBL because it performed better with default settings than RIPPER and because the findings in \n\t\t']",Positive
"['\n\t\t 1. Set aside the recognition hypotheses for one of the user utterances . \n\t', '\n\t\t Randomly split the remaining data into an 80 % training and 20 % test set . \n\t', '\n\t\t 3. Run TiMBL with all possible parameter settings on the generated training and test sets and store the best performing settings . \n\t', '\n\t\t 4. Classify the left-out hypotheses with the recorded parameter settings . \n\t', '\n\t\t 5. Iterate . \n\t', '\n\t\t Figure 2 : Parameter optimization described by \n\t\t']",Positive
"['\n\t\t Table 3 shows the classification results when we run TiMBL with optimized parameter settings and using all feature groups for training . \n\t', '\n\t\t User Utterance System Behavior accept clarify reject ignore in-grammar 159/2 11 16 0 out-of-grammar ( WER < 50 ) 0 25 5 0 out-of-grammar ( WER > 50 ) 6 6 50 0 crosstalk 2 5 0 16 Acc/wf-score ( 3 classes ) : 86.14/86.39 % Acc/wf-score ( 4 classes ) : 82.51/83.29 % Table 3 : TiMBL classification results with optimized parameters Table 3 shows a remarkable 9 % improvement for the 3-way and 4-way classification in both accuracy and weighted f-score , compared to using TiMBL with default parameter settings . \n\t', '\n\t\t In terms of WER , the baseline system ( cf. . \n\t', '\n\t\t Table 1 ) accepted 233 user utterances with a WER of 21.51 % , and in contrast , TiMBL with optimized parameters ( Ti OP ) only accepted 169 user utterances with a WER of 4.05 % . \n\t', '\n\t\t This low WER reflects the fact that if the machine learning system accepts an user utterance , it is almost certainly the correct one . \n\t', '\n\t\t Note that although the machine learning system in total accepted far fewer utterances ( 169 vs. 233 ) it accepted more correct utterances than the baseline ( 159 vs. 154 ) . \n\t', '\n\t\t 7.2 Evaluation The baseline accuracy for the 3-class problem is 65.68 % ( 61.81 % weighted f-score ) . \n\t', '\n\t\t Our best results , obtained by using TiMBL with parameter op- System or features used Acc/wf-score Acc/wf-score Acc/wf-score Acc/wf-score for classification ( 3 classes ) ( 4 classes ) ( 3 classes ) ( 4 classes ) Baseline 65.68/61.81 % TiMBL 67.66/67.51 % 63.04/63.03 % 68.98/68.32 % 64.03/63.08 % RIPPER 69.31/69.03 % 66.67/65.14 % 72.61/72.33 % 70.30/68.61 % REC REC+UTT REC+UTT+DIAL 77.56/77.59 % 72.94/73.70 % 74.92/75.34 % 71.29/71.62 % REC+UTT+DIAL+TASK 77.89/77.91 % 73.27/74.12 % 75.25/75.61 % 70.63/71.54 % TiMBL ( optimized params . \n\t', '\n\t\t ) Oracle 86.14/86.39 % 82.51/83.29 % 94.06/94.17 % 94.06/94.18 % Table 2 : Classification Results timization , show a 25 % weighted f-score improvement over the baseline system . \n\t', '\n\t\t We can compare these results to a hypothetical \x93oracle\x94 system in order to obtain an upper bound on classification performance . \n\t', '\n\t\t This is an imaginary system which performs perfectly on the experimental data given the 10-best recognition output . \n\t', '\n\t\t The oracle results reveal that for 18 of the in-grammar utterances the 10-best recognition hypotheses do not include the correct logical form at all and therefore have to be classified as clarify or reject ( i.e. it is not possible to achieve 100 % accuracy on the experimental data ) . \n\t', '\n\t\t Table 2 shows that our best results are only 8%/12 % ( absolute ) away from the optimal performance . \n\t', '\n\t\t 7.2.1 Costs and k2 Levels of Significance We use the k2 test of independence to statistically compare the different classification results . \n\t', '\n\t\t However , since k2 only tells us whether two classifications are different from each other , we introduce a simple cost measure ( Table 4 ) for the 3-way classification problem to complement the k2 results.6 System behavior accept reject ignore 0 2 2 4 2 2 4 2 0 User utterance in-grammar out-of-grammar crosstalk Table 4 : Cost measure Table 4 captures the intuition that the correct behavior of a dialogue system is to accept correctly recognized utterances and ignore crosstalk ( cost 0 ) . \n\t', '\n\t\t The worst a system can do is to accept misrecognized utterances or utterances that were not addressed to the system . \n\t', '\n\t\t The remaining classes are as- 6 We only evaluate the 3-way classification problem because there are no baseline results for the 4-way classification available . \n\t', '\n\t\t signed a value in-between these two extremes . \n\t', '\n\t\t Note that the cost assignment is not validated against user judgments . \n\t', '\n\t\t We only use the costs to interpret the k2 levels of significance ( i.e. as an indicator to compare the relative quality of different systems ) . \n\t', '\n\t\t Table 5 shows the differences in cost and k2 levels of significance when we compare the classification results . \n\t', '\n\t\t Here , Ti OP stands for TiMBL with optimized parameters and the stars indicate the level of statistical significance as computed by the k2 statis- tics ( *** indicates significance at p = .001 , ** at p = .01 , and * at p = .05).7 Baseline RIPPER TiMBL Ti OP Oracle \x97232*** \x97116*** \x97100*** \x9756 Ti OP \x97176*** \x9760* \x9744 TiMBL \x97132*** \x9716 RIPPER \x97116*** Table 5 : Cost comparisons and k2 levels of significance for 3-way classification The cost measure shows the strict ordering : Oracle < Ti OP < TiMBL < RIPPER < Baseline . \n\t', '\n\t\t Note however that according to the k2 test there is no significant difference between the oracle system and TiMBL with optimized parameters . \n\t', '\n\t\t Table 5 also shows that all of our experiments significantly outperform the baseline system . \n\t', '\n\t\t 8 Conclusion We used a combination of acoustic confidence and pragmatic plausibility features ( i.e. computed from dialogue context ) to predict the quality of incoming recognition hypotheses to a multi-modal dialogue system . \n\t', '\n\t\t We classified hypotheses as accept , ( clarify ) , reject , or ignore : functional categories that 7Following \n\t\t']",Positive
"['\n\t\t can be used by a dialogue manager to decide appropriate system reactions . \n\t', '\n\t\t The approach is novel in combining machine learning with n-best processing for spoken dialogue systems using the Information State Update approach . \n\t', '\n\t\t Our best results , obtained using TiMBL with optimized parameters , show a 25 % weighted f-score improvement over a baseline system that uses a \x93grammar-switching\x94 approach to context-sensitive speech recognition , and are only 8 % away from the optimal performance that can be achieved on the data . \n\t', '\n\t\t Clearly , this improvement would result in better dialogue system performance overall . \n\t', '\n\t\t Parameter optimization improved the classification results by 9 % compared to using the learner with default settings , which shows the importance of such tuning . \n\t', '\n\t\t Future work points in two directions : first , integrating our methodology into working ISU-based dialogue systems and determining whether or not they improve in terms of standard dialogue evaluation metrics ( e.g. task completion ) . \n\t', '\n\t\t The ISU approach is a particularly useful testbed for our methodology because it collects information pertaining to dialogue context in a central data structure from which it can be easily extracted . \n\t', '\n\t\t This avenue will be further explored in the TALK project8 . \n\t', '\n\t\t Second , it will be interesting to investigate the impact of different dialogue and task features for classification and to introduce a distinction between \x93generic\x94 features that are domain independent and \x93application-specific\x94 features which reflect properties of individual systems and application scenarios . \n\t', '\n\t\t Acknowledgments We thank Nuance Communications Inc. for the use of their speech recognition and synthesis software and Alexander Koller and Dan Shapiro for reading draft versions of this paper . \n\t', '\n\t\t Oliver Lemon was partially supported by Scottish Enterprise under the Edinburgh-Stanford Link programme . \n\t', '\n\t\t References M. Boros , W. Eckert , F. Gallwitz , G. G¨orz , G. Hanrieder , and H. Niemann. 1996 . \n\t', '\n\t\t Towards Understanding Spontaneous Speech : Word Accuracy vs. . \n\t', '\n\t\t Concept Accuracy . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ICSLP-96 . \n\t', '\n\t\t Ananlada Chotimongkol and Alexander I. Rudnicky . \n\t', '\n\t\t 2001. N-best Speech Hypotheses Reordering Using Linear Regression . \n\t', '\n\t\t In Proceedings ofEuroSpeech 2001 , pages 1829\x961832 . \n\t', '\n\t\t William W. Cohen . \n\t', '\n\t\t 1995. Fast Effective Rule Induction . \n\t', '\n\t\t In Proceedings of the 12th International Conference on Machine Learning . \n\t', '\n\t\t 8EC FP6 IST-507802 , http://www.talk-project.org Walter Daelemans and V´eronique Hoste . \n\t', '\n\t\t 2002. Evaluation of Machine Learning Methods for Natural Language Processing Tasks . \n\t', '\n\t\t In Proceedings ofLREC-02 . \n\t', '\n\t\t Walter Daelemans , Jakub Zavrel , Ko van der Sloot , and Antal van den Bosch . \n\t', '\n\t\t 2002. TIMBL : Tilburg Memory Based Learner , version 4.2 , Reference Guide . \n\t', '\n\t\t In ILK Technical Report 02-01 . \n\t', '\n\t\t John Dowding , Jean Mark Gawron , Doug Appelt , John Bear , Lynn Cherny , Robert Moore , and Douglas Moran . \n\t', '\n\t\t 1993. GEMINI : a natural language system for spoken-language understanding . \n\t', '\n\t\t In Proceedings ofACL-93 . \n\t', '\n\t\t Malte Gabsdil . \n\t', '\n\t\t 2003. Classifying Recognition Results for Spoken Dialogue Systems . \n\t', '\n\t\t In Proceedings of the Student Research Workshop at ACL03 . \n\t', '\n\t\t Perry R. Hinton . \n\t', '\n\t\t 1995. Statistics Explained \x96 A Guide For Social Science Students . \n\t', '\n\t\t Routledge . \n\t', '\n\t\t Oliver Lemon and Alexander Gruenstein . \n\t', '\n\t\t 2004. Multithreaded context for robust conversational interfaces : context-sensitive speech recognition and interpretation of corrective fragments . \n\t', '\n\t\t ACM Transactions on Computer-Human Interaction . \n\t', '\n\t\t ( to appear ) . \n\t', '\n\t\t Oliver Lemon , Alexander Gruenstein , and Stanley Peters . \n\t', '\n\t\t 2002. Collaborative activities and multitasking in dialogue systems . \n\t', '\n\t\t Traitement Automatique des Langues , 43(2):131\x96154 . \n\t', '\n\t\t Oliver Lemon . \n\t', '\n\t\t 2004. Context-sensitive speech recognition in ISU dialogue systems : results for the grammar switching approach . \n\t', '\n\t\t In Proceedings of the 8th Workshop on the Semantics and Pragmatics ofDialogue , CATALOG\x9204 . \n\t', '\n\t\t Diane J. Litman , Julia Hirschberg , and Marc Swerts . \n\t', '\n\t\t 2000. Predicting Automatic Speech Recognition Performance Using Prosodic Cues . \n\t', '\n\t\t In Proceedings ofNAACL-00 . \n\t', '\n\t\t Erwin Marsi , Martin Reynaert , Antal van den Bosch , Walter Daelemans , and V´eronique Hoste . \n\t', '\n\t\t 2003. Learning to predict pitch accents and prosodic boundaries in Dutch . \n\t', '\n\t\t In Proceedings of ACL-03 . \n\t', '\n\t\t David Traum , Johan Bos , Robin Cooper , Staffan Larsson , Ian Lewin , Colin Matheson , and Massimo Poesio . \n\t', '\n\t\t 1999. A Model of Dialogue Moves and Information State Revision . \n\t', '\n\t\t Technical Report D2.1 , Trindi Project . \n\t', '\n\t\t Marilyn Walker , Jerry Wright , and Irene Langkilde . \n\t', '\n\t\t 2000. Using Natural Language Processing and Discourse Features to Identify Understanding Errors in a Spoken Dialogue System . \n\t', '\n\t\t In Proceedings ofICML-2000 . \n\t', '\n\t\t Predicting Student Emotions in Computer-Human Tutoring Dialogues Diane J. Litman Kate Forbes-Riley University of Pittsburgh University of Pittsburgh Department of Computer Science Learning Research and Development Center Learning Research and Development Center Pittsburgh PA , 15260 , USA Pittsburgh PA , 15260 , USA forbesk@pitt.edu litman@cs.pitt.edu Abstract We examine the utility of speech and lexical features for predicting student emotions in computer- human spoken tutoring dialogues . \n\t', '\n\t\t We first annotate student turns for negative , neutral , positive and mixed emotions . \n\t', '\n\t\t We then extract acoustic-prosodic features from the speech signal , and lexical items from the transcribed or recognized speech . \n\t', '\n\t\t We compare the results of machine learning experiments using these features alone or in combination to predict various categorizations of the annotated student emotions . \n\t', '\n\t\t Our best results yield a 19-36 % relative improvement in error reduction over a baseline . \n\t', '\n\t\t Finally , we compare our results with emotion prediction in human-human tutoring dialogues . \n\t', '\n\t\t 1 Introduction This paper explores the feasibility of automatically predicting student emotional states in a corpus of computer-human spoken tutoring dialogues . \n\t', '\n\t\t Intelligent tutoring dialogue systems have become more prevalent in recent years \n\t\t']",Positive
"['\n\t\t Another method for closing this performance gap has been to incorporate affective reasoning into computer tutoring systems , independently of whether or not the tutor is dialogue-based \n\t\t']",Positive
"['\n\t\t For example , \n\t\t']",Positive
"['\n\t\t Our long-term goal is to merge these lines of dialogue and affective tutoring research , by enhancing our intelligent tutoring spoken dialogue system to automatically predict and adapt to student emotions , and to investigate whether this improves learning and other measures of performance . \n\t', '\n\t\t Previous spoken dialogue research has shown that predictive models of emotion distinctions ( e.g. , emotional vs. non-emotional , negative vs. non- negative ) can be developed using features typically available to a spoken dialogue system in real-time ( e.g , acoustic-prosodic , lexical , dialogue , and/or contextual ) \n\t\t']",Positive
"['\n\t\t In prior work we built on and generalized such research , by defining a three-way distinction between negative , neutral , and positive student emotional states that could be reliably annotated and accurately predicted in human-human spoken tutoring dialogues \n\t\t']",Positive
"['\n\t\t Like the non-tutoring studies , our results showed that combining feature types yielded the highest predictive accuracy . \n\t', '\n\t\t In this paper we investigate the application of our approach to a comparable corpus of computer- human tutoring dialogues , which displays many different characteristics , such as shorter utterances , little student initiative , and non-overlapping speech . \n\t', '\n\t\t We investigate whether we can annotate and predict student emotions as accurately and whether the relative utility of speech and lexical features as predictors is the same , especially when the output of the speech recognizer is used ( rather than a human transcription of the student speech ) . \n\t', '\n\t\t Our best models for predicting three different types of emotion classifications achieve accuracies of 66-73 % , representing relative improvements of 19-36 % over majority class baseline errors . \n\t', '\n\t\t Our computer-human results also show interesting differences compared with comparable analyses of human-human data . \n\t', '\n\t\t Our results provide an empirical basis for enhancing our spoken dialogue tutoring system to automatically predict and adapt to a student model that includes emotional states . \n\t', '\n\t\t 2 Computer-Human Dialogue Data Our data consists of student dialogues with IT- SPOKE ( Intelligent Tutoring SPOKEn dialogue system ) \n\t\t']",Positive
"['\n\t\t In ITSPOKE , a student first types an essay answering a qualitative physics problem . \n\t', '\n\t\t IT- SPOKE then analyzes the essay and engages the student in spoken dialogue to correct misconceptions and to elicit complete explanations . \n\t', '\n\t\t First , the Why2-Atlas back-end parses the student essay into propositional representations , in order to find useful dialogue topics . \n\t', '\n\t\t It uses 3 different approaches ( symbolic , statistical and hybrid ) competitively to create a representation for each sentence , then resolves temporal and nominal anaphora and constructs proofs using abductive reasoning \n\t\t']",Positive
"['\n\t\t During the dialogue , student speech is digitized from microphone input and sent to the Sphinx2 recognizer , whose stochastic language models have a vocabulary of 1240 words and are trained with 7720 student utterances from evaluations of Why2-Atlas and from pilot studies of IT- SPOKE . \n\t', '\n\t\t Sphinx2\x92s best \x93transcription\x94 ( recognition output ) is then sent to the Why2-Atlas back-end for syntactic , semantic and dialogue analysis . \n\t', '\n\t\t Finally , the text response produced by Why2-Atlas is sent to the Cepstral text-to-speech system and played to the student . \n\t', '\n\t\t After the dialogue , the student revises the essay , thereby ending the tutoring or causing another round of tutoring/essay revision . \n\t', '\n\t\t Our corpus of dialogues with ITSPOKE was collected from November 2003 - April 2004 , as part of an evaluation comparing ITSPOKE , Why2-Atlas , and human tutoring \n\t\t']",Positive
"['\n\t\t Subjects are University of Pittsburgh students who have never taken college physics , and who are native English speakers . \n\t', '\n\t\t Subjects first read a small document of background physics material , then work through 5 problems ( dialogues ) with ITSPOKE . \n\t', '\n\t\t The corpus contains 100 dialogues ( physics problems ) from 20 subjects , with a total of 2445 student turns and 398 unique words . \n\t', '\n\t\t 15 dialogues have been annotated for emotion as described in Section 3 . \n\t', '\n\t\t On average , our dialogues last 19.4 minutes and contain 25 student turns . \n\t', '\n\t\t While ITSPOKE\x92s word error rate on this corpus is 31.2 % , semantic accuracy is more useful for dialogue evaluation as it does not penalize for unimportant word errors . \n\t', '\n\t\t Semantic analysis based on speech recognition is the same as based on perfect transcription 92.4 % of the time . \n\t', '\n\t\t An emotion- annotated corpus example is shown in Figure 1. 3 Annotating Student Turns In our data , student \x93emotions\x941 can only be identified indirectly : via what is said and/or how it is ... dialogue excerpt at 18.3 min . \n\t', '\n\t\t into session ... \n\t', '\n\t\t ITSPOKE : What is the magnitude of the acceleration of the packet in the horizontal direction ? \n\t', '\n\t\t STUDENT : i don\x92t know ( NEGATIVE ) ITSPOKE : Ok , let\x92s consider how forces in the vertical direction affect the motion of a body in a horizontal direction in a different scenario . \n\t', '\n\t\t Say an apple falls from a tree . \n\t', '\n\t\t What force(s) are acting on the apple as it falls ? \n\t', '\n\t\t STUDENT : gravity ( NEGATIVE ) ITSPOKE : Yeah . \n\t', '\n\t\t Are there any other forces acting on the apple as it falls ? \n\t', '\n\t\t STUDENT : no ( NEGATIVE ) ITSPOKE : Fine . \n\t', '\n\t\t The only force acting on the apple as it falls to the earth is the earth\x92s gravitational force . \n\t', '\n\t\t In which direction does gravitational force act ? \n\t', '\n\t\t STUDENT : down ( POSITIVE ) ITSPOKE : Yep . \n\t', '\n\t\t Therefore , what is the magnitude of this gravitational force in the horizontal direction ? \n\t', '\n\t\t STUDENT : in the direction of the airplane ( NEUTRAL ) Figure 1 : Annotated Spoken Dialogue Excerpt said . \n\t', '\n\t\t In \n\t\t']",Positive
"['\n\t\t Negative : a student turn that expresses emotions such as confused , bored , irritated . \n\t', '\n\t\t Evidence of a negative emotion can come from many knowledge sources such as lexical items ( e.g. , \x93I don\x92t know\x94 in student in Figure 1 ) , and/or acoustic-prosodic features ( e.g. , prior-turn pausing in student ) . \n\t', '\n\t\t Positive : a student turn expressing emotions such as confident , enthusiastic . \n\t', '\n\t\t An example is student , which displays louder speech and faster tempo . \n\t', '\n\t\t Neutral : a student turn not expressing a negative or positive emotion . \n\t', '\n\t\t An example is student , where evidence comes from moderate loudness , pitch and tempo . \n\t', '\n\t\t We also distinguish Mixed : a student turn expressing both positive and negative emotions . \n\t', '\n\t\t To avoid influencing the annotator\x92s intuitive understanding of emotion expression , and because particular emotional cues are not used consistently 1We use the term \x93emotion\x94 loosely to cover both affects and attitudes that can impact student learning . \n\t', '\n\t\t 2Weak and strong expressions of emotions are annotated . \n\t', '\n\t\t or unambiguously across speakers , our annotation manual does not associate particular cues with particular emotion labels . \n\t', '\n\t\t Instead , it contains examples of labeled dialogue excerpts ( as in Figure 1 , except on human-human data ) with links to corresponding audio files . \n\t', '\n\t\t The cues mentioned in the discussion of Figure 1 above were elicited during post-annotation discussion of the emotions , and are presented here for expository use only . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t To analyze the reliability of the scheme on our new computer-human data , we selected 15 transcribed dialogues from the corpus described in Section 2 , yielding a dataset of 333 student turns , where approximately 30 turns came from each of 10 subjects . \n\t', '\n\t\t The 333 turns were separately annotated by two annotators following the emotion annotation scheme described above . \n\t', '\n\t\t We focus here on three analyses of this data , itemized below . \n\t', '\n\t\t While the first analysis provides the most fine-grained distinctions for triggering system adaptation , the second and third ( simplified ) analyses correspond to those used in \n\t\t']",Positive
"['\n\t\t These represent alternative potentially useful triggering mechanisms , and are worth exploring as they might be easier to annotate and/or predict . \n\t', '\n\t\t Negative , Neutral , Positive ( NPN ) : mixeds are conflated with neutrals . \n\t', '\n\t\t Negative , Non-Negative ( NnN ) : positives , mixeds , neutrals are conflated as non- negatives . \n\t', '\n\t\t Emotional , Non-Emotional ( EnE ) : negatives , positives , mixeds are conflated as Emotional ; neutrals are Non-Emotional . \n\t', '\n\t\t Tables 1-3 provide a confusion matrix for each analysis summarizing inter-annotator agreement . \n\t', '\n\t\t The rows correspond to the labels assigned by annotator 1 , and the columns correspond to the labels assigned by annotator 2 . \n\t', '\n\t\t For example , the annotators agreed on 89 negatives in Table 1 . \n\t', '\n\t\t In the NnN analysis , the two annotators agreed on the annotations of 259/333 turns achieving 77.8 % agreement , with Kappa = 0.5 . \n\t', '\n\t\t In the EnE analysis , the two annotators agreed on the annotations of 220/333 turns achieving 66.1 % agreement , with Kappa = 0.3 . \n\t', '\n\t\t In the NPN analysis , the two annotators agreed on the annotations of 202/333 turns achieving 60.7 % agreement , with Kappa = 0.4 . \n\t', '\n\t\t This inter-annotator agreement is on par with that of prior studies of emotion annotation in naturally occurring computer-human dialogues ( e.g. , agreement of 71 % and Kappa of 0.47 in \n\t\t']",Positive
"['\n\t\t A number of researchers have accommodated for this low agreement by exploring ways of achieving consensus between disagreed annotations , to yield 100 % agreement ( e.g \n\t\t']",Positive
['\n\t\t As in \n\t\t'],Positive
"['\n\t\t negative non-negative negative 89 36 non-negative 38 170 Table 1 : NnN Analysis Confusion Matrix emotional non-emotional emotional 129 43 non-emotional 70 91 Table 2 : EnE Analysis Confusion Matrix negative neutral positive negative 89 30 6 neutral 32 94 38 positive 6 19 19 Table 3 : NPN Analysis Confusion Matrix 4 Extracting Features from Turns For each of the 333 student turns described above , we next extracted the set of features itemized in Figure 2 , for use in the machine learning experiments described in Section 5 . \n\t', '\n\t\t Motivated by previous studies of emotion prediction in spontaneous dialogues \n\t\t']",Positive
"['\n\t\t We further restrict our features to those that can be computed automatically and in real-time , since our goal is to use such features to trigger online adaptation in IT- SPOKE based on predicted student emotions . \n\t', '\n\t\t F0 and RMS values , representing measures of pitch and loudness , respectively , are computed using Entropic Research Laboratory\x92s pitch tracker , get f0 , with no post-correction . \n\t', '\n\t\t Amount of Silence is approximated as the proportion of zero f0 frames for the turn . \n\t', '\n\t\t Turn Duration and Prior Pause Duration are computed Acoustic-Prosodic Features 4 fundamental frequency ( f0 ) : max , min , mean , standard deviation 4 energy ( RMS ) : max , min , mean , standard deviation 4 temporal : amount of silence in turn , turn duration , duration of pause prior to turn , speaking rate Lexical Features human-transcribed lexical items in the turn ITSPOKE-recognized lexical items in the turn Identifier Features : subject , gender , problem Figure 2 : Features Per Student Turn automatically via the start and end turn boundaries in ITSPOKE logs . \n\t', '\n\t\t Speaking Rate is automatically calculated as #syllables per second in the turn . \n\t', '\n\t\t While acoustic-prosodic features address how something is said , lexical features representing what is said have also been shown to be useful for predicting emotion in spontaneous dialogues \n\t\t']",Positive
"['\n\t\t Our first set of lexical features represents the human transcription of each student turn as a word occurrence vector ( indicating the lexical items that are present in the turn ) . \n\t', '\n\t\t This feature represents the \x93ideal\x94 performance of ITSPOKE with respect to speech recognition . \n\t', '\n\t\t The second set represents ITSPOKE\x92s actual best speech recognition hypothesis of what is said in each student turn , again as a word occurrence vector . \n\t', '\n\t\t Finally , we recorded for each turn the 3 \x93identifier\x94 features shown last in Figure 2 . \n\t', '\n\t\t Prior studies \n\t\t']",Positive
"['\n\t\t \x93Subject\x94 and \x93problem\x94 are particularly important in our tutoring domain because students will use our system repeatedly , and problems are repeated across students . \n\t', '\n\t\t 5 Predicting Student Emotions 5.1 Feature Sets and Method We next created the 10 feature sets in Figure 3 , to study the effects that various feature combinations had on predicting emotion . \n\t', '\n\t\t We compare an acoustic-prosodic feature set ( \x93sp\x94 ) , a human- transcribed lexical items feature set ( \x93lex\x94 ) and an ITSPOKE-recognized lexical items feature set ( \x93asr\x94 ) . \n\t', '\n\t\t We further compare feature sets combining acoustic-prosodic and either transcribed or recognized lexical items ( \x93sp+lex\x94 , \x93sp+asr\x94 ) . \n\t', '\n\t\t Finally , we compare each of these 5 feature sets with an identical set supplemented with our 3 identifier features ( \x93+id\x94 ) . \n\t', '\n\t\t sp : 12 acoustic-prosodic features lex : human-transcribed lexical items asr : ITSPOKE recognized lexical items sp+lex : combined sp and lex features sp+asr : combined sp and asr features +id : each above set + 3 identifier features Figure 3 : Feature Sets for Machine Learning We use the Weka machine learning software \n\t\t']",Positive
['\n\t\t In our human- human dialogue studies \n\t\t'],Positive
['\n\t\t 5.2 Predicting Agreed Turns As in \n\t\t'],Positive
"['\n\t\t Tables 4-6 show , for each emotion classification , the mean accuracy ( %correct ) and standard error ( SE ) for our 10 feature sets ( Figure 3 ) , computed across 10 runs of 10-fold cross-validation . \n\t', ""\n\t\t ' For comparison , the accuracy of a standard baseline algorithm ( MAJ ) , which always predicts the majority class , is shown in each caption . \n\t"", '\n\t\t For example , Table 4\x92s caption shows that for NnN , always predicting the majority class of non-negative yields an accuracy of 65.65 % . \n\t', '\n\t\t In each table , the accuracies are labeled for how they compare statistically to the relevant baseline accuracy ( = worse , = same , = better ) , as automatically computed in Weka using a two-tailed t-test ( p .05 ) . \n\t', '\n\t\t First note that almost every feature set significantly outperforms the majority class baseline , across all emotion classifications ; the only exceptions are the speech-only feature sets without identifier features ( \x93sp-id\x94 ) in the NnN and EnE tables , which perform the same as the baseline . \n\t', '\n\t\t These results suggest that without any subject or task specific information , acoustic-prosodic features alone 3For each cross-validation , the training and test data are drawn from utterances produced by the same set of speakers . \n\t', '\n\t\t A separate experiment showed that testing on one speaker and training on the others , averaged across all speakers , does not significantly change the results . \n\t', '\n\t\t are not useful predictors for our two binary classification tasks , at least in our computer-human dialogue corpus . \n\t', '\n\t\t As will be discussed in Section 6 , however , \x93sp-id\x94 feature sets are useful predictors in human-human tutoring dialogues . \n\t', '\n\t\t Feat . \n\t', '\n\t\t Set -id SE +id SE sp 64.10 0.80 70.66 0.76 lex 68.20 0.41 72.74 0.58 asr 72.30 0.58 70.51 0.59 sp+lex 71.78 0.77 72.43 0.87 sp+asr 69.90 0.57 71.44b 0.68 Table 4 : %Correct , NnN Agreed , MAJ ( non- negative ) = 65.65 % Feat . \n\t', '\n\t\t Set -id SE +id SE sp 59.18 0.75 70.68 0.89 lex 63.18 0.82 75.64 0.37 asr 66.36 0.54 72.91 0.35 sp+lex 63.86 0.97 69.59 0.48 sp+asr 65.14 0.82 69.64 0.57 Table 5 : %Correct , EnE Agreed , MAJ ( emotional ) = 58.64 % Feat . \n\t', '\n\t\t Set -id SE +id SE sp 55.49 1.01 62.03 0.91 lex 52.66 0.62 67.84 0.66 asr 57.95 0.67 65.70 0.50 sp+lex 62.08 0.56 63.52 0.48 sp+asr 61.22 1.20 62.23 0.86 Table 6 : %Correct , NPN Agreed , MAJ ( neutral ) = 46.52 % Further note that adding identifier features to the \x93-id\x94 feature sets almost always improves performance , although this difference is not always significant4 ; across tables the \x93+id\x94 feature sets outperform their \x93-id\x94 counterparts across all feature sets and emotion classifications except one ( NnN \x93asr\x94 ) . \n\t', '\n\t\t Surprisingly , while \n\t\t']",Positive
"['\n\t\t Also note that with the addition of identifier features , the speech-only feature sets ( sp+id ) now do outperform the majority class baselines for all three emotion classifications . \n\t', '\n\t\t 4For any feature set , the mean +/- 2*SE = the 95 % confidence interval . \n\t', '\n\t\t If the confidence intervals for two feature sets are non-overlapping , then their mean accuracies are significantly different with 95 % confidence . \n\t', '\n\t\t With respect to the relative utility of lexical versus acoustic-prosodic features , without identifier features , using only lexical features ( \x93lex\x94 or \x93asr\x94 ) almost always produces statistically better performance than using only speech features ( \x93sp\x94 ) ; the only exception is NPN \x93lex\x94 , which performs statistically the same as NPN \x93sp\x94 . \n\t', '\n\t\t This is consistent with others\x92 findings , e.g. , \n\t\t']",Positive
"['\n\t\t When identifier features are added to both , the lexical sets don\x92t always significantly outperform the speech set ; only in NPN and EnE \x93lex+id\x94 is this the case . \n\t', '\n\t\t For NnN , just as using \x93sp+id\x94 rather than \x93sp-id\x94 improved performance when compared to the majority baseline , the addition of the identifier features also improves the utility of the speech features when compared to the lexical features . \n\t', '\n\t\t Interestingly , although we hypothesized that the \x93lex\x94 feature sets would present an upper bound on the performance of the \x93asr\x94 sets , because the human transcription is more accurate than the speech recognizer , we see that this is not consistently the case . \n\t', '\n\t\t In fact , in the \x93-id\x94 sets , \x93asr\x94 always significantly outperforms \x93lex\x94 . \n\t', '\n\t\t A comparison of the decision trees produced in either case , however , does not reveal why this is the case ; words chosen as predictors are not very intuitive in either case ( e.g. , for NnN , an example path through the learned \x93lex\x94 decision tree says predict negative if the utterance contains the word will but does not contain the word decrease ) . \n\t', '\n\t\t Understanding this result is an area for future research . \n\t', '\n\t\t Within the \x93+id\x94 sets , we see that \x93lex\x94 and \x93asr\x94 perform the same in the NnN and NPN classifications ; in EnE \x93lex+id\x94 significantly outperforms \x93asr+id\x94 . \n\t', '\n\t\t The utility of the \x93lex\x94 features compared to \x93asr\x94 also increases when combined with the \x93sp\x94 features ( with and without identifiers ) , for both NnN and NPN . \n\t', '\n\t\t Moreover , based on results in \n\t\t']",Positive
"['\n\t\t We instead found that the relative performance of these sets depends both on the emotion classification being predicted and the presence or absence of \x93id\x94 features . \n\t', '\n\t\t Although consistently with prior research we find that the combined feature sets usually outperform the speech-only feature sets , the combined feature sets frequently perform worse than the lexical-only feature sets . \n\t', '\n\t\t However , we will see in Section 6 that combining knowledge sources does improve prediction performance in human-human dialogues . \n\t', '\n\t\t Finally , the bolded accuracies in each table sum- marize the best-performing feature sets with and without identifiers , with respect to both the %Corr figures shown in the tables , as well as to relative improvement in error reduction over the baseline ( MAJ ) error5 , after excluding all the feature sets containing \x93lex\x94 features . \n\t', '\n\t\t In this way we give a better estimate of the best performance our system could accomplish , given the features it can currently access from among those discussed . \n\t', '\n\t\t These best- performing feature sets yield relative improvements over their majority baseline errors ranging from 19- 36 % . \n\t', '\n\t\t Moreover , although the NPN classification yields the lowest raw accuracies , it yields the highest relative improvement over its baseline . \n\t', '\n\t\t 5.3 Predicting Consensus Turns Following \n\t\t']",Positive
"['\n\t\t For our consensus labeling , the original annotators revisited each originally disagreed case , and through discussion , sought a consensus label . \n\t', '\n\t\t Due to consensus labeling , agreement rose across all three emotion classifications to 100 % . \n\t', '\n\t\t Tables 7- 9 show , for each emotion classification , the mean accuracy ( %correct ) and standard error ( SE ) for our 10 feature sets . \n\t', '\n\t\t Feat . \n\t', '\n\t\t Set -id SE +id SE sp 59.10 0.57 64.20 0.52 lex 63.70 0.47 68.64 0.41 asr 66.26 0.71 68.13 0.56 sp+lex 64.69 0.61 65.40 0.63 sp+asr 65.99 0.51 67.55 0.48 Table 7 : %Corr. , NnN Consensus , MAJ=62.47 % Feat . \n\t', '\n\t\t Set -id SE +id SE sp 56.13 0.94 59.30 0.48 lex 52.07 0.34 65.37 0.47 asr 53.78 0.66 64.13 0.51 sp+lex 60.96 0.76 63.01 0.62 sp+asr 57.84 0.73 60.89 0.38 Table 8 : %Corr. , EnE Consensus , MAJ=55.86 % A comparison with Tables 4-6 shows that overall , using consensus-labeled data decreased the performance across all feature sets and emotion classifications . \n\t', '\n\t\t This was also found in \n\t\t']",Positive
"['\n\t\t Moreover , it is no longer the case that every feature 5Relative improvement over the baseline ( MAJ ) error for feature set x = , where error(x) is 100 minus the %Corr(x) value shown in Tables 4-6 . \n\t', '\n\t\t Feat . \n\t', '\n\t\t Set -id SE +id SE sp 48.97 0.66 51.90 0.40 lex 47.86 0.54 57.28 0.44 asr 51.09 0.66 53.41 0.66 sp+lex 53.41 0.62 54.20 0.86 sp+asr 52.50 0.42 53.84 0.42 Table 9 : %Corr. , NPN Consensus , MAJ=48.35 % set performs as well as or better than their baselines6 ; within the \x93-id\x94 sets , NnN \x93sp\x94 and EnE \x93lex\x94 perform significantly worse than their baselines . \n\t', '\n\t\t However , again we see that the \x93+id\x94 sets do consistently better than the \x93-id\x94 sets and moreover always outperform the baselines . \n\t', '\n\t\t We also see again that using only lexical features almost always yields better performance than using only speech features . \n\t', '\n\t\t In addition , we again see that the \x93lex\x94 feature sets perform comparably to the \x93asr\x94 feature sets , rather than outperforming them as we first hypothesized . \n\t', '\n\t\t And finally , we see again that while in most cases combining speech and lexical features yields better performance than using only speech features , the combined feature sets in most cases perform the same or worse than the lexical feature sets . \n\t', '\n\t\t As above , the bolded accuracies summarize the best-performing feature sets from each emotion classification , after excluding all the feature sets containing \x93lex\x94 to give a better estimate of actual system performance . \n\t', '\n\t\t The best-performing feature sets in the consensus data yield an 11%-19 % relative improvement in error reduction compared to the majority class prediction , which is a lower error reduction than seen for agreed data . \n\t', '\n\t\t Moreover , the NPN classification yields the lowest accuracies and the lowest improvements over its baseline . \n\t', '\n\t\t 6 Comparison with Human Tutoring While building ITSPOKE , we collected a corresponding corpus of spoken human tutoring dialogues , using the same experimental methodology as for our computer tutoring corpus ( e.g. same subject pool , physics problems , web and audio interface , etc ) ; the only difference between the two corpora is whether the tutor is human or computer . \n\t', '\n\t\t As discussed in \n\t\t']",Positive
"['\n\t\t Here , we perform the exper- 6The majority class for EnE Consensus is non-emotional ; all others are unchanged . \n\t', '\n\t\t NnN EnE NPN FS -id SE +id SE -id SE +id SE -id SE +id SE sp 77.46 0.42 77.56 0.30 84.71 0.39 84.66 0.40 73.09 0.68 74.18 0.40 lex 80.74 0.42 80.60 0.34 88.86 0.26 86.23 0.34 78.56 0.45 77.18 0.43 sp+lex 81.37 0.33 80.79 0.41 87.74 0.36 88.31 0.29 79.06 0.38 78.03 0.33 Table 10 : Human-Human %Correct , NnN MAJ=72.21 % ; EnE MAJ=50.86 % ; NPN MAJ=53.24 % iments from Section 5.2 on this annotated human tutoring data , as a step towards understand the differences between annotating and predicting emotion in human versus computer tutoring dialogues . \n\t', '\n\t\t With respect to inter-annotator agreement , in the NnN analysis , the two annotators had 88.96 % agreement ( Kappa = 0.74 ) . \n\t', '\n\t\t In the EnE analysis , the annotators had 77.26 % agreement ( Kappa = 0.55 ) . \n\t', '\n\t\t In the NPN analysis , the annotators had 75.06 % agreement ( Kappa = 0.60 ) . \n\t', '\n\t\t A comparison with the results in Section 3 shows that all of these figures are higher than their computer tutoring counterparts . \n\t', '\n\t\t With respect to predictive accuracy , Table 10 shows our results for the agreed data . \n\t', '\n\t\t A comparison with Tables 4-6 shows that overall , the human- human data yields increased performance across all feature sets and emotion classifications , although it should be noted that the human-human corpus is over 100 turns larger than the computer-human corpus . \n\t', '\n\t\t Every feature set performs significantly better than their baselines . \n\t', '\n\t\t However , unlike the computer- human data , we don\x92t see the \x93+id\x94 sets performing better than the \x93-id\x94 sets ; rather , both sets perform about the same . \n\t', '\n\t\t We do see again the \x93lex\x94 sets yielding better performance than the \x93sp\x94 sets . \n\t', '\n\t\t However , we now see that in 5 out of 6 cases , combining speech and lexical features yields better performance than using either \x93sp\x94 or \x93lex\x94 alone . \n\t', '\n\t\t Finally , these feature sets yield a relative error reduction of 42.45%-77.33 % compared to the majority class predictions , which is far better than in our computer tutoring experiments . \n\t', '\n\t\t Moreover , the EnE classification yields the highest raw accuracies and relative improvements over baseline error . \n\t', '\n\t\t We hypothesize that such differences arise in part due to differences between the two corpora : 1 ) student turns with the computer tutor are much shorter than with the human tutor ( and thus contain less emotional content - making both annotation and prediction more difficult ) , 2 ) students respond to the computer tutor differently and perhaps more idiosyncratically than to the human tutor , 3 ) the computer tutor is less \x93flexible\x94 than the human tutor ( allowing little student initiative , questions , groundings , contextual references , etc. ) , which also effects student emotional response and its expression . \n\t', '\n\t\t 7 Conclusions and Current Directions Our results show that acoustic-prosodic and lexical features can be used to automatically predict student emotion in computer-human tutoring dialogues . \n\t', '\n\t\t We examined emotion prediction using a classification scheme developed for our prior human- human tutoring studies ( negative/positive/neutral ) , as well as using two simpler schemes proposed by other dialogue researchers ( negative/non-negative , emotional/non-emotional ) . \n\t', '\n\t\t We used machine learning to examine the impact of different feature sets on prediction accuracy . \n\t', '\n\t\t Across schemes , our feature sets outperform a majority baseline , and lexical features outperform acoustic-prosodic features . \n\t', '\n\t\t While adding identifier features typically also improves performance , combining lexical and speech features does not . \n\t', '\n\t\t Our analyses also suggest that prediction in consensus-labeled turns is harder than in agreed turns , and that prediction in our computer- human corpus is harder and based on somewhat different features than in our human-human corpus . \n\t', '\n\t\t Our continuing work extends this methodology with the goal of enhancing ITSPOKE to predict and adapt to student emotions . \n\t', '\n\t\t We continue to manually annotate ITSPOKE data , and are exploring partial automation via semi-supervised machine learning \n\t\t']",Positive
"['\n\t\t Further manual annotation might also improve reliability , as understanding systematic disagreements can lead to coding manual revisions . \n\t', '\n\t\t We are also expanding our feature set to include features suggested in prior dialogue research , tutoring-dependent features ( e.g. , pedagogical goal ) , and other features available in our logs ( e.g. , semantic analysis ) . \n\t', '\n\t\t Finally , we will explore how the recognized emotions can be used to improve system performance . \n\t', '\n\t\t First , we will label human tutor adaptations to emotional student turns in our human tutoring corpus ; this labeling will be used to formulate adaptive strategies for ITSPOKE , and to determine which of our three prediction tasks best triggers adaptation . \n\t', '\n\t\t Acknowledgments This research is supported by NSF Grants 9720359 & 0328431 . \n\t', '\n\t\t Thanks to the Why2-Atlas team and S. Silliman for system design and data collection . \n\t', '\n\t\t References G. Aist , B. Kort , R. Reilly , J. Mostow , and R. Picard . \n\t', '\n\t\t 2002. Experimentally augmenting an intelligent tutoring system with human-supplied capabilities : Adding Human-Provided Emotional Scaffolding to an Automated Reading Tutor that Listens . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t Intelligent Tutoring Systems . \n\t', '\n\t\t V. Aleven and C. P. Rose , editors . \n\t', '\n\t\t 2003. Proc . \n\t', '\n\t\t AI in Education Workshop on Tutorial Dialogue Systems : With a View toward the Classroom . \n\t', '\n\t\t J. Ang , R. Dhillon , A. Krupski , E.Shriberg , and A. Stolcke . \n\t', '\n\t\t 2002. Prosody-based automatic detection of annoyance and frustration in human- computer dialog . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t International Conf . \n\t', '\n\t\t on Spoken Language Processing ( ICSLP ) . \n\t', '\n\t\t A. Batliner , K. Fischer , R. Huber , J. Spilker , and E. N¨oth . \n\t', '\n\t\t 2000. Desperately seeking emotions : Actors , wizards , and human beings . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ISCA Workshop on Speech and Emotion . \n\t', '\n\t\t A. Batliner , K. Fischer , R. Huber , J. Spilker , and E. Noth . \n\t', '\n\t\t 2003. How to find trouble in communication . \n\t', '\n\t\t Speech Communication , 40:117\x96143 . \n\t', '\n\t\t K. Bhatt , M. Evens , and S. Argamon . \n\t', '\n\t\t 2004. Hedged responses and expressions of affect in human/human and human/computer tutorial interactions . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t Cognitive Science . \n\t', '\n\t\t C. Conati , R. Chabbal , and H. Maclaren . \n\t', '\n\t\t 2003. A study on using biometric sensors for monitoring user emotions in educational games . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t User Modeling Workshop on Assessing and Adapting to User Attitudes and Effect : Why , When , and How ? \n\t', '\n\t\t L. Devillers , L. Lamel , and I. Vasilescu . \n\t', '\n\t\t 2003. Emotion detection in task-oriented spoken dialogs . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t IEEE International Conference on Multimedia & Expo ( ICME ) . \n\t', '\n\t\t K. Forbes-Riley and D. Litman . \n\t', '\n\t\t 2004. Predicting emotion in spoken dialogue from multiple knowledge sources . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t Human Language Technology Conf . \n\t', '\n\t\t of the North American Chap . \n\t', '\n\t\t of the Assoc. for Computational Linguistics ( HLT/NAACL ) . \n\t', '\n\t\t A. Graesser , K. VanLehn , C. Rose , P. Jordan , and D. Harter . \n\t', '\n\t\t 2002. Intelligent tutoring systems with conversational dialogue . \n\t', '\n\t\t AI Magazine . \n\t', '\n\t\t P. W. Jordan , M. Makatchev , and K. VanLehn . \n\t', '\n\t\t 2004. Combining competing language understanding approaches in an intelligent tutoring system . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t Intelligent Tutoring Systems . \n\t', '\n\t\t B. Kort , R. Reilly , and R. W. Picard . \n\t', '\n\t\t 2001. An affective model of interplay between emotions and learning : Reengineering educational pedagogy - building a learning companion . \n\t', '\n\t\t In International Conf . \n\t', '\n\t\t on Advanced Learning Technologies . \n\t', '\n\t\t C.M. Lee , S. Narayanan , and R. Pieraccini . \n\t', '\n\t\t 2001. Recognition of negative emotions from the speech signal . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t IEEE Automatic Speech Recognition and Understanding Workshop . \n\t', '\n\t\t C.M. Lee , S. Narayanan , and R. Pieraccini . \n\t', '\n\t\t 2002. Combining acoustic and language information for emotion recognition . \n\t', '\n\t\t In International Conf . \n\t', '\n\t\t on Spoken Language Processing ( ICSLP ) . \n\t', '\n\t\t D. Litman and K. Forbes-Riley . \n\t', '\n\t\t 2004. Annotating student emotional states in spoken tutoring dialogues . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 5th SIGdial Workshop on Discourse and Dialogue . \n\t', '\n\t\t D. Litman and K. Forbes . \n\t', '\n\t\t 2003. Recognizing emotion from student speech in tutoring dialogues . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t IEEE Automatic Speech Recognition and Understanding Workshop ( ASRU ) . \n\t', '\n\t\t D. Litman and S. Silliman . \n\t', '\n\t\t 2004. ITSPOKE : An intelligent tutoring spoken dialogue system . \n\t', '\n\t\t In Companion Proc . \n\t', '\n\t\t of the Human Language Technology Conf . \n\t', '\n\t\t of the North American Chap . \n\t', '\n\t\t of the Assoc. for Computational Linguistics ( HLT/NAACL ) . \n\t', '\n\t\t D. J. Litman , C. P. Rose , K. Forbes-Riley , K. VanLehn , D. Bhembe , and S. Silliman . \n\t', '\n\t\t 2004. Spoken versus typed human and computer dialogue tutoring . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t Intelligent Tutoring Systems . \n\t', '\n\t\t B. Maeireizo-Tokeshi , D. Litman , and R. Hwa . \n\t', '\n\t\t 2004. Co-training for predicting emotions with spoken dialogue data . \n\t', '\n\t\t In Companion Proc . \n\t', '\n\t\t Assoc. for Computational Linguistics ( ACL ) . \n\t', '\n\t\t S. Narayanan . \n\t', '\n\t\t 2002. Towards modeling user behavior in human-machine interaction : Effect of errors and emotions . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ISLE Workshop on Dialogue Tagging for Multi-modal Human Computer Interaction . \n\t', '\n\t\t P-Y. Oudeyer . \n\t', '\n\t\t 2002. The production and recognition of emotions in speech : Features and Algorithms . \n\t', '\n\t\t International Journal of Human Computer Studies , 59(1-2):157\x96183 . \n\t', '\n\t\t I. Shafran , M. Riley , and M. Mohri . \n\t', '\n\t\t 2003. Voice signatures . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t IEEE Automatic Speech Recognition and Understanding Workshop . \n\t', '\n\t\t K. VanLehn , P. W. Jordan , C. P. Ros´e , D. Bhembe , M. B¨ottner , A. Gaydos , M. Makatchev , U. Pappuswamy , M. Ringenberg , A. Roque , S. Siler , R. Srivastava , and R. Wilson . \n\t', '\n\t\t 2002. The architecture of Why2-Atlas : A coach for qualitative physics essay writing . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t Intelligent Tutoring Systems . \n\t', '\n\t\t I. H. Witten and E. Frank . \n\t', '\n\t\t 1999. Data Mining : Practical Machine Learning Tools and Techniques with Java implementations . \n\t', '\n\t\t Building Verb Predicates : A Computational View Fernando Gomez Dept. of Computer Science University of Central Florida , Orlando , FL 32816 gomez@cs.ucf.edu Abstract A method for the definition of verb predicates is proposed . \n\t', '\n\t\t The definition of the predicates is essentially tied to a semantic interpretation algorithm that determines the predicate for the verb , its semantic roles and adjuncts . \n\t', '\n\t\t As predicate definitions are complete , they can be tested by running the algorithm on some sentences and verifying the resolution of the predicate , semantic roles and adjuncts in those sentences . \n\t', '\n\t\t The predicates are defined semiautomatically with the help of a software environment that uses several sections of a corpus to provide feedback for the definition of the predicates , and then for the subsequent testing and refining of the definitions . \n\t', '\n\t\t The method is very flexible in adding a new predicate to a list of already defined predicates for a given verb . \n\t', '\n\t\t The method builds on an existing approach that defines predicates for WordNet verb classes , and that plans to define predicates for every English verb . \n\t', '\n\t\t The definitions of the predicates and the semantic interpretation algorithm are being used to automatically create a corpus of annotated verb predicates , semantic roles and adjuncts . \n\t', '\n\t\t 1 Introduction This paper deals with the definition of verb predicates which will make possible the determination of verb meaning , semantic roles , adjuncts , and attachment and meaning of post- verbal PPs . \n\t', '\n\t\t Hence , the adequacy of the definitions is measured by comparing the output of a semantic interpretation algorithm with the solution of those semantic interpretation tasks on sentences randomly taken from any corpus , or typed by a user at the console . \n\t', '\n\t\t The algorithm , thus , must provide immediate feedback by testing the definitions on these randomly selected sentences . \n\t', '\n\t\t In \n\t\t']",Positive
"['\n\t\t The semantic roles of the predicates are linked to the selectional restrictions ( categories in WordNet ontology for nouns ) and the grammatical relations that realize them . \n\t', '\n\t\t The selectional restrictions of the predicates are solidly grounded on the WordNet ontology for nouns \n\t\t']",Positive
"['\n\t\t However , we have found out that the initial idea in that work of defining predicates for a WN verb class which will be also valid for most of the verbs under that class has proven to be too optimistic because many of the verb forms included under that class realize their semantic roles by different selectional restrictions and grammatical relations . \n\t', '\n\t\t This is due to the fact that many of the verbs under a given class have been grouped in many instances by some kind of implication , or troponymy \n\t\t']",Positive
"['\n\t\t Even in many cases , some of the verbs in the same synset list differ semantically and syntactically between them . \n\t', '\n\t\t For instance , the third sense of "" gain "" in WN is : "" profit , gain , benefit ( derive benefit from ) . \n\t', '\n\t\t "" The theme , the thing obtained , is syntactically realized by the PP [ from NP ] for two of the verbs listed in that synset "" benefit "" and "" profit , "" while the theme for "" gain "" is realized by a direct object . \n\t', '\n\t\t The differences in grammatical relations are even more prevailing within the verbs under one given class . \n\t', '\n\t\t But , in those cases in which the verb polysemy is not high , one predicate definition for the verb class may apply to many of the verb forms under that class . \n\t', '\n\t\t Notwithstanding these problems , WN verb classes have provided an important basis for the construction of a general ontology of predicates that will cover every English verb . \n\t', '\n\t\t Moreover , if one considers that there are 5752 verbs in WordNet 1.6 having only one sense and 2199 verbs having exactly two senses , the pred- icates that have been constructed for the verb classes of 7,951 verbs are very close to be done . \n\t', '\n\t\t However , the method explained in this paper deviates considerably from the WN approach to constructing verb meaning . \n\t', '\n\t\t In particular , it eschews the synset list in favor of predicate definitions for individual verbs as opposed to a list of synonymous verbs , and allows for an easy integration of a new predicate into a list of already defined predicates for a given verb . \n\t', '\n\t\t Briefly , the algorithm \n\t\t']",Positive
"['\n\t\t For every verb in a sentence , we provide a list of predicates for that verb . \n\t', '\n\t\t These predicates can be viewed as contenders for the meaning of the verb . \n\t', '\n\t\t The goals of the algorithm are to select one predicate from that list , thus determining the sense of the verb , identify its semantic roles , and adjuncts and attach post-verbal PPs . \n\t', '\n\t\t All these tasks are simultaneously achieved . \n\t', '\n\t\t For each grammatical relation ( GR ) in the clause ( starting with the NP complements ) and for every predicate in the list of predicates , the algorithm checks if the predicate explains the GR . \n\t', '\n\t\t A predicate explains a GR if there is a semantic role in the predicate realized by the grammatical relation and the selectional restrictions of the semantic role subsume the ontological category of the head noun of the grammatical relation . \n\t', '\n\t\t This process is repeated for each GR in the clause and each predicate in the list of predicates for the verb of the clause . \n\t', '\n\t\t Then , the predicate that explains the most GRs is selected as the meaning of the verb . \n\t', '\n\t\t The semantic roles of the predicate have been identified as a result of this process . \n\t', '\n\t\t In case of ties , the predicate that has the greatest number of semantic roles realized is selected . \n\t', '\n\t\t Every grammatical relation that has not been mapped to a semantic role must be an adjunct or an NP modifier . \n\t', '\n\t\t The entries for adjuncts are stored in the root node action or description ( for stative verbs ) and are inherited by all predicates in those categories . \n\t', '\n\t\t Adjuncts are identified after the predicate of the verb has been determined because adjuncts are not part of the argument structure of the predicate . \n\t', '\n\t\t In the next section , we will show how to define predicates for individual verbs as different from WN verb classes , how these predicates for individual verbs can reuse entries in the generic predicates for WordNet verb classes , and how they are integrated into the ontology of predicates that have been defined with the help of WN verb classes . \n\t', '\n\t\t Section 3 provides a discus sion of the semiautomatic construction of the predicates . \n\t', '\n\t\t Section 4 gives a view of the upper- level ontology of predicates , section 5 discusses the testing , and sections 6 and 7 related research and conclusions , respectively . \n\t', '\n\t\t 2 Defining Predicates for Individual Verbs The approach to defining predicates for individual verbs is similar to the way in which the senses for a verb are given in a dictionary . \n\t', '\n\t\t The difference lies in the level of detail provided . \n\t', '\n\t\t For each verb sense , one must provide : a ) a predicate and a hierarchy where to insert it , b ) the semantic roles for the predicate , and c ) the selectional restrictions and grammatical relations for each role . \n\t', '\n\t\t Of course , in the construction of the definitions one must keep in mind the interpretation algorithm because the definitions must determine verb meaning , semantic roles and adjuncts . \n\t', '\n\t\t Except for that , our approach is similar to a dictionary definition in the sense that the predicates for that verb are listed in the same way as the senses of a verb are listed in the dictionary . \n\t', '\n\t\t The lexical definition of the predicates is a frame-like representation containing the selectional restrictions followed by the grammatical relations for that role given those selectional restrictions . \n\t', '\n\t\t The syntax for a semantic role is : ( role ( ( <slr> ) ( <grs> ) ( <slr> ) ( <grs> ) ( <slr> ( <grs> ) ) ) ) Where < slr > stands for any number of selectional restrictions , and "" < grs > "" for any number of grammatical relations . \n\t', '\n\t\t The order in which the grammatical relations are listed is irrelevant . \n\t', '\n\t\t However , the order of the selectional restrictions is relevant in the sense that the first selectional restriction that matches is selected and the others in the list are not tried . \n\t', '\n\t\t Hence , the list of selectional restrictions is a preference list \n\t\t']",Positive
"['\n\t\t This preference list is critical in selecting the correct sense for many head nouns in the noun phrases of the verb arguments . \n\t', '\n\t\t A selectional restriction preceded by the sign "" - "" ( -slr ) means that the semantic role is not realized by that ontological category . \n\t', '\n\t\t The grammatical relations for PPs are represented by writing "" prep "" followed by the prepositions that realize the semantic role , e.g. ( prep about on ... ) . \n\t', '\n\t\t Some of the grammatical relations are : subj ( the sentence has a subject ) , subj-if-obj ( the sentence has a subject and a direct object ) , obj ( direct object ) , obj2 ( second post- verbal NP ) , cp ( any complement clause ) , cpinf ( an infinitival complement clause ) , cp-thatclause ( a complement clause introduced by a that clause ) , cp-ing ( a complement clause introduced by gerund ) , prep-cp ( a complement clause introduced by a preposition ) , etc. . \n\t', '\n\t\t Suppose that one wants to define predicates for the verb "" gain . \n\t', '\n\t\t "" The first three predicates are : ( gain [ gain-weight ;gain9 in WN ( is-a(c-change-state-of-animate)) ( agent(human animal)(subj-if-obj)) ( theme(weight_unitl )(obj)) ] [ gain-reach-a-location;gain4 in WN ( is-a(reach-a-location)) ( to-loc(location) ( obj ) ) ] [ gain-possession;gain8in WN ( is-a(transfer-of-possession) ) (theme(possession)(obj)) (from-poss(human-agent)((prep from ) ) ) ( source-t(-human-agent thing ) ( ( prep from))(prep-cp by from ) ) ) ] We have included the WN verb senses following the predicate , but they can be omitted . \n\t', '\n\t\t The reason for putting them is to produce an output that can be checked against WN verb senses ( see Section 5 ) . \n\t', '\n\t\t We could have also included the sense numbers of Longman Dictionary of Contemporary English ( Longman Group , UK Limited ) , one of the dictionaries that we have consulted in constructing the predicates . \n\t', '\n\t\t The first predicate gain-weight is a subpredicate of cchange-state-of-animate , cause-change-of-stateof-animate-being , which has been already defined for a class of WN verbs . \n\t', '\n\t\t In fact , one does not need to indicate the agent role because it is already indicated in its parent predicate . \n\t', '\n\t\t The only roles that one needs to list are those that differ from those in its immediate super- predicate . \n\t', '\n\t\t The predicate gain-reach-a-location is a subpredicate of reach-a-location , which has been already defined in principle for all WN verb forms that fall under the synsets "" reachl "" and "" scale4 . \n\t', '\n\t\t "" The to-loc role , which is identical to the parent predicate , does not need to be listed either . \n\t', '\n\t\t Hence , all the work already done in defining predicates for WN classes can be reused in defining predicates for individual verbs which , because of their high degree of polysemy , cannot be handled by the predicates defined for WN verb classes . \n\t', '\n\t\t The predicate gain-possession expresses the sense of the verb when the things gained are money , assets , etc. . \n\t', '\n\t\t When the source of the things transferred are humans or social groups ( "" human-agent "" stands for humans or social groups ) the semantic role is called from-pons , e.g. . \n\t', '\n\t\t "" She gained much money from the firm . \n\t', '\n\t\t "" The role source-t is used when the source is not a human-agent , e.g , "" He gained much money from the mines or from/by selling the estate . \n\t', '\n\t\t "" The preposition that realizes this role is "" from . \n\t', '\n\t\t "" The prep-cp stands for a complement clause introduced by a preposition(s) . \n\t', '\n\t\t In this case , the prepositions "" from "" ( from selling ) and "" by "" ( by selling ) . \n\t', '\n\t\t The -human-agent in the source-t indicates that this role is not realized by the ontological category of human- agent . \n\t', '\n\t\t The next two : [ gain-something;gainl,gain2,gain3 in WN ( is-a(transfer-of-something) ) ( theme(thing) ( obj ) ) (from-poss(human-agent)((prep from ) ) ) ( source-t(-human-agent thing ) ( ( prep from))(prep-cp by from ) ) ) ] [ gain-increase;gain6,gain7 in WN ( is-a(increase)) (agent(nil)(nil)) ( theme(substance magnitude process action thing ) ( obj ) ) ( belonging-to:theme(-human-agent -animal thing ) ( subj-if-obj ) ) ] The predicate gain-something is the most gen- eral , because the selectional restriction of its theme , thing , is the most general concept in the ontology . \n\t', '\n\t\t This predicate will be also se- lected by the algorithm as the second choice for the verb "" gain "" in "" She gained much money from the lottery . \n\t', '\n\t\t "" The first predicate will be gain-possession and the second predicate gain- something . \n\t', '\n\t\t If one does not want this to hap- pen and limit the choices just to one predicate , namely gain-possession , for this sentence , one needs to rewrite the entry for the theme of gain- something as : ( theme ( -possession thing ) ( obj ) ) . \n\t', '\n\t\t This will exclude the things that have possession as a hypernym , or superconcept . \n\t', '\n\t\t The predicate gain-something is a default predicate for "" gain . \n\t', '\n\t\t "" That predicate contains implicitly many predicates that are unanalyzed , in particular those whose theme is a nominal , e.g. , "" gain control/independence/success , etc. . \n\t', '\n\t\t "" The meaning of these predicates can be inferred , or generated from the meaning of the nominal . \n\t', '\n\t\t The predicate gain-increase is a subpredicate of increase . \n\t', '\n\t\t The entry "" ( agent(nil) ( nil ) ) "" means that this predicate does not have an agent , so it will not inherit this role from any superpredicate . \n\t', '\n\t\t The meaning of this predicate expresses those propositions in which entities other than humans or animals gain something . \n\t', '\n\t\t The role belonging-to:theme means that the filler of this role is in the relationship of belonging-to to the filler of the role theme . \n\t', '\n\t\t The relation belonging- to is a very general relationship including many other relations such as at-loc , attribute , and others . \n\t', '\n\t\t This role is also shared by other predicates . \n\t', '\n\t\t Thus , the analysis of the sentence "" the pond gained water "" is something caused the water belonging-to the pond to increase . \n\t', '\n\t\t In this sentence , the belonging-to relation stands for at-loc , something that can be inferred by additional analysis of the ontology of the arguments of the relation . \n\t', '\n\t\t A similar analysis is produced for "" the object gained speed uniformly . \n\t', '\n\t\t "" Both sentences can be rephrased by saying "" the water in the pond increased , "" and "" the speed of the object increased . \n\t', '\n\t\t "" Order of the Predicates As indicated , the order of the predicates is relevant in the sense that , in case of ties ( two or more predicates realize all their semantic roles ) , they will be preferred as the predicate for that verb in the order in which they are defined . \n\t', '\n\t\t This order has nothing to do with frequency of senses for a given verb , but rather with the generality of the ontological categories in the selectional restrictions of the semantic roles . \n\t', '\n\t\t For instance , had the predicate gain-weight been defined after gain-something , gain-weight would have been ranked second after gain-something as the predicate of "" gain "" in the sentence "" The teacher gained weight . \n\t', '\n\t\t "" If the ontological category of the selectional restriction of at least one role , says , rl , in predicate , say , pi , is not in the same path as the selectional restriction of role rl in predicate pi , then it makes no difference in which order the predicates are defined . \n\t', '\n\t\t That is the case for gain-weight and gain-reacha-location , because the ontological categories of their themes are not in the same path . \n\t', '\n\t\t The definition of the predicates is provided without any knowledge about what could be the meaning of the nouns in the sentence . \n\t', '\n\t\t The goal is that the definition of the predicates help determine the senses of the nouns as much as possible . \n\t', '\n\t\t For instance , if one types the sentence "" the kite flew into the sky , "" the system selects two predicates for "" fly "" : the first one : fly which is a subpredicate of change-location , ( an animate being changes location from a place to another ) . \n\t', '\n\t\t This is so because one of the senses of "" kite "" in WN is a hawk(kite2) . \n\t', '\n\t\t Thus , kite2 is selected as the agent of fly . \n\t', '\n\t\t The second predicate is cause-to-fly ( a subpredicate of causeto-change-location ) , in which "" kite "" is taken as plaything , a toy , ( kitel ) . \n\t', '\n\t\t In this case , "" kitel "" is the theme and the agent or inanimate-cause are unknown . \n\t', '\n\t\t During one of our demos , somebody typed the sentence "" She left the keys in her car , "" and the system came up with the following two predicates for "" left "" : leave-a-place and leave-behind . \n\t', '\n\t\t Our first reaction was that something had gone very wrong in selecting the predicate leave-a-place . \n\t', '\n\t\t But upon looking into it , we found out that one of the senses for "" keys "" in WN is the "" Florida keys , "" a location . \n\t', '\n\t\t Hence , the interpretation of the system for "" left "" was correct , given that sense for "" keys , "" : "" somebody left a place ( the Keys ) using her car as the instrument for the change of location . \n\t', '\n\t\t 3 A software Environment for the Semiautomatic Definition of Predicates We have developed a software environment that has the following components : a parser , the semantic interpreter that uses the predicates to interpret the sentence , a corpus ( The World Book Encyclopedia , World Book , Inc. . \n\t', '\n\t\t Chicago ) divided into different sections for defining , refining and testing the predicate definitions , a skimmer that searches for sentences in the corpus containing the verb for which predicates are being defined , and a mechanism for dragging in sentences from the corpus into the system for testing . \n\t', '\n\t\t An interface has been also implemented for those who abhor Lisp parentheses . \n\t', '\n\t\t The interface interactively asks for the semantic roles , the selectional restrictions and the grammatical relations for a given predicate . \n\t', '\n\t\t Suppose that one wants to define predicates for the three senses of the verb "" confuse "" provided by Longman , namely "" to cause to be mixed up in the mind , "" "" to mix up in one \'s mind ( with ) "" and "" to put in disorder , make less clear or more difficult to deal with .. "" pl:[cause-to-confuse-perplex ( is-a(c-change-state-of-animate)) ( agent(human-agent animal ) ( subj-if-obj ) ) ( theme(human-agent animal)(obj)) ( inanimate-cause(thing) ( subj-if-obj )1 ] p2 : [ confuse-something-with-something ( is-a(misunderstand)) ( agent(human-agent animal ) ( subj-if-obj ) ) ( theme ( thing ) ( obj ) ) (co-theme(thing)((prep with ) ) ) ( require(co-theme)) ] P3 : [ confuse-issue-muddy ( is-a ( obscure-ideas-information ) ) ( agent(human-agent animal ) ( subj-if-obj ) ) ( theme(-physical-thing communication thing ) ( obj ) ) ] The first predicate is straightforward . \n\t', '\n\t\t The inanimate-cause is needed because anything in principle can cause to confuse a living thing , e.g. , "" The light confused the bird and was trapped , "" or "" These questions confuse even the experts . \n\t', '\n\t\t "" The second predicate has no restrictions in the roles theme and co-theme . \n\t', '\n\t\t Anything can in principle be confused with anything else . \n\t', '\n\t\t The require slot can be used in any predicate to indicate that in order for that predicate to be selected by the algorithm as the meaning of the verb , the role expressed in the require slot must have been filled by the algorithm . \n\t', '\n\t\t This is not absolutely needed in this case , but it minimizes the number of predicates selected for a verb . \n\t', '\n\t\t Note that , if one removes the require slot from p2 , and one types the sentence "" He confused the teacher , "" predicates p1 and p2 will be selected because both predicates will explain the two grammatical relations in the sentence . \n\t', '\n\t\t However , if one types "" She confused the crane with a heron , "" only p2 will be selected because p2 will explain the PP [ with a heron ] and p1 will not be able to . \n\t', '\n\t\t The selectional restriction of the theme for predicate p3 is anything that is not a physical-thing in the ontology , preferring those concepts having communication as a hypernym . \n\t', '\n\t\t There are two ways in which one can proceed in the definition of these predicates . \n\t', '\n\t\t One way is to provide the definitions given above using the interface , which reduces to responding to some commands/questions by the interface , namely "" enter role , "" "" enter selectional restrictions , "" "" enter grammatical relations for that role , "" "" done with this role ? \n\t', '\n\t\t "" etc. . \n\t', '\n\t\t Then , once the definitions are completed one can skim a section of the corpus for sentences containing the verb for which predicates are being defined and try them ( parse and interpret them ) in the system in order to test the definitions provided . \n\t', '\n\t\t Most sentences are parsed and interpreted in few seconds . \n\t', '\n\t\t Another way is to skim the corpus for sentences containing the verb for which predicates are being defined , provide tentative definitions based on the sentences retrived by the skimmer , and then refine and test the definitions by searching the section of the corpus for refining predicates , and loop as needed . \n\t', '\n\t\t The interface provides with all kinds of functions for searching the noun ontology , which facilitates greatly the task of defining the predicates . \n\t', '\n\t\t The placing of a predicate in the hierarchy of predicates is a semiautomatic process . \n\t', '\n\t\t The interface has a function that displays all the predicates already built for all the WN senses for a given verb . \n\t', '\n\t\t There are also functions that display the content of the predicates as well as all their superpredicates and subpredicates . \n\t', '\n\t\t This makes the insertion a rather simple task . \n\t', '\n\t\t Adding a New Predicate Suppose that one wants to add a new predicate to a list of predicates for a given verb which one thought it was complete . \n\t', '\n\t\t One may discover a new sense that WN or the dictionary has missed , something not uncommon . \n\t', '\n\t\t This is as simple as adding the new predicate definition to the list of predicates for that verb . \n\t', '\n\t\t Suppose that one wants to add the predicate gain-a-reputation to the list of predicates for the verb "" gain , "" because the verb "" gain "" is frequently used with this concept . \n\t', '\n\t\t This predicate is clearly a refinement of the predicate gain- something . \n\t', '\n\t\t Hence it needs to be inserted before it . \n\t', '\n\t\t Prior to introducing the new predicate one may want to check if there is a predicate whose theme contains the selectional restriction of "" reputation , "" or one of its hypernyms . \n\t', '\n\t\t This is a task fully automated . \n\t', '\n\t\t One needs only to type : ( find-predicates ( theme reputationl ) ) . \n\t', '\n\t\t This will retrieve all predicates whose theme has reputationl , or one of its hypernyms as its selectional restriction . \n\t', '\n\t\t In this case , the predicate establisha-reputation that has standingl as the selectional restriction for the theme will be retrieved . \n\t', '\n\t\t So , one needs only to insert the predicate gain- a-reputation immediately before gain-something as follows : ( gain-a-reputation ( is-a(establish-a-reputation))) 4 A Panorama of the Upper-level Ontology of Predicates We have defined 3,017 predicates . \n\t', '\n\t\t Some of them are very complex , while others contain just two or three semantic roles because other roles are inherited from their superpredicates . \n\t', '\n\t\t The major classes and some of their subclasses are briefly described . \n\t', '\n\t\t The first class is cause-change-of-state with 609 subpredicates . \n\t', '\n\t\t This class contains major subclasses with very little relation to each other . \n\t', '\n\t\t Some major subclasses ( the number of predicates un- der each class are given in parentheses following the class ) are cause-change-of-state-of- animate-being ( 140 subpredicates ) which in turn has subclasses such as arouse-feelings-emotions ( 52 ) cause-to-act ( 19 ) , ièjuåó-òuåä-sæêóbædé-æå- æèósólf ( 18 ) . \n\t', '\n\t\t Other major subclasses of causechange-of-state are : increase ( 31 ) , improve ( 19 ) , worsen ( 10 ) , terminate ( 16 ) , physical-change-ofstate ( 14 ) ( e.g. , solidify , li^uefy , etc. ) cause- change-of-integrity ( 22 ) , transform ( 9 ) and others . \n\t', '\n\t\t The next predicate class is cause-tochange-location with 379 predicates . \n\t', '\n\t\t The primary event expressed by this predicate is a cause of change of location of something other than the agent ; although the agent may have also changed location . \n\t', '\n\t\t In "" Linda carried the books to the library "" the agent also has been moved , but the primary event is the fact that Linda caused a change of location of the books . \n\t', '\n\t\t In a sentence such as "" The moon circles the earth , "" "" the moon "" is the theme and the agent or inanimate-cause is unknown . \n\t', '\n\t\t The predicate cause-to-change-location is not to be confused with change-location ( see next class ) in which the agent and the theme are the same animate being , e.g. , "" Kelly went to the store . \n\t', '\n\t\t "" The major subclasses of cause-to-change-location are : put ( 75 ) , remove ( 53 ) , transport ( 23 ) , propel ( 20 ) , connect ( 22 ) , flow ( 12 ) , and pull , push and send with 9 predicates each . \n\t', '\n\t\t The next class is change-location with 238 predicates . \n\t', '\n\t\t Major subclasses are : walk ( 14 ) ( some subpredicates of walk are hike , march , sneak-walk , and others ) , enter ( 10 ) , leave-aplace ( 11 ) , travel , arrive-to-a-place ( 10 ) and others . \n\t', '\n\t\t The next class is interact with 372 predicates . \n\t', '\n\t\t Whithin this class , the major subclasses are : communicate ( 243 ) , treat-an-animal-orhuman ( 26 ) , whose mayor subpredicates are treat-unjustly-somebody , and behave ( 9 ) , jæiè-agåæuç-æå- a-human ( 9 ) and others . \n\t', '\n\t\t The next class is transfer-of-something with 293 . \n\t', '\n\t\t Its major class is transfer-ofpossession ( 231 ) , which in turn contains such subpredicates as give ( 31 ) , get ( 132 ) and others . \n\t', '\n\t\t The next class is make-or-create-something with 144 predicates . \n\t', '\n\t\t Some major subclasses are initiate-something ( 30 ) , create-art ( 28 ) , produce ( 10 ) , prepare-something ( 10 ) and others . \n\t', '\n\t\t Another major class is judge with 182 predicates . \n\t', '\n\t\t Some of its subpredicates are pass-a-negative-judgment-on ( 36 ) ( e.g. , disapprove , oppose-ideas , oppose-people , and others . \n\t', '\n\t\t ) , put-value-to-something ( 12 ) , praise ( 7 ) , accept-admit-a-fact ( 30 ) , confirm-corroborate ( 11 ) , deny-something-to-somebody ( 14 ) , accuse ( 11 ) and others . \n\t', '\n\t\t The next class is experi- ence with 110 predicates . \n\t', '\n\t\t Some of its subclasses are : fóól-iè-a-säaäó-æå-óêæäiæès ( 25 ) , perceive ( 21 ) , like-something ( 36 ) , experienceevent-state-abstraction ( 11 ) and others . \n\t', '\n\t\t The next class is think with 115 predicates . \n\t', '\n\t\t Some of its major subclasses are analyze ( 31 ) , plan ( 13 ) , reason-conclude ( 9 ) , associate ( 7 ) and others . \n\t', '\n\t\t The next class is decide with 92 predicates . \n\t', '\n\t\t Its major subclass is exert-control-over ( 65 ) , which in turn contains major subclasses such as restrain(22) and manage ( 23 ) . \n\t', '\n\t\t Another class is touch with 55 predicates . \n\t', '\n\t\t Its major subclasses are handle-operate ( 18 ) and hit- something ( 24 ) . \n\t', '\n\t\t Another major class with 52 predicates is spend-something , which has ingest ( 31 ) as one of its main subclasses . \n\t', '\n\t\t Some other predicate classes are : prevent ( 45 ) , move-body-position ( 33 ) , know ( 31 ) , fail-to-do-something ( 33 ) , fight ( 31 ) , expelsubstance-from-the-body ( 23 ) , do-act ( 27 ) , appoint-somebody ( 15 ) , allow-something ( 21 ) , support-something ( 24 ) , succeed ( 23 ) , and utilize ( 11 ) . \n\t', '\n\t\t To these we need to add 223 predicates for stative verb senses ( e.g. be-at-aplace , include , etc. ) 5 Testing We have tested the predicates in 500 sentences taken from the The World Book Encyclopedia . \n\t', ""\n\t\t The manually corrected output of the semantic interpreter on these 500 sentences has been made public on November 26 , 03 on the Table 1 : Statistics for the Sentences Predicate for the Verb 87 % Semantic Roles : 91 % Adjuncts : 82 % Post Verbal PP attachment : 89 % authors ' homepage ( www.cs.ucf.edu/ gomez ) . \n\t"", '\n\t\t These sentences and their subclauses contain close to 400 distinct verbs . \n\t', '\n\t\t Many of these verbs appear in the 500 sentence corpus several times with different senses , while others appear just one or two times . \n\t', '\n\t\t Space limitations prevent us from listing the verbs in this paper . \n\t', '\n\t\t Table 1 summarizes the results of the semantic interpreter on these 500 sentences . \n\t', '\n\t\t We counted as a correct predicate for the verb only the first predicate selected by the algorithm . \n\t', '\n\t\t Semantic roles were computed only for those predicates which were correctly selected by the algorithm . \n\t', '\n\t\t As one can see , the results are very good , and , as a consequence , the manual correction of the errors in the sentences takes very little time . \n\t', '\n\t\t In fact , we have made public another set of 500 sentences on March 24 , 2004 . \n\t', '\n\t\t This new set of sentences tests many verbs that did not appear in the initial corpus , and which exhibit many different senses . \n\t', '\n\t\t The preliminary results on this new set are comparable to the results for the first set . \n\t', '\n\t\t The selectional restrictions are aimed at capturing the possible noun categories for a given predicate , and are not biased to any corpus . \n\t', '\n\t\t The main reasons for failing to assign meaning to a constituent are : over-specification or over-generalization of the selectional restrictions , missing a grammatical relation , sense not contemplated in the predicate definitions for that verb , and insufficient information in the sentence to unequivocally determine the predicate of the verb . \n\t', '\n\t\t The semantic interpreter has become an excellent tool to automatically build a corpus of annotated predicates , semantic roles and adjuncts , requiring very little human effort in validating and correcting the final output . \n\t', '\n\t\t Our additions and restructuring to the upper level noun ontology together with the preference list in the selectional restrictions is making possible for the interpreter to resolve many noun senses , which otherwise would have remained unresolved . \n\t', '\n\t\t Figure 1 depicts the output of the interpreter for one of the sentence in the initial corpus . \n\t', '\n\t\t The interpretation of the sentences in the corpus include relativization , ( Clause CL364 ( SUBJ:((ADJ MANY)(ADJ WHITE)(NOUN TRADERS ) ) ( MERCHANTI TRADERI ) <(AGENT)> )(ADVERB : ( ( ADVERB LATER ) ) LATER )(VERB : OPENED <OPEN-AN-ENTERPRISE:(OPEN2) supported by 2 SRs> )(OBJ : ( ( NOUN POSTS ) ) ( POSITION6 POST3 ) <(THEME)>) ( PREP : ON ( PREP-NP : ( ( PN INDIAN ) ( NOUN RESERVATIONS ) ) ( RESERVATIONI INDIAN-RESERVATIONI ) <(AT-LOC)>) Attach : Verb Confidence : WEAK ) )AND ( Clause CL360 ( SUBJ : ( ( ADJ MANY ) ( ADJ WHITE ) ( NOUN TRADERS ) ) ( MERCHANTI TRADERI ) <(AGENT)> )(ADVERB : ( ( ADVERB LATER ) ) LATER )(VERB : CHARGED <CHARGE-BILL:(CHARGE3) supported by 2 SRs> ) ( OBJ : ( ( ADJ UNFAIR ) ( NOUN PRICES ) ) ( POSSESSION PRICEI PRICE6 ) <(THEME)> ) ) Figure 1 : Interpreter output for Many white traders later opened posts on Indian reservations and charged unfair prices complement phrases , coordination , subordination and adverbial clauses . \n\t', '\n\t\t The grammatical relation is listed first followed by the semantic role for that grammatical relation in the form < ( semantic \x97 role ) > . \n\t', '\n\t\t Hence , the pair <> ( placed to the right of the grammatical relation ) identifies semantic roles . \n\t', '\n\t\t The program prints the predicate followed by the WN verb senses and the phrase "" supported by ` a-number \' SRs , "" ( where ` a-number \' stands for a cardinal num- ber ) , which indicates the number of grammatical relations ( SRs ) that have realized semantic roles . \n\t', '\n\t\t If the output for that sentence has more semantic roles than the number listed in the phrase "" supported by , "" those semantic roles are adjuncts . \n\t', '\n\t\t Prepositions can be attached to the verb with a confidence of strong or weak . \n\t', '\n\t\t All post-verbal PPs deemed to be arguments of the verb are attached as strong , while all PPs that are deemed adjuncts can be attached as strong or weak . \n\t', '\n\t\t The WN sense number for the verb is printed in parentheses immediately following the predicate . \n\t', '\n\t\t Sometimes more than one WN verb sense is given for a predicate . \n\t', '\n\t\t For instance , for the predicate travel-change-location given for the verb "" travel "" in the sentence "" She traveled to Spain , "" the following WN senses of "" travel "" are listed ( travel1 travel4 travel5 ) . \n\t', '\n\t\t This means that all WN senses of "" travel "" in that list are coalesced into the predicate travel-change-location \n\t\t']",Positive
"['\n\t\t Hence , all these WN senses of the verb have the same meaning . \n\t', '\n\t\t The WN noun senses follow the noun in parentheses . \n\t', '\n\t\t In some instances , the algorithm may provide more than one sense for a noun . \n\t', '\n\t\t To the contrary to verb senses , this list is a ranked list in which the first noun sense listed by the algorithm is the preferred one . \n\t', '\n\t\t Besides this output , the system lists all the superpredicates of the predicates selected for the verbs in the sentence . \n\t', '\n\t\t Thus , open-an-enterprise is-a initiate-an-enterpriseproject is-a organize is-a initiate-something is- a give-rise-to-something is-a make-or-createsomething , and charge-bill is-a transfer-ofpossession is-a transfer-of-something is-a action . \n\t', '\n\t\t 6 Related Research This work has benefited from several sources . \n\t', '\n\t\t In linguistics , the work reported in \n\t\t']",Positive
"['\n\t\t In computational linguistics , the VerbNet project \n\t\t']",Positive
"['\n\t\t Major differences are that our project aims at defining predicates for every English verb in a systematic manner , linking the selectional re- strictions for their semantic roles to a well es- tablished and widely used ontology for nouns , namely WordNet , and placing them in a hierar- chy of predicates where inferences and semantic roles can be inherited . \n\t', '\n\t\t Moreover , the definition of our predicates can be tested by running the semantic interpretation algorithm on any cor- pus or on sentences typed by the user at the console . \n\t', '\n\t\t 7 Conclusions An approach to building verb predicates has been presented . \n\t', '\n\t\t The construction of the predicates is essentially linked to an algorithm for determining the semantic roles of the predicates . \n\t', '\n\t\t The definitions of the predicates are done semi- automatically and are being refined and tested with the help of a semantic interpretation algorithm that uses the definitions for determining verb meaning and semantic roles for sentences selected from a corpus , or entered at the terminal . \n\t', '\n\t\t The algorithm and the predicates are being used to automatically construct a corpus of semantic annotated sentences . \n\t', '\n\t\t References Hoan Trang Dang , Karin Kipper , Martha Palmer , and Joseph Rosenzweig . \n\t', '\n\t\t 1998. Investigating regular sense extensions based on intersective levin classes . \n\t', '\n\t\t In In COLING-ACL , pages 293-299 , Montreal , Quebec . \n\t', '\n\t\t C. Fellbaum . \n\t', '\n\t\t 1998. A semantic network of english verbs . \n\t', '\n\t\t In C. Fellbaum , editor , WordNet : An electronic Lexical Database and some of its applications , page chapter 3. MIT Press , 1998 , Cambridge , Mass. C. J. Fillmore , C. R. Johnson , and M. R.L. Petruck . \n\t', '\n\t\t 2003. Background to framenet . \n\t', '\n\t\t In- ternational Journal of Lexicography , 16:235- 250 . \n\t', '\n\t\t F. Gomez . \n\t', '\n\t\t 2001. An algorithm for aspects of semantic interpretation using an enhanced wordnet . \n\t', '\n\t\t In Proccedings of the 2nd North American Meeting of the North American Association for Computational Linguistics , NAACL-2001 , pages 87-94 . \n\t', '\n\t\t F. Gomez . \n\t', '\n\t\t 2004 . \n\t', '\n\t\t Grounding the ontology on the semantic interpretation algorithm . \n\t', '\n\t\t In Proceedings of the Second International WordNet Conference , pages 124-129 , Masaryk University , Brno . \n\t', '\n\t\t J. Grimshaw . \n\t', '\n\t\t 1990. Argument Structure . \n\t', '\n\t\t MIT Press , Cambridge , Mass. George Miller . \n\t', '\n\t\t 1998. Nouns in wordnet . \n\t', '\n\t\t In C. Fellbaum , editor , WordNet : An electronic Lexical Database and some of its applications , page chapter 1. MIT Press , 1998 , Cambridge , Mass. S. Pinker . \n\t', '\n\t\t 1989. Learnability and Cognition . \n\t', '\n\t\t MIT Press , Cambridge , Mass. B. L. Pritchett . \n\t', '\n\t\t 1992. Grammatical Competence and Parsing Performance . \n\t', '\n\t\t The University of Chicago Press , Chicago,Illinois . \n\t', '\n\t\t Y.A. Wilks . \n\t', '\n\t\t 1975. Preference semantics . \n\t', '\n\t\t In E.L. Keenan , editor , Formal Semantics of Natural Language . \n\t', '\n\t\t Cambridge University Press , Cambridge , UK . \n\t', '\n\t\t Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II Treebank Ruth O\x92Donovan , Michael Burke , Aoife Cahill , Josef van Genabith , Andy Way National Centre for Language Technology and School of Computing Dublin City University Glasnevin Dublin 9 Ireland {rodonovan,mburke,acahill,josef,away}@computing.dcu.ie Abstract In this paper we present a methodology for extracting subcategorisation frames based on an automatic LFG f-structure annotation algorithm for the Penn-II Treebank . \n\t', '\n\t\t We extract abstract syntactic function-based subcategorisation frames ( LFG semantic forms ) , traditional CFG category- based subcategorisation frames as well as mixed function/category-based frames , with or without preposition information for obliques and particle information for particle verbs . \n\t', '\n\t\t Our approach does not predefine frames , associates probabilities with frames conditional on the lemma , distinguishes between active and passive frames , and fully reflects the effects of long-distance dependencies in the source data structures . \n\t', '\n\t\t We extract 3586 verb lemmas , 14348 semantic form types ( an average of 4 per lemma ) with 577 frame types . \n\t', '\n\t\t We present a large-scale evaluation of the complete set of forms extracted against the full COMLEX resource . \n\t', '\n\t\t 1 Introduction Lexical resources are crucial in the construction of wide-coverage computational systems based on modern syntactic theories ( e.g. LFG , HPSG , CCG , LTAG etc. ) . \n\t', '\n\t\t However , as manual construction of such lexical resources is time-consuming , error- prone , expensive and rarely ever complete , it is often the case that limitations of NLP systems based on lexicalised approaches are due to bottlenecks in the lexicon component . \n\t', '\n\t\t Given this , research on automating lexical acquisition for lexically-based NLP systems is a particularly important issue . \n\t', '\n\t\t In this paper we present an approach to automating subcategorisation frame acquisition for LFG \n\t\t']",Positive
"['\n\t\t LFG has two levels of structural representation : c(onstituent)- structure , and f(unctional)-structure . \n\t', '\n\t\t LFG differentiates between governable ( argument ) and non- governable ( adjunct ) grammatical functions . \n\t', '\n\t\t Sub- categorisation requirements are enforced through semantic forms specifying the governable grammatical functions required by a particular predicate ( e.g. FOCUS((T SUBJ)(T OBL , , , , , ) ) ) . \n\t', '\n\t\t Our approach is based on earlier work on LFG semantic form extraction ( van Genabith et al. , 1999 ) and recent progress in automatically annotating the Penn-II treebank with LFG f-structures \n\t\t']",Positive
"['\n\t\t Depending on the quality of the f-structures , reliable LFG semantic forms can then be generated quite simply by recursively reading off the subcategorisable grammatical functions for each local pre d value at each level of embedding in the f-structures . \n\t', '\n\t\t The work reported in ( van Genabith et al. , 1999 ) was small scale ( 100 trees ) , proof of concept and required considerable manual annotation work . \n\t', '\n\t\t In this paper we show how the extraction process can be scaled to the complete Wall Street Journal ( WSJ ) section of the Penn-II treebank , with about 1 million words in 50,000 sentences , based on the automatic LFG f-structure annotation algorithm described in \n\t\t']",Positive
"['\n\t\t In addition to extracting grammatical function-based subcategorisation frames , we also include the syntactic categories of the predicate and its subcategorised arguments , as well as additional details such as the prepositions required by obliques , and particles accompanying particle verbs . \n\t', '\n\t\t Our method does not predefine the frames to be extracted . \n\t', '\n\t\t In contrast to many other approaches , it discriminates between active and passive frames , properly reflects long distance dependencies and assigns conditional probabilities to the semantic forms associated with each predicate . \n\t', '\n\t\t Section 2 reviews related work in the area of automatic subcategorisation frame extraction . \n\t', '\n\t\t Our methodology and its implementation are presented in Section 3 . \n\t', '\n\t\t Section 4 presents the results of our lexical extraction . \n\t', '\n\t\t In Section 5 we evaluate the complete extracted lexicon against the COMLEX resource \n\t\t']",Positive
"['\n\t\t To our knowledge , this is the largest evaluation of subcategorisation frames for English . \n\t', '\n\t\t In Section 6 , we conclude and give suggestions for future work . \n\t', '\n\t\t 2 Related Work Creating a ( subcategorisation ) lexicon by hand is time-consuming , error-prone , requires considerable linguistic expertise and is rarely , if ever , complete . \n\t', '\n\t\t In addition , a system incorporating a manually constructed lexicon cannot easily be adapted to specific domains . \n\t', '\n\t\t Accordingly , many researchers have attempted to construct lexicons automatically , especially for English . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t The frames do not include details of specific prepositions . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t Manning feeds the output from a stochastic tagger into a finite state parser , and applies statistical filtering to the parsing results . \n\t', '\n\t\t He predefines 19 different subcategorisation frames , including details of prepositions . \n\t', '\n\t\t Applying this technique to approx . \n\t', '\n\t\t 4 million words of New York Times newswire , Manning acquires 4900 sub- categorisation frames for 3104 verbs , an average of 1.6 per verb . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t In addition , all prepositional phrases are treated as adjuncts . \n\t', '\n\t\t For 1565 tokens of 33 selected verbs , they report an accuracy rate of 83 % . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t attempt to derive relative subcategorization frequency for individual predicates\x94 . \n\t', '\n\t\t In contrast , the system of \n\t\t']",Positive
['\n\t\t More recent work by \n\t\t'],Positive
"['\n\t\t Korhonen experiments with the use of linguistic verb classes for obtaining more accurate back-off estimates for use in hypothesis selection . \n\t', '\n\t\t Using this extended approach , the average results for 45 semantically classified test verbs evaluated against hand judgements are precision 87.1 % and recall 71.2 % . \n\t', '\n\t\t By comparison , the average results for 30 verbs not classified semantically are precision 78.2 % and recall 58.7 % . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t The extracted frames do not contain details of prepositions . \n\t', '\n\t\t More recently , a number of researchers have applied similar techniques to derive resources for other languages , especially German . \n\t', '\n\t\t One of these , \n\t\t']",Positive
"['\n\t\t Using sentences of limited length , she extracts 38 distinct frame types , which contain maximally three arguments each . \n\t', '\n\t\t The frames may optionally contain details of particular prepositional use . \n\t', '\n\t\t Her evaluation on over 3000 frequently occurring verbs against the German dictionary Duden - Das Stilw¨orterbuch is similar in scale to ours and is discussed further in Section 5 . \n\t', '\n\t\t There has also been some work on extracting subcategorisation details from the Penn Treebank . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t They manually examined the 150+ possible sequences of tags , both functional and categorial , in Penn-II and determined whether the sequence in question denoted a modifier , argument or optional argument . \n\t', '\n\t\t Arguments were then mapped to traditional syntactic functions . \n\t', '\n\t\t As they do not include an evaluation , currently it is impossible to say how effective this technique is . \n\t', '\n\t\t \n\t\t']",Positive
['\n\t\t Both techniques implement variations on the approaches of \n\t\t'],Positive
['\n\t\t In the case of \n\t\t'],Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t For each tree , the algorithm annotates the nodes with CCG categories in a top- down recursive manner . \n\t', '\n\t\t In order to examine the coverage of the extracted lexicon in a manner similar to \n\t\t']",Positive
"['\n\t\t It was found that the reference CCG lexicon contained 95.09 % of the entries in the test lexicon , while 94.03 % of the entries in the test TAG lexicon also occurred in the reference lexicon . \n\t', '\n\t\t Both approaches involve extensive correction and clean-up of the treebank prior to lexical extraction . \n\t', '\n\t\t 3 Our Methodology The first step in the application of our methodology is the production of a treebank annotated with LFG f-structure information . \n\t', '\n\t\t F-structures are feature structures which represent abstract syntactic information , approximating to basic predicate-argumentmodifier structures . \n\t', '\n\t\t We utilise the automatic annotation algorithm of \n\t\t']",Positive
"['\n\t\t Trees are traversed top-down , and annotation is driven by categorial , basic configurational , trace and Penn-II functional tag information in local subtrees of mostly depth one ( i.e. CFG rules ) . \n\t', '\n\t\t The annotation procedure is dependent on locating the head daughter , for which the scheme of \n\t\t']",Positive
"['\n\t\t The head is annotated with the LFG equation T= J. Linguistic generalisations are provided over the left ( the prefix ) and the right ( suffix ) context of the head for each syntactic category occurring as the mother node of such heads . \n\t', '\n\t\t To give a simple example , the rightmost NP to the left of a VP head under an S is likely to be its subject ( T SUBJ =J ) , while the leftmost NP to the right of the V head of a VP is most probably its object ( T OBJ =J ) . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t Distinguishing between argument and adjunct is an inherent step in the automatic assignment of functional annotations . \n\t', '\n\t\t The satisfactory treatment of long distance dependencies by the annotation algorithm is imperative for the extraction of accurate semantic forms . \n\t', '\n\t\t The Penn Treebank employs a rich arsenal of traces and empty productions ( nodes which do not realise any lexical material ) to co-index displaced material with the position where it should be interpreted semantically . \n\t', '\n\t\t The algorithm of \n\t\t']",Positive
"['\n\t\t Passive movement is also captured and expressed at f-structure level using a pas s ive : + annotation . \n\t', '\n\t\t Once a treebank tree is annotated with feature structure equations by the annotation algorithm , the equations are collected and passed to a constraint solver which produces the f-structures . \n\t', '\n\t\t In order to ensure the quality of the seman- S signs treaty ^SUBJ [ PRED U.N. ] TOPIC PRED sign J _OBJ LPRED treaty~ ^ ^SUBJ ( SPEC the 1 LPRED headline J ^ PRED say COMP 1 Figure 1 : Penn-II style tree with long distance dependency trace and corresponding reentrancy in f-structure tic forms extracted by our method , we must first ensure the quality of the f-structure annotations . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t The algorithm currently achieves an F-score of 96.3 % for complete f-structures and 93.6 % for preds-only f-structures.1 Our semantic form extraction methodology is based on the procedure of ( van Genabith et al. , 1999 ) : For each f-structure generated , for each level of embedding we determine the local PRED value and collect the subcategorisable grammatical functions present at that level of embedding . \n\t', '\n\t\t Consider the f-structure in Figure 1 . \n\t', '\n\t\t From this we recursively extract the following non- empty semantic forms : s a y ( [ s ub j , c omp ] ) , sign ( [ subj , obj ] ) . \n\t', '\n\t\t In effect , in both ( van Genabith et al. , 1999 ) and our approach semantic forms are reverse engineered from automatically generated f-structures for treebank trees . \n\t', '\n\t\t We extract the following subcategorisable syntactic functions : SUBJ , OBJ , OBJ2 , OBLpTep , OBL2pTep , COMP , XCOMP and PART . \n\t', '\n\t\t Adjuncts ( e.g. ADJ , APP etc ) are not included in the semantic forms . \n\t', '\n\t\t PART is not a syntactic function in the strict sense but we capture the relevant co-occurrence patterns of verbs and particles in the semantic forms . \n\t', '\n\t\t Just as OBL includes the prepositional head of the PP , PART includes the actual particle which occurs e.g. add([subj,obj,part:up]) . \n\t', '\n\t\t In the work presented here we substantially extend the approach of ( van Genabith et al. , 1999 ) as 1Preds-only measures only paths ending in PRED:VALUE so features such as number , person etc are not included . \n\t', '\n\t\t S-TPC- 1 NP VP Det S N V VP the NP U.N. headline said T- 1 V NP ^ ^^^^^^ 1 regards coverage , granularity and evaluation : First , we scale the approach of ( van Genabith et al. , 1999 ) which was proof of concept on 100 trees to the full WSJ section of the Penn-II Treebank . \n\t', '\n\t\t Second , our approach fully reflects long distance dependencies , indicated in terms of traces in the Penn-II Tree- bank and corresponding re-entrancies at f-structure . \n\t', '\n\t\t Third , in addition to abstract syntactic function- based subcategorisation frames we compute frames for syntactic function-CFG category pairs , both for the verbal heads and their arguments and also generate pure CFG-based subcat frames . \n\t', '\n\t\t Fourth , our method differentiates between frames captured for active or passive constructions . \n\t', '\n\t\t Fifth , our method associates conditional probabilities with frames . \n\t', '\n\t\t In contrast to much of the work reviewed in the previous section , our system is able to produce surface syntactic as well as abstract functional subcategorisation details . \n\t', '\n\t\t To incorporate CFG details into the extracted semantic forms , we add an extra feature to the generated f-structures , the value of which is the syntactic category of the pred at each level of embedding . \n\t', '\n\t\t Exploiting this information , the extracted semantic form for the verb sign looks as follows : sign ( v , [ subj ( np ) , obj ( np ) ] ) . \n\t', '\n\t\t We have also extended the algorithm to deal with passive voice and its effect on subcategorisation behaviour . \n\t', '\n\t\t Consider Figure 2 : not taking voice into account , the algorithm extracts an intransitive frame outlaw ( [ subj ] ) for the transitive outlaw . \n\t', '\n\t\t To correct this , the extraction algorithm uses the feature value pair p a s s ive : + , which appears in the f-structure at the level of embedding of the verb in question , to mark that predicate as occurring in the passive : outlaw ( [ subj ] , p ) . \n\t', '\n\t\t In order to estimate the likelihood of the cooccurrence of a predicate with a particular argument list , we compute conditional probabilities for sub- categorisation frames based on the number of token occurrences in the corpus . \n\t', '\n\t\t Given a lemma l and an argument list s , the probability of s given l is estimated as : count ( l , s ) P(s|l) := .i=1 count(l , si ) We use thresholding to filter possible error judgements by our system . \n\t', '\n\t\t Table 1 shows the attested semantic forms for the verb accept with their associated conditional probabilities . \n\t', '\n\t\t Note that were the distinction between active and passive not taken into account , the intransitive occurrence of accept would have been assigned an unmerited probability . \n\t', '\n\t\t subj : spec : quant : pred : all adjunct : 2 : pred : almost adjunct : 3 : pred:remain participle : pres 4 : obj : adjunct : 5 : pred : cancer-causing pers : 3 pred : asbestos num : sg pform : of pers : 3 pred : use num : pl passive : + adjunct : 1 : obj : pred : 1997 pform : by xcomp : subj : spec : quant : pred : all adjunct : 2 : pred : almost ... ... passive : + xcomp : subj : spec : quant : pred : all adjunct : 2 : pred : almost ... ... passive : + pred : outlaw tense : past pred : be pred : will modal : + Figure 2 : Automatically generated f-structure for the string wsj 0003 23\x93B y 1997 , almost all remaining uses of cancer-causing asbestos will be outlawed.\x94 Semantic Form accept([subj,obj]) - accept ( [ subj ] , p ) accept([subj,comp]) -accept([subj,obl:as],p) accept([subj,obj,obl:as]) accept([subj,obj,obl:from]) - accept ( [ subj ] ) accept([subj,obj,obl:at]) accept([subj,obj,obl:for]) accept([subj,obj,xcomp]) Table 1 : Semantic Forms for the verb accept marked with p for passive use . \n\t', '\n\t\t 4 Results We extract non-empty semantic forms2 for 3586 verb lemmas and 10969 unique verbal semantic form types ( lemma followed by non-empty argument list ) . \n\t', '\n\t\t Including prepositions associated with the OBLs and particles , this number rises to 14348 , an average of 4.0 per lemma ( Table 2 ) . \n\t', '\n\t\t The number of unique frame types ( without lemma ) is 38 without specific prepositions and particles , 577 with ( Table 3 ) . \n\t', '\n\t\t F-structure annotations allow us to distinguish passive and active frames . \n\t', '\n\t\t 5 COMLEX Evaluation We evaluated our induced ( verbal ) semantic forms against COMLEX \n\t\t']",Positive
"['\n\t\t COM- 2Frames with at least one subcategorised grammatical function . \n\t', '\n\t\t Frequency Probability 122 0.813 9 0.060 5 0.033 3 0.020 3 0.020 3 0.020 2 0.013 1 0.007 1 0.007 1 0.007 Without Prep/Part With Prep/Part Sem . \n\t', '\n\t\t Form Types 10969 14348 Active 8516 11367 Passive 2453 2981 Table 2 : Number of Semantic Form Types Without Prep/Part With Prep/Part # Frame Types 38 577 # Singletons 1 243 # Twice Occurring 1 84 # Occurring maz . \n\t', '\n\t\t 5 7 415 # Occurring > 5 31 162 Table 3 : Number of Distinct Frames for Verbs ( not including syntactic category for grammatical function ) LEX defines 138 distinct verb frame types without the inclusion of specific prepositions or particles . \n\t', '\n\t\t The following is a sample entry for the verb reimburse : ( VERB :ORTH \x93reimburse\x94 :SUBC ( ( NP-NP ) ( NP-PP :PVAL ( \x93for\x94 ) ) ( NP ) ) ) Each verb has a :SUBC feature , specifying its subcategorisation behaviour . \n\t', '\n\t\t For example , reimburse can occur with two noun phrases ( NP-NP ) , a noun phrase and a prepositional phrase headed by \x93for\x94 ( NP-PP :PVAL ( \x93for\x94 ) ) or a single noun phrase ( NP ) . \n\t', '\n\t\t Note that the details of the subject noun phrase are not included in COMLEX frames . \n\t', '\n\t\t Each of the complement types which make up the value of the :SUBC feature is associated with a formal frame definition which looks as follows : ( vp-frame np-np :cs ( ( np 2)(np 3 ) ) :gs ( :subject 1 :obj 2 :obj2 3 ) :ex \x93she asked him his name\x94 ) The value of the :cs feature is the constituent structure of the subcategorisation frame , which lists the syntactic CF-PSG constituents in sequence . \n\t', '\n\t\t The value of the :gs feature is the grammatical structure which indicates the functional role played by each of the CF-PSG constituents . \n\t', '\n\t\t The elements of the constituent structure are indexed , and referenced in the :gs field . \n\t', '\n\t\t This mapping between constituent structure and functional structure makes the information contained in COMLEX suitable as an evaluation standard for the LFG semantic forms which we induce . \n\t', '\n\t\t 5.1 COMLEX-LFG Mapping We devised a common format for our induced semantic forms and those contained in COMLEX . \n\t', '\n\t\t This is summarised in Table 4 . \n\t', '\n\t\t COMLEX does not distinguish between obliques and objects so we converted Obji to OBLi as required . \n\t', '\n\t\t In addition , COMLEX does not explicitly differentiate between COMPs and XCOMPs , but does encode control information for any Comps which occur , thus allowing us to deduce the distinction automatically . \n\t', '\n\t\t The manually constructed COMLEX entries provided us with a gold standard against which we evaluated the automatically induced frames for the 2992 ( active ) verbs that both resources have in common . \n\t', '\n\t\t LFG COMLEX Merged SUBJ Subject SUBJ OBJ Object OBJ OBJ2 Obj2 OBJ2 OBL Obj3 OBL OBL2 Obj4 OBL2 COMP Comp COMP XCOMP Comp XCOMP PART Part PART Table 4 : COMLEX and LFG Syntactic Functions We use the computed conditional probabilities to set a threshold to filter the selection of semantic forms . \n\t', '\n\t\t As some verbs occur less frequently than others we felt it was important to use a relative rather than absolute threshold . \n\t', '\n\t\t For a threshold of 1 % , we disregard any frames with a conditional probability of less than or equal to 0.01 . \n\t', '\n\t\t We carried out the evaluation in a similar way to \n\t\t']",Positive
"['\n\t\t The scale of our evaluation is comparable to hers . \n\t', '\n\t\t This allows us to make tentative comparisons between our respective results . \n\t', '\n\t\t The figures shown in Table 5 are the results of three different kinds of evaluation with the threshold set to 1 % and 5 % . \n\t', '\n\t\t The effect of the threshold increase is obvious in that Precision goes up for each of the experiments while Recall goes down . \n\t', '\n\t\t For Ezp 1 , we excluded prepositional phrases entirely from the comparison , i.e. assumed that PPs were adjunct material ( e.g. [ subj,obl:for ] becomes [ subj ] ) . \n\t', '\n\t\t Our results are better for Precision than for Recall compared to Schulte im Walde ( op cit . \n\t', '\n\t\t ) , who reports Precision of 74.53 % , Recall of 69.74 % and an F-score of 72.05 % . \n\t', '\n\t\t Ezp 2 includes prepositional phrases but not parameterised for particular prepositions ( e.g. [ subj,obl:for ] becomes [ subj,obl ] ) . \n\t', '\n\t\t While our figures for Recall are again lower , our results for Precision are considerably higher than those of Schulte im Walde ( op cit . \n\t', '\n\t\t ) who recorded Precision of 60.76 % , Recall of 63.91 % and an F-score of 62.30 % . \n\t', '\n\t\t For Ezp . \n\t', '\n\t\t 3 , we used semantic forms which contained details of specific prepositions for any subcategorised prepositional phrase . \n\t', '\n\t\t Our Precision figures are again high ( in comparison to 65.52 % as recorded by \n\t\t']",Negative
"['\n\t\t However , Threshold 1 % Threshold 5 % P R F-Score P R F-Score Exp . \n\t', '\n\t\t 1 79.0 % 59.6 % 68.0 % 83.5 % 54.7 % 66.1 % Exp . \n\t', '\n\t\t 2 77.1 % 50.4 % 61.0 % 81.4 % 44.8 % 57.8 % Exp . \n\t', '\n\t\t 2a 76.4 % 44.5 % 56.3 % 80.9 % 39.0 % 52.6 % Exp . \n\t', '\n\t\t 3 73.7 % 22.1 % 34.0 % 78.0 % 18.3 % 29.6 % Exp . \n\t', '\n\t\t 3a 73.3 % 19.9 % 31.3 % 77.6 % 16.2 % 26.8 % Precision Recall F-Score Experiment 3 81.7 % 40.8 % 54.4 % Experiment 3a 83.1 % 35.4 % 49.7 % Table 6 : COMLEX Comparison using p-dir(Threshold of 1 % ) Table 5 : COMLEX Comparison our Recall is very low ( compared to the 50.83 % that Schulte im Walde ( op cit . \n\t', '\n\t\t ) reports ) . \n\t', '\n\t\t Consequently our F-score is also low ( Schulte im Walde ( op cit . \n\t', '\n\t\t ) records an F-score of 57.24 % ) . \n\t', '\n\t\t Experiments 2a and 3a are similar to Experiments 2 and 3 respectively except they include the specific particle associated with each PART . \n\t', '\n\t\t 5.1.1 Directional Prepositions There are a number of possible reasons for our low recall scores for Experiment 3 in Table 5 . \n\t', '\n\t\t It is a well-documented fact \n\t\t']",Positive
"['\n\t\t We have extracted frames from one domain ( the WSJ ) whereas COMLEX was built using examples from the San Jose Mercury News , the Brown Corpus , several literary works from the Library of America , scientific abstracts from the U.S. Department of Energy , and the WSJ . \n\t', '\n\t\t For this reason , it is likely to contain a greater variety of subcategorisation frames than our induced lexicon . \n\t', '\n\t\t It is also possible that due to human error COMLEX contains subcategorisation frames , the validity of which may be in doubt . \n\t', '\n\t\t This is due to the fact that the aim of the COMLEX project was to construct as complete a set of subcategorisation frames as possible , even for infrequent verbs . \n\t', '\n\t\t Lexicographers were allowed to extrapolate from the citations found , a procedure which is bound to be less certain than the assignment of frames based entirely on existing examples . \n\t', '\n\t\t Our recall figure was particularly low in the case of evaluation using details of prepositions ( Experiment 3 ) . \n\t', '\n\t\t This can be accounted for by the fact that COMLEX errs on the side of overgeneration when it comes to preposition assignment . \n\t', '\n\t\t This is particularly true of directional prepositions , a list of 31 of which has been prepared and is assigned in its entirety by default to any verb which can potentially appear with any directional preposition . \n\t', '\n\t\t In a subsequent experiment , we incorporate this list of directional prepositions by default into our semantic form induction process in the same way as the creators of COMLEX have done . \n\t', '\n\t\t Table 6 shows the results of this experiment . \n\t', '\n\t\t As expected there is a significant im- Passive Precision Recall F-Score Experiment 2 80.2 % 54.7 % 65.1 % Experiment 2a 79.7 % 46.2 % 58.5 % Experiment 3 72.6 % 33.4 % 45.8 % Experiment 3a 72.3 % 29.3 % 41.7 % Table 7 : Passive evaluation ( Threshold of 1 % ) provement in the recall figure , being almost double the figures reported in Table 5 for Experiments 3 and 3a . \n\t', '\n\t\t 5.1.2 Passive Evaluation Table 7 presents the results of our evaluation of the passive semantic forms we extract . \n\t', '\n\t\t It was carried out for 1422 verbs which occur with passive frames and are shared by the induced lexicon and COMLEX . \n\t', '\n\t\t As COMLEX does not provide explicit passive entries , we applied Lexical Redundancy Rules \n\t\t']",Positive
"['\n\t\t For example , the COMLEX entry see ( [ subj , obj ] ) is converted to s e e ( [ s ub j ] ) . \n\t', '\n\t\t The resulting precision is very high , a slight increase on that for the active frames . \n\t', '\n\t\t The recall score drops for passive frames ( from 54.7 % to 29.3 % ) in a similar way to that for active frames when prepositional details are included . \n\t', '\n\t\t 5.2 Lexical Accession Rates As well as evaluating the quality of our extracted semantic forms , we also examine the rate at which they are induced . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t We were interested in discovering whether the acquisition of lexical material on the same data displays a similar propensity . \n\t', '\n\t\t Figure 3 displays the accession rates for the semantic forms induced by our method for sections 0\x9624 of the WSJ section of the Penn-II treebank . \n\t', '\n\t\t When we do not distinguish semantic forms by category , all semantic forms together with those for verbs display smaller accession rates than for the PCFG . \n\t', '\n\t\t We also examined the coverage of our system in a similar way to \n\t\t']",Positive
"['\n\t\t We extracted a verb-only reference lexicon from Sections 02-21 of the WSJ and subsequently compared this to a test lexicon constructed in the same way from 0 5 10 15 20 25 WSJ Section Figure 3 : Accession Rates for Semantic Forms and CFG Rules Entries also in reference lexicon : 89.89 % Entries not in reference lexicon : 10.11 % Known words : 7.85 % -Known words , known frames : 7.85 % -Known words , unknown frames : - Unknown words : 2.32 % -Unknown words , known frames : 2.32 % -Unknown words , unknown frames : - Table 8 : Coverage of induced lexicon on unseen data ( Verbs Only ) Section 23 . \n\t', '\n\t\t Table 8 shows the results of this experiment . \n\t', '\n\t\t 89.89 % of the entries in the test lexicon appeared in the reference lexicon . \n\t', '\n\t\t 6 Conclusions We have presented an algorithm and its implementation for the extraction of semantic forms or subcategorisation frames from the Penn-II Treebank , automatically annotated with LFG f-structures . \n\t', '\n\t\t We have substantially extended an earlier approach by ( van Genabith et al. , 1999 ) . \n\t', '\n\t\t The original approach was small-scale and \x91proof of concept\x92 . \n\t', '\n\t\t We have scaled our approach to the entire WSJ Sections of Penn- II ( 50,000 trees ) . \n\t', '\n\t\t Our approach does not predefine the subcategorisation frames we extract as many other approaches do . \n\t', '\n\t\t We extract abstract syntactic function-based subcategorisation frames ( LFG semantic forms ) , traditional CFG category-based frames as well as mixed function-category based frames . \n\t', '\n\t\t Unlike many other approaches to subcategorisation frame extraction , our system properly reflects the effects of long distance dependencies and distinguishes between active and passive frames . \n\t', '\n\t\t Finally our system associates conditional probabilities with the frames we extract . \n\t', '\n\t\t We carried out an extensive evaluation of the complete induced lexicon ( not just a sample ) against the full COMLEX resource . \n\t', '\n\t\t To our knowledge , this is the most extensive qualitative evaluation of subcategorisation extraction in English . \n\t', '\n\t\t The only evaluation of a similar scale is that carried out by \n\t\t']",Positive
"['\n\t\t Our results compare well with hers . \n\t', '\n\t\t We believe our semantic forms are fine-grained and by choosing to evaluate against COMLEX we set our sights high : COMLEX is considerably more detailed than the OALD or LDOCE used for other evaluations . \n\t', '\n\t\t Currently work is under way to extend the coverage of our acquired lexicons by applying our methodology to the Penn-III treebank , a more balanced corpus resource with a number of text genres ( in addition to the WSJ sections ) . \n\t', '\n\t\t It is important to realise that the induction of lexical resources is part of a larger project on the acquisition of wide-coverage , robust , probabilistic , deep unification grammar resources from treebanks . \n\t', '\n\t\t We are already using the extracted semantic forms in parsing new text with robust , wide-coverage PCFG-based LFG grammar approximations automatically acquired from the f-structure annotated Penn-II tree- bank \n\t\t']",Positive
"['\n\t\t We hope to be able to apply our lexical acquisition methodology beyond existing parse-annotated corpora ( Penn-II and Penn- III ) : new text is parsed by our PCFG-based LFG approximations into f-structures from which we can then extract further semantic forms . \n\t', '\n\t\t The work reported here is part of the core component for bootstrapping this approach . \n\t', '\n\t\t As the extraction algorithm we presented derives semantic forms at f-structure level , it is easily applied to other , even typologically different , languages . \n\t', '\n\t\t We have successfully ported our automatic annotation algorithm to the TIGER Treebank , despite German being a less configurational language than English , and extracted wide-coverage , probabilistic LFG grammar approximations and lexical resources for German \n\t\t']",Positive
"['\n\t\t Currently , we are migrating the technique to Spanish , which has freer word order than English and less morphological marking than German . \n\t', '\n\t\t Preliminary results have been very encouraging . \n\t', '\n\t\t 7 Acknowledgements The research reported here is supported by Enterprise Ireland Basic Research Grant SC/2001/186 and an IRCSET PhD fellowship award . \n\t', '\n\t\t All SF Frames All Verbs All SF Frames , no category All Verbs , no category 25000 20000 15000 10000 5000 0 References M. Brent . \n\t', '\n\t\t 1993. From Grammar to Lexicon : Unsupervised Learning of Lexical Syntax . \n\t', '\n\t\t Computational Linguistics , 19(2):203\x96222 . \n\t', '\n\t\t E. Briscoe and J. Carroll . \n\t', '\n\t\t 1997. Automatic Extraction of Subcategorization from Corpora . \n\t', '\n\t\t In Proceedings of the 5th ACL Conference on Applied Natural Language Processing , pages 356\x96363 , Washington , DC . \n\t', '\n\t\t A. Cahill , M. Forst , M. McCarthy , R. O\x92Donovan , C. Rohrer , J. van Genabith , and A. Way . \n\t', '\n\t\t 2003. Treebank-Based Multilingual Unification- Grammar Development . \n\t', '\n\t\t In Proceedings of the Workshop on Ideas and Strategies for Multilingual Grammar Development at the 15th ESSLLI , pages 17\x9624 , Vienna , Austria . \n\t', '\n\t\t A. Cahill , M. Burke , R. O\x92Donovan , J. van Genabith , and A. Way . \n\t', '\n\t\t 2004a . \n\t', '\n\t\t Long-Distance Dependency Resolution in Automatically Acquired Wide-Coverage PCFG-Based LFG Approximations . \n\t', '\n\t\t In Proceedings of the 42nd Annual Conference of the Association for Computational Linguistics ( ACL-04 ) , Barcelona , Spain . \n\t', '\n\t\t A. Cahill , M. McCarthy , M. Burke , R. O\x92Donovan , J. van Genabith , and A. Way . \n\t', '\n\t\t 2004b . \n\t', '\n\t\t Evaluating Automatic F-Structure Annotation for the Penn- II Treebank . \n\t', '\n\t\t Journal of Research on Language and Computation . \n\t', '\n\t\t G. Carroll and M. Rooth . \n\t', '\n\t\t 1998. Valence Induction with a Head-Lexicalised PCFG . \n\t', '\n\t\t In Proceedings of the 3rd Conference on Empirical Methods in Natural Language Processing , pages 36\x96 45 , Granada , Spain . \n\t', '\n\t\t E. Charniak . \n\t', '\n\t\t 1996. Tree-bank Grammars . \n\t', '\n\t\t In AAAI96 : Proceedings of the Thirteenth National Conference on Artificial Intelligence , MIT Press , pages 1031\x961036 , Cambridge , MA . \n\t', '\n\t\t J. Chen and K. Vijay-Shanker . \n\t', '\n\t\t 2000. Automated Extraction of TAGs from the Penn Treebank . \n\t', '\n\t\t In Proceedings of the 38th Annual Meeting of the Association of Computational Linguistics , pages 65\x9676 , Hong Kong . \n\t', '\n\t\t M. Collins . \n\t', '\n\t\t 1997. Three generative lexicalised models for statistical parsing . \n\t', '\n\t\t In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics , pages 16\x9623 . \n\t', '\n\t\t J. Hockenmaier , G. Bierner , and J. Baldridge . \n\t', '\n\t\t 2002. Extending the Coverage of a CCG System . \n\t', '\n\t\t Journal ofLanguage and Computation , ( 2 ) . \n\t', '\n\t\t R. Kaplan and J. Bresnan . \n\t', '\n\t\t 1982. Lexical Functional Grammar : A Formal System for Grammatical Representation . \n\t', '\n\t\t In Joan Bresnan , editor , The Mental Representation of Grammatical Re- lations , pages 206\x96250 . \n\t', '\n\t\t MIT Press , Cambridge , MA , Mannheim , 8th Edition . \n\t', '\n\t\t A. Kinyon and C. Prolo . \n\t', '\n\t\t 2002. Identifying Verb Arguments and their Syntactic Function in the Penn Treebank . \n\t', '\n\t\t In Proceedings of the 3rd LREC Conference , pages 1982\x961987 , Las Palmas , Spain . \n\t', '\n\t\t A. Korhonen . \n\t', '\n\t\t 2002. Subcategorization Acquisition . \n\t', '\n\t\t PhD thesis published as Techical Report UCAMCL-TR-530 , Computer Laboratory , University of Cambridge , UK . \n\t', '\n\t\t A. Krotov , M. Hepple , R. Gaizauskas , and Y. Wilks . \n\t', '\n\t\t 1998. Compacting the Penn Treebank Grammar . \n\t', '\n\t\t In Proceedings of COLING -ACL\x9298 , pages 669\x96 703 , Montreal , Canada . \n\t', '\n\t\t C. MacLeod , R. Grishman , and A. Meyers . \n\t', '\n\t\t 1994. The Comlex Syntax Project : The First Year . \n\t', '\n\t\t In Proceedings of the ARPA Workshop on Human Language Technology , pages 669\x96703 , Princeton , NJ . \n\t', '\n\t\t D. Magerman . \n\t', '\n\t\t 1994. Natural Language Parsing as Statistical Pattern Recognition . \n\t', '\n\t\t PhD Thesis , Stanford University , CA . \n\t', '\n\t\t C. Manning . \n\t', '\n\t\t 1993. Automatic Acquisition of a Large Subcategorisation Dictionary from Corpora . \n\t', '\n\t\t In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics , pages 235\x96242 , Columbus , OH . \n\t', '\n\t\t S. Schulte im Walde . \n\t', '\n\t\t 2002. Evaluating Verb Sub- categorisation Frames learned by a German Statistical Grammar against Manual Definitions in the Duden Dictionary . \n\t', '\n\t\t In Proceedings of the 10th EURALEX International Congress , pages 187\x96 197 , Copenhagen , Denmark . \n\t', '\n\t\t A. Ushioda , D. Evans , T. Gibson , and A. Waibel . \n\t', '\n\t\t 1993. The Automatic Acquisition of Frequencies of Verb Subcategorization Frames from Tagged Corpora . \n\t', '\n\t\t In SIGLEX ACL Workshop on the Acquisition of Lexical Knowledge from Text , pages 95\x96106 , Columbus , OH . \n\t', '\n\t\t J. van Genabith , A. Way , and L. Sadler . \n\t', '\n\t\t 1999. Data- driven Compilation of LFG Semantic Forms . \n\t', '\n\t\t In EACL-99 Workshop on Linguistically Interpreted Corpora , pages 69\x9676 , Bergen , Norway . \n\t', '\n\t\t F. Xia , M. Palmer , and A. Joshi . \n\t', '\n\t\t 2000. A Uniform Method of Grammar Extraction and its Applications . \n\t', '\n\t\t In Proceedings of the Conference on Empirical Methods in Natural Language Processing ( EMNLP-2000 ) , pages 53\x9662 , Hong Kong . \n\t', '\n\t\t Inducing Frame Semantic Verb Classes from WordNet and LDOCE Rebecca Green,*\x86\x87 Bonnie J. Dorr,*\x86 and Philip Resnik*\x86 Institute for Advanced Computer Studies \x86 Department of Computer Science College of Information Studies University of Maryland College Park , MD 20742 USA { rgreen , bonnie , resnik}@umiacs.umd.edu * \x87 Abstract This paper presents SemFrame , a system that induces frame semantic verb classes from WordNet and LDOCE . \n\t', '\n\t\t Semantic frames are thought to have significant potential in resolving the paraphrase problem challenging many language- based applications . \n\t', ""\n\t\t When compared to the handcrafted FrameNet , SemFrame achieves its best recall-precision balance with 83.2 % recall ( based on SemFrame 's coverage of FrameNet frames ) and 73.8 % precision ( based on SemFrame verbs\x92 semantic relatedness to frame-evoking verbs ) . \n\t"", '\n\t\t The next best performing semantic verb classes achieve 56.9 % recall and 55.0 % precision . \n\t', '\n\t\t 1 Introduction Semantic content can almost always be expressed in a variety of ways . \n\t', '\n\t\t Lexical synonymy ( She esteemed him highly vs. . \n\t', '\n\t\t She respected him greatly ) , syntactic variation ( John paid the bill vs. . \n\t', '\n\t\t The bill was paid by John ) , overlapping meanings ( Anna turned at Elm vs. Anna rounded the corner at Elm ) , and other phenomena interact to produce a broad range of choices for most language generation tasks \n\t\t']",Positive
"['\n\t\t At the same time , natural language understanding must recognize what remains constant across paraphrases . \n\t', '\n\t\t The paraphrase phenomenon affects many computational linguistic applications , including information retrieval , information extraction , question-answering , and machine translation . \n\t', '\n\t\t For example , documents that express the same content using different linguistic means should typically be retrieved for the same queries . \n\t', '\n\t\t Information sought to answer a question needs to be recognized no matter how it is expressed . \n\t', '\n\t\t Semantic frames \n\t\t']",Positive
"['\n\t\t Semantic frame types of an intermediate granularity have the potential to fulfill an interlingua role within a solution to the paraphrase problem . \n\t', '\n\t\t Until now , semantic frames have been generated by hand ( as in Fillmore and Atkins , 1992 ) , based on native speaker intuition ; the FrameNet project ( http://www.icsi.berkeley.edu/ \x97framenet ; Johnson et al. , 2002 ) now couples this generation with empirical validation . \n\t', '\n\t\t Only recently has this project begun to achieve relative breadth in its inventory of semantic frames . \n\t', '\n\t\t To have a comprehensive inventory of semantic frames , however , we need the capacity to generate semantic frames semi-automatically ( the need for manual post-editing is assumed ) . \n\t', '\n\t\t To address these challenges , we have developed SemFrame , a system that induces semantic frames automatically . \n\t', '\n\t\t Overall , the system performs two primary functions : ( 1 ) identification of sets of verb senses that evoke a common semantic frame ( in the sense that lexical units call forth corresponding conceptual structures ) ; and ( 2 ) identification of the conceptual structure of semantic frames . \n\t', '\n\t\t This paper explores the first task of identifying frame semantic verb classes . \n\t', '\n\t\t These classes have several types of uses . \n\t', '\n\t\t First , they are the basis for identifying the internal structure of the frame proper , as set forth in Green and Dorr , 2004 . \n\t', '\n\t\t Second , they may be used to extend FrameNet . \n\t', '\n\t\t Third , they support applications needing access to sets of semantically related words , for example , text segmentation and word sense disambiguation , as explored to a limited degree in Green , 2004 . \n\t', '\n\t\t Section 2 presents related research efforts on developing semantic verb classes . \n\t', '\n\t\t Section 3 summarizes the features of WordNet ( http://www.cogsci.princeton.edu/\x97wn ) and LDOCE \n\t\t']",Positive
"['\n\t\t Section 5 thus mapping easily to the corresponding frame presents a brief synopsis of SemFrame\x92s results , element . \n\t', '\n\t\t Corpus data , however , are more likely while Section 6 presents an evaluation of to include instantiated participants , which may SemFrame\x92s ability to identify semantic verb not generalize to the frame element . \n\t', '\n\t\t Second , classes of a FrameNet-like nature . \n\t', '\n\t\t Section 7 lexical resources provide a consistent amount of summarizes our work and motivates directions for data for word senses , while the amount of data in further development of SemFrame . \n\t', '\n\t\t a corpus for word senses is likely to vary widely . \n\t', '\n\t\t 2 Previous Work The EAGLES ( 1998 ) report on semantic encoding differentiates between two approaches to the development of semantic verb classes : those based on syntactic behavior and those based on semantic criteria . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t Verb classes at the bottom of Levin\x92s shallow network group together ( quasi- ) synonyms , hierarchically related verbs , and antonyms , alongside verbs with looser semantic relationships . \n\t', '\n\t\t The verb categories based on \n\t\t']",Positive
"['\n\t\t The resulting clusters contain synonyms , hierarchically related verbs , and antonyms , as well as verbs more loosely related from the perspective of paraphrase . \n\t', '\n\t\t The handcrafted WordNet \n\t\t']",Positive
"['\n\t\t Each collection of a top-level node supplemented by its descendants may be seen as a semantic verb class . \n\t', '\n\t\t In all fairness , resolution of the paraphrase problem is not the explicit goal of most efforts to build semantic verb classes . \n\t', '\n\t\t However , they can process some paraphrases through lexical synonymy , hierarchically related terms , and antonymy . \n\t', '\n\t\t 3 Resources Used in SemFrame We adopt an approach that relies heavily on pre-existing lexical resources . \n\t', '\n\t\t Such resources have several advantages over corpus data in identifying semantic frames . \n\t', '\n\t\t First , both Third , lexical resources provide their data in a more systematic fashion than do corpora . \n\t', '\n\t\t Most centrally , the syntactic arguments of the verbs used in a definition often correspond to the semantic arguments of the verb being defined . \n\t', '\n\t\t For example , Table 1 gives the definitions of several verb senses in LDOCE that evoke the COMMERCIAL TRANSACTION frame , which includes as its semantic arguments a Buyer , a Seller , some Merchandise , and Money . \n\t', '\n\t\t Words corresponding to the Money ( money , value ) , the Merchandise ( property , goods ) , and the Buyer ( buyer , buyers ) are present in , and to some extent shared across , the definitions ; however , no words corresponding to the Seller are present . \n\t', '\n\t\t Verb sense LDOCE Definition buy 1 to obtain ( something ) by giving money ( or something else of value ) buy 2 to obtain in exchange for something , often something of great value buy 3 to be exchangeable for purchase 1 to gain ( something ) at the cost of effort , suffering , or loss of something of value sell 1 to give up ( property or goods ) to another for money or other value sell 2 to offer ( goods ) for sale sell 3 to be bought ; get a buyer or buyers ; gain a sale Table 1 . \n\t', '\n\t\t LDOCE Definitions for Verbs Evoking the COMMERCIAL TRANSACTION Frame Of available machine-readable dictionaries , LDOCE appears especially useful for this research . \n\t', '\n\t\t It uses a restricted vocabulary of about 2000 words in its definitions and example sentences , thus increasing the likelihood that words with closely related meanings will use the same words in their definitions and support WordNet verb synsets and LDOCE verb senses the pattern of discovery envisioned . \n\t', '\n\t\t LDOCE\x92s relies on finding matches between the data subject field codes also accomplish some of the available for the verb senses in each resource same type of grouping as semantic frames . \n\t', '\n\t\t ( e.g. , other words in the synset ; words in WordNet is a machine-readable lexico- definitions and example sentences ; words closely semantic database whose primary organizational related to these words ; and stems of these words ) . \n\t', '\n\t\t structure is the synset\x97a set of synonymous word The similarity measure used is the average of the senses . \n\t', '\n\t\t A limited number of relationship types proportion of words on each side of the ( e.g. , antonymy , hyponymy , meronymy , comparison that are matched in the other . \n\t', '\n\t\t This troponymy , entailment ) also relate synsets within mapping is used both to relate LDOCE verb senses , a part of speech . \n\t', '\n\t\t ( Version 1.7.1 was used . \n\t', '\n\t\t ) that map to the same WordNet synset ( fig . \n\t', '\n\t\t 3f ) and to \n\t\t']",Positive
"['\n\t\t frame semantics\x94 ( p. 5 ) . \n\t', '\n\t\t Through the relational In the third stage , the resulting verb sense structure of WordNet , buy , purchase , sell , and pay pairs are merged into a single data set , retaining are related together : buy and purchase comprise one only those pairs whose cumulative support synset ; they entail paying and are opposed to sell . \n\t', '\n\t\t exceeds thresholds for either the number of The relationship of buy , purchase , sell , and supporting data sources or strength of support , pay to other COMMERCIAL TRANSACTION thus achieving higher precision in the merged verbs\x97for example , cost , price , and the demand data set than in the input data sets . \n\t', '\n\t\t Then , the payment sense of charge\x97is not made explicit in graph formed by the verb sense pairs in the WordNet , however . \n\t', '\n\t\t Further , as Roger Chaffin merged data set is analyzed to find the fully has noted , the specialized vocabulary of , for connected components . \n\t', '\n\t\t example , tennis ( e.g. racket , court , lob ) is not co- Finally , these groups of verb senses become located , but is dispersed across different branches input to a clustering operation \n\t\t']",Positive
"['\n\t\t of the noun network ( Miller , 1998 , p. 34 ) . \n\t', '\n\t\t Those groups whose similarity ( due to overlap in 4 SemFrame Approach SemFrame gathers evidence about frame semantic relatedness between verb senses by analyzing LDOCE and WordNet data from a variety of perspectives . \n\t', '\n\t\t The overall approach used is shown in Figure 1 . \n\t', '\n\t\t The first stage of processing extracts pairs of LDOCE and WordNet verb senses that potentially evoke the same frame . \n\t', '\n\t\t By exploiting many different clues to semantic relatedness , we overgenerate these pairs , favoring recall ; subsequent stages improve the precision of the resulting data . \n\t', '\n\t\t Figures 2 and 3 give details of the algorithms for extracting verb pairs based on different types of evidence . \n\t', '\n\t\t These include : clustering LDOCE verb senses/WordNet synsets on the basis of words in their definitions and example sentences ( fig . \n\t', '\n\t\t 2 ) ; relating LDOCE verb senses defined in terms of the same verb ( fig . \n\t', '\n\t\t 3a ) ; relating LDOCE verb senses that share a common stem ( fig . \n\t', '\n\t\t 3b ) ; extracting explicit sense-linking relationships in LDOCE ( fig . \n\t', '\n\t\t 3c ) ; relating verb senses that share general or specific subject field codes in LDOCE ( fig . \n\t', '\n\t\t 3d ) ; and extracting ( direct or extended ) semantic relationships in WordNet ( fig . \n\t', '\n\t\t 3e ) . \n\t', '\n\t\t In the second stage , mapping between membership ) exceed a threshold are merged together , thus reducing the number of verb sense groups . \n\t', '\n\t\t The verb senses within each resulting group are hypothesized to evoke the same semantic frame and constitute a frameset . \n\t', '\n\t\t Extract verb sense pairs from WordNet Map WordNet synsets to LDOCE senses Merge pairs , filtering out those not meeting threshold criteria Build fully-connected verb groups Cluster related verb groups Verb sense framesets Figure 1 . \n\t', '\n\t\t Approach for Building Frame Semantic Verb Classes Extract verb sense pairs from LDOCE Figure 2. Algorithm for Generating Clustering-based Verb Pairs 5 Results We explored a range of thresholds in the final stage of the algorithm.1 In general , the lower the threshold , the looser the verb grouping . \n\t', '\n\t\t The number of verb senses retained ( out of 12,663 non-phrasal verb senses in LDOCE ) and the verb sense groups produced by using these thresholds are recorded in Table 2 . \n\t', ""\n\t\t 6 Evaluation One of our goals is to produce sets of verb senses capable of extending FrameNet 's coverage while requiring reasonably little post-editing . \n\t"", '\n\t\t This goal has two subgoals : identifying new frames and identifying additional lexical units that evoke 1 Threshold Num verb senses Num groups 0.5 6461 1338 1.0 6414 1759 1.5 5607 1421 2.0 5604 1563 Table 2 . \n\t', '\n\t\t Results of Frame Clustering Process previously recognized frames . \n\t', ""\n\t\t We use the handcrafted FrameNet , which is of reliably high precision , as a gold standar& for the initial evaluation of SemFrame 's ability to achieve these subgoals . \n\t"", '\n\t\t For the first , we evaluate SemFrame\x92s ability to generate frames that correspond to FrameNet\x92s frames , reasoning that the system must be able to identify a large proportion of known frames if the quality of its output is good enough to identify new frames . \n\t', '\n\t\t ( At this stage we do not measure the quality of new frames . \n\t', '\n\t\t ) For the second subgoal we can be more concrete : For frames identified by both systems , we measure the degree to which the verbs identified by SemFrame can be shown to evoke those frames , even if FrameNet has not identified them as frame-evoking verbs . \n\t', '\n\t\t FrameNet includes hierarchically organized frames of varying levels of generality : Some semantic areas are covered by a general frame , some by a combination of specific frames , and some by a mix of general and specific frames . \n\t', '\n\t\t Because of this variation we determined the degree to which SemFrame and FrameNet overlap by automatically finding and comparing corresponding frames instead of fully equivalent frames . \n\t', ""\n\t\t Frames correspond if the semantic scope of one frame is included within the semantic 2Certain constraints imposed by FrameNet 's development strategy restrict its use as a full-fledged gold standard for evaluating semantic frame induction . \n\t"", '\n\t\t ( 1 ) As of summer 2003 , only 382 frames had been identified within the FrameNet project . \n\t', '\n\t\t ( 2 ) Low recall affects not only the set of semantic frames identified by FrameNet , but also the sets of frame-evoking units listed for each frame . \n\t', ""\n\t\t No verbs are listed for 38.5 % of FrameNet 's frames , while another 13.1 % of them list only 1 or 2 verbs . \n\t"", '\n\t\t The comparison here is limited to the 197 FrameNet frames for which at least one verb is listed with a counterpart in LDOCE . \n\t', '\n\t\t ( 3 ) Some of Input . \n\t', '\n\t\t SW , a set of stop words ; M , a set of ( word , stem ) pairs ; F , a set of ( word , frequency ) pairs ; DE , a set of ( verb_sense_id , def+ex ) pairs , where def+exd = the set of words in the definitions and example sentences of verb_sense_idd Step 1. forall d E DE , append to def+exd : verb_sense_idd and remove from def+exd any word w E SW Step 2. forall d E DE forall . \n\t', '\n\t\t E M if word . \n\t', '\n\t\t exists in def+exd , substitute ste .. for word . \n\t', '\n\t\t Step 3. forall f E F if frequencyf > 1 , wgtwordf 1 frequency f , else if frequencyf == 1 , wgtwordf .01 Step 4 . \n\t', ""\n\t\t O Voorhees\x92 average link clustering algorithm applied to DE , with initial weights forall t in def+ex set to wgtt Step 5. forall o E O return all combinations of two members from o For the clustering algorithm used , the clustering FrameNet 's frames are more syntactically than threshold range is open-ended . \n\t"", '\n\t\t The values semantically motivated ( e.g. , EXPERIENCER-OBJECT , investigated in the evaluation are fairly low . \n\t', '\n\t\t EXPERIENCER-SUBJECT ) . \n\t', '\n\t\t a. Relates LDOCE verb senses that are defined in terms of the same verb Input . \n\t', '\n\t\t D , a set of ( verb\x97sense\x97id , def\x97verb ) pairs , where def\x97verbd = the verb in terms of which verb\x97sense\x97idd is defined Step 1. forall v that exist as def\x97verb in D , form DVv c : D , by extracting all ( verb\x97sense\x97id , def\x97verb ) pairs where v = def\x97verb Step 2. remove all DVv for which | DVv | > 40 Step 3. forall v that exist as def\x97verb in D , return all combinations of two members from DVv b. Relates LDOCE verb senses that share a common stem Input . \n\t', '\n\t\t D , a set of ( verb\x97sense\x97id , verb\x97stem ) pairs , where verb\x97stemd = the stem for the verb on which verb\x97sense\x97idd is based Step 1. forall m that exist as verb\x97stem in D , form DVm c : D , by extracting all ( verb\x97sense\x97id , verb \x97stem ) pairs where m = verb\x97stem Step 2. forall m that exist as verb\x97stem in D , return all combinations of two members from DVv c . \n\t', '\n\t\t Extracts explicit sense-linking relationships in LDOCE Input . \n\t', '\n\t\t D , a set of ( verb\x97sense\x97id , def ) pairs , where defd = the definition for verb\x97sense\x97idd Step 1. forall d E D , if defd contains compare or opposite note , extract related\x97verb from note ; generate ( verb\x97sense\x97idd , related\x97verbd ) pair Step 2. forall d E D , if defd defines verb\x97sense\x97idd in terms of a related standalone verb ( in BLOCK CAPS ) , extract related\x97verb from definition ; generate ( verb\x97sense\x97idd , related\x97verbd ) pair Step 3. forall ( verb\x97sense\x97idd , related\x97verbd ) pairs , if there is only one sense of related\x97verbd , choose it and return ( verb\x97sense\x97idd , related\x97verb\x97sense\x97idd ) , else apply generalized mapping algorithm to return ( verb\x97sense\x97idd , related\x97verb\x97sense\x97idd ) pairs where overlap occurs in the glosses of verb\x97sense\x97idd and related\x97verb\x97sense\x97idd d. Relates verb senses that share general or specific subject field codes in LDOCE Input . \n\t', '\n\t\t D , a set of ( verb\x97sense\x97id , subject\x97code ) pairs , where subject\x97coded = any 2- or 4-character subject field code assigned to verb\x97sense\x97id Step 1. forall c that exist as subject\x97code in D , form DVc c : D , by extracting all ( verb\x97sense\x97id , subject\x97code ) pairs where c = subject\x97code Step 2. forall c that exist as subject\x97code in D , return all combinations of two members from DVv e . \n\t', '\n\t\t Extracts ( direct or extended ) semantic relationships in WordNet Input . \n\t', '\n\t\t WordNet data file for verb synsets Step 1. forall synset lines in input file return ( synset , related \x97synset ) pairs for all synsets directly related through hyponymy , antonymy , entailment , or cause\x97to relationships in WordNet ( for extended relationship pairs , also return ( synset , related\x97synset ) pairs for all synsets within hyponymy tree , i.e. , no matter how many levels removed ) f. Relates LDOCE verb senses that map to the same WordNet synset Input . \n\t', '\n\t\t mapping of LDOCE verb senses to WordNet synsets Step 1. forall lines in input file return all combinations of two LDOCE verb senses mapped to the same WordNetlsynset Figure 3. Algorithms for Generating Non-clustering-based Verb Pairs scope of the other frame or if the semantic scopes SemFrame\x92s verb classes list specific LDOCE of the two frames have significant overlap . \n\t', '\n\t\t Since verb senses . \n\t', '\n\t\t In extending FrameNet , verbs from FrameNet lists evoking words , without SemFrame would be word-sense-disambiguated specification of word sense , the comparison was in the same way that FrameNet verbs currently done on the word level rather than on the word are , through the correspondence of lexeme and sense level , as if LDOCE verb senses were not frame . \n\t', '\n\t\t specified in SemFrame . \n\t', '\n\t\t However , it is clearly Incompleteness in the listing of evoking verbs specific word senses that evoke frames , and in FrameNet and SemFrame precludes a straight- forward detection of correspondences between incrust , and ornament . \n\t', '\n\t\t Two of the verbs\x97adorn their frames . \n\t', '\n\t\t Instead , correspondence between and decorate\x97are shared . \n\t', '\n\t\t In addition , the frame FrameNet and SemFrame frames is established names are semantically related through a using either of two somewhat indirect approaches . \n\t', '\n\t\t WordNet synset consisting of decorate , adorn In the first approach , a SemFrame frame is ( which CatVar relates to ADORNING ) , grace , deemed to correspond to a FrameNet frame if the ornament ( which CatVar relates to two frames meet both a minimal-overlap ORNAMENTATION ) , embellish , and beautify . \n\t', '\n\t\t The criterion ( i.e. , there is some , perhaps small , two frames are therefore designated as overlap between the FrameNet and SemFrame corresponding frames by meeting both the framesets ) and a frame-name-relatedness minimal-overlap and the frame-name relatedness criterion . \n\t', '\n\t\t The minimal-overlap criterion is met if criteria . \n\t', '\n\t\t either of two conditions is met : ( 1 ) If the In the second approach , a SemFrame frame is FrameNet frame lists four or fewer verbs ( true of deemed to correspond to a FrameNet frame if the over one-third of the FrameNet frames that list two frames meet either of two relatively stringent associated verbs ) , minimal overlap occurs when verb overlap criteria , the majority-match criterion any one verb associated with the FrameNet frame or the majority-related criterion , in which case matches a verb associated with a SemFrame examination of frame names is unnecessary . \n\t', '\n\t\t frame . \n\t', '\n\t\t ( 2 ) If the FrameNet frame lists five or The majority-match criterion is met if the set more verbs , minimal overlap occurs when two or of verbs shared by FrameNet and SemFrame more verbs in the FrameNet frame are matched by framesets account for half or more of the verbs in verbs in the SemFrame frame . \n\t', '\n\t\t either frameset . \n\t', '\n\t\t For example , the APPLY_HEAT The looseness of the minimal overlap frame in FrameNet includes 22 verbs : bake , criterion is tightened by also requiring that the blanch , boil , braise , broil , brown , char , coddle , names of the FrameNet and SemFrame frames be cook , fry , grill , microwave , parboil , poach , roast , closely related . \n\t', ""\n\t\t Establishing this frame-name saute , scald , simmer , steam , steep , stew , and relatedness involves identifying individual toast , while the BOILING frame in SemFrame components of each frame name ' and augmenting includes 7 verbs : boil , coddle , jug , parboil , this set with morphological variants from CatVar poach , seethe , and simmer . \n\t"", '\n\t\t Five of these \n\t\t']",Positive
"['\n\t\t The resulting set for verbs\x97boil , coddle , parboil , poach , and each FrameNet and SemFrame frame name is simmer\x97are shared across the two frames and then searched in both the noun and verb WordNet constitute over half of the SemFrame frameset . \n\t', '\n\t\t networks to find all the synsets that might Therefore the two frames are deemed to correspond to the frame name . \n\t', '\n\t\t To these sets are correspond by meeting the majority-match also added all synsets directly related to the criterion . \n\t', '\n\t\t synsets corresponding to the frame names . \n\t', '\n\t\t If the The majority-related criterion is met if half or resulting set of synsets gathered for a FrameNet more of the verbs from the SemFrame frame are frame name intersects with the set of synsets semantically related to verbs from the FrameNet gathered for a SemFrame frame name , the two frame ( that is , if the precision of the SemFrame frame names are deemed to be semantically verb set is at least 0.5 ) . \n\t', '\n\t\t To evaluate this criterion , related . \n\t', '\n\t\t each FrameNet and SemFrame verb is associated For example , the FrameNet ADORNING frame with the WordNet verb synsets it occurs in , contains 17 verbs : adorn , blanket , cloak , coat , augmented by the synsets to which the initial sets cover , deck , decorate , dot , encircle , envelop , of synsets are directly related . \n\t', '\n\t\t If the sets of festoon , fill , film , line , pave , stud , and wreathe . \n\t', '\n\t\t synsets corresponding to two verbs share one or The SemFrame ORNAMENTATION frame contains more synsets , the two verbs are deemed to be 12 verbs : adorn , caparison , decorate , embellish , semantically related . \n\t', ""\n\t\t This process is extended embroider , garland , garnish , gild , grace , hang , one further level , such that a SemFrame verb found by this process to be semantically related to a SemFrame verb , whose semantic relationship to ' a FrameNet verb has already been established , will also be designated a frame-evoking verb . \n\t"", '\n\t\t If half or more of the verbs listed for a SemFrame frame are established as evoking the same frame as the list of WordNet verbs , then the FrameNet All SemFrame frame names are nouns . \n\t', '\n\t\t ( See Green and Dorr , 2004 for an explanation of their selection . \n\t', '\n\t\t ) FrameNet frame names ( e.g. , ABUNDANCE , ACTIVITY_START , CAUSE_TO_BE_WET , INCHOATIVE_ATTACHING ) , however , exhibit considerable variation . \n\t', '\n\t\t and SemFrame frames are hypothesized to bound on the task , i.e. , 100 % recall and 100 % correspond through the majority-related criterion . \n\t', '\n\t\t precision . \n\t', '\n\t\t The Lin & Pantel results are here a For example , the FrameNet ABUNDANCE lower bound for automatically induced semantic frame includes 4 verbs : crawl , swarm , teem , and verb classes and probably reflect the limitations of throng . \n\t', '\n\t\t The SemFrame FLOW frame likewise using only corpus data . \n\t', '\n\t\t Among efforts to develop includes 4 verbs : pour , teem , stream , and semantic verb classes , SemFrame\x92s results pullulate . \n\t', '\n\t\t Only one verb\x97teem\x97is shared , so correspond more closely to semantic frames than the majority-match criterion is not met , nor is the do others . \n\t', '\n\t\t related-frame-name criterion met , as the frame names are not semantically related . \n\t', '\n\t\t The majority- related criterion , however , is met through a WordNet verb synset that includes pour , swarm , stream , teem , and pullulate . \n\t', '\n\t\t Of the 197 FrameNet frames that include at least one LDOCE verb , 175 were found to have a corresponding SemFrame frame . \n\t', '\n\t\t But this 88.8 % recall level should be balanced against the precision ratio of SemFrame verb framesets . \n\t', '\n\t\t After all , we could get 100 % recall by listing all verbs in every SemFrame frame . \n\t', '\n\t\t The majority-related function computes the precision ratio of the SemFrame frame for each pair of FrameNet and SemFrame frames being compared . \n\t', '\n\t\t By modifying the minimum precision threshold , the balance between recall and precision , as measured using F-score , can be investigated . \n\t', '\n\t\t The best balance for the SemFrame version is based on a clustering threshold of 2.0 and a minimum precision threshold of 0.4 , which yields a recall of 83.2 % and overall precision of 73.8 % . \n\t', '\n\t\t To interpret these results meaningfully , one would like to know if SemFrame achieves more FrameNet-like results than do other available verb category data , more specifically the 258 verb classes from Levin , the 357 semantic verb classes of WordNet 1.7.1 , or the 272 verb clusters of Lin and Pantel , as described in Section 2 . \n\t', '\n\t\t For purposes of comparison with FrameNet , Levin\x92s verb class names have been hand-edited to isolate the word that best captures the semantic sense of the class ; the name of a WordNet-based frame is taken from the words for the root-level synset ; and the name of each Lin and Pantel cluster is taken to be the first verb in the cluster.4 Evaluation results for the best balance between recall and precision ( i.e. , the maximum F-score ) of the four comparisons are summarized in Table 3 . \n\t', '\n\t\t FrameNet itself constitutes the upper 4Lin and Pantel have taken a similar approach , \x93naming\x94 their verb clusters by the first three verbs listed for a cluster , i.e. , the three most similar verbs . \n\t', '\n\t\t Semantic verb classes Precision threshold at max F- Recall Precision score SemFrame 0.40 0.832 0.738 Levin 0.20 0.569 0.550 WordNet 0.15 0.528 0.466 Lin & Pantel 0.15 0.472 0.407 Table 3 . \n\t', '\n\t\t Best Recall-Precision Balance When Compared with FrameNet 7 Conclusions and Future Work We have demonstrated that sets of verbs evoking a common semantic frame can be induced from existing lexical tools . \n\t', '\n\t\t In a head-to-head comparison with frames in FrameNet , the frame semantic verb classes developed by the SemFrame approach achieve a recall of 83.2 % and the verbs listed for frames achieve a precision of 73.8 % ; these results far outpace those of other semantic verb classes . \n\t', '\n\t\t On a practical level , a large number of frame semantic verb classes have been identified . \n\t', '\n\t\t Associated with clustering threshold 1.5 are 1421 verb classes , averaging 14.1 WordNet verb synsets . \n\t', '\n\t\t Associated with clustering threshold 2.0 are 1563 verb classes , averaging 6.6 WordNet verb synsets . \n\t', '\n\t\t Despite these promising results , we are limited by the scope of our input data set . \n\t', '\n\t\t While LDOCE and WordNet data are generally of high quality , the relative sparseness of these resources has an adverse impact on recall . \n\t', '\n\t\t In addition , the mapping technique used for picking out corresponding word senses in WordNet and LDOCE is shallow , thus constraining the recall and precision of SemFrame outputs . \n\t', '\n\t\t Finally , the multi-step process of merging smaller verb groups into verb groups that are intended to correspond to frames sometimes fails to achieve an appropriate degree of correspondence ( all the verb classes discovered are not distinct ) . \n\t', '\n\t\t In our future work , we will experiment with the more recent release of WordNet ( 2.0 ) . \n\t', '\n\t\t This version provides derivational morphology links between nouns and verbs , which will promote far greater precision in the linking of verb senses based on morphology than was possible in our initial implementation . \n\t', ""\n\t\t Another significant addition to WordNet 2.0 is the inclusion of category domains , which co-locate words pertaining to a subject and perform the same function as LDOCE 's subject field codes . \n\t"", '\n\t\t Finally , data sparseness issues may be addressed by supplementing the use of the lexical resources used here with access to , for example , the British National Corpus , with its broad coverage and carefully-checked parse trees . \n\t', '\n\t\t Acknowledgments This research has been supported in part by a National Science Foundation Graduate Research Fellowship NSF ITR grant #IIS-0326553 , and NSF CISE Research Infrastructure Award EIA0130422 . \n\t', '\n\t\t References Boguraev , Bran and Ted Briscoe . \n\t', '\n\t\t 1989. Introduction . \n\t', '\n\t\t In B. Boguraev and T. Briscoe ( Eds . \n\t', '\n\t\t ) , Computational Lexicography for Natural Language Processing , 1- 40. London : Longman . \n\t', '\n\t\t EAGLES Lexicon Interest Group . \n\t', '\n\t\t 1998. EAGLES Preliminary Recommendations on Semantic Encoding : Interim Report , <http:// www.ilc.cnr.it/EAGLES96/rep2/ rep2.html> . \n\t', '\n\t\t Fellbaum , Christiane ( Ed . \n\t', '\n\t\t 1998a . \n\t', '\n\t\t WordNet : An Electronic Lexical Database . \n\t', '\n\t\t Cambridge , MA : The MIT Press . \n\t', '\n\t\t Fellbaum , Christiane . \n\t', '\n\t\t 1998b . \n\t', '\n\t\t Introduction . \n\t', '\n\t\t In C. Fellbaum , 1998a , 1-17 . \n\t', '\n\t\t Fillmore , Charles J. 1982 . \n\t', '\n\t\t Frame semantics . \n\t', '\n\t\t In Linguistics in the Morning Calm , 111-137 . \n\t', '\n\t\t Seoul : Hanshin . \n\t', '\n\t\t Fillmore , Charles J. and B. T. S. Atkins . \n\t', '\n\t\t 1992. Towards a frame-based lexicon : The semantics of RISK and its neighbors . \n\t', '\n\t\t In A. Lehrer and E. F. Kittay ( Eds . \n\t', '\n\t\t ) , Frames , Fields , and Contrasts , 75- 102 . \n\t', '\n\t\t Hillsdale , NJ : Erlbaum . \n\t', '\n\t\t Green , Rebecca . \n\t', '\n\t\t 2004. Inducing Semantic Frames from Lexical Resources . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t dissertation , University of Maryland . \n\t', '\n\t\t Green , Rebecca and Bonnie J. Dorr . \n\t', '\n\t\t 2004. Inducing A Semantic Frame Lexicon from WordNet Data . \n\t', '\n\t\t In Proceedings of the 2nd Workshop on Text Meaning and Interpretation ( ACL 2004 ) . \n\t', '\n\t\t Habash , Nizar and Bonnie Dorr . \n\t', '\n\t\t 2003. A categorial variation database for English . \n\t', '\n\t\t In Proceedings of North American Association for Computational Linguistics , 96-102 . \n\t', '\n\t\t Hirst , Graeme . \n\t', '\n\t\t 2003 . \n\t', '\n\t\t Paraphrasing paraphrased . \n\t', '\n\t\t Keynote address for The Second International Workshop on Paraphrasing : Paraphrase Acquisition and Applications , ACL 2003 , <http://nlp.nagaokaut.ac.jp/IWP2003/pdf/ Hirst-slides.pdf> . \n\t', '\n\t\t Johnson , Christopher R. , Charles J. Fillmore , Miriam R. L. Petruck , Collin F. Baker , Michael Ellsworth , Josef Ruppenhofer , and Esther J. Wood . \n\t', '\n\t\t 2002. FrameNet : Theory and Practice , version 1.0 , <http://www.icsi.berkeley.edu/ ~framenet/book/book.html> . \n\t', '\n\t\t Kozlowski , Raymond , Kathleen F. McCoy , and K. Vijay-Shanker . \n\t', '\n\t\t 2003. Generation of single-sentence paraphrases from predicate/argument structure using lexico-grammatical resources . \n\t', '\n\t\t In The Second International Workshop on Paraphrasing : Paraphrase Acquisition and Applications ( IWP2003 ) , ACL 2003 , 1-8 . \n\t', '\n\t\t Levin , Beth . \n\t', '\n\t\t 1993. English Verb Classes and Alternations : A Preliminary Investigation . \n\t', '\n\t\t Chicago : University of Chicago Press . \n\t', '\n\t\t Lin , Dekang and Patrick Pantel . \n\t', '\n\t\t 2001 . \n\t', '\n\t\t Induction of semantic classes from natural language text . \n\t', '\n\t\t In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining , 317-322 . \n\t', '\n\t\t Litkowski , Ken . \n\t', '\n\t\t 2004. Senseval-3 task : Word-sense disambiguation of WordNet glosses , <http://www.clres.com/SensWNDisamb.html> . \n\t', '\n\t\t Miller , George A. 1998 . \n\t', '\n\t\t Nouns in WordNet . \n\t', '\n\t\t In C. Fellbaum , 1998a , 23-67 . \n\t', '\n\t\t Pantel , Patrick and Dekang Lin . \n\t', '\n\t\t 2002 . \n\t', '\n\t\t Discovering word senses from text . \n\t', '\n\t\t In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 613- 619 . \n\t', '\n\t\t Procter , Paul ( Ed . \n\t', '\n\t\t 1978. Longman Dictionary of Contemporary English . \n\t', '\n\t\t Longman Group Ltd. , Essex , UK . \n\t', '\n\t\t Rinaldi , Fabio , James Dowdall , Kaarel Kaljurand , Michael Hess , and Diego Mollá . \n\t', '\n\t\t 2003. Exploiting paraphrases in a question answering system . \n\t', '\n\t\t In The Second International Workshop on Paraphrasing : Paraphrase Acquisition and Applications ( IWP2003 ) , ACL 2003 , 25-32 . \n\t', '\n\t\t Voorhees , Ellen . \n\t', '\n\t\t 1986. Implementing agglomerative hierarchic clustering algorithms for use in document retrieval . \n\t', '\n\t\t Information Processing & Management 22/6 : 465-476 . \n\t', '\n\t\t Paragraph- , word- , and coherence-based approaches to sentence ranking : A comparison of algorithm and human performance Florian WOLF Massachusetts Institute of Technology MIT NE20-448 , 3 Cambridge Center Cambridge , MA 02139 , USA fwolf@mit.edu Abstract Sentence ranking is a crucial part of generating text summaries . \n\t', '\n\t\t We compared human sentence rankings obtained in a psycholinguistic experiment to three different approaches to sentence ranking : A simple paragraph-based approach intended as a baseline , two word-based approaches , and two coherence-based approaches . \n\t', '\n\t\t In the paragraph-based approach , sentences in the beginning of paragraphs received higher importance ratings than other sentences . \n\t', '\n\t\t The word-based approaches determined sentence rankings based on relative word frequencies ( \n\t\t']",Positive
['\n\t\t Coherence-based approaches determined sentence rankings based on some property of the coherence structure of a text ( \n\t\t'],Positive
"['\n\t\t Our results suggest poor performance for the simple paragraph-based approach , whereas word- based approaches perform remarkably well . \n\t', '\n\t\t The best performance was achieved by a coherence-based approach where coherence structures are represented in a non-tree structure . \n\t', '\n\t\t Most approaches also outperformed the commercially available MSWord summarizer . \n\t', '\n\t\t 1 Introduction Automatic generation of text summaries is a natural language engineering application that has received considerable interest , particularly due to the ever-increasing volume of text information available through the internet . \n\t', '\n\t\t The task of a human generating a summary generally involves three subtasks ( \n\t\t']",Positive
"['\n\t\t Like most approaches to summarization , we are concerned with the second subtask ( e.g. \n\t\t']",Positive
"['\n\t\t Furthermore , we are concerned with obtaining generic rather than query-relevant importance rankings ( cf. \n\t\t']",Positive
"['\n\t\t We evaluated different approaches to sentence ranking against human sentence rankings . \n\t', '\n\t\t To obtain human sentence rankings , we asked people to read 15 texts from the Wall Street Journal on a wide variety of topics ( e.g. economics , foreign and domestic affairs , political commentaries ) . \n\t', '\n\t\t For each of the sentences in the text , they provided a ranking of how important that sentence is with respect to the content of the text , on an integer scale from 1 ( not important ) to 7 ( very important ) . \n\t', '\n\t\t The approaches we evaluated are a simple paragraph-based approach that serves as a baseline , two word-based algorithms , and two coherence- based approaches1 . \n\t', '\n\t\t We furthermore evaluated the MSWord summarizer . \n\t', '\n\t\t 2 Approaches to sentence ranking 2.1 Paragraph-based approach Sentences at the beginning of a paragraph are usually more important than sentences that are further down in a paragraph , due in part to the way people are instructed to write . \n\t', '\n\t\t Therefore , probably the simplest approach conceivable to sentence ranking is to choose the first sentences of each 1 We did not use any machine learning techniques to boost performance of the algorithms we tested . \n\t', '\n\t\t Therefore performance of the algorithms tested here will almost certainly be below the level of performance that could be reached if we had augmented the algorithms with such techniques ( e.g. \n\t\t']",Positive
"['\n\t\t However , we think that a comparison between \x91bare-bones\x92 algorithms is viable because it allows to see how performance differs due to different basic approaches to sentence ranking , and not due to potentially different effects of different machine learning algorithms on different basic approaches to sentence ranking . \n\t', '\n\t\t In future research we plan to address the impact of machine learning on the algorithms tested here . \n\t', '\n\t\t paragraph as important , and the other sentences as not important . \n\t', '\n\t\t We included this approach merely as a simple baseline . \n\t', '\n\t\t 2.2 Word-based approaches Word-based approaches to summarization are based on the idea that discourse segments are important if they contain \x93important\x94 words . \n\t', '\n\t\t Different approaches have different definitions of what an important word is . \n\t', '\n\t\t For example , \n\t\t']",Positive
"['\n\t\t Significant words are words that are not in some predefined stoplist of words with high overall corpus frequency2 . \n\t', '\n\t\t Once significant words are marked in a text , clusters of significant words are formed . \n\t', '\n\t\t A cluster has to start and end with a significant word , and fewer than n insignificant words must separate any two significant words ( we chose n = 3 , cf. \n\t\t']",Positive
"['\n\t\t Then , the weight of each cluster is calculated by dividing the square of the number of significant words in the cluster by the total number of words in the cluster . \n\t', '\n\t\t Sentences can contain multiple clusters . \n\t', '\n\t\t In order to compute the weight of a sentence , the weights of all clusters in that sentence are added . \n\t', '\n\t\t The higher the weight of a sentence , the higher is its ranking . \n\t', '\n\t\t A more recent and frequently used word-based method used for text piece ranking is tf.idf ( e.g. Manning & \n\t\t']",Positive
"['\n\t\t The tf.idf measure relates the frequency of words in a text piece , in the text , and in a collection of texts respectively . \n\t', '\n\t\t The intuition behind tf. idf is to give more weight to sentences that contain terms with high frequency in a document but low frequency in a reference corpus . \n\t', '\n\t\t Figure 1 shows a formula for calculating tf.idf , where dsij is the tf.idf weight of sentence i in document j , nsi is the number of words in sentence i , k is the kth word in sentence i , tfjk is the frequency of word k in document j , nd is the number of documents in the reference corpus , and dfk is the number of documents in the reference corpus in which word k appears . \n\t', '\n\t\t n , . \n\t', '\n\t\t nd k=1 = ~ t ^ log df ~jk k ~ ~ Figure 1 . \n\t', '\n\t\t Formula for calculating tf.idf ( Salton & \n\t\t']",Positive
"['\n\t\t 2 Instead of stoplists , tf.idf values have also been used to determine significant words ( e.g. \n\t\t']",Positive
"['\n\t\t We compared both Luhn (1958)\x92s measure and tf.idf scores to human rankings of sentence importance . \n\t', '\n\t\t We will show that both methods performed remarkably well , although one coherence-based method performed better . \n\t', '\n\t\t 2.3 Coherence-based approaches The sentence ranking methods introduced in the two previous sections are solely based on layout or on properties of word distributions in sentences , texts , and document collections . \n\t', '\n\t\t Other approaches to sentence ranking are based on the informational structure of texts . \n\t', '\n\t\t With informational structure , we mean the set of informational relations that hold between sentences in a text . \n\t', '\n\t\t This set can be represented in a graph , where the nodes represent sentences , and labeled directed arcs represent informational relations that hold between the sentences ( cf. \n\t\t']",Positive
"['\n\t\t Often , informational structures of texts have been represented as trees ( e.g. \n\t\t']",Positive
"['\n\t\t We will present one coherence-based approach that assumes trees as a data structure for representing discourse structure , and one approach that assumes less constrained graphs . \n\t', '\n\t\t As we will show , the approach based on less constrained graphs performs better than the tree-based approach when compared to human sentence rankings . \n\t', '\n\t\t 3 Coherence-based summarization revisited This section will discuss in more detail the data structures we used to represent discourse structure , as well as the algorithms used to calculate sentence importance , based on discourse structures . \n\t', '\n\t\t 3.1 Representing coherence structures 3.1.1 Discourse segments Discourse segments can be defined as non- overlapping spans of prosodic units ( Hirschberg & \n\t\t']",Positive
"['\n\t\t We adopted a sentence unit-based definition of discourse segments for the coherence-based approach that assumes non-tree graphs . \n\t', '\n\t\t For the coherence-based approach that assumes trees , we used Marcu (2000)\x92s more fine-grained definition of discourse segments because we used the discourse trees from Carlson et al . \n\t', '\n\t\t (2002)\x92s database of coherence- annotated texts . \n\t', '\n\t\t 3.1.2 Kinds of coherence relations We assume a set of coherence relations that is similar to that of \n\t\t']",Positive
"['\n\t\t Below are examples of each coherence relation . \n\t', '\n\t\t ij ds ( 1 ) Cause-Effect [ There was bad weather at the airport]a [ and so our flight got delayed.]b ( 2 ) Violated Expectation [ The weather was nice]a [ but our flight got delayed.]b ( 3 ) Condition [ If the new software works,]a [ everyone will be happy.]b ( 4 ) Similarity [ There is a train on Platform A.]a [ There is another train on Platform B.]b ( 5 ) Contrast [ John supported Bush]a [ but Susan opposed him.]b ( 6 ) Elaboration [ A probe to Mars was launched this week.]a [ The European-built \x91Mars Express\x92 is scheduled to reach Mars by late December.]b ( 7 ) Attribution [ John said that]a [ the weather would be nice tomorrow.]b ( 8 ) Temporal Sequence [ Before he went to bed,]a [ John took a shower.]b Cause-effect , violated expectation , condition , elaboration , temporal sequence , and attribution are asymmetrical or directed relations , whereas similarity , contrast , and temporal sequence are symmetrical or undirected relations ( Mann & Thompson , 1988 ; Marcu , 2000 ) . \n\t', '\n\t\t In the non-treebased approach , the directions of asymmetrical or directed relations are as follows : cause 4 effect for cause-effect ; cause 4 absent effect for violated expectation ; condition 4 consequence for condition ; elaborating 4 elaborated for elaboration , and source 4 attributed for attribution . \n\t', '\n\t\t In the tree-based approach , the asymmetrical or directed relations are between a more important discourse segment , or a Nucleus , and a less important discourse segment , or a Satellite ( \n\t\t']",Positive
"['\n\t\t The Nucleus is the equivalent of the arc destination , and the Satellite is the equivalent of the arc origin in the non-treebased approach . \n\t', '\n\t\t The symmetrical or undirected relations are between two discourse elements of equal importance , or two Nuclei . \n\t', '\n\t\t Below we will explain how the difference between Satellites and Nuclei is considered in tree-based sentence rankings . \n\t', '\n\t\t 3.1.3 Data structures for representing discourse coherence As mentioned above , we used two alternative representations for discourse structure , tree- and non-tree based . \n\t', '\n\t\t In order to illustrate both data structures , consider ( 9 ) as an example : ( 9 ) Example text 0 . \n\t', '\n\t\t Susan wanted to buy some tomatoes . \n\t', '\n\t\t 1. She also tried to find some basil . \n\t', '\n\t\t 2. The basil would probably be quite expensive at this time of the year . \n\t', '\n\t\t Figure 2 shows one possible tree representation of the coherence structure of (9)3 . \n\t', '\n\t\t Sim represents a similarity relation , and elab an elaboration relation . \n\t', '\n\t\t Furthermore , nodes with a \x93Nuc\x94 subscript are Nuclei , and nodes with a \x93Sat\x94 subscript are Satellites . \n\t', '\n\t\t Figure 2. Coherence tree for ( 9 ) . \n\t', '\n\t\t Figure 3 shows a non-tree representation of the coherence structure of ( 9 ) . \n\t', '\n\t\t Here , the heads of the arrows represent the directionality of a relation . \n\t', '\n\t\t Figure 3. Non-tree coherence graph for ( 9 ) . \n\t', '\n\t\t 3.2 Coherence-based sentence ranking This section explains the algorithms for the tree- and the non-tree-based sentence ranking approach . \n\t', '\n\t\t 3.2.1 Tree-based approach We used Marcu (2000)\x92s algorithm to determine sentence rankings based on tree discourse structures . \n\t', '\n\t\t In this algorithm , sentence salience is determined based on the tree level of a discourse segment in the coherence tree . \n\t', '\n\t\t Figure 4 shows Marcu (2000)\x92s algorithm , where r(s,D,d) is the rank of a sentence s in a discourse tree D with depth d . \n\t', '\n\t\t Every node in a discourse tree D has a promotion set promotion(D) , which is the union of all Nucleus children of that node . \n\t', '\n\t\t Associated with every node in a discourse tree D is also a set of parenthetical nodes parentheticals(D) ( for example , in \x93Mars \x96 half the size of Earth \x96 is red\x94 , \x93half the size of earth\x94 would be a parenthetical node in a discourse tree ) . \n\t', '\n\t\t Both promotion(D) and parentheticals(D) can be empty sets . \n\t', '\n\t\t Furthermore , each node has a left subtree , 3 Another possible tree structure might be ( elab ( par ( 0 1 ) 2 ) ) . \n\t', '\n\t\t sim 0Nuc 1 Nuc 2Sat elabNuc elab sim 0 1 2 lc(D) , and a right subtree , rc(D) . \n\t', '\n\t\t Both lc(D) and rc(D) can also be empty . \n\t', '\n\t\t calculated PageRanks for a set to values between 0.05 and 0.95 , in increments of 0.05 ; changing a did not affect performance . \n\t', '\n\t\t 0 if D is NIL PR 1 PR n ^ n =1^^+^ d if s ^ promotion(D) , 1 d ^1 if s ^ parentheticals(D) , max(r(s , lc(D) , d 1 on Figure 5 . \n\t', '\n\t\t Formula for calculating PageRank ( \n\t\t']",Positive
"['\n\t\t r(s,D,d) ~ f , ~ ~ ~ r d ^1 ) ) otherwise ( s , rc(D) , Figure 4 . \n\t', '\n\t\t Formula for calculating coherence-treebased sentence rank ( \n\t\t']",Positive
"['\n\t\t The discourse segments in Carlson et al . \n\t', '\n\t\t (2002)\x92s database are often sub-sentential . \n\t', '\n\t\t Therefore , we had to calculate sentence rankings from the rankings of the discourse segments that form the sentence under consideration . \n\t', '\n\t\t We did this by calculating the average ranking , the minimal ranking , and the maximal ranking of all discourse segments in a sentence . \n\t', '\n\t\t Our results showed that choosing the minimal ranking performed best , followed by the average ranking , followed by the maximal ranking ( cf. . \n\t', '\n\t\t Section 4.4 ) . \n\t', '\n\t\t 3.2.2 Non-tree-based approach We used two different methods to determine sentence rankings for the non-tree coherence graphs4 . \n\t', '\n\t\t Both methods implement the intuition that sentences are more important if other sentences relate to them ( \n\t\t']",Positive
"['\n\t\t The first method consists of simply determining the in-degree of each node in the graph . \n\t', '\n\t\t A node represents a sentence , and the in-degree of a node represents the number of sentences that relate to that sentence . \n\t', '\n\t\t The second method uses Page et al . \n\t', '\n\t\t (1998)\x92s PageRank algorithm , which is used , for example , in the GoogleTM search engine . \n\t', '\n\t\t Unlike just determining the in-degree of a node , PageRank takes into account the importance of sentences that relate to a sentence . \n\t', '\n\t\t PageRank thus is a recursive algorithm that implements the idea that the more important sentences relate to a sentence , the more important that sentence becomes . \n\t', '\n\t\t Figure 5 shows how PageRank is calculated . \n\t', '\n\t\t PRn is the PageRank of the current sentence , PRn_1 is the PageRank of the sentence that relates to sentence n , on_1 is the out-degree of sentence n_1 , and a is a damping parameter that is set to a value between 0 and 1 . \n\t', '\n\t\t We report results for a set to 0.85 because this is a value often used in applications of PageRank ( e.g. \n\t\t']",Positive
"['\n\t\t We also 4 Neither of these methods could be implemented for coherence trees since Marcu (2000)\x92s tree-based algorithm assumes binary branching trees . \n\t', '\n\t\t Thus , the in- degree for all non-terminal nodes is always 2 . \n\t', '\n\t\t 4 Experiments In order to test algorithm performance , we compared algorithm sentence rankings to human sentence rankings . \n\t', '\n\t\t This section describes the experiments we conducted . \n\t', '\n\t\t In Experiment 1 , the texts were presented with paragraph breaks ; in Experiment 2 , the texts were presented without paragraph breaks . \n\t', '\n\t\t This was done to control for the effect of paragraph information on human sentence rankings . \n\t', '\n\t\t 4.1 Materials for the coherence-based approaches In order to test the tree-based approach , we took coherence trees for 15 texts from a database of 385 texts from the Wall Street Journal that were annotated for coherence ( \n\t\t']",Positive
"['\n\t\t The database was independently annotated by six annotators . \n\t', '\n\t\t Inter-annotator agreement was determined for six pairs of two annotators each , resulting in kappa values ( \n\t\t']",Positive
"['\n\t\t No kappa values for just the 15 texts we used were available . \n\t', '\n\t\t For the non-tree based approach , we used coherence graphs from a database of 135 texts from the Wall Street Journal and the AP Newswire , annotated for coherence . \n\t', '\n\t\t Each text was independently annotated by two annotators . \n\t', '\n\t\t For the 15 texts we used , kappa was 0.78 , for the whole database , kappa was 0.84 . \n\t', '\n\t\t 4.2 Experiment 1 : With paragraph information 15 participants from the MIT community were paid for their participation . \n\t', '\n\t\t All were native speakers of English and were naïve as to the purpose of the study ( i.e. none of the subjects was familiar with theories of coherence in natural language , for example ) . \n\t', '\n\t\t Participants were asked to read 15 texts from the Wall Street Journal , and , for each sentence in each text , to provide a ranking of how important that sentence is with respect to the content of the text , on an integer scale from 1 to 7 ( 1 = not important ; 7 = very important ) . \n\t', '\n\t\t The texts were selected so Figure 6 . \n\t', '\n\t\t Human ranking results for one text ( wsj_1306 ) . \n\t', '\n\t\t 8 7 6 5 4 3 2 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 KbParagraph With Paragraph sentence number that there was a coherence tree annotation available in Carlson et al . \n\t', '\n\t\t (2002)\x92s database . \n\t', '\n\t\t Text lengths for the 15 texts we selected ranged from 130 to 901 words ( 5 to 47 sentences ) ; average text length was 442 words ( 20 sentences ) , median was 368 words ( 16 sentences ) . \n\t', '\n\t\t Additionally , texts were selected so that they were about as diverse topics as possible . \n\t', '\n\t\t The experiment was conducted in front of personal computers . \n\t', '\n\t\t Texts were presented in a web browser as one webpage per text ; for some texts , participants had to scroll to see the whole text . \n\t', '\n\t\t Each sentence was presented on a new line . \n\t', '\n\t\t Paragraph breaks were indicated by empty lines ; this was pointed out to the participants during the instructions for the experiment . \n\t', '\n\t\t 4.3 Experiment 2 : Without paragraph information The method was the same as in Experiment 1 , except that texts in Experiment 2 did not include paragraph information . \n\t', '\n\t\t Each sentence was presented on a new line . \n\t', '\n\t\t None of the 15 participants who participated in Experiment 2 had participated in Experiment 1 . \n\t', '\n\t\t 4.4 Results of the experiments Human sentence rankings did not differ significantly between Experiment 1 and Experiment 2 for any of the 15 texts ( all Fs < 1 ) . \n\t', '\n\t\t This suggests that paragraph information does not have a big effect on human sentence rankings , at least not for the 15 texts that we examined . \n\t', '\n\t\t Figure 6 shows the results from both experiments for one text . \n\t', '\n\t\t We compared human sentence rankings to different algorithmic approaches . \n\t', '\n\t\t The paragraph- based rankings do not provide scaled importance rankings but only \x93important\x94 vs. \x93not important\x94 . \n\t', '\n\t\t Therefore , in order to compare human rankings to the paragraph-based baseline approach , we calculated point biserial correlations ( cf. \n\t\t']",Positive
"['\n\t\t We obtained significant correlations between paragraph-based rankings and human rankings only for one of the 15 texts . \n\t', '\n\t\t All other algorithms provided scaled importance rankings . \n\t', '\n\t\t Many evaluations of scalable sentence ranking algorithms are based on precision/recall/Fscores ( e.g. \n\t\t']",Positive
"['\n\t\t However , \n\t\t']",Positive
"['\n\t\t For example , imagine a situation where the human ranking for a given sentence is \x937\x94 ( \x93very important\x94 ) on an integer scale ranging from 1 to 7 , and Algorithm A gives the same sentence a ranking of \x937\x94 on the same scale , Algorithm B gives a ranking of \x936\x94 , and Algorithm C gives a ranking of \x932\x94 . \n\t', '\n\t\t Intuitively , Algorithm B , although it does not reach perfect performance , still performs better than Algorithm C. Precision/recall/F-scores do not account for that difference and would rate Algorithm A as \x93hit\x94 but Algorithm B as well as Algorithm C as \x93miss\x94 . \n\t', '\n\t\t In order to collect performance measures that are more adequate to the evaluation of scaled importance rankings , we computed Spearman\x92s rank correlation coefficients . \n\t', '\n\t\t The rank correlation coefficients were corrected for tied ranks because in our rankings it was possible for more than one sentence to have the same importance rank , i.e. to have tied ranks ( \n\t\t']",Positive
"['\n\t\t In addition to evaluating word-based and coherence-based algorithms , we evaluated one commercially available summarizer , the MSWord summarizer , against human sentence rankings . \n\t', '\n\t\t Our reason for including an evaluation of the MSWord summarizer was to have a more useful baseline for scalable sentence rankings than the paragraph-based approach provides . \n\t', '\n\t\t Figure 7 . \n\t', '\n\t\t Average rank correlations of algorithm and human sentence rankings . \n\t', '\n\t\t NoParagraph WithParagraph 0.6 0.5 0.4 0.3 0.2 0.1 0 MSWord Luhn tf.idf MarcuAvg MarcuMin MarcuMax in-degree PageRank Figure 7 shows average rank correlations ( ^avg ) of each algorithm and human sentence ranking for the 15 texts . \n\t', '\n\t\t MarcuAvg refers to the version of Marcu (2000)\x92s algorithm where we calculated sentence rankings as the average of the rankings of all discourse segments that constitute that sentence ; for MarcuMin , sentence rankings were the minimum of the rankings of all discourse segments in that sentence ; for MarcuMax we selected the maximum of the rankings of all discourse segments in that sentence . \n\t', '\n\t\t Figure 7 shows that the MSWord summarizer performed numerically worse than most other algorithms , except MarcuMin . \n\t', '\n\t\t Figure 7 also shows that PageRank performed numerically better than all other algorithms . \n\t', '\n\t\t Performance was significantly better than most other algorithms ( MSWord , NoParagraph : F(1,28) = 21.405 , p = 0.0001 ; MSWord , WithParagraph : F(1,28) = 26.071 , p = 0.0001 ; Luhn , WithParagraph : F(1,28) = 5.495 , p = 0.026 ; MarcuAvg , NoParagraph : F(1,28) = 9.186 , p = 0.005 ; MarcuAvg , WithParagraph : F(1,28) = 9.097 , p = 0.005 ; MarcuMin , NoParagraph : F(1,28) = 4.753 , p = 0.038 ; MarcuMax , NoParagraph F(1,28) = 24.633 , p = 0.0001 ; MarcuMax , WithParagraph : F(1,28) = 31.430 , p =0.0001 ) . \n\t', '\n\t\t Exceptions are Luhn , NoParagraph ( F(1,28) = 1.859 , p = 0.184 ) ; tf.idf , NoParagraph ( F(1,28) = 2.307 , p = 0.14 ) ; MarcuMin , WithParagraph ( F(1,28) = 2.555 , p = 0.121 ) . \n\t', '\n\t\t The difference between PageRank and tf.idf , WithParagraph was marginally significant ( F(1,28) = 3.113 , p = 0.089 ) . \n\t', '\n\t\t As mentioned above , human sentence rankings did not differ significantly between Experiment 1 and Experiment 2 for any of the 15 texts ( all Fs < 1 ) . \n\t', '\n\t\t Therefore , in order to lend more power to our statistical tests , we collapsed the data for each text for the WithParagraph and the NoParagraph condition , and treated them as one experiment . \n\t', '\n\t\t Figure 8 shows that when the data from Experiments 1 and 2 are collapsed , PageRank performed significantly better than all other algorithms except in-degree ( two-tailed t-test results : MSWord : F(1 , 58 ) = 48.717 , p = 0.0001 ; Luhn : F(1,58) = 6.368 , p = 0.014 ; tf.idf : F(1,58) = 5.522 , p = 0.022 ; MarcuAvg : F(1,58) = 18.922 , p = 0.0001 ; MarcuMin : F(1,58) = 7.362 , p = 0.009 ; MarcuMax : F(1,58) = 56.989 , p = 0.0001 ; in- degree : F(1,58) < 1 ) . \n\t', '\n\t\t Figure 8 . \n\t', '\n\t\t Average rank correlations of algorithm and human sentence rankings with collapsed data . \n\t', '\n\t\t 5 Conclusion The goal of this paper was to evaluate the results of three different kinds of sentence ranking algorithms and one commercially available summarizer . \n\t', '\n\t\t In order to evaluate the algorithms , we compared their sentence rankings to human sentence rankings of fifteen texts of varying length from the Wall Street Journal . \n\t', '\n\t\t Our results indicated that a simple paragraph- based algorithm that was intended as a baseline performed very poorly , and that word-based and some coherence-based algorithms showed the best performance . \n\t', '\n\t\t The only commercially available summarizer that we tested , the MSWord summarizer , showed worse performance than most other algorithms . \n\t', '\n\t\t Furthermore , we found that a coherence-based algorithm that uses PageRank and takes non-tree coherence graphs as input performed better than most versions of a 0.5 0.4 0.3 0.2 0.1 0 MSWord Luhn tf.idf MarcuAvg MarcuMin MarcuMax in-degree PageRank coherence-based algorithm that operates on coherence trees . \n\t', '\n\t\t When data from Experiments 1 and 2 were collapsed , the PageRank algorithm performed significantly better than all other algorithms , except the coherence-based algorithm that uses in-degrees of nodes in non-tree coherence graphs . \n\t', '\n\t\t References Jürgen Bortz . \n\t', '\n\t\t 1999. Statistik für Sozialwissen- schaftler . \n\t', '\n\t\t Berlin : Springer Verlag . \n\t', '\n\t\t Ronald Brandow , Karl Mitze , & Lisa F Rau . \n\t', '\n\t\t 1995. Automatic condensation of electronic publications by sentence selection . \n\t', '\n\t\t Information Processing and Management , 31(5) , 675-685 . \n\t', '\n\t\t Orkut Buyukkokten , Hector Garcia-Molina , & Andreas Paepcke . \n\t', '\n\t\t 2001. Seeing the whole in parts : Text summarization for web browsing on handheld devices . \n\t', '\n\t\t Paper presented at the 10th International WWW Conference , Hong Kong , China . \n\t', '\n\t\t Jean Carletta . \n\t', '\n\t\t 1996. Assessing agreement on classification tasks : The kappa statistic . \n\t', '\n\t\t Computational Linguistics , 22(2) , 249- 254 . \n\t', ""\n\t\t Lynn Carlson , John M Conroy , Daniel Marcu , Dianne P O'Leary , Mary E Okurowski , Anthony Taylor , et al . 2001. An empirical study on the relation between abstracts , extracts , and the discourse structure of texts . \n\t"", '\n\t\t Paper presented at the DUC-2001 , New Orleans , LA , USA . \n\t', '\n\t\t Lynn Carlson , Daniel Marcu , & Mary E Okurowski . \n\t', '\n\t\t 2002. RST Discourse Treebank . \n\t', '\n\t\t Philadelphia , PA : Linguistic Data Consortium . \n\t', '\n\t\t Lynn Carlson , Daniel Marcu , & Mary E Okurowski . \n\t', '\n\t\t 2003. Building a discourse- tagged corpus in the framework of rhetorical structure theory . \n\t', '\n\t\t In J. van Kuppevelt & R. Smith ( Eds . \n\t', '\n\t\t ) , Current directions in discourse and dialogue . \n\t', '\n\t\t New York : Kluwer Academic Publishers . \n\t', '\n\t\t Simon Corston-Oliver . \n\t', '\n\t\t 1998. Computing representations of the structure of written discourse . \n\t', '\n\t\t Redmont , WA . \n\t', '\n\t\t Chris Ding , Xiaofeng He , Perry Husbands , Hongyuan Zha , & Horst Simon . \n\t', '\n\t\t 2002. PageRank , HITS , and a unified framework for link analysis . \n\t', '\n\t\t ( No. 49372 ) . \n\t', '\n\t\t Berkeley , CA , USA . \n\t', '\n\t\t Jade Goldstein , Mark Kantrowitz , Vibhu O Mittal , & Jamie O Carbonell . \n\t', '\n\t\t 1999 . \n\t', '\n\t\t Summarizing text documents : Sentence selection and evaluation metrics . \n\t', '\n\t\t Paper presented at the SIGIR-99 , Melbourne , Australia . \n\t', '\n\t\t Yihong Gong , & Xin Liu . \n\t', '\n\t\t 2001. Generic text summarization using relevance measure and latent semantic analysis . \n\t', '\n\t\t Paper presented at the Annual ACM Conference on Research and Development in Information Retrieval , New Orleans , LA , USA . \n\t', '\n\t\t Barbara J Grosz , & Candace L Sidner . \n\t', '\n\t\t 1986. Attention , intentions , and the structure of discourse . \n\t', '\n\t\t Computational Linguistics , 12(3) , 175-204 . \n\t', '\n\t\t Julia Hirschberg , & Christine H Nakatani . \n\t', '\n\t\t 1996. A prosodic analysis of discourse segments in direction-giving monologues . \n\t', '\n\t\t Paper presented at the 34th Annual Meeting of the Association for Computational Linguistics , Santa Cruz , CA . \n\t', '\n\t\t Jerry R Hobbs . \n\t', '\n\t\t 1985. On the coherence and structure of discourse . \n\t', '\n\t\t Stanford , CA . \n\t', '\n\t\t D Horn . \n\t', '\n\t\t 1942. A correction for the effect of tied ranks on the value of the rank difference correlation coefficient . \n\t', '\n\t\t Journal of Educational Psychology , 33 , 686-690 . \n\t', '\n\t\t Hongyan Jing , Kathleen R McKeown , Regina Barzilay , & Michael Elhadad . \n\t', '\n\t\t 1998. Summarization evaluation methods : Experiments and analysis . \n\t', '\n\t\t Paper presented at the AAAI-98 Spring Symposium on Intelligent Text Summarization , Stanford , CA , USA . \n\t', '\n\t\t Alex Lascarides , & Nicholas Asher . \n\t', '\n\t\t 1993 . \n\t', '\n\t\t Temporal interpretation , discourse relations and common sense entailment . \n\t', '\n\t\t Linguistics and Philosophy , 16(5) , 437- 493 . \n\t', '\n\t\t Hans Peter Luhn . \n\t', '\n\t\t 1958. The automatic creation of literature abstracts . \n\t', '\n\t\t IBM Journal of Research and Development , 2(2) , 159-165 . \n\t', '\n\t\t William C Mann , & Sandra A Thompson . \n\t', '\n\t\t 1988 . \n\t', '\n\t\t Rhetorical structure theory : Toward a functional theory of text organization . \n\t', '\n\t\t Text , 8(3) , 243-281 . \n\t', '\n\t\t Christopher D Manning , & Hinrich Schuetze . \n\t', '\n\t\t 2000. Foundations of statistical natural language processing . \n\t', '\n\t\t Cambridge , MA , USA : MIT Press . \n\t', '\n\t\t Daniel Marcu . \n\t', '\n\t\t 2000. The theory and practice of discourse parsing and summarization . \n\t', '\n\t\t Cambridge , MA : MIT Press . \n\t', '\n\t\t Mandar Mitra , Amit Singhal , & Chris Buckley . \n\t', '\n\t\t 1997. Automatic text summarization by paragraph extraction . \n\t', '\n\t\t Paper presented at the ACL/EACL-97 Workshop on Intelligent Scalable Text Summarization , Madrid , Spain . \n\t', '\n\t\t Kenji Ono , Kazuo Sumita , & Seiji Miike . \n\t', '\n\t\t 1994. Abstract generation based on rhetorical structure extraction . \n\t', '\n\t\t Paper presented at the COLING-94 , Kyoto , Japan . \n\t', '\n\t\t Lawrence Page , Sergey Brin , Rajeev Motwani , & Terry Winograd . \n\t', '\n\t\t 1998. The PageRank citation ranking : Bringing order to the web. Stanford , CA . \n\t', '\n\t\t Dragomir R Radev , Eduard Hovy , & Kathleen R McKeown . \n\t', '\n\t\t 2002. Introduction to the special issue on summarization . \n\t', '\n\t\t Computational Linguistics , 28(4) , 399- 408 . \n\t', '\n\t\t Gerard Salton , & Christopher Buckley . \n\t', '\n\t\t 1988. Term-weighting approaches in automatic text retrieval . \n\t', '\n\t\t Information Processing and Management , 24(5) , 513-523 . \n\t', '\n\t\t Karen Sparck-Jones . \n\t', '\n\t\t 1993. What might be in a summary ? \n\t', '\n\t\t In G. Knorz , J. Krause & C. Womser-Hacker ( Eds . \n\t', '\n\t\t ) , Information retrieval 93 : Von der Modellierung zur Anwendung ( pp. 9-26 ) . \n\t', '\n\t\t Konstanz : Universitaetsverlag . \n\t', '\n\t\t Karen Sparck-Jones , & Tetsuya Sakai . \n\t', '\n\t\t 2001 , September 2001 . \n\t', '\n\t\t Generic summaries for indexing in IR . \n\t', '\n\t\t Paper presented at the ACM SIGIR-2001 , New Orleans , LA , USA . \n\t', '\n\t\t Klaus Zechner . \n\t', '\n\t\t 1996. Fast generation of abstracts from general domain text corpora by extracting relevant sentences . \n\t', '\n\t\t Paper presented at the COLING-96 , Copenhagen , Denmark . \n\t', ""\n\t\t Evaluating Centering-based metrics of coherence for text structuring using a reliably annotated corpus Nikiforos Karamanis,4 Massimo Poesio,* Chris Mellish,' and Jon Oberlander4 4School of Informatics , University of Edinburgh , UK , {nikiforo,jon}@ed.ac.uk *Dept . \n\t"", '\n\t\t of Computer Science , University of Essex , UK , poesio at essex dot ac dot uk *Dept . \n\t', '\n\t\t of Computing Science , University of Aberdeen , UK , cmellish@csd.abdn.ac.uk Abstract We use a reliably annotated corpus to compare metrics of coherence based on Centering Theory with respect to their potential usefulness for text structuring in natural language generation . \n\t', '\n\t\t Previous corpus-based evaluations of the coherence of text according to Centering did not compare the coherence of the chosen text structure with that of the possible alternatives . \n\t', '\n\t\t A corpus- based methodology is presented which distinguishes between Centering-based metrics taking these alternatives into account , and represents therefore a more appropriate way to evaluate Centering from a text structuring perspective . \n\t', '\n\t\t 1 Motivation Our research area is descriptive text generation ( O\x92Donnell et al. , 2001 ; Isard et al. , 2003 ) , i.e. the generation of descriptions of objects , typically museum artefacts , depicted in a picture . \n\t', '\n\t\t Text ( 1 ) , from the GNOME corpus \n\t\t']",Positive
"['\n\t\t ( b ) Its present arrangement , twisted into three rings , may be a modern alteration ; ( c ) it should probably be a single ring , worn around the neck . \n\t', '\n\t\t ( d ) The terminals are in the form of goats\x92 heads . \n\t', '\n\t\t According to Centering Theory \n\t\t']",Positive
"['\n\t\t It is often claimed in current work on in natural language generation that the constraints on felicitous text proposed by the theory are useful to guide text structuring , in combination with other factors ( see \n\t\t']",Positive
"['\n\t\t However , how successful Centering\x92s constraints are on their own in generating a felicitous text structure is an open question , already raised by the seminal papers of the theory \n\t\t']",Positive
"['\n\t\t In this work , we explored this question by developing an approach to text structuring purely based on Centering , in which the role of other factors is deliberately ignored . \n\t', '\n\t\t In accordance with recent work in the emerging field of text-to-text generation \n\t\t']",Positive
"['\n\t\t The output of text structuring is merely an ordering of these clauses , rather than the tree-like structure of database facts often used in traditional deep generation \n\t\t']",Positive
"['\n\t\t Our approach is further characterized by two key insights . \n\t', '\n\t\t The first distinguishing feature is that we assume a search-based approach to text structuring \n\t\t']",Positive
['\n\t\t The second novel aspect is that our approach is based on the position that the most straightforward way of using Centering for text structuring is by defining a Centering-based metric of coherence \n\t\t'],Positive
"['\n\t\t Together , these two assumptions lead to a view of text planning in which the constraints of Centering act not as filters , but as ranking factors , and the text planner may be forced to choose a sub-optimal solution . \n\t', '\n\t\t However , \n\t\t']",Positive
"['\n\t\t Hence , a general methodology for identifying which of these metrics represent the most promising candidates for text structuring is required , so that at least some of them can be compared empirically . \n\t', '\n\t\t This is the second research question that this paper addresses , building upon previous work on corpus-based evaluations of Centering , and particularly the methods used by \n\t\t']",Positive
['\n\t\t We use the GNOME corpus \n\t\t'],Positive
"['\n\t\t To sum up , in this paper we try to identify the most promising Centering-based metric for text structuring , and to evaluate how useful this metric is for that purpose , using corpus- based methods instead of generally more expensive psycholinguistic techniques . \n\t', '\n\t\t The paper is structured as follows . \n\t', '\n\t\t After discussing how the GNOME corpus has been used in previous work to evaluate the coherence of a text according to Centering we discuss why such evaluations are not sufficient for text structuring . \n\t', '\n\t\t We continue by showing how Centering can be used to define different metrics of coherence which might be useful to drive a text planner . \n\t', '\n\t\t We then outline a corpus-based methodology to choose among these metrics , estimating how well they are expected to do when used by a text planner . \n\t', '\n\t\t We conclude by discussing our experiments in which this methodology is applied using a subset of the GNOME corpus . \n\t', '\n\t\t 2 Evaluating the coherence of a corpus text according to Centering In this section we briefly introduce Centering , as well as the methodology developed in \n\t\t']",Positive
"['\n\t\t 2.1 Computing CF lists , CPs and CBs According to \n\t\t']",Positive
"['\n\t\t The members of the CF list are \x93ranked\x94 in order of prominence , the first element being the preferred center CP . \n\t', '\n\t\t In this paper , we used what we considered to be the most common definitions of the central notions of Centering ( its \x91parameters\x92 ) . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t Following most mainstream work on Centering for English , we assume that an \x93utterance\x94 corresponds to what is annotated as a finite unit in the GNOME corpus.2 The spans of text with the indexes ( a ) to ( d ) in example ( 1 ) are examples . \n\t', '\n\t\t This definition of utterance is not optimal from the point of view of minimizing Centering violations \n\t\t']",Positive
"['\n\t\t Similarly , we use grammatical function ( gf ) combined with linear order within the unit ( what \n\t\t']",Positive
"['\n\t\t In this configuration , the CP is the referent of the first NP within the unit that is annotated as a subject for its gf.3 Example ( 2 ) shows the relevant annotation features of unit u210 which corresponds to utterance ( a ) in example ( 1 ) . \n\t', '\n\t\t According to gftherelin , the CP of ( a ) is the referent of ne410 \x93144\x94. ( 2 ) <unit finite=\x92finite-yes\x92 id=\x92u210\x92> <ne id=""ne410"" gf=""subj"">144</ne> is <ne id=""ne411"" gf=""predicate""> a torc</ne> </unit> . \n\t', '\n\t\t The ranking of the CFs other than the CP is defined according to the following preference on their gf \n\t\t']",Positive
"['\n\t\t CFs with the same gf are ranked according to the linear order of the corresponding NPs in the utterance . \n\t', '\n\t\t The second column of Table 1 shows how the utterances in example ( 1 ) are automatically translated by the scripts developed by \n\t\t']",Positive
"['\n\t\t 3Or as a post-copular subject in a there-clause . \n\t', '\n\t\t U CF list : CB Transition CHEAPNESS CBn=CPn^1 { CP , other CFs1 ( a ) { de374 , de3751 n.a. n.a. n.a. ( b ) { de376 , de374 , de3771 de374 RETAIN + ( c ) { de374 , de3791 de374 CONTINUE * ( d ) { de380 , de381 , de3821 - NOCB + Table 1 : CP , CFs other than CP , CB , NOCB or standard ( see Table 2 ) transition and violations of CHEAPNESS ( denoted with an asterisk ) for each utterance ( U ) in example ( 1 ) Table 2 : COHERENCE , SALIENCE and the table of standard transitions SALIENCE* : CBn ~=CPn SALIENCE : CBn=CPn or NOCB in CFn_1 CBn=CBn_1 COHERENCE : CONTINUE RETAIN SMOOTH-SHIFT COHERENCE* : CBn ~=CBn_1 ROUGH-SHIFT sequence of CF lists , each decomposed into the CP and the CFs other than the CP , according to the chosen setting of the Centering parameters . \n\t', '\n\t\t Note that the CP of ( a ) is the center de374 and that the same center is used as the referent of the other NPs which are annotated as coreferring with ne410 . \n\t', '\n\t\t Given two subsequent utterances Un_1 and Un , with CF lists CFn_1 and CFn respectively , the backward looking center of Un , CBn , is defined as the highest ranked element of CFn_1 which also appears in CFn ( Centering\x92s Constraint 3 ) . \n\t', '\n\t\t For instance , the CB of ( b ) is de374 . \n\t', '\n\t\t The third column of Table 1 shows the CB for each utterance in (1).4 2.2 Computing transitions As the fourth column of Table 1 shows , each utterance , with the exception of ( a ) , is also marked with a transition from the previous one . \n\t', '\n\t\t When CFn and CFn_1 do not have any centers in common , we compute the NOCB transition \n\t\t']",Positive
"['\n\t\t 5In this study we do not take indirect realisation into account , i.e. , we ignore the bridging reference ( annotated in the corpus ) between the referent of \x93it\x94 de374 in ( c ) and the referent of \x93the terminals\x94 de380 in ( d ) , by virtue of which de374 might be thought as being a member of the CF list of ( d ) . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t However , in this work we are treating CF lists as an abstract representation Following again the terminology in \n\t\t']",Positive
"['\n\t\t Each of these principles can be satisfied or violated while their various combinations give rise to the standard transitions of Centering shown in Table 2 ; Poesio et al\x92s scripts compute these violations.6 We also make note of the preference between these transitions , known as Centering\x92s Rule 2 \n\t\t']",Positive
"['\n\t\t Finally , the scripts determine whether CBn is the same as CPn_1 , known as the principle of CHEAPNESS \n\t\t']",Positive
"['\n\t\t The last column of Table 1 shows the violations of CHEAPNESS ( denoted with an asterisk ) in (1).7 2.3 Evaluating the coherence of a text and text structuring The statistics about transitions computed as just discussed can be used to determine the degree to which a text conforms with , or violates , Centering\x92s principles . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t 6If the second utterance in a sequence U2 has a CB , then it is taken to be either a CONTINUE or a RETAIN , although U1 is not classified as a NOCB . \n\t', '\n\t\t 7As for the other two principles , no violation of CHEAPNESS is computed for ( a ) or when Un is marked as a NOCB . \n\t', '\n\t\t of the transitions in the GNOME corpus in configurations such as the one used in this paper . \n\t', '\n\t\t More generally , a significant percentage of NOCBs ( at least 20 % ) and other \x93dispreferred\x94 transitions was found with all parameter configurations tested by \n\t\t']",Positive
['\n\t\t These results led \n\t\t'],Positive
"['\n\t\t These studies , however , do not investigate the question that is most important from the text structuring perspective adopted in this paper : whether there would be alternative ways of structuring the text that would result in fewer violations of Centering\x92s constraints \n\t\t']",Positive
"['\n\t\t Consider the NOCB utterance ( d ) in ( 1 ) . \n\t', '\n\t\t Simply observing that this transition is \x91dispreferred\x92 ignores the fact that every other ordering of utterances ( b ) to ( d ) would result in more NOCBs than those found in ( 1 ) . \n\t', '\n\t\t Even a text- structuring algorithm functioning solely on the basis of the Centering constraints might therefore still choose the particular order in ( 1 ) . \n\t', '\n\t\t In other words , a metric of text coherence purely based on Centering principles\x96trying to minimize the number of NOCBs\x96may be sufficient to explain why this order of clauses was chosen , at least in this particular genre , without need to involve more complex explanations . \n\t', '\n\t\t In the rest of the paper , we consider several such metrics , and use the texts in the GNOME corpus to choose among them . \n\t', '\n\t\t We return to the issue of coherence ( i.e. , whether additional coherence- inducing factors need to be stipulated in addition to those assumed in Centering ) in the Discussion . \n\t', '\n\t\t 3 Centering-based metrics of coherence As said previously , we assume a text structuring system taking as input a set of utterances represented in terms of their CF lists . \n\t', '\n\t\t The system orders these utterances by applying a bias in favour of the best scoring ordering among the candidate solutions for the preferred output . \n\t', '\n\t\t $ In this section , we discuss how the Centering 8Additional assumptions for choosing between the orderings that are assigned the best score are presented in the next section . \n\t', '\n\t\t concepts just described can be used to define metrics of coherence which might be useful for text structuring . \n\t', '\n\t\t The simplest way to define a metric of coherence using notions from Centering is to classify each ordering of propositions according to the number of NOCBs it contains , and pick the ordering with the fewest NOCBs . \n\t', '\n\t\t We call this metric M.NOCB , following \n\t\t']",Positive
"['\n\t\t Because of its simplicity , M.NOCB serves as the baseline metric in our experiments . \n\t', '\n\t\t We consider three more metrics . \n\t', '\n\t\t M.CHEAP is biased in favour of the ordering with the fewest violations of CHEAPNESS . \n\t', '\n\t\t M.KP sums up the NOCBs and the violations of CHEAPNESS , COHERENCE and SALIENCE , preferring the ordering with the lowest total cost \n\t\t']",Positive
"['\n\t\t Finally , M.BFP employs the preferences between standard transitions as expressed by Rule 2 . \n\t', '\n\t\t More specifically , M.BFP selects the ordering with the highest number of CONTINUEs . \n\t', '\n\t\t If there exist several orderings which have the most CONTINUEs , the one which has the most RETAINs is favoured . \n\t', '\n\t\t The number of SMOOTH-SHIFTs is used only to distinguish between the orderings that score best for CONTINUEs as well as for RETAINs , etc. . \n\t', '\n\t\t In the next section , we present a general methodology to compare these metrics , using the actual ordering of clauses in real texts of a corpus to identify the metric whose behavior mimics more closely the way these actual orderings were chosen . \n\t', '\n\t\t This methodology was implemented in a program called the System for Evaluating Entity Coherence ( SEEC ) . \n\t', '\n\t\t 4 Exploring the space of possible orderings In section 2 , we discussed how an ordering of utterances in a text like ( 1 ) can be translated into a sequence of CF lists , which is the representation that the Centering-based metrics operate on . \n\t', '\n\t\t We use the term Basis for Comparison ( BfC ) to indicate this sequence of CF lists . \n\t', '\n\t\t In this section , we discuss how the BfC is used in our search-oriented evaluation methodology to calculate a performance measure for each metric and compare them with each other . \n\t', '\n\t\t In the next section , we will see how our corpus was used to identify the most promising Centering-based metric for a text classifier . \n\t', '\n\t\t 4.1 Computing the classification rate The performance measure we employ is called the classification rate of a metric M on a cer- tain BfC B . \n\t', '\n\t\t The classification rate estimates the ability of M to produce B as the output of text structuring according to a specific generation scenario . \n\t', '\n\t\t The first step of SEEC is to search through the space of possible orderings defined by the permutations of the CF lists that B consists of , and to divide the explored search space into sets of orderings that score better , equal , or worse than B according to M . \n\t', '\n\t\t Then , the classification rate is defined according to the following generation scenario . \n\t', '\n\t\t We assume that an ordering has higher chances of being selected as the output of text structuring the better it scores for M . \n\t', '\n\t\t This is turn means that the fewer the members of the set of better scoring orderings , the better the chances of B to be the chosen output . \n\t', '\n\t\t Moreover , we assume that additional factors play a role in the selection of one of the orderings that score the same for M . \n\t', '\n\t\t On average , B is expected to sit in the middle of the set of equally scoring orderings with respect to these additional factors . \n\t', '\n\t\t Hence , half of the orderings with the same score will have better chances than B to be selected by M . \n\t', '\n\t\t The classification rate v of a metric M on B expresses the expected percentage of orderings with a higher probability of being generated than B according to the scores assigned by M and the additional biases assumed by the generation scenario as follows : ( 3 ) Classification rate : v(M , B ) = Better(M) + Equ2 ( M ) Better(M) stands for the percentage of orderings that score better than B according to M , whilst Equal(M) is the percentage of orderings that score equal to B according to M . \n\t', '\n\t\t If v(M , , , B ) is the classification rate of M , , on B , and v(My , B ) is the classification rate of My on B , My is a more suitable candidate than M , , for generating B if v(My , B ) is smaller than v(M , , , B ) . \n\t', '\n\t\t 4.2 Generalising across many BfCs In order for the experimental results to be reliable and generalisable , M , , and My should be compared on more than one BfC from a corpus C . \n\t', '\n\t\t In our standard analysis , the BfCs B1 , ... , Bm from C are treated as the random factor in a repeated measures design since each BfC contributes a score for each metric . \n\t', '\n\t\t Then , the classification rates for M , , and My on the BfCs are compared with each other and significance is tested using the Sign Test . \n\t', '\n\t\t After calculating the number of BfCs that return a lower classification rate for M , , than for My and vice versa , the Sign Test reports whether the difference in the number of BfCs is significant , that is , whether there are significantly more BfCs with a lower classification rate for M , , than the BfCs with a lower classification rate for My ( or vice versa).9 Finally , we summarise the performance of M on m BfCs from C in terms of the average classification rate Y : ( 4 ) Average classification rate : Y(M , C ) = v(M,B1)+...+v(M,Bm) m 5 Using the GNOME corpus for a search-based comparison of metrics We will now discuss how the methodology discussed above was used to compare the Centering-based metrics discussed in Section 3 , using the original ordering of texts in the GNOME corpus to compute the average classification rate of each metric . \n\t', '\n\t\t The GNOME corpus contains texts from different genres , not all of which are of interest to us . \n\t', '\n\t\t In order to restrict the scope of the experiment to the text-type most relevant to our study , we selected 20 \x93museum labels\x94 , i.e. , short texts that describe a concrete artefact , which served as the input to SEEC together with the metrics in section 3.10 5.1 Permutation and search strategy In specifying the performance of the metrics we made use of a simple permutation heuristic exploiting a piece of domain-specific communication knowledge \n\t\t']",Positive
['\n\t\t Like \n\t\t'],Positive
"['\n\t\t Hence , we restricted the orderings considered by the SEEC 9The Sign Test was chosen over its parametric alternatives to test significance because it does not carry specific assumptions about population distributions and variance . \n\t', '\n\t\t It is also more appropriate for small samples like the one used in this study . \n\t', '\n\t\t 10Note that example ( 1 ) is characteristic of the genre , not the length , of the texts in our subcorpus . \n\t', '\n\t\t The number of CF lists that the BfCs consist of ranges from 4 to 16 ( average cardinality : 8.35 CF lists ) . \n\t', '\n\t\t Pair M.NOCB p Winner lower greater ties M.NOCB vs M.CHEAP 18 2 0 0.000 M.NOCB M.NOCB vs M.KP 16 2 2 0.001 M.NOCB M.NOCB vs M.BFP 12 3 5 0.018 M.NOCB N 20 Table 3 : Comparing M.NOCB with M.CHEAP , M.KP and M.BFP in GNOME to those in which the first CF list of B , CF1 , appears in first position.11 For very short texts like ( 1 ) , which give rise to a small BfC , the search space of possible orderings can be enumerated exhaustively . \n\t', '\n\t\t However , when B consists of many more CF lists , it is impractical to explore the search space in this way . \n\t', '\n\t\t Elsewhere we show that even in these cases it is possible to estimate v(M , B ) reliably for the whole population of orderings using a large random sample . \n\t', '\n\t\t In the experiments reported here , we had to resort to random sampling only once , for a BfC with 16 CF lists . \n\t', '\n\t\t 5.2 Comparing M.NOCB with other metrics The experimental results of the comparisons of the metrics from section 3 , computed using the methodology in section 4 , are reported in Table 3 . \n\t', '\n\t\t In this table , the baseline metric M.NOCB is compared with each of M.CHEAP , M.KP and M.BFP . \n\t', '\n\t\t The first column of the Table identifies the comparison in question , e.g. M.NOCB versus M.CHEAP . \n\t', '\n\t\t The exact number of BfCs for which the classification rate of M.NOCB is lower than its competitor for each comparison is reported in the next column of the Table . \n\t', '\n\t\t For example , M.NOCB has a lower classification rate than M.CHEAP for 18 ( out of 20 ) BfCs from the GNOME corpus . \n\t', '\n\t\t M.CHEAP only achieves a lower classification rate for 2 BfCs , and there are no ties , i.e. cases where the classification rate of the two metrics is the same . \n\t', '\n\t\t The p value returned by the Sign Test for the difference in the number of BfCs , rounded to the third decimal place , is reported in the fifth column of the Table . \n\t', '\n\t\t The last column of the Table 3 shows M.NOCB as the \x93winner\x94 of the comparison with M.CHEAP since it has a lower classifica- 11Thus , we assume that when the set of CF lists serves as the input to text structuring , CF1 will be identified as the initial CF list of the ordering to be generated using annotation features such as the unit type which distinguishes ( a ) from the other utterances in ( 1 ) . \n\t', '\n\t\t tion rate than its competitor for significantly more BfCs in the corpus . \n\t', '\n\t\t 12 Overall , the Table shows that M.NOCB does significantly better than the other three metrics which employ additional Centering concepts . \n\t', '\n\t\t This result means that there exist proportionally fewer orderings with a higher probability of being selected than the BfC when M.NOCB is used to guide the hypothetical text structuring algorithm instead of the other metrics . \n\t', '\n\t\t Hence , M.NOCB is the most suitable among the investigated metrics for structuring the CF lists in GNOME . \n\t', '\n\t\t This in turn indicates that simply avoiding NOCB transitions is more relevant to text structuring than the combinations of the other Centering notions that the more complicated metrics make use of . \n\t', '\n\t\t ( However , these notions might still be appropriate for other tasks , such as anaphora resolution . \n\t', '\n\t\t ) 6 Discussion : the performance of M.NOCB We already saw that \n\t\t']",Positive
"['\n\t\t However , we also explained in section 2.3 that what really matters when trying to determine whether a text might have been generated only paying attention to Centering constraints is the extent to which it would be possible to \x91improve\x92 upon the ordering chosen in that text , given the information that the text structuring algorithm had to convey . \n\t', '\n\t\t The average classification rate of M.NOCB is an esti- 12 No winner is reported for a comparison when the p value returned by the Sign Test is not significant ( ns ) , i.e. greater than 0.05 . \n\t', '\n\t\t Note also that despite conducting more than one pairwise comparison simultaneously we refrain from further adjusting the overall threshold of significance ( e.g. according to the Bonferroni method , typically used for multiple planned comparisons that employ parametric statistics ) since it is assumed that choosing a conservative statistic such as the Sign Test already provides substantial protection against the possibility of a type I error . \n\t', '\n\t\t Pair M.NOCB p Winner lower greater ties M.NOCB vs M.CHEAP 110 12 0 0.000 M.NOCB M.NOCB vs M.KP 103 16 3 0.000 M.NOCB M.NOCB vs M.BFP 41 31 49 0.121 ns N 122 Table 4 : Comparing M.NOCB with M.CHEAP , M.KP and M.BFP using the novel methodology in MPIRO mate of exactly this variable , indicating whether M.NOCB is likely to arrive at the BfC during text structuring . \n\t', '\n\t\t The average classification rate Y for M.NOCB on the subcorpus of GNOME studied here , for the parameter configuration of Centering we have assumed , is 19.95 % . \n\t', '\n\t\t This means that on average the BfC is close to the top 20 % of alternative orderings when these orderings are ranked according to their probability of being selected as the output of the algorithm . \n\t', '\n\t\t On the one hand , this result shows that although the ordering of CF lists in the BfC might not completely minimise the number of observed NOCB transitions , the BfC tends to be in greater agreement with the preference to avoid NOCBs than most of the alternative orderings . \n\t', '\n\t\t In this sense , it appears that the BfC optimises with respect to the number of potential NOCBs to a certain extent . \n\t', '\n\t\t On the other hand , this result indicates that there are quite a few orderings which would appear more likely to be selected than the BfC . \n\t', '\n\t\t We believe this finding can be interpreted in two ways . \n\t', '\n\t\t One possibility is that M.NOCB needs to be supplemented by other features in order to explain why the original text was structured this way . \n\t', '\n\t\t This is the conclusion arrived at by \n\t\t']",Positive
"['\n\t\t There is also a second possibility , however : we might want to reconsider the assumption that human text planners are trying to ensure that each utterance in a text is locally coherent . \n\t', '\n\t\t They might do all of their planning just on the basis of Centering constraints , at least in this genre \x96perhaps because of resource limitations\x96 and simply accept a certain degree of incoherence . \n\t', '\n\t\t Further research on this issue will require psycholinguistic methods ; our analysis nevertheless sheds more light on two previously un addressed questions in the corpus-based evaluation of Centering \x96 a ) which of the Centering notions are most relevant to the text structuring task , and b ) to which extent Centering on its own can be useful for this purpose . \n\t', '\n\t\t 7 Further results In related work , we applied the methodology discussed here to a larger set of existing data ( 122 BfCs ) derived from the MPIRO system and ordered by a domain expert \n\t\t']",Positive
"['\n\t\t As Table 4 shows , the results from MPIRO verify the ones reported here , especially with respect to M.KP and M.CHEAP which are overwhelmingly beaten by the baseline in the new domain as well . \n\t', '\n\t\t Also note that since M.BFP fails to overtake M.NOCB in MPIRO , the baseline can be considered the most promising solution among the ones investigated in both domains by applying Occam\x92s logical principle . \n\t', '\n\t\t We also tried to account for some additional constraints on coherence , namely local rhetorical relations , based on some of the assumptions in \n\t\t']",Positive
"['\n\t\t These results , reported in \n\t\t']",Positive
"['\n\t\t Hence , it remains unclear to us how to improve upon M.NOCB . \n\t', '\n\t\t In our future work , we would like to experiment with more metrics . \n\t', '\n\t\t Moreover , although we consider the parameter configuration of Centering used here a plausible choice , we intend to apply our methodology to study different instantiations of the Centering parameters , e.g. by investigating whether \x93indirect realisation\x94 reduces the classification rate for M.NOCB compared to \x93direct realisation\x94 , etc. Acknowledgements Special thanks to James Soutter for writing the program which translates the output produced by GNOME\x92s scripts into a format appropriate for SEEC . \n\t', '\n\t\t The first author was able to engage in this research thanks to a scholarship from the Greek State Scholarships Foundation ( IKY ) . \n\t', '\n\t\t References Regina Barzilay , Noemie Elhadad , and Kathleen McKeown . \n\t', '\n\t\t 2002. Inferring strategies for sentence ordering in multidocument news summarization . \n\t', '\n\t\t Journal of Artificial Intelligence Research , 17:35\x9655 . \n\t', '\n\t\t Susan E. Brennan , Marilyn A. Friedman [ Walker ] , and Carl J. Pollard . \n\t', '\n\t\t 1987. A centering approach to pronouns . \n\t', '\n\t\t In Proceedings of ACL 1987 , pages 155\x96162 , Stanford , California . \n\t', '\n\t\t Barbara Di Eugenio . \n\t', '\n\t\t 1998 . \n\t', '\n\t\t Centering in Italian . \n\t', '\n\t\t In Walker et al . \n\t', '\n\t\t \n\t\t', '\n\t\t Aggeliki Dimitromanolaki and Ion Androutsopoulos . \n\t', '\n\t\t 2003. Learning to order facts for discourse planning in natural language generation . \n\t', '\n\t\t In Proceedings of the 9th European Workshop on Natural Language Generation , Budapest , Hungary . \n\t', '\n\t\t Barbara J. Grosz , Aravind K. Joshi , and Scott Weinstein . \n\t', '\n\t\t 1995 . \n\t', '\n\t\t Centering : A framework for modeling the local coherence of discourse . \n\t', '\n\t\t Computational Linguistics , 21(2):203\x96225 . \n\t', '\n\t\t Amy Isard , Jon Oberlander , Ion Androutsopoulos , and Colin Matheson . \n\t', '\n\t\t 2003. Speaking the users\x92 languages . \n\t', '\n\t\t IEEE Intelligent Systems Magazine , 18(1):40\x9645 . \n\t', '\n\t\t Nikiforos Karamanis and Hisar Maruli Manurung . \n\t', '\n\t\t 2002. Stochastic text structuring using the principle of continuity . \n\t', '\n\t\t In Proceedings of INLG 2002 , pages 81\x9688 , Harriman , NY , USA , July . \n\t', '\n\t\t Nikiforos Karamanis . \n\t', '\n\t\t 2003. Entity Coherence for Descriptive Text Structuring . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , Division of Informatics , University of Edinburgh . \n\t', '\n\t\t Rodger Kibble and Richard Power . \n\t', '\n\t\t 2000. An integrated framework for text planning and pronominalisation . \n\t', '\n\t\t In Proceedings of INLG 2000 , pages 77\x9684 , Israel . \n\t', '\n\t\t Rodger Kibble . \n\t', '\n\t\t 2001. A reformulation of Rule 2 of Centering Theory . \n\t', '\n\t\t Computational Linguistics , 27(4):579\x96587 . \n\t', '\n\t\t Richard Kittredge , Tanya Korelsky , and Owen Rambow . \n\t', '\n\t\t 1991. On the need for domain com- munication knowledge . \n\t', '\n\t\t Computational Intelligence , 7:305\x96314 . \n\t', '\n\t\t Alistair Knott , Jon Oberlander , Mick O\x92Donnell , and Chris Mellish . \n\t', '\n\t\t 2001. Beyond elaboration : The interaction of relations and focus in coherent text . \n\t', '\n\t\t In T. Sanders , J. Schilperoord , and W. Spooren , editors , Text Representation : Linguistic and Psycholinguistic Aspects , chapter 7 , pages 181\x96196 . \n\t', '\n\t\t John Benjamins . \n\t', '\n\t\t Mirella Lapata . \n\t', '\n\t\t 2003. Probabilistic text structuring : Experiments with sentence ordering . \n\t', '\n\t\t In Proceedings of ACL 2003 , Saporo , Japan , July . \n\t', '\n\t\t Chris Mellish , Alistair Knott , Jon Oberlander , and Mick O\x92Donnell . \n\t', '\n\t\t 1998. Experiments using stochastic search for text planning . \n\t', '\n\t\t In Proceedings of the 9th International Workshop on NLG , pages 98\x96107 , Niagara-on-theLake , Ontario , Canada . \n\t', '\n\t\t Eleni Miltsakaki . \n\t', '\n\t\t 2002. Towards an aposynthesis of topic continuity and intrasentential anaphora . \n\t', '\n\t\t Computational Linguistics , 28(3):319\x96355 . \n\t', '\n\t\t Mick O\x92Donnell , Chris Mellish , Jon Oberlander , and Alistair Knott . \n\t', '\n\t\t 2001. ILEX : An architecture for a dynamic hypertext generation system . \n\t', '\n\t\t Natural Language Engineering , 7(3):225\x96250 . \n\t', '\n\t\t Rebecca J. Passoneau . \n\t', '\n\t\t 1998. Interaction of discourse structure with explicitness of discourse anaphoric phrases . \n\t', '\n\t\t In Walker et al . \n\t', '\n\t\t \n\t\t', '\n\t\t Massimo Poesio , Rosemary Stevenson , Barbara Di Eugenio , and Janet Hitzeman . \n\t', '\n\t\t 2004 . \n\t', '\n\t\t Centering : a parametric theory and its instantiations . \n\t', '\n\t\t Computational Linguistics , 30(3) . \n\t', '\n\t\t Ehud Reiter and Robert Dale . \n\t', '\n\t\t 2000. Building Natural Language Generation Systems . \n\t', '\n\t\t Cambridge . \n\t', '\n\t\t Michael Strube and Udo Hahn . \n\t', '\n\t\t 1999. Functional centering : Grounding referential coherence in information structure . \n\t', '\n\t\t Computational Linguistics , 25(3):309\x96344 . \n\t', '\n\t\t Marilyn A. Walker , Aravind K. Joshi , and Ellen F. Prince . \n\t', '\n\t\t 1998a . \n\t', '\n\t\t Centering in naturally occuring discourse : An overview . \n\t', '\n\t\t In Walker et al . \n\t', '\n\t\t \n\t\t', '\n\t\t Marilyn A. Walker , Aravind K. Joshi , and Ellen F. Prince , editors . \n\t', '\n\t\t 1998b . \n\t', '\n\t\t Centering Theory in Discourse . \n\t', '\n\t\t Clarendon Press , Oxford . \n\t', '\n\t\t Computing Locally Coherent Discourses Alexander Koller Dept. of Computational Linguistics Saarland University Saarbr¨ucken , Germany koller@coli.uni-sb.de Ernst Althaus LORIA Universit´e Henri Poincar´e Vand\x9cuvre-l`es-Nancy , France althaus@loria.fr Nikiforos Karamanis School of Informatics University of Edinburgh Edinburgh , UK N.Karamanis@sms.ed.ac.uk Abstract We present the first algorithm that computes optimal orderings of sentences into a locally coherent discourse . \n\t', '\n\t\t The algorithm runs very efficiently on a variety of coherence measures from the literature . \n\t', '\n\t\t We also show that the discourse ordering problem is NP-complete and cannot be approximated . \n\t', '\n\t\t 1 Introduction One central problem in discourse generation and summarisation is to structure the discourse in a way that maximises coherence . \n\t', '\n\t\t Coherence is the property of a good human-authored text that makes it easier to read and understand than a randomly- ordered collection of sentences . \n\t', '\n\t\t Several papers in the recent literature \n\t\t']",Positive
"['\n\t\t This is in contrast to theories of global coherence , which can consider relations between larger chunks of the discourse and e.g. structures them into a tree \n\t\t']",Positive
"['\n\t\t Measures of local coherence specify which ordering of the sentences makes for the most coherent discourse , and can be based e.g. on Centering Theory \n\t\t']",Positive
"['\n\t\t But while formal models of local coherence have made substantial progress over the past few years , the question of how to efficiently compute an ordering of the sentences in a discourse that maximises local coherence is still largely unsolved . \n\t', '\n\t\t The fundamental problem is that any of the factorial number of permutations of the sentences could be the optimal discourse , which makes for a formidable search space for nontrivial discourses . \n\t', '\n\t\t \n\t\t']",Negative
"['\n\t\t This paper presents the first algorithm that computes optimal locally coherent discourses , and establishes the complexity of the discourse ordering problem . \n\t', '\n\t\t We first prove that the discourse ordering problem for local coherence measures is equivalent to the Travelling Salesman Problem ( TSP ) . \n\t', '\n\t\t This means that discourse ordering is NP-complete , i.e. there are probably no polynomial algorithms for it . \n\t', '\n\t\t Worse , our result implies that the problem is not even approximable ; any polynomial algorithm will compute arbitrarily bad solutions on unfortunate inputs . \n\t', '\n\t\t Note that all approximation algorithms for the TSP assume that the underlying cost function is a metric , which is not the case for the coherence measures we consider . \n\t', '\n\t\t Despite this negative result , we show that by applying modern algorithms for TSP , the discourse ordering problem can be solved efficiently enough for practical applications . \n\t', '\n\t\t We define a branch-and-cut algorithm based on linear programming , and evaluate it on discourse ordering problems based on the GNOME corpus \n\t\t']",Positive
"['\n\t\t If the local coherence measure depends only on the adjacent pairs of sentences in the discourse , we can order discourses of up to 50 sentences in under a second . \n\t', '\n\t\t If it is allowed to depend on the left-hand context of the sentence pair , computation is often still efficient , but can become expensive . \n\t', '\n\t\t The structure of the paper is as follows . \n\t', '\n\t\t We will first formally define the discourse ordering problem and relate our definition to the literature on local coherence measures in Section 2 . \n\t', '\n\t\t Then we will prove the equivalence of discourse ordering and TSP ( Section 3 ) , and present algorithms for solving it in Section 4 . \n\t', '\n\t\t Section 5 evaluates our algorithms on examples from the literature . \n\t', '\n\t\t We compare our approach to various others in Section 6 , and then conclude in Section 7 . \n\t', '\n\t\t 2 The Discourse Ordering Problem We will first give a formal definition of the problem of computing locally coherent discourses , and demonstrate how some local coherence measures from the literature fit into this framework . \n\t', '\n\t\t 2.1 Definitions We assume that a discourse is made up of discourse units ( depending on the underlying theory , these could be utterances , sentences , clauses , etc. ) , which must be ordered to achieve maximum local coherence . \n\t', '\n\t\t We call the problem of computing the optimal ordering the discourse ordering problem . \n\t', '\n\t\t We formalise the problem by assigning a cost to each unit-to-unit transition , and a cost for the discourse to start with a certain unit . \n\t', '\n\t\t Transition costs may depend on the local context , i.e. a fixed number of discourse units to the left may influence the cost of a transition . \n\t', '\n\t\t The optimal ordering is the one which minimises the sum of the costs . \n\t', '\n\t\t Definition 1 . \n\t', '\n\t\t A d -place transition cost function for a set U of discourse units is a function cT : Ud ^ R. Intuitively , cT(un l u1 , ... , ud_1 ) is the cost of the transition ( ud_1 , ud ) given that the immediately preceding units were u1 , ... , ud_2 . \n\t', '\n\t\t A d -place initial costfunction for U is a function cI : Ud ^ IR , . \n\t', '\n\t\t Intuitively , cI(u1 , ... , ud ) is the cost for the fact that the discourse starts with the sequence u1 , ... , ud . \n\t', '\n\t\t The d -place discourse ordering problem is defined as follows : Given a set U = { u1 , ... , un } , a d-place transition cost function cT and a ( d \x97 1)- place initial cost function cI , compute a permutation ^ of { 1 , ... , n } such that cI(uw(1) , ... , uw(d_1)) cT ( uw(i+d_1) l uw(i) , ... , uw(i+d_2)) is minimal . \n\t', '\n\t\t The notation for the cost functions is suggestive : The transition cost function has the character of a conditional probability , which specifies that the cost of continuing the discourse with the unit ud depends on the local context u1 , ... , ud_1 . \n\t', '\n\t\t This local context is not available for the first d \x97 1 units of the discourse , which is why their costs are summarily covered by the initial function . \n\t', '\n\t\t 2.2 Centering-Based Cost Functions One popular class of coherence measures is based on Centering Theory ( CT , \n\t\t']",Positive
"['\n\t\t We will briefly sketch its basic notions and then show how some CT-based coherence measures can be cast into our framework . \n\t', '\n\t\t The standard formulation of CT e.g. in \n\t\t']",Positive
"['\n\t\t The members of Cf(ui) correspond to the referents of the NPs in ui and are ranked in order of prominence , the first element being the preferred centre Cp(ui) . \n\t', '\n\t\t The backward-looking centre Cb(ui) of ui is defined as the highest ranked element of Cf ( ui ) which also appears in Cf ( ui_1 ) , and serves as the link between the two subsequent utterances ui_1 and ui . \n\t', '\n\t\t Each utterance has at most one Cb . \n\t', '\n\t\t If ui and ui_1 have no forward-looking centres in common , or if ui is the first utterance in the discourse , then ui does not have a Cb at all . \n\t', '\n\t\t Based on these concepts , CT classifies the transitions between subsequent utterances into different types . \n\t', '\n\t\t Table 1 shows the most common classification into the four types CONTINUE , RETAIN , SMOOTH-SHIFT , and ROUGH-SHIFT , which are predicted to be less and less coherent in this order \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t Finally , a transition is considered to satisfy the CHEAPNESS constraint \n\t\t']",Positive
"['\n\t\t Table 2 summarises some cost functions from the literature , in the reconstruction of \n\t\t']",Positive
"['\n\t\t Each line shows the name of the coherence measure , the arity d from Definition 1 , and the initial and transition cost functions . \n\t', '\n\t\t To fit the definitions in one line , we use terms of the form fk , which abbreviate applications of f to the last k arguments of the cost functions , i.e. f ( ud_k+ 1 , . \n\t', '\n\t\t .. , ud ) . \n\t', '\n\t\t The most basic coherence measure , M.NOCB \n\t\t']",Positive
"['\n\t\t The definition of cT(u2lu1) , which decodes to nocb(u1 , u2 ) , only looks at the two units in the transition , and no further context . \n\t', '\n\t\t The initial costs for this coherence measure are always zero . \n\t', '\n\t\t The measure M.KP \n\t\t']",Positive
"['\n\t\t This is an instance of the 3-place discourse ordering problem because COHERENCE depends on Cb(ui_1) , which itself depends on Cf(ui_2) ; hence nocoh must take n_d+1 X i=1 + COHERENCE : COHERENCE* : Cb(ui) = Cb(ui-1) Cb(ui) =~ Cb(ui-1) SALIENCE : Cb(ui) = Cp(ui) CONTINUE SMOOTH-SHIFT SALIENCE* : Cb(ui) =~ Cp(ui) RETAIN ROUGH-SHIFT Table 1 : COHERENCE , SALIENCE and the table of standard transitions d M.NOCB 2 M.KP 3 M.BFP 3 M.LAPATA 2 initial cost cI(u1 , . \n\t', '\n\t\t .. , ud-1 ) 0 nocb2 + nocheap2 + nosal2 ( 1\x97 nosal2 , nosal2 , 0 , 0 ) \x97 log P(u1) transition cost cT(udl u1 , . \n\t', '\n\t\t .. , ud-1 ) nocb2 nocb2 + nocheap2 + nosal2 + nocoh3 ( cont3 , ret3 , ss3 , rs3 ) \x97 log P(u2 lu1 ) Table 2 : Some cost functions from the literature . \n\t', '\n\t\t three arguments . \n\t', '\n\t\t Finally , the measure M.BFP \n\t\t']",Positive
"['\n\t\t cT and all four functions it is computed from take three arguments because the classification depends on COHERENCE . \n\t', '\n\t\t As the first transition in the discourse is coherent by default ( it has no Cb ) , we can compute cI by distinguishing RETAIN and CONTINUE via SALIENCE . \n\t', '\n\t\t The tuple-valued cost functions can be converted to real-valued functions by choosing a sufficiently large number M and using the value M3 \x95 cont + M2 \x95 ret + M \x95 ss + rs. 2.3 Probability-Based Cost Functions A fundamentally different approach to measure discourse coherence was proposed by \n\t\t']",Positive
"['\n\t\t It uses a statistical bigram model that assigns each pair ui , uk of utterances a probability P(uk lui ) of appearing in subsequent positions , and each utterance a probability P(ui) of appearing in the initial position of the discourse . \n\t', '\n\t\t The probabilities are estimated on the grounds of syntactic features of the discourse units . \n\t', '\n\t\t The probability of the entire discourse u1 ... un is the product P(u1) \x95 P(u2lu1) \x95 ... \x95 P(unlun-1) . \n\t', '\n\t\t We can transform Lapata\x92s model straightforwardly into our cost function framework , as shown under M.LAPATA in Table 2 . \n\t', '\n\t\t The discourse that minimizes the sum of the negative logarithms will also maximise the product of the probabilities . \n\t', '\n\t\t We have d = 2 because it is a bigram model in which the transition probability does not depend on the previous discourse units . \n\t', '\n\t\t 3 Equivalence of Discourse Ordering and TSP Now we show that discourse ordering and the travelling salesman problem are equivalent . \n\t', '\n\t\t In order to do this , we first redefine discourse ordering as a graph problem . \n\t', '\n\t\t d-place discourse ordering problem ( dPDOP ) : Given a directed graph G = ( V , E ) , a node s E V and a function c : V d \x97* IR , , compute a simple directed path P = ( s = v0 , v1 , ... , vn ) from s through all vertices in V which minimisesn-d+1 c v v , v We ~ ( z , z+1 , . \n\t', '\n\t\t i-0 \x95 \x95 , z+d-1)\x95 write instances of dPDOP as ( V , E , s , c ) . \n\t', '\n\t\t The nodes v1 , ... , vn correspond to the discourse units . \n\t', '\n\t\t The cost function c encodes both the initial and the transition cost functions from Section 2 by returning the initial cost if its first argument is the ( new ) start node s . \n\t', '\n\t\t Now let\x92s define the version of the travelling salesman problem we will use below . \n\t', '\n\t\t Generalised asymmetric TSP ( GATSP ) : Given a directed graph G = ( V , E ) , edge weights c : E \x97* IR , , and a partition ( V1 , ... , Vk ) of the nodes V , compute the shortest directed cycle that visits exactly one node of each Vi . \n\t', '\n\t\t We call such a cycle a tour and write instances of GATSP as ( ( V1 , ... , Vk ) , E , c ) . \n\t', '\n\t\t The usual definition of the TSP , in which every node must be visited exactly once , is the special case of GATSP where each Vi contains exactly one node . \n\t', '\n\t\t We call this case asymmetric travelling salesman problem , ATSP . \n\t', '\n\t\t ATSP 2PDOP Figure 1 : Reduction of ATSP to 2PDOP We will show that ATSP can be reduced to 2PDOP , and that any dPDOP can be reduced to GATSP . \n\t', '\n\t\t 3.1 Reduction of ATSP to 2PDOP First , we introduce the reduction of ATSP to 2PDOP , which establishes NP-completeness of dPDOP for all d > 1 . \n\t', '\n\t\t The reduction is approximation preserving , i.e. if we can find a solution of 2PDOP that is worse than the optimum only by a factor of E ( an E-approximation ) , it translates to a solution of ATSP that is also an E-approximation . \n\t', '\n\t\t Since it is known that there can be no polynomial algorithms that compute E-approximations for general ATSP , for any E \n\t\t']",Positive
"['\n\t\t The reduction works as follows . \n\t', '\n\t\t Let G = ( ( V1 , ... , Vk ) , E , c ) be an instance of ATSP , and V = V1 U ... \n\t', '\n\t\t U Vk . \n\t', '\n\t\t We choose an arbitrary node v E V and split it into two nodes vs and vt . \n\t', '\n\t\t We assign all edges with source node v to vs and all edges with target node v to vt ( compare Figure 1 ) . \n\t', ""\n\t\t Finally we make vs the source node of our 2PDOP instance G ' . \n\t"", ""\n\t\t For every tour in G , we have a path in G ' starting at vs visiting all other nodes ( and ending in vt ) with the same cost by replacing the edge ( v , u ) out of v by ( vs , u ) and the edge ( w , v ) into v by ( w , vt ) . \n\t"", '\n\t\t Conversely , for every path starting at vs visiting all nodes , we have an ATSP tour of the same cost , since all such paths will end in vt ( as vt has no outgoing edges ) . \n\t', '\n\t\t An example is shown in Fig . \n\t', '\n\t\t 1. The ATSP instance on the left has the tour ( 1 , 3 , 2 , 1 ) , indicated by the solid edges . \n\t', '\n\t\t The node 1 is split into the two nodes 1s and 1t , and the tour translates to the path ( 1s , 3 , 2 , 1t ) in the 2PDOP instance . \n\t', ""\n\t\t 3.2 Reduction of dPDOP to GATSP Conversely , we can encode an instance G = ( V , E , s , c ) of dPDOP as an instance G ' = 3PDOP GATSP Figure 2 : Reduction of dPDOP to GATSP . \n\t"", '\n\t\t Edges to the source node [ s , s ] are not drawn . \n\t', ""\n\t\t ((V'u)uEV , E ' , c ' ) of GATSP , in such a way that the optimal solutions correspond . \n\t"", '\n\t\t The cost of traversing an edge in dPDOP depends on the previous d \x97 1 nodes ; we compress these costs into ordinary costs of single edges in the reduction to GATSP . \n\t', '\n\t\t The GATSP instance has a node [ u1 , ... , ud_1 ] for every d \x97 1-tuple of nodes of V . \n\t', '\n\t\t It has an edge from [ u1 , ... , ud_1 ] to [ u2 , ... , ud_1 , ud ] iff there is an edge from ud_1 to ud in G , and it has an edge from each node into [ s , ... , s ] . \n\t', ""\n\t\t The idea is to encode a path P = ( s = u0 , u1 , ... , un ) in Gas a tour TP in G ' that successively visits the nodes [ uz_d+1 , ... uz ] , i = 0 , ... n , where we assume that uj = s for all j G 0 ( compare Figure 2 ) . \n\t"", '\n\t\t The cost of TP can be made equal to the cost of P by making the cost of the edge from [ u1 , ... , ud_1 ] to [ u2 , ... , ud ] equal to c(u1 , ... ud ) . \n\t', ""\n\t\t ( We set c'(e) to 0 for all edges e between nodes with first component s and for the edges e with target node [ sd_1 ] . \n\t"", ""\n\t\t ) Finally , we define Vu ' to be the set of all nodes in G ' with last component u . \n\t"", ""\n\t\t It is not hard to see that for any simple path of length n in G , we find a tour TP in G ' with the same cost . \n\t"", ""\n\t\t Conversely , we can find for every tour in G ' a simple path of length n in G with the same cost . \n\t"", ""\n\t\t Note that the encoding G ' will contain many unnecessary nodes and edges . \n\t"", '\n\t\t For instance , all nodes that have no incoming edges can never be used in a tour , and can be deleted . \n\t', '\n\t\t We can safely delete such unnecessary nodes in a post-processing step . \n\t', '\n\t\t An example is shown in Fig . \n\t', '\n\t\t 2. The 3PDOP instance on the left has a path ( s , 3 , 1 , 2 ) , which translates to the path ( [ s , s ] , [ s , 3 ] , [ 3 , 1 ] , [ 1 , 2 ] ) in the GATSP instance shown on the right . \n\t', '\n\t\t This path can be completed by a tour by adding the edge ( [ 1 , 2 ] , [ s , s ] ) , of cost 0 . \n\t', '\n\t\t The tour indeed visits each Vu0 ( i.e. , each column ) exactly once . \n\t', '\n\t\t Nodes with last component s which are not [ s , s ] are unreachable and are not shown . \n\t', '\n\t\t For the special case of d = 2 , the GATSP is simply an ordinary ATSP . \n\t', '\n\t\t The graphs of both problems look identical in this case , except that the GATSP instance has edges of cost 0 from any node to the source [ s ] . \n\t', '\n\t\t 4 Computing Optimal Orderings The equivalence of dPDOP and GATSP implies that we can now bring algorithms from the vast literature on TSP to bear on the discourse ordering problem . \n\t', '\n\t\t One straightforward method is to reduce the GATSP further to ATSP \n\t\t']",Positive
['\n\t\t Then one can solve the reduced ATSP instance ; see \n\t\t'],Positive
"['\n\t\t We choose the alternative of developing a new algorithm for solving GATSP directly , which uses standard techniques from combinatorial optimisation , gives us a better handle on optimising the algorithm for our problem instances , and runs more efficiently in practice . \n\t', '\n\t\t Our algorithm translates the GATSP instance into an integer linear program ( ILP ) and uses the branch-and-cut method \n\t\t']",Positive
"['\n\t\t Integer linear programs consist of a set of linear equations and inequalities , and are solved by integer variable assignments which maximise or minimise a goal function while satisfying the other conditions . \n\t', '\n\t\t Let G = ( V , E ) be a directed graph and 5 ^ V . \n\t', '\n\t\t We define S+(5) = { ( u , v ) E E I u E 5 and v E~ 5 } and S\x97 ( 5 ) = { ( u , v ) E E I u E/ 5 and v E 5 } , i.e. S+(5) and S\x97(5) are the sets of all incoming and outgoing edges of 5 , respectively . \n\t', '\n\t\t We assume that the graph G has no edges within one partition Vu , since such edges cannot be used by any solution . \n\t', '\n\t\t With this assumption , GATSP can be phrased as an ILP as follows ( this formulation is similar to the one proposed by \n\t\t']",Positive
"['\n\t\t The intention is that xe has value 1 if e is used in the tour , and 0 otherwise . \n\t', '\n\t\t Thus the cost of the tour can be written as PeEE cexe . \n\t', '\n\t\t The three conditions enforce the variable assignment to encode a valid GATSP tour . \n\t', '\n\t\t ( 1 ) ensures that all integer solutions encode a set of cycles . \n\t', '\n\t\t ( 2 ) guarantees that every partition Vi is visited by exactly one cycle . \n\t', '\n\t\t The inequalities ( 3 ) say that every subset of the partitions has an outgoing edge ; this makes sure a solution encodes one cycle , rather than a set of multiple cycles . \n\t', '\n\t\t To solve such an ILP using the branch-and-cut method , we drop the integrality constraints ( i.e. we replace xe E { 0 , 1 } by 0 < xe < 1 ) and solve the corresponding linear programming ( LP ) relaxation . \n\t', '\n\t\t If the solution of the LP is integral , we found the optimal solution . \n\t', '\n\t\t Otherwise we pick a variable with a fractional value and split the problem into two subproblems by setting the variable to 0 and 1 , respectively . \n\t', '\n\t\t We solve the subproblems recursively and disregard a subproblem if its LP bound is worse than the best known solution . \n\t', '\n\t\t Since our ILP contains an exponential number of inequalities of type ( 3 ) , solving the complete LPs directly would be too expensive . \n\t', '\n\t\t Instead , we start with a small subset of these inequalities , and test ( efficiently ) whether a solution of the smaller LP violates an inequality which is not in the current LP . \n\t', '\n\t\t If so , we add the inequality to the LP , resolve it , and iterate . \n\t', '\n\t\t Otherwise we found the solution of the LP with the exponential number of inequalities . \n\t', '\n\t\t The inequalities we add by need are called cutting planes ; algorithms that find violated cutting planes are called separation algorithms . \n\t', '\n\t\t To keep the size of the branch-and-cut tree small , our algorithm employs some heuristics to find further upper bounds . \n\t', '\n\t\t In addition , we improve lower bound from the LP relaxations by adding further inequalities to the LP that are valid for all integral solutions , but can be violated for optimal solutions of the LP . \n\t', '\n\t\t One major challenge here was to find separation algorithms for these inequalities . \n\t', '\n\t\t We cannot go into these details for lack of space , but will discuss them in a separate paper . \n\t', '\n\t\t 5 Evaluation We implemented the algorithm and ran it on some examples to evaluate its practical efficiency . \n\t', '\n\t\t The runtimes are shown in Tables 3 and 4 for an implementation using a branch-and-cut ILP solver which is free for all academic purposes ( ILP-FS ) and a commercial branch-and-cut ILP solver ( ILP-CS ) . \n\t', '\n\t\t Our implementations are based on LEDA 4.4.1 min X cexe eEE Xs.t . \n\t', '\n\t\t eEd+(v) X eEd\x97(Vi) X eEd+(UiEIVi) xe E { 0 , 1 } Xxe = xe ^vEV ( 1 ) eEd\x97(v) xe = 1 1 < i < n ( 2 ) xe ^ 1 I C { 1 , ... , n } ( 3 ) Instance Size ILP-FS ILP-CS lapata-10 13 0.05 0.05 coffers1 M.NOCB 10 0.04 0.02 cabinet1 M.NOCB 15 0.07 0.01 random ( avg ) 20 0.09 0.07 random ( avg ) 40 0.28 0.17 random ( avg ) 60 1.39 0.40 random ( avg ) 100 6.17 1.97 Table 3 : Some runtimes for d = 2 ( in seconds ) . \n\t', '\n\t\t ( www. algorithmic- solutions.com ) for the data structures and the graph algorithms and on SCIL 0.8 ( www.mpi-sb.mpg.de/SCIL ) for implementing the ILP-based branch-and-cut algorithm . \n\t', '\n\t\t SCIL can be used with different branch-and-cut core codes . \n\t', '\n\t\t We used CPLEX 9.0 ( www. i log . \n\t', '\n\t\t com ) as commercial core and SCIP 0.68 ( www.zib.de/Optimization/ Software/SCIP/ ) based on SOPLEX 1.2.2a ( www.zib.de/Optimization/Software/ S o p l e x / ) as the free implementation . \n\t', '\n\t\t Note that all our implementations are still preliminary . \n\t', '\n\t\t The software is publicly available ( www. mpi- sb. mpg.de/\x98althaus/PDOP.html ) . \n\t', '\n\t\t We evaluate the implementations on three classes of inputs . \n\t', '\n\t\t First , we use two discourses from the GNOME corpus , taken from \n\t\t']",Positive
"['\n\t\t Second , we use twelve discourses from the BLLIP corpus taken from \n\t\t']",Positive
"['\n\t\t These discourses are 4 to 13 discourse units long ; the table only shows the instance with the highest running time . \n\t', '\n\t\t Finally , we generate random instances of 2PDOP of size 20\x96100 , and of 3PDOP of size 10 , 15 , and 20 . \n\t', '\n\t\t A random instance is the complete graph , where c(ui , ... , ud ) is chosen uniformly at random from { 0 , ... , 999 } . \n\t', '\n\t\t The results for the 2-place instances are shown in Table 3 , and the results for the 3-place instances are shown in Table 4 . \n\t', '\n\t\t The numbers are runtimes in seconds on a Pentium 4 ( Xeon ) processor with 3.06 GHz . \n\t', '\n\t\t Note that a hypothetical baseline implementation which naively generates and evaluates all permutations would run over 77 years for a discourse of length 20 , even on a highly optimistic platform that evaluates one billion permutations per second . \n\t', '\n\t\t For d = 2 , all real-life instances and all random instances of size up to 50 can be solved in less than one second , with either implementation . \n\t', '\n\t\t The problem becomes more challenging for d = 3 . \n\t', '\n\t\t Here the algorithm quickly establishes good LP bounds for Instance Size ILP-FS ILP-CS coffers 1 M.KP 10 0.05 0.05 coffers1 M.BFP 10 0.08 0.06 cabinet1 M.KP 15 0.40 1.12 cabinet1 M.BFP 15 0.39 0.28 random ( avg ) 10 1.00 0.42 random ( avg ) 15 35.1 5.79 random ( avg ) 20 - 115.8 Table 4 : Some runtimes for d = 3 ( in seconds ) . \n\t', '\n\t\t the real-life instances , and thus the branch-and-cut trees remain small . \n\t', '\n\t\t The LP bounds for the random instances are worse , in particular when the number of units gets larger . \n\t', '\n\t\t In this case , the further optimisations in the commercial software make a big difference in the size of the branch-and-cut tree and thus in the solution time . \n\t', '\n\t\t An example output for cabinet1 with M.NOCB is shown in Fig . \n\t', '\n\t\t 3 ; we have modified referring expressions to make the text more readable , and have marked discourse unit boundaries with \x93/\x94 and expressions that establish local coherence with square brackets . \n\t', '\n\t\t This is one of many possible optimal solutions , which have cost 2 because of the two NOCB transitions at the very start of the discourse . \n\t', '\n\t\t Details on the comparison of different centering-based coherence measures are discussed by \n\t\t']",Positive
"['\n\t\t 6 Comparison to Other Approaches There are two approaches in the literature that are similar enough to ours that a closer comparison is in order . \n\t', '\n\t\t The first is a family of algorithms for discourse ordering based on genetic programming \n\t\t']",Positive
"['\n\t\t This is a very flexible and powerful approach , which can be applied to measures of local coherence that do not seem to fit in our framework trivially . \n\t', '\n\t\t For example , the measure from \n\t\t']",Positive
"['\n\t\t However , our algorithm is several orders of magnitude faster where a direct comparison is possible ( Manurung , p.c. ) , and it is guaranteed to find an optimal ordering . \n\t', '\n\t\t The nonapproximability result for TSP means that a genetic ( or any other ) algorithm which is restricted to polynomial runtime could theoretically deliver arbitrarily bad solutions . \n\t', '\n\t\t Second , the discourse ordering problem we have discussed in this paper looks very similar to the Majority Ordering problem that arises in the context of multi-document summarisation ( Barzilay et al. , Both cabinets probably entered England in the early nineteenth century / after the French Revolution caused the dispersal of so many French collections . \n\t', '\n\t\t / The pair to [ this monumental cabinet ] still exists in Scotland . \n\t', '\n\t\t / The fleurs-de-lis on the top two drawers indicate that [ the cabinet ] was made for the French King Louis XIV . \n\t', '\n\t\t / [ It ] may have served as a royal gift , / as [ it ] does not appear in inventories of [ his ] possessions . \n\t', '\n\t\t / Another medallion inside shows [ him ] a few years later . \n\t', '\n\t\t / The bronze medallion above [ the central door ] was cast from a medal struck in 1661 which shows [ the king ] at the age of twenty-one . \n\t', '\n\t\t / A panel of marquetry showing the cockerel of [ France ] standing triumphant over both the eagle of the Holy Roman Empire and the lion of Spain and the Spanish Netherlands decorates [ the central door ] . \n\t', '\n\t\t / In [ the Dutch Wars ] of 1672 - 1678 , [ France ] fought simultaneously against the Dutch , Spanish , and Imperial armies , defeating them all . \n\t', '\n\t\t / [ The cabinet ] celebrates the Treaty of Nijmegen , which concluded [ the war ] . \n\t', '\n\t\t / The Sun King\x92s portrait appears twice on [ this work ] . \n\t', '\n\t\t / Two large figures from Greek mythology , Hercules and Hippolyta , Queen of the Amazons , representatives of strength and bravery in war appear to support [ the cabinet ] . \n\t', '\n\t\t / The decoration on [ the cabinet ] refers to [ Louis XIV\x92s ] military victories . \n\t', '\n\t\t / On the drawer above the door , gilt-bronze military trophies flank a medallion portrait of [ the king ] . \n\t', '\n\t\t Figure 3 : An example output based on M.NOCB . \n\t', '\n\t\t 2002 ) . \n\t', '\n\t\t The difference between the two problems is that Barzilay et al . \n\t', '\n\t\t minimise the sum of all costs Cij for any pair i , j of discourse units with i < j , whereas we only sum over the Cij for i = j \x97 1 . \n\t', '\n\t\t This makes their problem amenable to the approximation algorithm by \n\t\t']",Positive
"['\n\t\t However , a Majority Ordering algorithm is not guaranteed to compute good solutions to the discourse ordering problem , as \n\t\t']",Positive
"['\n\t\t 7 Conclusion We have shown that the problem of ordering clauses into a discourse that maximises local coherence is equivalent to the travelling salesman problem : Even the two-place discourse ordering problem can encode ATSP . \n\t', '\n\t\t This means that the problem is NP- complete and doesn\x92t even admit polynomial approximation algorithms ( unless P=NP ) . \n\t', '\n\t\t On the other hand , we have shown how to encode the discourse ordering problems of arbitrary arity d into GATSP . \n\t', '\n\t\t We have demonstrated that modern branch-and-cut algorithms for GATSP can easily solve practical discourse ordering problems if d = 2 , and are still usable for many instances with d = 3 . \n\t', '\n\t\t As far as we are aware , this is the first algorithm for discourse ordering that can make any guarantees about the solution it computes . \n\t', '\n\t\t Our efficient implementation can benefit generation and summarisation research in at least two respects . \n\t', '\n\t\t First , we show that computing locally coherent orderings of clauses is feasible in practice , as such coherence measures will probably be applied on sentences within the same paragraph , i.e. on problem instances of limited size . \n\t', '\n\t\t Second , our system should be a useful experimentation tool in developing new measures of local coherence . \n\t', '\n\t\t We have focused on local coherence in this paper , but it seems clear that notions of global coherence , which go beyond the level of sentence-to-sentence transitions , capture important aspects of coherence that a purely local model cannot . \n\t', '\n\t\t However , our algorithm can still be useful as a subroutine in a more complex system that deals with global coherence \n\t\t']",Positive
"['\n\t\t Whether our methods can be directly applied to the tree structures that come up in theories of global coherence is an interesting question for future research . \n\t', '\n\t\t Acknowledgments . \n\t', '\n\t\t We would like to thank Mirella Lapata for providing the experimental data and Andrea Lodi for providing an efficiency baseline by running his ATSP solver on our inputs . \n\t', '\n\t\t We are grateful to Malte Gabsdil , Ruli Manurung , Chris Mellish , Kristina Striegnitz , and our reviewers for helpful comments and discussions . \n\t', '\n\t\t References R. Barzilay , N. Elhadad , and K. R. McKeown . \n\t', '\n\t\t 2002. Inferring strategies for sentence ordering in multidocument news summarization . \n\t', '\n\t\t Journal ofArtificial Intelligence Research , 17:35\x9655 . \n\t', '\n\t\t S. Brennan , M. Walker Friedman , and C. Pollard . \n\t', '\n\t\t 1987. A centering approach to pronouns . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 25th ACL , pages 155\x96162 , Stanford . \n\t', '\n\t\t W. Cohen , R. Schapire , and Y . \n\t', '\n\t\t Singer . \n\t', '\n\t\t 1999. Learn- ing to order things . \n\t', '\n\t\t Journal of Artificial Intelli- gence Research , 10:243\x96270 . \n\t', '\n\t\t T. H. Cormen , C. E. Leiserson , and R. L. Rivest . \n\t', '\n\t\t 1990. Introduction to Algorithms . \n\t', '\n\t\t MIT Press , Cambridge . \n\t', '\n\t\t M. Fischetti , A. Lodi , and P. Toth . \n\t', '\n\t\t 2001. Solv- ing real-world ATSP instances by branch-andcut . \n\t', '\n\t\t Combinatorial Optimization . \n\t', '\n\t\t M. Fischetti , A. Lodi , and P. Toth . \n\t', '\n\t\t 2002 . \n\t', '\n\t\t Exact methods for the asymmmetric traveling salesman problem . \n\t', '\n\t\t In G. Gutin and A. Punnen , editors , The Traveling Salesman Problem and its Variations . \n\t', '\n\t\t Kluwer . \n\t', '\n\t\t N. Karamanis and H. M. Manurung . \n\t', '\n\t\t 2002. Stochastic text structuring using the principle of continuity . \n\t', '\n\t\t In Proceedings of INLG-02 , pages 81\x9688 , New York . \n\t', '\n\t\t N. Karamanis , M. Poesio , C. Mellish , and J. Oberlander . \n\t', '\n\t\t 2004. Evaluating centering-based metrics of coherence for text structuring using a reliably annotated corpus . \n\t', '\n\t\t In Proceedings of the 42nd ACL , Barcelona . \n\t', '\n\t\t N. Karamanis . \n\t', '\n\t\t 2003. Entity Coherence for Descriptive Text Structuring . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , Division of Informatics , University of Edinburgh . \n\t', '\n\t\t R. Kibble and R. Power . \n\t', '\n\t\t 2000. An integrated framework for text planning and pronominalisation . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t INLG 2000 , pages 77\x9684 , Mitzpe Ramon . \n\t', '\n\t\t M. Lapata . \n\t', '\n\t\t 2003. Probabilistic text structuring : Experiments with sentence ordering . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 41st ACL , pages 545\x96552 , Sapporo , Japan . \n\t', '\n\t\t G. Laporte , H. Mercure , and Y. Nobert . \n\t', '\n\t\t 1987. Generalized travelling salesman problem through n sets of nodes : the asymmetrical case . \n\t', '\n\t\t Discrete Applied Mathematics , 18:185\x96197 . \n\t', '\n\t\t W. Mann and S. Thompson . \n\t', '\n\t\t 1988 . \n\t', '\n\t\t Rhetorical structure theory : A theory of text organization . \n\t', '\n\t\t Text , 8(3):243\x96281 . \n\t', '\n\t\t D. Marcu . \n\t', '\n\t\t 1997. From local to global coherence : A bottom-up approach to text planning . \n\t', '\n\t\t In Proceedings of the 14th AAAI , pages 629\x96635 . \n\t', '\n\t\t C. Mellish , A. Knott , J. Oberlander , and M. O\x92Donnell . \n\t', '\n\t\t 1998. Experiments using stochastic search for text planning . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 9th INLG , pages 98\x96107 , Niagara-on-the-Lake . \n\t', '\n\t\t G.L. Nemhauser and L.A. Wolsey . \n\t', '\n\t\t 1988. Integer and Combinatorial Optimization . \n\t', '\n\t\t John Wiley & Sons . \n\t', '\n\t\t C.E. . \n\t', '\n\t\t Noon and J.C. Bean . \n\t', '\n\t\t 1993. An efficient transformation of the generalized traveling salesman problem . \n\t', '\n\t\t Information Systems and Operational Research , 31(1) . \n\t', '\n\t\t M. Strube and U. Hahn . \n\t', '\n\t\t 1999. Functional centering : Grounding referential coherence in information structure . \n\t', '\n\t\t Computational Linguistics , 25(3) . \n\t', '\n\t\t M. Walker , A. Joshi , and E. Prince . \n\t', '\n\t\t 1998 . \n\t', '\n\t\t Centering in naturally occuring discourse : An overview . \n\t', '\n\t\t In M. Walker , A. Joshi , and E. Prince , edi- tors , Centering Theory in Discourse , pages 1\x9630 . \n\t', '\n\t\t Clarendon Press , Oxford . \n\t', '\n\t\t B. Webber , A. Knott , M. Stone , and A. Joshi . \n\t', '\n\t\t 1999. What are little trees made of : A structural and presuppositional account using Lexicalized TAG . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 36th ACL , pages 151\x96156 , College Park . \n\t', '\n\t\t Generating Referring Expressions in Open Domains Advaith Siddharthan Ann Copestake Computer Science Department Computer Laboratory Columbia University University of Cambridge as372@cs.columbia.edu aac10@cl.cam.ac.uk Abstract We present an algorithm for generating referring expressions in open domains . \n\t', '\n\t\t Existing algorithms work at the semantic level and assume the availability of a classification for attributes , which is only feasible for restricted domains . \n\t', '\n\t\t Our alternative works at the realisation level , relies on Word- Net synonym and antonym sets , and gives equivalent results on the examples cited in the literature and improved results for examples that prior approaches cannot handle . \n\t', '\n\t\t We believe that ours is also the first algorithm that allows for the incremental incorporation of relations . \n\t', '\n\t\t We present a novel corpus-evaluation using referring expressions from the Penn Wall Street Journal Treebank . \n\t', '\n\t\t 1 Introduction Referring expression generation has historically been treated as a part of the wider issue of generating text from an underlying semantic representation . \n\t', '\n\t\t The task has therefore traditionally been approached at the semantic level . \n\t', '\n\t\t Entities in the real world are logically represented ; for example ( ignoring quantifiers ) , a big brown dog might be repre- sented as big1 ( x ) n brown1 ( x ) n dog1 ( x ) , where the predicates big1 , brown1 and dog1 represent different attributes of the variable ( entity ) x . \n\t', '\n\t\t The task of referring expression generation has traditionally been framed as the identification of the shortest logical description for the referent entity that differentiates it from all other entities in the discourse domain . \n\t', '\n\t\t For example , if there were a small brown dog ( small1 ( x ) n brown1 ( x ) n dog1 ( x ) ) in context , the minimal description for the big brown dog would be big1 ( x ) n dog1 (x)1 . \n\t', '\n\t\t This semantic framework makes it difficult to apply existing referring expression generation algorithms to the many regeneration tasks that are important today ; for example , summarisation , open- ended question answering and text simplification . \n\t', ""\n\t\t Unlike in traditional generation , the starting point in ' The predicate dog1 is selected because it has a distin- guished status , referred to as type in \n\t\t""]",Positive
"['\n\t\t One such predicate has to to be present in the description . \n\t', '\n\t\t these tasks is unrestricted text , rather than a semantic representation of a small domain . \n\t', '\n\t\t It is difficult to extract the required semantics from unrestricted text ( this task would require sense disambiguation , among other issues ) and even harder to construct a classification for the extracted predicates in the manner that existing approaches require ( cf. , § 2 ) . \n\t', '\n\t\t In this paper , we present an algorithm for generating referring expressions in open domains . \n\t', '\n\t\t We discuss the literature and detail the problems in applying existing approaches to reference generation to open domains in § 2 . \n\t', '\n\t\t We then present our approach in § 3 , contrasting it with existing approaches . \n\t', '\n\t\t We extend our approach to handle relations in § 3.3 and present a novel corpus-based evaluation on the Penn WSJ Treebank in § 4 . \n\t', '\n\t\t 2 Overview of Prior Approaches The incremental algorithm \n\t\t']",Positive
"['\n\t\t It takes as input the intended referent and a contrast set of distractors ( other entities that could be confused with the intended referent ) . \n\t', '\n\t\t Entities are represented as attribute value matrices ( AVMs ) . \n\t', '\n\t\t The algorithm also takes as input a *preferred-attributes* list that contains , in order of preference , the attributes that human writers use to reference objects . \n\t', '\n\t\t For example , the preference might be { colour , size , shape ... } . \n\t', '\n\t\t The algorithm then repeatedly selects attributes from *preferred- attributes* that rule out at least one entity in the contrast set until all distractors have been ruled out . \n\t', '\n\t\t It is instructive to look at how the incremental algorithm works . \n\t', '\n\t\t Consider an example where a large brown dog needs to be referred to . \n\t', '\n\t\t The contrast set contains a large black dog . \n\t', '\n\t\t These are represented by the AVMs shown below . \n\t', '\n\t\t ~ ~ type dog ~size large ~ colour black Assuming that the *preferred-attributes* list is [ size , colour , ... ] , the algorithm would first com- pare the values of the size attribute ( both large ) , type dog size large colour brown ~ ~ 1 disregard that attribute as not being discriminating , compare the values of the colour attribute and return the brown dog . \n\t', '\n\t\t Subsequent work on referring expression generation has expanded the logical framework to allow reference by negation ( the dog that is not black ) and references to multiple entities ( the brown or black dogs ) ( van Deemter , 2002 ) , explored different search algorithms for finding the minimal description ( e.g. , \n\t\t']",Negative
"['\n\t\t However , all these approaches are based on very similar formalisations of the problem , and all make the following assumptions : 1 . \n\t', '\n\t\t A semantic representation exists . \n\t', '\n\t\t 2. A classification scheme for attributes exists . \n\t', '\n\t\t 3. The linguistic realisations are unambiguous . \n\t', '\n\t\t Attributes cannot be reference modifying . \n\t', '\n\t\t All these assumptions are violated when we move from generation in a very restricted domain to regeneration in an open domain . \n\t', '\n\t\t In regeneration tasks such as summarisation , open-ended question answering and text simplification , AVMs for entities are typically constructed from noun phrases , with the head noun as the type and pre-modifiers as attributes . \n\t', '\n\t\t Converting words into semantic labels would involve sense disambiguation , adding to the cost and complexity of the analysis module . \n\t', '\n\t\t Also , attribute classification is a hard problem and there is no existing classification scheme that can be used for open domains like newswire ; for example , WordNet \n\t\t']",Positive
"['\n\t\t In addition , selecting attributes at the semantic level is risky because their linguistic realisation might be ambiguous and many common adjectives are polysemous ( cf. , example 1 in § 3.1 ) . \n\t', '\n\t\t Reference modification , which has not been considered in the referring expression generation literature , raises further issues ; for example , referring to an alleged murderer as the murderer is potentially libellous . \n\t', '\n\t\t In addition to the above , there is the issue of overlap between values of attributes . \n\t', '\n\t\t The case of subsumption ( for example , that the colour red subsumes crimson and the type dog subsumes chihuahua ) has received formal treatment in the literature ; \n\t\t']",Negative
"['\n\t\t As mentioned earlier , such hierarchical knowledge bases do not exist for open domains . \n\t', '\n\t\t Further , a treatment of subsumption is insufficient , and degrees of intersection between attribute values also require consideration . \n\t', '\n\t\t van \n\t\t']",Negative
"['\n\t\t However , when applying referring expression generation to regeneration tasks where the representation of entities is derived from text rather than a knowledge base , we have to consider the case where the grading of attributes is not explicit . \n\t', '\n\t\t For example , we might need to compare the attribute dark with black , light or white . \n\t', '\n\t\t In contrast to previous approaches , our algorithm works at the level of words , not semantic labels , and measures the relatedness of adjectives ( lexicalised attributes ) using the lexical knowledge base Word- Net rather than a semantic classification . \n\t', '\n\t\t Our approach also addresses the issue of comparing intersective attributes that are not explicitly graded , by making novel use of the synonymy and antonymy links in WordNet . \n\t', '\n\t\t Further , it treats discriminating power as only one criteria for selecting attributes and allows for the easy incorporation of other considerations such as reference modification ( § 5 ) . \n\t', '\n\t\t 3 The Lexicalised Approach 3.1 Quantifying Discriminating Power We define the following three quotients . \n\t', '\n\t\t Similarity Quotient ( 5Q ) We define similarity as transitive synonymy . \n\t', '\n\t\t The idea is that if X is a synonym of Y and Y is a synonym of Z , then X is likely to be similar to Z . \n\t', '\n\t\t The degree of similarity between two adjectives depends on how many steps must be made through WordNet synonymy lists to get from one to the other . \n\t', '\n\t\t Suppose we need to find a referring expression for e0 . \n\t', '\n\t\t For each adjective aj describing e0 , we calculate a similarity quotient 5Qj by initialising it to 0 , forming a set of WordNet synonyms 51 of aj , forming a synonymy set 52 containing all the Word- Net synonyms of all the adjectives in 51 and forming 53 from 52 similarly . \n\t', '\n\t\t Now for each adjective describing any distractor , we increment 5Qj by 4 if it is present in 51 , by 2 if it is present in 52 , and by 1 if it is present in 53 . \n\t', '\n\t\t 5Qj now measures how similar aj is to other adjectives describing distractors . \n\t', '\n\t\t Contrastive Quotient ( CQ ) Similarly , we define contrastive in terms of antonymy relationships . \n\t', '\n\t\t We form the set C1 of strict WordNet antonyms of aj . \n\t', '\n\t\t The set C2 con- sists of strict WordNet antonyms of members of 51 and WordNet synonyms of members of C1 . \n\t', '\n\t\t C3 is similarly constructed from 52 and C2 . \n\t', '\n\t\t We now initialise CQj to zero and for each adjective describing each distractor , we add w =E { 4,2 , 1 } to CQj , depending on whether it is a member of C1 , C2 or C3 . \n\t', '\n\t\t CQj now measures how contrasting aj is to other adjectives describing distractors . \n\t', '\n\t\t Discriminating Quotient ( DQ ) An attribute that has a high value of 5Q has bad discriminating power . \n\t', '\n\t\t An attribute that has a high value of CQ has good discriminating power . \n\t', '\n\t\t We can now define the Discriminating Quotient ( DQ ) as DQ = CQ ^ 5Q . \n\t', '\n\t\t We now have an order ( decreasing DQs ) in which to incorporate attributes . \n\t', '\n\t\t This constitutes our *preferred* list . \n\t', '\n\t\t We illustrate the benefits of our approach with two examples . \n\t', '\n\t\t Example 1 : The Importance of Lexicalisation Previous referring expression generation algorithms ignore the issue of realising the logical description for the referent . \n\t', '\n\t\t The semantic labels are chosen such that they have a direct correspondence with their linguistic realisation and the realisation is thus considered trivial . \n\t', '\n\t\t Ambiguity and syntactically optional arguments are ignored . \n\t', '\n\t\t To illustrate one problem this causes , consider the two entities below : e1 e2 type president age young tenure past If we followed the strict typing system used by previous algorithms , with *preferred*={age , tenure } , to refer to e1 we would compare the age attributes and rule out e2 and generate the old president . \n\t', '\n\t\t This expression is ambiguous since old can also mean previous . \n\t', '\n\t\t Models that select attributes at the semantic level will run into trouble when their linguistic realisations are ambiguous . \n\t', '\n\t\t In contrast , our algorithm , given flattened attribute lists : e1 e2 r head president II head president 1 L attrib old , current J L attrib young , past J successfully picks the current president as current has a higher DQ ( 2 ) than old ( 0 ) : attribute distractor CQ SQ DQ old current e 2 { young , past } e 2 { young , past } 4 4 0 2 0 2 In this example , old is a WordNet antonym of young and a WordNet synonym of past . \n\t', '\n\t\t Current is a WordNet synonym of present , which is a WordNet antonym of past . \n\t', '\n\t\t Note that WordNet synonym and antonym links capture the implicit gradation in the lexicalised values of the age and tenure attributes . \n\t', '\n\t\t Example 2 : Naive Incrementality To illustrate another problem with the original incremental algorithm , consider three dogs : e 1(a big black dog ) , e 2 ( a small black dog ) and e 3 ( a tiny white dog ) . \n\t', '\n\t\t Consider using the original incremental algorithm to refer to e1 with *preferred*={colour , size } . \n\t', '\n\t\t The colour attribute black rules out e3 . \n\t', '\n\t\t We then we have to select the size attribute big as well to rule out e2 , thus generating the sub-optimal expression the big black dog . \n\t', '\n\t\t Here , the use of a predetermined *preferred* list fails to capture what is obvious from the context : that e 1 stands out not because it is black , but because it is big . \n\t', '\n\t\t In our approach , for each of e 1\x92s attributes , we calculate DQ with respect to e 2 and e 3 : attribute distractor CQ SQ DQ big e2{small , black } 4 0 4 big e 3 { tiny , white } 2 0 2 black e2{small , black } 1 4 -3 black e 3 { tiny , white } 2 1 1 Overall , big has a higher discriminating power ( 6 ) than black ( -2 ) and rules out both e 2 and e 3 . \n\t', '\n\t\t We therefore generate the big dog . \n\t', '\n\t\t Our incremental approach thus manages to select the attribute that stands out in context . \n\t', '\n\t\t This is because we construct the *preferred* list after observing the context . \n\t', '\n\t\t We discuss this issue further in the next section . \n\t', '\n\t\t Note again that WordNet antonym and synonym links capture the gradation in the lexicalised size and colour attributes . \n\t', '\n\t\t However , this only works where the gradation is along one axis ; in particular , this approach will not work for colours in general , and cannot be used to deduce the relative similarity between yellow and orange as compared to , say , yellow and blue . \n\t', '\n\t\t 3.2 Justifying our Algorithm The psycholinguistic justification for the incremental algorithm ( IA ) hinges on two premises : 1 . \n\t', '\n\t\t Humans build referring expressions incrementally . \n\t', '\n\t\t 2. There is a preferred order in which humans select attributes ( e.g. , colour>shape> size ... ) . \n\t', '\n\t\t Our algorithm is also incremental . \n\t', '\n\t\t However , it departs significantly from premise 2 . \n\t', '\n\t\t We assume that speakers pick out attributes that are distinctive in context ( cf. , example 2 , previous section ) . \n\t', '\n\t\t Averaged over contexts , some attributes have more discriminating power than others ( largely because of the way we visualise entities ) and premise 2 is an approximation to our approach . \n\t', '\n\t\t We now quantify the extra effort we are making to identify attributes that \x93stand out\x94 in a given context . \n\t', '\n\t\t Let N be the maximum number of entities in type president age old tenure current ~ ~ 1[ the contrast set and n be the maximum number of attributes per entity . \n\t', '\n\t\t The table below compares the computational complexity of an optimal algorithm ( such as \n\t\t']",Positive
"['\n\t\t Incremental Algo Our Algorithm Optimal Algo O(nN) O(n2N) O(n2 N ) Both the IA and our algorithm are linear in the number of entities N . \n\t', '\n\t\t This is because neither algorithm allows backtracking ; an attribute , once selected , cannot be discarded . \n\t', '\n\t\t In contrast , an optimal search requires O(2N) comparisons . \n\t', '\n\t\t As our algorithm compares each attribute of the discourse referent to every attribute of every distractor , it is quadratic in n . \n\t', '\n\t\t The IA compares each attribute of the discourse referent to only one attribute per distractor and is linear in n . \n\t', '\n\t\t Note , however , that values for n of over 4 are rare . \n\t', '\n\t\t 3.3 Relations Semantically , attributes describe an entity ( e.g. , the small grey dog ) and relations relate an entity to other entities ( e.g. , the dog in the bin ) . \n\t', '\n\t\t Relations are troublesome because in relating an entity e , , to el , we need to recursively generate a referring expression for el . \n\t', '\n\t\t The IA does not consider relations and the referring expression is constructed out of attributes alone . \n\t', '\n\t\t The \n\t\t']",Negative
"['\n\t\t To incorporate relational descriptions in the incremental framework would require a classification system which somehow takes into account the relations themselves and the secondary entities el etc. . \n\t', '\n\t\t This again suggests that the existing algorithms force the incrementality at the wrong stage in the generation process . \n\t', '\n\t\t Our approach computes the order in which attributes are incorporated after observing the context , by quantifying their utility through the quotient DQ . \n\t', '\n\t\t This makes it easy for us to extend our algorithm to handle relations , because we can compute DQ for relations in much the same way as we did for attributes.We illustrate this for prepositions . \n\t', '\n\t\t 3.4 Calculating DQ for Relations Suppose the referent entity eref contains a relation [ prep , , e , , ] that we need to calculate the three quotients for ( cf. , figure 1 for representation of relations in AVMs ) . \n\t', '\n\t\t We consider each entity ez in the contrast set for eref in turn . \n\t', '\n\t\t If ez does not have a prep , , relation then the relation is useful and we increment CQ by 4 . \n\t', '\n\t\t If ez has a prep , , relation then two cases arise . \n\t', '\n\t\t If the object of ez\x92s prep , , relation is e , , then we increment 5Q by 4 . \n\t', '\n\t\t If it is not e , , , the relation is useful and we increment CQ by 4 . \n\t', '\n\t\t This is an efficient non-recursive way of computing the quotients CQ and 5Q for relations . \n\t', '\n\t\t We now discuss how to calculate DQ . \n\t', '\n\t\t For attributes , we defined DQ = CQ \x97 5Q . \n\t', '\n\t\t However , as the linguistic realisation of a relation is a phrase and not a word , we would like to normalise the discriminating power of a relation with the length of its linguistic realisation . \n\t', '\n\t\t Calculating the length involves recursively generating referring expressions for the object of the preposition , an expensive task that we want to avoid unless we are actually using that relation in the final referring expression . \n\t', '\n\t\t We therefore initially approximate the length as follows . \n\t', '\n\t\t The realisation of a relation [ prep , , e , , ] consists of prep , , , a determiner and the referring expression for e , , . \n\t', '\n\t\t If none of eref\x92s distractors have a prep , , relation then we only require the head noun of e , , in the referring expression and length = 3 . \n\t', '\n\t\t In this case , the relation is sufficient to identify both entities ; for example , even if there were multiple bins in figure 1 , as long as only one dog is in a bin , the reference the dog in the bin succeeds in uniquely referencing both the dog and the bin . \n\t', '\n\t\t If n distractors of eref contain a prep , , relation with a non-e , , object that is distractor for e , , , we set length = 3 + n . \n\t', '\n\t\t This is an estimate for the word length of the realised relation that assumes one extra attribute for distinguishing e , , from each distractor . \n\t', '\n\t\t Normalisation by estimated length is vital ; if e , , requires a long description , the relations\x92s DQ should be small so that shorter possibilities are considered first in the incremental process . \n\t', '\n\t\t The formula for DQ for relations is therefore DQ = ( CQ \x97 5Q)/length . \n\t', '\n\t\t This approach can also be extended to allow for relations such as comparatives which have syntactically optional arguments ( e.g. , the earlier flight vs the flight earlier than UA941 ) which are not allowed for by approaches which ignore realisation . \n\t', '\n\t\t 3.5 The Lexicalised Context-Sensitive IA Our lexicalised context-sensitive incremental algorithm ( below ) generates a referring expression for Entity . \n\t', '\n\t\t As it recurses , it keeps track of entities it has used up in order to avoid entering loops like the dog in the bin containing the dog in the bin .... To generate a referring expression for an entity , the algorithm calculates the DQs for all its attributes and approximates the DQs for all its relations ( 2 ) . \n\t', '\n\t\t It then forms the *preferred* list ( 3 ) and constructs the referring expression by adding elements of *preferred* till the contrast set is empty ( 4 ) . \n\t', '\n\t\t This is straightforward for attributes ( 5 ) . \n\t', '\n\t\t For relations ( 6 ) , it needs to recursively generate the prepositional phrase first . \n\t', '\n\t\t It checks that it hasn\x92t entered a loop ( 6a ) , generates a new contrast set for the object of the relation (6(a)i) , recursively generates a referring expression for the object of the preposition (6(a)ii) , recalculates DQ (6(a)iii) and either incorporates the relation in the referring expression or shifts the relation down the *preferred* list (6(a)iv) . \n\t', '\n\t\t This step ensures that an initial mis-estimation in the word length of a relation doesn\x92t force its inclusion at the expense of shorter possibilities . \n\t', '\n\t\t If after incorporating all attributes and relations , the contrast set is still non- empty , the algorithm returns the best expression it can find ( 7 ) . \n\t', '\n\t\t set generate-ref-exp(Entity , ContrastSet , UsedEntities ) 1 . \n\t', '\n\t\t IF ContrastSet= [ ] THEN RETURN { Entity.head } 2 . \n\t', '\n\t\t Calculate CQ , SQ and DQ for each attribute and relation of Entity ( as in Sec 3.1 and 3.4 ) 3 . \n\t', '\n\t\t Let *preferred* be the list of attributes/ relations sorted in decreasing order of DQs . \n\t', '\n\t\t FOR each element ( Mod ) of *preferred* DO steps 4 , 5 and 6 4 . \n\t', '\n\t\t IF ContrastSet = [ ] THEN RETURN RefExp U { Entity.head } 5 . \n\t', '\n\t\t IF Mod is an Attribute THEN ( a ) LET RefExp = { Mod } U RefExp ( b ) Remove from ContrastSet , any entities Mod rules out 6 . \n\t', '\n\t\t IF Mod is a Relation [ prepi ei ] THEN ( a ) IF ei E UsedEntities THEN i . \n\t', '\n\t\t Set DQ = \x97oo ii . \n\t', '\n\t\t Move Mod to the end of *preferred* ELSE i. LET ContrastSet2 be the set of non-ei entities that are the objects of prepi relations in members of ContrastSet ii . \n\t', '\n\t\t LET RE = generate-referring-exp(ei , ContrastSet2 , {ei}UUsedEntities ) iii . \n\t', '\n\t\t recalculate DQ using length = 2 + length(RE) iv . \n\t', '\n\t\t IF position in *preferred* is lowered THEN re-sort *preferred* ELSE ( a ) SET Re xp = Re . \n\t', '\n\t\t . \n\t', '\n\t\t U { [ prepildeterminerlRE ] } ( o ) Remove from ContrastSet , any entities that Mod rules out 7 . \n\t', '\n\t\t RETURN RefExp U { Entity.head } An Example Trace : We now trace the algorithm above as it generates a referring expression for d1 in figure 1. call generate-ref-exp(d1,[d2],[]) \x95 step 1 : ContrastSet is not empty \x95 step 2 : DQsmall = \x974 , DQgrey = \x974 DQ[in b1 ] = 4/3 , DQnear d2 ] = 4/4 \x95 step 3 : *preferred* = [ [ in b1{ [ near d2 ] , small , grey ] head dog attrib [ small , grey ] outside b1 near d1 head bin attrib [ large , steel ] containing d1 near d2 Figure 1 : AVMs for two dogs and a bin \x95 Iteration 1 \x97 mod = [ in b 1 ] \x96 step 6(a)i : ContrastSet2 = [ ] \x96 step 6(a)ii : call generate-ref-exp(b1,[],[d1]) * step 1 : ContrastSet = [ ] return { bin } \x96 step 6(a)iii : DQ[in b1 ] = 4/3 \x96 step 6(a)iva : RefExp = { [ in , the , { bin } ] } \x96 step 6(a)ivo : ContrastSet = [ ] \x95 Iteration 2 \x97 mod = [ near d2 ] \x96 step 4 : ContrastSet = [ ] return { [ in the { bin } ] , dog } The algorithm presented above is designed to return the shortest referring expression that uniquely identifies an entity . \n\t', '\n\t\t If the scene in figure 1 were cluttered with bins , the algorithm would still refer to d1 as the dog in the bin as there is only one dog that is in a bin . \n\t', '\n\t\t The user gets no help in locating the bin . \n\t', '\n\t\t If helping the user locate entities is important to the discourse plan , we need to change step 6(a)(ELSE)i so that the contrast set includes all bins in context , not just bins that are objects of in relations of distractors of d1 . \n\t', '\n\t\t 3.6 Compound Nominals Our analysis so far has assumed that attributes are adjectives . \n\t', '\n\t\t However , many nominals introduced through relations can also be introduced in compound nominals , for example : 1 . \n\t', '\n\t\t a church in Paris H a Paris church 2 . \n\t', '\n\t\t a novel by Archer H an Archer novel 3 . \n\t', '\n\t\t a company from London H a London company d1 d2 b1 head dog attrib [ small , grey ] in b1 near d2 ~ ~ ~ ~ d1 ~ ~ ~ ~ d2 1 1 ~ ~ b1 ~ ~ ~ ~ This is an important issue for regeneration applications , where the AVMs for entities are constructed from text rather than a semantic knowledge base ( which could be constructed such that such cases are stored in relational form , though possibly with an underspecified relation ) . \n\t', '\n\t\t We need to augment our algorithm so that it can compare AVMs like : ~ ~head church ~head church ~ in [ head Paris ] and attrib [ Paris ] Formally , the algorithm for calculating SQ and CQ for a nominal attribute a,,,om of entity eo is : FOR each distractor ei of e , DO 1 . \n\t', '\n\t\t IF a , , , .. is similar to any nominal attribute of ei THEN SQ = SQ + 4 2 . \n\t', '\n\t\t IF a , , , .. is similar to the head noun of the object of any relation of ei THEN ( a ) SQ = SQ + 4 ( b ) flatten that relation for ei , i.e. , add the attributes of the object of the relation to the attribute list for ei In step 2 , we compare a nominal attribute a,,,om of eo to the head noun of the object of a relation of ei . \n\t', '\n\t\t If they are similar , it is likely that any attributes of that object might help distinguish eo from ei . \n\t', '\n\t\t We then add those attributes to the attribute list of ei . \n\t', '\n\t\t Now , if SQ is non-zero , the nominal attribute a,,,om has bad discriminating power and we set DQ = ^SQ . \n\t', '\n\t\t If SQ = 0 , then a,,,om has good discriminating power and we set DQ = 4 . \n\t', '\n\t\t We also extend the algorithm for calculating DQ for a relation [ prep ] ej ] of eo as follows : 1 . \n\t', '\n\t\t IF any distractor ei has a nominal attribute a , , , .. THEN ( a ) IF a , , , .. is similar to the head of ej THEN i . \n\t', '\n\t\t Add all attributes of e , to the attribute list and calculate their DQs 2. calculate DQ for the relation as in section 3.4 We can demonstrate how this approach works using entities extracted from the following sentence ( from the Wall Street Journal ) : Also contributing to the firmness in copper , the analyst noted , was a report by Chicago purchasing agents , which precedes the full purchasing agents report that is due out today and gives an indication of what the full report might hold . \n\t', '\n\t\t Consider generating a referring expression for eo when the distractor is el : head report ~ ~ head agents by ~attrib [ Chicago , ~ purchasing ] ~ ~_ head report e1 attributes [ full , purchasing , agents ] The distractor the full purchasing agents report contains the nominal attribute agents . \n\t', '\n\t\t To compare report by Chicago purchasing agents with full purchasing agents report , our algorithm flattens the former to Chicago purchasing agents report . \n\t', '\n\t\t Our algorithm now gives : DQagents = \x974 , DQpurchasing = \x974 , DQChicago = 4 , DQby Chicagopurchasing agents = 4/4 We thus generate the referring expression the Chicago report . \n\t', '\n\t\t This approach takes advantage of the flexibility of the relationships that can hold between nouns in a compound : although examples can be devised where removing a nominal causes ungrammaticality , it works well enough empirically . \n\t', '\n\t\t To generate a referring expression for e1 ( full purchasing agents report ) when the distractor is eo(report by Chicago purchasing agents ) , our algorithm again flattens eo to obtain : DQagents = \x974 , DQpurchasing = \x974 DQfull=4 The generated referring expression is the full report . \n\t', '\n\t\t This is identical to the referring expression used in the original text . \n\t', '\n\t\t 4 Evaluation As our algorithm works in open domains , we were able to perform a corpus-based evaluation using the Penn WSJ Treebank \n\t\t']",Positive
"['\n\t\t Our evaluation aimed to reproduce existing referring expressions ( NPs with a definite determiner ) in the Penn Treebank by providing our algorithm as input : 1 . \n\t', '\n\t\t The first mention NP for that reference . \n\t', '\n\t\t 2. The contrast set of distractor NPs For each referring expression ( NP with a definite determiner ) in the Penn Treebank , we automatically identified its first mention and all its distractors in a four sentence window , as described in § 4.1 . \n\t', '\n\t\t We then used our program to generate a referring expression for the first mention NP , giving it a contrast- set containing the distractor NPs . \n\t', '\n\t\t Our evaluation compared this generated description with the original WSJ reference that we had started out with . \n\t', '\n\t\t Our algorithm was developed using toy examples and counter-examples constructed by hand , and the Penn Treebank was unseen data for this evaluation . \n\t', '\n\t\t 4.1 Identifying Antecedents and Distractors For every definite noun phrase NPo in the Penn Treebank , we shortlisted all the noun phrases NPi in a discourse window of four sentences ( the two ~ ~ ~ ~ e , = preceding sentences , current sentence and the following sentence ) that had a head noun identical to or a WordNet synonym of the head noun of NPo . \n\t', '\n\t\t We compared the set of attributes and relations for each shortlisted NPZ that preceded NPo in the discourse window with that of NPo . \n\t', '\n\t\t If the attributes and relations set of NPZ was a superset of that of NPo , we assumed that NPo referred to NPZ and added NPZ to an antecedent set . \n\t', '\n\t\t We added all other NPZ to the contrast set of distractors . \n\t', '\n\t\t Similarly , we excluded any noun phrase NPZ that appeared in the discourse after NPo whose attributes and relations set was a subset of NPo\x92s and added the remaining NPZ to the contrast set . \n\t', '\n\t\t We then selected the longest noun phrase in the antecedent set to be the antecedent that we would try and generate a referring expression from . \n\t', '\n\t\t The table below gives some examples of distractors that our program found using WordNet synonyms to compare head nouns : Entity Distractors first half-free Soviet vote military construction bill steep fall in currency permanent insurance fair elections in the GDR fiscal measure drop in market stock death benefit coverage 4.2 Results There were 146 instances of definite descriptions in the WSJ where the following conditions ( that ensure that the referring expression generation task is nontrivial ) were satisfied : 1 . \n\t', '\n\t\t The definite NP ( referring expression ) contained at least one attribute or relation . \n\t', '\n\t\t 2. An antecedent was found for the definite NP . \n\t', '\n\t\t 3. There was at least one distractor NP in the discourse window . \n\t', '\n\t\t In 81.5 % of these cases , our program returned a referring expression that was identical to the one used in the WSJ . \n\t', '\n\t\t This is a surprisingly high accuracy , considering that there is a fair amount of variability in the way human writers use referring expressions . \n\t', '\n\t\t For comparison , the baseline of reproducing the antecedent NP performed at 48%2 . \n\t', '\n\t\t Some errors were due to non-recognition of multiword expessions in the antecedent ( for example , our program generated care product from personal care product ) . \n\t', '\n\t\t In many of the remaining error cases , it was difficult to decide whether what our program generated was acceptable or wrong . \n\t', '\n\t\t For example , the WSJ contained the referring expression the one-day limit , where the automatically detected antecedent was the maximum one-day limit for the 2We are only evaluating content selection ( the nouns and pre- and post-modifiers ) and ignore determiner choice . \n\t', '\n\t\t S&P 500 stock-index futures contract and the automatically detected contrast set was : { the five-point opening limit for the contract , the 12-point limit , the 30-point limit , the in- termediate limit of 20 points } Our program generated the maximum limit , where the WSJ writer preferred the one-day limit . \n\t', '\n\t\t 5 Further Issues 5.1 Reference Modifying Attributes The analysis thus far has assumed that all attributes modify the referent rather than the reference to the referent . \n\t', '\n\t\t However , for example , if e 1 is an alleged murderer , the attribute alleged modifies the reference murderer rather than the referent e 1 and referring to e 1 as the murderer would be factually incorrect . \n\t', '\n\t\t Logically e 1 could be represented as ( alleged1 ( murderer1 ) ) ( x ) , rather than alleged1 ( x ) n murderer1 ( x ) . \n\t', '\n\t\t This is no longer first-order , and presents new difficulties for the traditional formalisation of the reference generation problem . \n\t', '\n\t\t One ( inelegant ) solution would be to introduce a new predicate allegedMurderer1 ( x ) . \n\t', '\n\t\t A working approach in our framework would be to add a large positive weight to the DQs of reference modifying attributes , thus forcing them to be selected in the referring expression . \n\t', '\n\t\t 5.2 Discourse Context and Salience The incremental algorithm assumes the availability of a contrast set and does not provide an algorithm for constructing and updating it . \n\t', '\n\t\t The contrast set , in general , needs to take context into account . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t The black dog would then refer to the most salient entity in the discourse domain that is both black and a dog . \n\t', '\n\t\t Incorporating salience into our algorithm is straightforward . \n\t', '\n\t\t As described earlier , we compute the quotients SQ and CQ for each attribute or relation by adding an amount w E 14,2 , 11 to the relevant quotient based on a comparison with the attributes and relations of each distractor . \n\t', '\n\t\t We can incorporate salience by weighting w with the salience of the distractor whose attribute or relation we are considering . \n\t', '\n\t\t This will result in attributes and relations with high discriminating power with regard to more salient distractors getting selected first in the incremental process . \n\t', '\n\t\t 5.3 Discourse Plans In many situations , attributes and relations serve different discourse functions . \n\t', '\n\t\t For example , attributes might be used to help the hearer identify an entity while relations might serve to help locate the entity . \n\t', '\n\t\t This needs to be taken into account when generating a referring expression . \n\t', '\n\t\t If we were generating instructions for using a machine , we might want to include both attributes and relations ; so to instruct the user to switch on the power , we might say switch on the red button on the top-left corner . \n\t', '\n\t\t This would help the user locate the switch ( on the top-left corner ) and identify it ( red ) . \n\t', '\n\t\t If we were helping a chef find the salt in a kitchen , we might want to use only relations because the chef knows what salt looks like . \n\t', '\n\t\t The salt behind the corn flakes on the shelf above the fridge is in this context preferable to the white powder . \n\t', '\n\t\t If the discourse plan that controls generation requires our algorithm to preferentially select relations or attributes , it can add a positive amount a to their DQs . \n\t', '\n\t\t Then , the resultant formula is DQ = ( CQ \x97 SQ)/length + a , where length = 1 for attributes and by default a = 0 for both relations and attributes . \n\t', '\n\t\t 6 Conclusions and Future Work We have described an algorithm for generating referring expressions that can be used in any domain . \n\t', '\n\t\t Our algorithm selects attributes and relations that are distinctive in context . \n\t', '\n\t\t It does not rely on the availability of an adjective classification scheme and uses WordNet antonym and synonym lists instead . \n\t', '\n\t\t It is also , as far as we know , the first algorithm that allows for the incremental incorporation of relations and the first that handles nominals . \n\t', '\n\t\t In a novel evaluation , our algorithm successfully generates identical referring expressions to those in the Penn WSJ Treebank in over 80 % of cases . \n\t', '\n\t\t In future work , we plan to use this algorithm as part of a system for generation from a database of user opinions on products which has been automatically extracted from newsgroups and similar text . \n\t', '\n\t\t This is midway between regeneration and the classical task of generating from a knowledge base because , while the database itself provides structure , many of the field values are strings corresponding to phrases used in the original text . \n\t', '\n\t\t Thus , our lexicalised approach is directly applicable to this task . \n\t', '\n\t\t 7 Acknowledgements Thanks are due to Kees van Deemter and three anonymous ACL reviewers for useful feedback on prior versions of this paper . \n\t', '\n\t\t This document was generated partly in the context of the Deep Thought project , funded under the Thematic Programme User-friendly Information Society of the 5th Framework Programme of the European Community ( Contract N IST-2001-37836 ) References Robert Dale and Nicholas Haddock . \n\t', '\n\t\t 1991. Generating referring expressions involving relations . \n\t', '\n\t\t In Proceedings of the 5th Conference of the European Chapter of the Association for Computational Linguistics ( EACL\x9291 ) , pages 161\x96166 , Berlin , Germany . \n\t', '\n\t\t Robert Dale and Ehud Reiter . \n\t', '\n\t\t 1995. Computational interpretations of the Gricean maxims in the generation of referring expressions . \n\t', '\n\t\t Cognitive Science , 19:233\x96263 . \n\t', '\n\t\t Helmut Horacek . \n\t', '\n\t\t 2003. A best-first search algorithm for generating referring expressions . \n\t', '\n\t\t In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics ( EACL\x9203 ) , pages 103\x96106 , Budapest , Hungary . \n\t', '\n\t\t Emiel Krahmer and Mari¨et Theune . \n\t', '\n\t\t 2002. Efficient context-sensitive generation of referring expressions . \n\t', '\n\t\t In Kees van Deemter and Rodger Kibble , editors , Information Sharing : Givenness and Newness in Language Processing , pages 223\x96 264 . \n\t', '\n\t\t CSLI Publications , Stanford,California . \n\t', '\n\t\t Emiel Krahmer , Sebastiaan van Erk , and Andr´e Verleg . \n\t', '\n\t\t 2003. Graph-based generation of referring expressions . \n\t', '\n\t\t Computational Linguistics , 29(1):53\x9672 . \n\t', '\n\t\t Mitchell Marcus , Beatrice Santorini , and Mary Marcinkiewicz . \n\t', '\n\t\t 1993. Building a large natural language corpus of English : The Penn Treebank . \n\t', '\n\t\t Computational Linguistics , 19:313\x96330 . \n\t', '\n\t\t George A. Miller , Richard Beckwith , Christiane D. Fellbaum , Derek Gross , and Katherine Miller . \n\t', '\n\t\t 1993. Five Papers on WordNet . \n\t', '\n\t\t Technical report , Princeton University , Princeton , N.J. Ehud Reiter . \n\t', '\n\t\t 1990. The computational complexity of avoiding conversational implicatures . \n\t', '\n\t\t In Proceedings of the 28th Annual Meeting of Association for Computational Linguistics ( ACL\x9290 ) , pages 97\x96104 , Pittsburgh , Pennsylvania . \n\t', '\n\t\t Ehud Reiter and Robert Dale . \n\t', '\n\t\t 1992. A fast algorithm for the generation of referring expressions . \n\t', '\n\t\t In Proceedings of the 14th International Conference on Computational Linguistics ( COLING\x9292 ) , pages 232\x96238 , Nantes , France . \n\t', '\n\t\t Kees van Deemter . \n\t', '\n\t\t 2000. Generating vague descriptions . \n\t', '\n\t\t In Proceedings of the 1 st International Conference on Natural Language Generation ( INLG\x9200 ) , pages 179\x96185 , Mitzpe Ramon , Israel . \n\t', '\n\t\t Kees van Deemter . \n\t', '\n\t\t 2002. Generating referring expressions : Boolean extensions of the incremental algorithm . \n\t', '\n\t\t Computational Linguistics , 28(1):37\x96 52. \n\t', '\n\t\t Discovering Relations among Named Entities from Large Corpora Takaaki Hasegawa Cyberspace Laboratories Nippon Telegraph and Telephone Corporation 1-1 Hikarinooka , Yokosuka , Kanagawa 239-0847 , Japan hasegawa.takaaki@lab.ntt.co.jp Satoshi Sekine and Ralph Grishman Dept. of Computer Science New York University 715 Broadway , 7th floor , New York , NY 10003 , U.S.A. sekine,grishman @cs.nyu.edu Abstract Discovering the significant relations embedded in documents would be very useful not only for information retrieval but also for question answering and summarization . \n\t', '\n\t\t Prior methods for relation discovery , however , needed large annotated corpora which cost a great deal of time and effort . \n\t', '\n\t\t We propose an unsupervised method for relation discovery from large corpora . \n\t', '\n\t\t The key idea is clustering pairs of named entities according to the similarity of context words intervening between the named entities . \n\t', '\n\t\t Our experiments using one year of newspapers reveals not only that the relations among named entities could be detected with high recall and precision , but also that appropriate labels could be automatically provided for the relations . \n\t', '\n\t\t 1 Introduction Although Internet search engines enable us to access a great deal of information , they cannot easily give us answers to complicated queries , such as \x93a list of recent mergers and acquisitions of companies\x94 or \x93current leaders of nations from all over the world\x94 . \n\t', '\n\t\t In order to find answers to these types of queries , we have to analyze relevant documents to collect the necessary information . \n\t', '\n\t\t If many relations such as \x93Company A merged with Company B\x94 embedded in those documents could be gathered and structured automatically , it would be very useful not only for information retrieval but also for question answering and summarization . \n\t', '\n\t\t Information Extraction provides methods for extracting information such as particular events and relations between entities from text . \n\t', '\n\t\t However , it is domain dependent and it could not give answers to those types of queries from Web documents which include widely various domains . \n\t', '\n\t\t Our goal is automatically discovering useful relations among arbitrary entities embedded in large This work is supported by Nippon Telegraph and Telephone ( NTT ) Corporation\x92s one-year visiting program at New York University . \n\t', '\n\t\t text corpora . \n\t', '\n\t\t We defined a relation broadly as an affiliation , role , location , part-whole , social relationship and so on between a pair of entities . \n\t', '\n\t\t For example , if the sentence , \x93George Bush was inaugurated as the president of the United States.\x94 exists in documents , the relation , \x93George Bush\x94(PERSON) is the \x93President of\x94 the \x93United States\x94 ( GPEI ) , should be extracted . \n\t', '\n\t\t In this paper , we propose an unsupervised method of discovering relations among various entities from large text corpora . \n\t', '\n\t\t Our method does not need the richly annotated corpora required for supervised learning \x97 corpora which take great time and effort to prepare . \n\t', '\n\t\t It also does not need any instances of relations as initial seeds for weakly supervised learning . \n\t', '\n\t\t This is an advantage of our approach , since we cannot know in advance all the relations embedded in text . \n\t', '\n\t\t Instead , we only need a named entity ( NE ) tagger to focus on the named entities which should be the arguments of relations . \n\t', '\n\t\t Recently developed named entity taggers work quite well and are able to extract named entities from text at a practically useful level . \n\t', '\n\t\t The rest of this paper is organized as follows . \n\t', '\n\t\t We discuss prior work and their limitations in section 2 . \n\t', '\n\t\t We propose a new method of relation discovery in section 3 . \n\t', '\n\t\t Then we describe experiments and evaluations in section 4 and 5 , and discuss the approach in section 6 . \n\t', '\n\t\t Finally , we conclude with future work . \n\t', '\n\t\t 2 Prior Work The concept of relation extraction was introduced as part of the Template Element Task , one of the information extraction tasks in the Sixth Message Understanding Conference ( MUC-6 ) \n\t\t']",Positive
"['\n\t\t MUC-7 added a Template Relation Task , with three relations . \n\t', '\n\t\t Following MUC , the Automatic Content Extraction ( ACE ) meetings \n\t\t']",Positive
"['\n\t\t tion extraction . \n\t', '\n\t\t In the ACE Program 2 , Relation Detection and Characterization ( RDC ) was introduced as a task in 2002 . \n\t', '\n\t\t Most of approaches to the ACE RDC task involved supervised learning such as kernel methods \n\t\t']",Negative
"['\n\t\t The biggest problem with this approach is that it takes a great deal of time and effort to prepare annotated corpora large enough to apply supervised learning . \n\t', '\n\t\t In addition , the varieties of relations were limited to those defined by the ACE RDC task . \n\t', '\n\t\t In order to discover knowledge from diverse corpora , a broader range of relations would be necessary . \n\t', '\n\t\t Some previous work adopted a weakly supervised learning approach . \n\t', '\n\t\t This approach has the advantage of not needing large tagged corpora . \n\t', '\n\t\t Brin proposed the bootstrapping method for relation discovery \n\t\t']",Negative
"['\n\t\t Brin\x92s method acquired patterns and examples by bootstrapping from a small initial set of seeds for a particular relation . \n\t', '\n\t\t Brin used a few samples of book titles and authors , collected common patterns from context including the samples and finally found new examples of book title and authors whose context matched the common patterns . \n\t', '\n\t\t Agichtein improved Brin\x92s method by adopting the constraint of using a named entity tagger \n\t\t']",Negative
['\n\t\t Ravichandran also explored a similar method for question answering \n\t\t'],Negative
"['\n\t\t These approaches , however , need a small set of initial seeds . \n\t', '\n\t\t It is also unclear how initial seeds should be selected and how many seeds are required . \n\t', '\n\t\t Also their methods were only tried on functional relations , and this was an important constraint on their bootstrapping . \n\t', '\n\t\t The variety of expressions conveying the same relation can be considered an example of paraphrases , and so some of the prior work on paraphrase acquisition is pertinent to relation discovery . \n\t', '\n\t\t Lin proposed another weakly supervised approach for discovering paraphrase \n\t\t']",Positive
"['\n\t\t Firstly Lin focused on verb phrases and their fillers as subject or object . \n\t', '\n\t\t Lin\x92s idea was that two verb phrases which have similar fillers might be regarded as paraphrases . \n\t', '\n\t\t This approach , however , also needs a sample verb phrase as an initial seed in order to find similar verb phrases . \n\t', '\n\t\t 3 Relation Discovery 3.1 Overview We propose a new approach to relation discovery from large text corpora . \n\t', '\n\t\t Our approach is based on 2A research and evaluation program in information extraction organized by the U.S. Government . \n\t', '\n\t\t context based clustering of pairs of entities . \n\t', '\n\t\t We assume that pairs of entities occurring in similar context can be clustered and that each pair in a cluster is an instance of the same relation . \n\t', '\n\t\t Relations between entities are discovered through this clustering process . \n\t', '\n\t\t In cases where the contexts linking a pair of entities express multiple relations , we expect that the pair of entities either would not be clustered at all , or would be placed in a cluster corresponding to its most frequently expressed relation , because its contexts would not be sufficiently similar to contexts for less frequent relations . \n\t', '\n\t\t We assume that useful relations will be frequently mentioned in large corpora . \n\t', '\n\t\t Conversely , relations mentioned once or twice are not likely to be important . \n\t', '\n\t\t Our basic idea is as follows : 1. tagging named entities in text corpora 2. getting co-occurrence pairs of named entities and their context 3. measuring context similarities among pairs of named entities 4. making clusters of pairs of named entities 5. labeling each cluster of pairs of named entities We show an example in Figure 1 . \n\t', '\n\t\t First , we find the pair of ORGANIZATIONs ( ORG ) A and B , and the pair of ORGANIZATIONs ( ORG ) C and D , after we run the named entity tagger on our newspaper corpus . \n\t', '\n\t\t We collect all instances of the pair A and B occurring within a certain distance of one another . \n\t', '\n\t\t Then , we accumulate the context words intervening between A and B , such as \x93be offer to buy\x94 , \x93be negotiate to acquire\x94.3 In same way , we also accumulate context words intervening between C and D . \n\t', '\n\t\t If the set of contexts of A and B and those of C and D are similar , these two pairs are placed into the same cluster . \n\t', '\n\t\t A \x96 B and C \x96 D would be in the same relation , in this case , merger and acquisition ( M&A ) . \n\t', '\n\t\t That is , we could discover the relation between these ORGANIZATIONs . \n\t', '\n\t\t 3.2 Named entity tagging Our proposed method is fully unsupervised . \n\t', '\n\t\t We do not need richly annotated corpora or any initial manually selected seeds . \n\t', '\n\t\t Instead of them , we use a named entity ( NE ) tagger . \n\t', '\n\t\t Recently developed named entity taggers work quite well and extract named entities from text at a practically usable 3 We collect the base forms of words which are stemmed by a POS tagger \n\t\t']",Positive
"['\n\t\t But verb past participles are distinguished from other verb forms in order to distinguish the passive voice from the active voice . \n\t', '\n\t\t Figure 1 : Overview of our basic idea Similar context ? \n\t', ""\n\t\t Clustered ( the same relation ) ORGANIZATION A be offer to buy B NE tagger Context words A be offer to buy B Newspapers Tagged newspapers <ORG>A </ORG> Accumulated context <ORG> C </ORG> say it intend to buy agree to buy ' s purchase of plan to buy <ORG> B </ORG> <ORG> D </ORG> be offer to buy ' s propose acquisitions of ' s interest in be negotiate to acquire ' s plan purchase of level . \n\t"", '\n\t\t In addition , the set of types of named entities has been extended by several research groups . \n\t', '\n\t\t For example , Sekine proposed 150 types of named entities \n\t\t']",Positive
"['\n\t\t Extending the range of NE types would lead to more effective relation discovery . \n\t', '\n\t\t If the type ORGANIZATION could be divided into subtypes , COMPANY , MILITARY , GOVERNMENT and so on , the discovery procedure could detect more specific relations such as those between COMPANY and COMPANY . \n\t', '\n\t\t We use an extended named entity tagger \n\t\t']",Positive
"['\n\t\t 3.3 NE pairs and context We define the co-occurrence of NE pairs as follows : two named entities are considered to co-occur if they appear within the same sentence and are separated by at most N intervening words . \n\t', '\n\t\t We collect the intervening words between two named entities for each co-occurrence . \n\t', '\n\t\t These words , which are stemmed , could be regarded as the context of the pair of named entities . \n\t', '\n\t\t Different orders of occurrence of the named entities are also considered as different contexts . \n\t', '\n\t\t For example , and are collected as different contexts , where and represent named entities . \n\t', '\n\t\t Less frequent pairs of NEs should be eliminated because they might be less reliable in learning rela tions . \n\t', '\n\t\t So we have set a frequency threshold to remove those pairs . \n\t', '\n\t\t 3.4 Context similarity among NE pairs We adopt a vector space model and cosine similarity in order to calculate the similarities between the set of contexts of NE pairs . \n\t', '\n\t\t We only compare NE pairs which have the same NE types , e.g. , one PERSON \x96 GPE pair and another PERSON \x96 GPE pair . \n\t', '\n\t\t We define a domain as a pair of named entity types , e.g. , the PERSON-GPE domain . \n\t', '\n\t\t For example , we have to detect relations between PERSON and GPE in the PERSON-GPE domain . \n\t', '\n\t\t Before making context vectors , we eliminate stop words , words in parallel expressions , and expressions peculiar to particular source documents ( examples of these are given below ) , because these expressions would introduce noise in calculating similarities . \n\t', '\n\t\t A context vector for each NE pair consists of the bag of words formed from all intervening words from all co-occurrences of two named entities . \n\t', '\n\t\t Each word of a context vector is weighed by tf*idf , the product of term frequency and inverse document frequency . \n\t', '\n\t\t Term frequency is the number of occurrences of a word in the collected context words . \n\t', '\n\t\t The order of co-occurrence of the named entities is also considered . \n\t', '\n\t\t If a word occurred times in con- text and times in context , the term frequency of the word is defined as , where and are named entities . \n\t', '\n\t\t We think that this term frequency of a word in different orders would be effective to detect the direction of a relation if the arguments of a relation have the same NE types . \n\t', '\n\t\t Document frequency is the number of documents which include the word . \n\t', '\n\t\t If the norm of the context vector is ex- tremely small due to a lack of content words , the cosine similarity between the vector and others might be unreliable . \n\t', '\n\t\t So , we also define a norm threshold in advance to eliminate short context vectors . \n\t', '\n\t\t The cosine similarity between context vectors and is calculated by the following formula. Cosine similarity varies from to . \n\t', '\n\t\t A cosine similarity of would mean these NE pairs have exactly the same context words with the NEs appearing predominantly in the same order , and a cosine similarity of would mean these NE pairs have exactly the same context words with the NEs appearing predominantly in reverse order . \n\t', '\n\t\t 3.5 Clustering NE pairs After we calculate the similarity among context vectors of NE pairs , we make clusters of NE pairs based on the similarity . \n\t', '\n\t\t We do not know how many clusters we should make in advance , so we adopt hierarchical clustering . \n\t', '\n\t\t Many clustering methods were proposed for hierarchical clustering , but we adopt complete linkage because it is conservative in making clusters . \n\t', '\n\t\t The distance between clusters is taken to be the distance of the furthest nodes between clusters in complete linkage . \n\t', '\n\t\t 3.6 Labeling clusters If most of the NE pairs in the same cluster had words in common , the common words would represent the characterization of the cluster . \n\t', '\n\t\t In other words , we can regard the common words as the characterization of a particular relation . \n\t', '\n\t\t We simply count the frequency of the common words in all combinations of the NE pairs in the same cluster . \n\t', '\n\t\t The frequencies are normalized by the number of combinations . \n\t', '\n\t\t The frequent common words in a cluster would become the label of the cluster , i.e. they would become the label of the relation , if the cluster would consist of the NE pairs in the same relation . \n\t', '\n\t\t 4 Experiments We experimented with one year of The New York \n\t\t']",Positive
"['\n\t\t We determined three parameters for thresholds and identified the patterns for parallel expressions and expressions peculiar to The New York Times as ignorable context . \n\t', '\n\t\t We set the maximum context word length to 5 words and set the frequency threshold of co-occurring NE pairs to 30 empirically . \n\t', '\n\t\t We also used the patterns , \x93 , . \n\t', '\n\t\t *,\x94 , \x93and\x94 and \x93or\x94 for parallel expressions , and the pattern\x93 ) --\x94(used in datelines at the beginning of articles ) as peculiar to The New York Times . \n\t', '\n\t\t In our experiment , the norm threshold was set to 10 . \n\t', '\n\t\t We also used stop words when context vectors are made . \n\t', '\n\t\t The stop words include symbols and words which occurred under 3 times as infrequent words and those which occurred over 100,000 times as highly frequent words . \n\t', '\n\t\t We applied our proposed method to The New York Times 1995 , identified the NE pairs satisfying our criteria , and extracted the NE pairs along with their intervening words as our data set . \n\t', '\n\t\t In order to evaluate the relations detected automatically , we analyzed the data set manually and identified the relations for two different domains . \n\t', '\n\t\t One was the PERSON-GPE ( PER-GPE ) domain . \n\t', '\n\t\t We obtained 177 distinct NE pairs and classified them into 38 classes ( relations ) manually . \n\t', '\n\t\t The other was the COMPANY-COMPANY ( COM-COM ) domain . \n\t', '\n\t\t We got 65 distinct NE pairs and classified them into 10 classes manually . \n\t', '\n\t\t However , the types of both arguments of a relation are the same in the COM-COM domain . \n\t', '\n\t\t So the COM-COM domain includes symmetrical relations as well as asymmetrical relations . \n\t', '\n\t\t For the latter , we have to distinguish the different orders of arguments . \n\t', '\n\t\t We show the types of classes and the number in each class in Table 1 . \n\t', '\n\t\t The errors in NE tagging were eliminated to evaluate our method correctly . \n\t', '\n\t\t 5 Evaluation We evaluated separately the placement of the NE pairs into clusters and the assignment of labels to these clusters . \n\t', '\n\t\t In the first step , we evaluated clusters consisting of two or more pairs . \n\t', '\n\t\t For each cluster , we determined the relation ( R ) of the cluster as the most frequently represented relation ; we call this the major relation of the cluster . \n\t', '\n\t\t NE pairs with relation R in a cluster whose major relation was R were counted as correct ; the correct pair count , , is defined as the total number of correct pairs in all clusters . \n\t', '\n\t\t Other NE pairs in the cluster were counted as incorrect ; the incorrect pair count , , is also defined as the total number of incorrect pairs in all clusters . \n\t', '\n\t\t We evaluated clusters based on Recall , Precision and F-measure . \n\t', '\n\t\t We defined these mea- PER-GPE President Senator Governor Prime Minister Player Living # NE pairs 28 21 17 16 12 9 PER-GPE Republican Secretary Mayor Enemy Working others(2 and 3 ) # NE pairs 8 7 5 5 4 20 COM-COM M&A Rival Parent Alliance Joint Venture Trading # NE pairs 35 8 8 6 2 2 Coach 8 others(only 1 ) 17 others(only 1 ) 4 Table 1 : Manually classified relations which are extracted from Newspapers sures as follows . \n\t', '\n\t\t Recall ( R ) How many correct pairs are detected out of all the key pairs ? \n\t', '\n\t\t The key pair count , , is defined as the total number of pairs manually classified in clusters of two or more pairs . \n\t', '\n\t\t Recall is defined as follows : Precision ( P ) How many correct pairs are detected among the pairs clustered automatically ? \n\t', '\n\t\t Precision is defined as follows : 100 80 80 60 50 6050 40 40 0 0 s 20 20 10 0 10 0 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 Threshold of cosine similarity ecallure F-measure ( F ) F-measure is defined as a combination of recall and precision according to the following formula : These values vary depending on the threshold of cosine similarity . \n\t', '\n\t\t As the threshold is decreased , the clusters gradually merge , finally forming one big cluster . \n\t', '\n\t\t We show the results of complete linkage clustering for the PERSON-GPE ( PER-GPE ) domain in Figure 2 and for the COMPANY-COMPANY ( COM-COM ) domain in Figure 3 . \n\t', '\n\t\t With these metrics , precision fell as the threshold of cosine similarity was lowered . \n\t', '\n\t\t Recall increased until the threshold was almost 0 , at which point it fell because the total number of correct pairs in the remaining few big clusters decreased . \n\t', '\n\t\t The best F-measure was 82 in the PER-GPE domain , 77 in the COM-COM domain . \n\t', '\n\t\t In both domains , the best F-measure was found near 0 cosine similarity . \n\t', '\n\t\t Generally , it is difficult to determine the threshold of similarity in advance . \n\t', '\n\t\t Since the best threshold of cosine similarity was almost same in the two domains , we fixed the cosine threshold at a single value just above zero for both domains for simplicity . \n\t', '\n\t\t We also investigated each cluster with the threshold of cosine similarity just above 0 . \n\t', '\n\t\t We got 34 Figure 2 : F-measure , recall and precision by varying the threshold of cosine similarity in complete linkage clustering for the PERSON-GPE domain Precision 10090 90 80 90 80 70 60 70 70 60 J0 J0 40 40 30 20 30 20 10 10 0 0 -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 T res old ing the threshold of cosine similari ty in complete linkage clustering for the COMPANY-COMPANY domain Precision Recall F-measure PER-GPE 79 83 80 COM-COM 76 74 75 Figure 3 : F-measure , recall and precision by vary- Table 2 : F-measure , recall an d precision with the ilarity just above 0 100 Precision Recall F-measure Major relations Ratio Common words ( Relative frequency ) President 17/23 Senator 19/21 Prime Minister 15/16 Governor 15/16 Secretary 6/7 Republican 5/6 Coach 5/5 President ( 1.0 ) , president ( 0.415 ) , ... \n\t', '\n\t\t Sen. ( 1.0 ) , Republican ( 0.214 ) , Democrat ( 0.133 ) , republican ( 0.133 ) , ... \n\t', '\n\t\t Minister ( 1.0 ) , minister ( 0.875 ) , Prime ( 0.875 ) , prime ( 0.758 ) , ... \n\t', '\n\t\t Gov. ( 1.0 ) , governor ( 0.458 ) , Governor ( 0.3 ) , ... \n\t', '\n\t\t Secretary ( 1.0 ) , secretary ( 0. 143 ) , ... \n\t', '\n\t\t Rep. ( 1.0 ) , Republican ( 0.667 ) , ... coach ( 1.0 ) , ... \n\t', '\n\t\t M&A 10/11 M&A 9/9 Parent 7/7 Alliance 3/4 buy ( 1.0 ) , bid ( 0.3 82 ) , offer ( 0.273 ) , purchase ( 0.273 ) , ... acquire ( 1.0 ) , acquisition ( 0.583 ) , buy ( 0.583 ) , agree ( 0.417 ) , ... parent ( 1.0 ) , unit ( 0.476 ) , own ( 0. 143 ) , ... join ( 1.0 ) Table 3 : Major relations in clusters and the most frequent common words in each cluster PER-GPE clusters and 15 COM-COM clusters . \n\t', '\n\t\t We show the F-measure , recall and precision at this cosine threshold in both domains in Table 2 . \n\t', '\n\t\t We got 80 F-measure in the PER-GPE domain and 75 F- measure in the COM-COM domain . \n\t', '\n\t\t These values were very close to the best F-measure . \n\t', '\n\t\t Then , we evaluated the labeling of clusters of NE pairs . \n\t', '\n\t\t We show the larger clusters for each domain , along with the ratio of the number of pairs bearing the major relation to the total number of pairs in each cluster , on the left in Table 3. ( As noted above , the major relation is the most frequently represented relation in the cluster . \n\t', '\n\t\t ) We also show the most frequent common words and their relative frequency in each cluster on the right in Table 3 . \n\t', '\n\t\t If two NE pairs in a cluster share a particular context word , we consider these pairs to be linked ( with respect to this word ) . \n\t', '\n\t\t The relative frequency for a word is the number of such links , relative to the maxi- mal possible number of links ( for a cluster of pairs ) . \n\t', '\n\t\t If the relative frequency is , the word is shared by all NE pairs . \n\t', '\n\t\t Although we obtained some meaningful relations in small clusters , we have omitted the small clusters because the common words in such small clusters might be unreliable . \n\t', '\n\t\t We found that all large clusters had appropriate relations and that the common words which occurred frequently in those clusters accurately represented the relations . \n\t', '\n\t\t In other words , the frequent common words could be regarded as suitable labels for the relations . \n\t', '\n\t\t 6 Discussion The results of our experiments revealed good performance . \n\t', '\n\t\t The performance was a little higher in the PER-GPE domain than in the COM-COM domain , perhaps because there were more NE pairs with high cosine similarity in the PER-GPE domain than in the COM-COM domain . \n\t', '\n\t\t However , the graphs in both domains were similar , in particular when the cosine similarity was under 0.2 . \n\t', '\n\t\t We would like to discuss the differences between the two domains and the following aspects of our unsupervised method for discovering the relations : properties of relations appropriate context word length selecting best clustering method covering less frequent pairs We address each of these points in turn . \n\t', '\n\t\t 6.1 Properties of relations We found that the COM-COM domain was more difficult to judge than the PER-GPE domain due to the similarities of relations . \n\t', '\n\t\t For example , the pair of companies in M&A relation might also subsequently appear in the parent relation . \n\t', '\n\t\t Asymmetric properties caused additional difficulties in the COM-COM domain , because most relations have directions . \n\t', '\n\t\t We have to recognize the direction of relations , vs. , to distinguish , for example , \x93A is parent company of B\x94 and \x93B is parent company of A\x94 . \n\t', '\n\t\t In determining the similarities between the NE pairs A and B and the NE pairs C and D , we must calculate both the similarity with and the similarity with . \n\t', '\n\t\t Sometimes the wrong correspondence ends up being favored . \n\t', '\n\t\t This kind of error was observed in 2 out of the 15 clusters , due to the fact that words happened to be shared by NE pairs aligned in the wrong direction more than in right direction . \n\t', '\n\t\t 6.2 Context word length The main reason for undetected or mis-clustered NE pairs in both domains is the absence of common words in the pairs\x92 context which explicitly represent the particular relations . \n\t', '\n\t\t Mis-clustered NE pairs were clustered based on another common word which occurred by accident . \n\t', '\n\t\t If the maximum context length were longer than the limit of 5 words which we set in the experiments , we could detect additional common words , but the noise would also increase . \n\t', '\n\t\t In our experiments , we used only the words between the two NEs . \n\t', '\n\t\t Although the outer context words ( preceding the first NE or following the second NE ) may be helpful , extending the context in this way will have to be carefully evaluated . \n\t', '\n\t\t It is future work to determine the best context word length . \n\t', '\n\t\t 6.3 Clustering method We tried single linkage and average linkage as well as complete linkage for making clusters . \n\t', '\n\t\t Complete linkage was the best clustering method because it yielded the highest F-measure . \n\t', '\n\t\t Furthermore , for the other two clustering methods , the threshold of cosine similarity producing the best F-measure was different in the two domains . \n\t', '\n\t\t In contrast , for complete linkage the optimal threshold was almost the same in the two domains . \n\t', '\n\t\t The best threshold of cosine similarity in complete linkage was determined to be just above 0 ; when this threshold reaches 0 , the F-measure drops suddenly because the pairs need not share any words . \n\t', '\n\t\t A threshold just above 0 means that each combination of NE pairs in the same cluster shares at least one word in common \x97 and most of these common words were pertinent to the relations . \n\t', '\n\t\t We consider that this is relevant to context word length . \n\t', '\n\t\t We used a relatively small maximum context word length \x96 5 words \x96 making it less likely that noise words appear in common across different relations . \n\t', '\n\t\t The combination of complete linkage and small context word length proved useful for relation discovery . \n\t', '\n\t\t 6.4 Less frequent pairs As we set the frequency threshold of NE co- occurrence to 30 , we will miss the less frequent NE pairs . \n\t', '\n\t\t Some of those pairs might be in valuable relations . \n\t', '\n\t\t For the less frequent NE pairs , since the context varieties would be small and the norms of context vectors would be too short , it is difficult to reliably classify the relation based on those pairs . \n\t', '\n\t\t One way of addressing this defect would be through bootstrapping . \n\t', '\n\t\t The problem of bootstrapping is how to select initial seeds ; we could resolve this problem with our proposed method . \n\t', '\n\t\t NE pairs which have many context words in common in each cluster could be promising seeds . \n\t', '\n\t\t Once these seeds have been established , additional , lower-frequency NE pairs could be added to these clusters based on more relaxed keyword-overlap criteria . \n\t', '\n\t\t 7 Conclusion We proposed an unsupervised method for relation discovery from large corpora . \n\t', '\n\t\t The key idea was clustering of pairs of named entities according to the similarity of the context words intervening between the named entities . \n\t', '\n\t\t The experiments using one year\x92s newspapers revealed not only that the relations among named entities could be detected with high recall and precision , but also that appropriate labels could be automatically provided to the relations . \n\t', '\n\t\t In the future , we are planning to discover less frequent pairs of named entities by combining our method with bootstrapping as well as to improve our method by tuning parameters . \n\t', '\n\t\t 8 Acknowledgments This research was supported in part by the Defense Advanced Research Projects Agency as part of the Translingual Information Detection , Extraction and Summarization ( TIDES ) program , under Grant N66001-001-1-8917 from the Space and Naval Warfare Systems Center , San Diego , and by the National Science Foundation under Grant ITS- 00325657 . \n\t', '\n\t\t This paper does not necessarily reflect the position of the U.S. Government . \n\t', '\n\t\t We would like to thank Dr. Yoshihiko Hayashi at Nippon Telegraph and Telephone Corporation , currently at Osaka University , who gave one of us ( T.H. ) an opportunity to conduct this research . \n\t', '\n\t\t References Eugene Agichtein and Luis Gravano . \n\t', '\n\t\t 2000. Snowball : Extracting relations from large plain-text collections . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 5th ACM International Conference on Digital Libraries ( ACM DL\x9200 ) , pages 85\x9694 . \n\t', '\n\t\t Sergey Brin . \n\t', '\n\t\t 1998. Extracting patterns and relations from world wide web. . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of WebDB Workshop at 6th International Conference on Extending Database Technology ( WebDB\x9298 ) , pages 172\x96183 . \n\t', '\n\t\t Defense Advanced Research Projects Agency . \n\t', '\n\t\t 1995. Proceedings of the Sixth Message Understanding Conference ( MUC-6 ) . \n\t', '\n\t\t Morgan Kaufmann Publishers , Inc. . \n\t', '\n\t\t Dekang Lin and Patrick Pantel . \n\t', '\n\t\t 200 1 . \n\t', '\n\t\t Dirt - discov- ery of inference rules from text . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 7th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining ( KDD- 2001 ) , pages 323\x96328 . \n\t', '\n\t\t National Institute of Standards and Technology . \n\t', '\n\t\t 2000. Automatic Content Extraction . \n\t', '\n\t\t http://www.nist.gov/speech/tests/ace/index.htm . \n\t', '\n\t\t Deepak Ravichandran and Eduard Hovy . \n\t', '\n\t\t 2002. Learning surface text patterns for a question answering system . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 40th Annual Meeting of the Association for Computational Linguistics ( ACL-2002 ) , pages 41\x9647 . \n\t', '\n\t\t Satoshi Sekine , Kiyoshi Sudo , and Chikashi Nobata . \n\t', '\n\t\t 2002. Extended named entity hierarchy . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the Third International Conference on Language Resources and Evaluation ( LREC2002 ) , pages 1818\x961824 . \n\t', '\n\t\t Satoshi Sekine . \n\t', '\n\t\t 2001. OAK System ( English Sentence Analyzer ) . \n\t', '\n\t\t http://nlp.cs.nyu.edu/oak/ . \n\t', '\n\t\t Dmitry Zelenko , Chinatsu Aone , and Anthony Richardella. 2002 . \n\t', '\n\t\t Kernel methods for relation extraction . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the Conference on Empirical Methods in Natural Language Processing ( EMNLP-2002 ) , pages 71\x9678 . \n\t', '\n\t\t Dependency Tree Kernels for Relation Extraction Aron Culotta University of Massachusetts Amherst , MA 01002 USA culotta@cs.umass.edu Jeffrey Sorensen IBM T.J. Watson Research Center Yorktown Heights , NY 10598 USA sorenj@us.ibm.com Abstract We extend previous work on tree kernels to estimate the similarity between the dependency trees of sentences . \n\t', '\n\t\t Using this kernel within a Support Vector Machine , we detect and classify relations between entities in the Automatic Content Extraction ( ACE ) corpus of news articles . \n\t', '\n\t\t We examine the utility of different features such as Wordnet hypernyms , parts of speech , and entity types , and find that the dependency tree kernel achieves a 20 % F1 improvement over a \x93bag-of-words\x94 kernel . \n\t', '\n\t\t 1 Introduction The ability to detect complex patterns in data is limited by the complexity of the data\x92s representation . \n\t', '\n\t\t In the case of text , a more structured data source ( e.g. a relational database ) allows richer queries than does an unstructured data source ( e.g. a collection of news articles ) . \n\t', '\n\t\t For example , current web search engines would not perform well on the query , \x93list all California-based CEOs who have social ties with a United States Senator.\x94 Only a structured representation of the data can effectively provide such a list . \n\t', '\n\t\t The goal of Information Extraction ( IE ) is to discover relevant segments of information in a data stream that will be useful for structuring the data . \n\t', '\n\t\t In the case of text , this usually amounts to finding mentions of interesting entities and the relations that join them , transforming a large corpus of unstructured text into a relational database with entries such as those in Table 1 . \n\t', '\n\t\t IE is commonly viewed as a three stage process : first , an entity tagger detects all mentions of interest ; second , coreference resolution resolves disparate mentions of the same entity ; third , a relation extractor finds relations between these entities . \n\t', '\n\t\t Entity tagging has been thoroughly addressed by many statistical machine learning techniques , obtaining greater than 90 % F1 on many datasets \n\t\t']",Positive
"['\n\t\t Coreference resolution is an active area of research not investigated here ( Pa- Entity Type Location Apple Organization Organization Cupertino , CA Redmond , WA Microsoft Table 1 : An example of extracted fields sula et al. , 2002 ; McCallum and Wellner , 2003 ) . \n\t', '\n\t\t We describe a relation extraction technique based on kernel methods . \n\t', '\n\t\t Kernel methods are non- parametric density estimation techniques that compute a kernel function between data instances , where a kernel function can be thought of as a similarity measure . \n\t', '\n\t\t Given a set of labeled instances , kernel methods determine the label of a novel instance by comparing it to the labeled training instances using this kernel function . \n\t', '\n\t\t Nearest neighbor classification and support-vector machines ( SVMs ) are two popular examples of kernel methods \n\t\t']",Positive
"['\n\t\t An advantage of kernel methods is that they can search a feature space much larger than could be represented by a feature extraction-based approach . \n\t', '\n\t\t This is possible because the kernel function can explore an implicit feature space when calculating the similarity between two instances , as described in the Section 3 . \n\t', '\n\t\t Working in such a large feature space can lead to over-fitting in many machine learning algorithms . \n\t', '\n\t\t To address this problem , we apply SVMs to the task of relation extraction . \n\t', '\n\t\t SVMs find a boundary between instances of different classes such that the distance between the boundary and the nearest instances is maximized . \n\t', '\n\t\t This characteristic , in addition to empirical validation , indicates that SVMs are particularly robust to over-fitting . \n\t', '\n\t\t Here we are interested in detecting and classifying instances of relations , where a relation is some meaningful connection between two entities ( Table 2 ) . \n\t', '\n\t\t We represent each relation instance as an augmented dependency tree . \n\t', '\n\t\t A dependency tree represents the grammatical dependencies in a sentence ; we augment this tree with features for each node AT NEAR PART ROLE SOCIAL Based-In Located Residence Relative-location Part-of Affiliate , Founder Associate , Grandparent Subsidiary Citizen-of , Management Parent , Sibling Other Client , Member Spouse , Other-professional Owner , Other , Staff Other-relative , Other-personal Table 2 : Relation types and subtypes . \n\t', '\n\t\t ( e.g. part of speech ) We choose this representation because we hypothesize that instances containing similar relations will share similar substructures in their dependency trees . \n\t', '\n\t\t The task of the kernel function is to find these similarities . \n\t', '\n\t\t We define a tree kernel over dependency trees and incorporate this kernel within an SVM to extract relations from newswire documents . \n\t', '\n\t\t The tree kernel approach consistently outperforms the bag-ofwords kernel , suggesting that this highly-structured representation of sentences is more informative for detecting and distinguishing relations . \n\t', '\n\t\t 2 Related Work Kernel methods \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
['\n\t\t String kernels for text classification are explored in \n\t\t'],Positive
['\n\t\t Our algorithm is similar to that described by \n\t\t'],Positive
"['\n\t\t Our contributions are a richer sentence representation , a more general framework to allow feature weighting , as well as the use of composite kernels to reduce kernel sparsity . \n\t', '\n\t\t \n\t\t']",Negative
['\n\t\t \n\t\t'],Positive
['\n\t\t Whereas \n\t\t'],Positive
"['\n\t\t Also , \n\t\t']",Positive
"['\n\t\t We experiment with a more challenging set of relation types and a larger corpus . \n\t', '\n\t\t 3 Kernel Methods In traditional machine learning , we are provided a set of training instances S = { x1 ... xN } , where each instance xZ is represented by some d- dimensional feature vector . \n\t', '\n\t\t Much time is spent on the task of feature engineering \x96 searching for the optimal feature set either manually by consulting domain experts or automatically through feature induction and selection \n\t\t']",Positive
"['\n\t\t For example , in entity detection the original instance representation is generally a word vector corresponding to a sentence . \n\t', '\n\t\t Feature extraction and induction may result in features such as part-ofspeech , word n-grams , character n-grams , capitalization , and conjunctions of these features . \n\t', '\n\t\t In the case of more structured objects , such as parse trees , features may include some description of the object\x92s structure , such as \x93has an NP-VP subtree.\x94 Kernel methods can be particularly effective at reducing the feature engineering burden for structured objects . \n\t', '\n\t\t By calculating the similarity between two objects , kernel methods can employ dynamic programming solutions to efficiently enumerate over substructures that would be too costly to explicitly include as features . \n\t', '\n\t\t Formally , a kernel function K is a mapping K : X x X ^ [ 0 , oc ] from instance space X to a similarity score K(x , y ) = PZ OZ(x)OZ(y) = O(x) · O(y) . \n\t', '\n\t\t Here , OZ(x) is some feature function over the instance x . \n\t', '\n\t\t The kernel function must be symmetric [ K(x , y ) = K(y , x ) ] and positivesemidefinite . \n\t', '\n\t\t By positive-semidefinite , we require that the if x 1 , ... , xn E X , then the n x n matrix G defined by GZj = K(xZ , xj ) is positive semi- definite . \n\t', '\n\t\t It has been shown that any function that takes the dot product of feature vectors is a kernel function \n\t\t']",Positive
"['\n\t\t A simple kernel function takes the dot product of the vector representation of instances being compared . \n\t', '\n\t\t For example , in document classification , each document can be represented by a binary vector , where each element corresponds to the presence or absence of a particular word in that document . \n\t', '\n\t\t Here , OZ(x) = 1 if word i occurs in document x . \n\t', '\n\t\t Thus , the kernel function K(x , y ) returns the num- ber of words in common between x and y . \n\t', '\n\t\t We refer to this kernel as the \x93bag-of-words\x94 kernel , since it ignores word order . \n\t', '\n\t\t When instances are more structured , as in the case of dependency trees , more complex kernels become necessary . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t As an example , consider a kernel over strings . \n\t', '\n\t\t To determine the similarity between two strings , string kernels \n\t\t']",Positive
"['\n\t\t Thus , Oi ( x ) is the number of times string x contains the subsequence referenced by i . \n\t', '\n\t\t These matches can be found efficiently through a dynamic program , allowing string kernels to examine long-range features that would be computationally infeasible in a feature-based method . \n\t', '\n\t\t Given a training set S = { xs ... xN } , kernel methods compute the Gram matrix G such that Gib = K(xi,xb) . \n\t', '\n\t\t Given G , the classifier finds a hyperplane which separates instances of different classes . \n\t', '\n\t\t To classify an unseen instance x , the classifier first projects x into the feature space defined by the kernel function . \n\t', '\n\t\t Classification then consists of determining on which side of the separating hyper- plane x lies . \n\t', '\n\t\t A support vector machine ( SVM ) is a type of classifier that formulates the task of finding the separating hyperplane as the solution to a quadratic programming problem \n\t\t']",Positive
"['\n\t\t Support vector machines attempt to find a hyperplane that not only separates the classes but also maximizes the margin between them . \n\t', '\n\t\t The hope is that this will lead to better generalization performance on unseen instances . \n\t', '\n\t\t 4 Augmented Dependency Trees Our task is to detect and classify relations between entities in text . \n\t', '\n\t\t We assume that entity tagging has been performed ; so to generate potential relation instances , we iterate over all pairs of entities occurring in the same sentence . \n\t', '\n\t\t For each entity pair , we create an augmented dependency tree ( described below ) representing this instance . \n\t', '\n\t\t Given a labeled training set of potential relations , we define a tree kernel over dependency trees which we then use in an SVM to classify test instances . \n\t', '\n\t\t A dependency tree is a representation that denotes grammatical relations between words in a sentence ( Figure 1 ) . \n\t', '\n\t\t A set of rules maps a parse tree to a dependency tree . \n\t', '\n\t\t For example , subjects are dependent on their verbs and adjectives are dependent Figure 1 : A dependency tree for the sentence Troops advanced near Tikrit . \n\t', '\n\t\t Feature Example word troops , Tikrit part-of-speech ( 24 values ) NN , NNP general-pos ( 5 values ) noun , verb , adj chunk-tag NP , VP , ADJP entity-type person , geo-political-entity entity-level name , nominal , pronoun Wordnet hypernyms social group , city relation-argument ARG A , ARG B Table 3 : List of features assigned to each node in the dependency tree . \n\t', '\n\t\t on the nouns they modify . \n\t', '\n\t\t Note that for the purposes of this paper , we do not consider the link labels ( e.g. \x93object\x94 , \x93subject\x94 ) ; instead we use only the dependency structure . \n\t', '\n\t\t To generate the parse tree of each sentence , we use MXPOST , a maximum entropy statistical parser1 ; we then convert this parse tree to a dependency tree . \n\t', '\n\t\t Note that the left-to-right ordering of the sentence is maintained in the dependency tree only among siblings ( i.e. the dependency tree does not specify an order to traverse the tree to recover the original sentence ) . \n\t', '\n\t\t For each pair of entities in a sentence , we find the smallest common subtree in the dependency tree that includes both entities . \n\t', '\n\t\t We choose to use this subtree instead of the entire tree to reduce noise and emphasize the local characteristics of relations . \n\t', '\n\t\t We then augment each node of the tree with a feature vector ( Table 3 ) . \n\t', '\n\t\t The relation-argument feature specifies whether an entity is the first or second argument in a relation . \n\t', '\n\t\t This is required to learn asymmetric relations ( e.g. X OWNS Y ) . \n\t', '\n\t\t Formally , a relation instance is a dependency tree 1http://www.cis.upenn.edu/\x98adwait/statnlp.html Troops t 1 t2 advanced t0 near Tikrit t3 T with nodes It0 ... tn } . \n\t', '\n\t\t The features of node ti are given by 0(ti) = Iv1 ... vd } . \n\t', '\n\t\t We refer to the jth child of node ti as ti[j] , and we denote the set of all children of node ti as ti[c] . \n\t', '\n\t\t We reference a subset j of children of ti by ti [ j ] C_ ti [ c ] . \n\t', '\n\t\t Finally , we refer to the parent of node ti as ti.p . \n\t', '\n\t\t From the example in Figure 1 , t0 [ 1 ] = t2 , t0[I0,1}]= It1 , t2 } , and t1.p = t0 . \n\t', '\n\t\t 5 Tree kernels for dependency trees We now define a kernel function for dependency trees . \n\t', '\n\t\t The tree kernel is a function K(T1 , T2 ) that returns a normalized , symmetric similarity score in the range ( 0 , 1 ) for two trees T1 and T2 . \n\t', '\n\t\t We define a slightly more general version of the kernel described by \n\t\t']",Positive
"['\n\t\t We first define two functions over the features of tree nodes : a matching function m(ti , tj ) E I0 , 1 } and a similarity function s(ti , tj ) E ( 0 , oc ] . \n\t', '\n\t\t Let the feature vector 0(ti) = Iv1 ... vd } consist of two possibly overlapping subsets 0m(ti) C_ 0(ti) and 03(ti) C_ 0(ti) . \n\t', '\n\t\t We use 0m(ti) in the matching function and 03(ti) in the similarity function . \n\t', '\n\t\t We define ~ m(ti , tj 1 if 0m ( ti ) = 0m(tj) ) = 0 otherwise and s(ti , tj ) = X X C(vq , vr ) vy ^Os ( ti ) vr^Os(tj) where C(vq , vr ) is some compatibility function between two feature values . \n\t', '\n\t\t For example , in the simplest case where ~ C(vq , vr ) = 1 if vq = vr 0 otherwise s(ti , tj ) returns the number of feature values in common between feature vectors 03 ( ti ) and 03 ( tj ) . \n\t', '\n\t\t We can think of the distinction between functions m(ti , tj ) and s(ti , tj ) as a way to discretize the similarity between two nodes . \n\t', '\n\t\t If 0m(ti) =~ 0m(tj) , then we declare the two nodes completely dissimilar . \n\t', '\n\t\t However , if 0m(ti) = 0m(tj) , then we proceed to compute the similarity s(ti , tj ) . \n\t', '\n\t\t Thus , restricting nodes by m(ti , tj ) is a way to prune the search space of matching subtrees , as shown below . \n\t', '\n\t\t For two dependency trees T1 , T2 , with root nodes r1 and r2 , we define the tree kernel K(T1 , T2 ) as 0 if m(r1 , r2 ) = 0 s(r1 , r2)+ Kc(r1 [ c ] , r2 [ c ] ) otherwise where Kc is a kernel function over children . \n\t', '\n\t\t Let a and b be sequences of indices such that a is a sequence a1 < a2 < ... < an , and likewise for b . \n\t', '\n\t\t Let d(a) = an ^ a1 + 1 and l(a) be the length of a . \n\t', '\n\t\t Then we have Kc ( ti [ c ] , tj [ c ] ) = X Ad(a)Ad(b)K ( ti [ a ] , tj [ b ] ) a,b,l(a)=l(b) The constant 0 < A < 1 is a decay factor that penalizes matching subsequences that are spread out within the child sequences . \n\t', '\n\t\t See \n\t\t']",Positive
"['\n\t\t Intuitively , whenever we find a pair of matching nodes , we search for all matching subsequences of the children of each node . \n\t', '\n\t\t A matching subsequence of children is a sequence of children a and b such that m(ai , bi ) = 1 ( bi < n ) . \n\t', '\n\t\t For each matching pair of nodes ( ai , bi ) in a matching subsequence , we accumulate the result of the similarity function s(ai , bj ) and then recursively search for matching subsequences of their children ai [ c ] , bj [ c ] . \n\t', '\n\t\t We implement two types of tree kernels . \n\t', '\n\t\t A contiguous kernel only matches children subsequences that are uninterrupted by non-matching nodes . \n\t', '\n\t\t Therefore , d(a) = l(a) . \n\t', '\n\t\t A sparse tree kernel , by contrast , allows non-matching nodes within matching subsequences . \n\t', '\n\t\t Figure 2 shows two relation instances , where each node contains the original text plus the features used for the matching function , 0m ( ti ) = Igeneralpos , entity-type , relation-argument } . \n\t', '\n\t\t ( \x93NA\x94 denotes the feature is not present for this node . \n\t', '\n\t\t ) The contiguous kernel matches the following substructures : It0 [ 0 ] , u0 [ 0 ] } , It0 [ 2 ] , u0 [ 1 ] } , It3 [ 0 ] , u2 [ 0 ] } . \n\t', '\n\t\t Because the sparse kernel allows non-contiguous matching sequences , it matches an additional substructure It0 [ 0 , * , 2 ] , u0 [ 0 , *,1 ] } , where ( * ) indicates an arbitrary number of non-matching nodes . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t 6 Experiments We extract relations from the Automatic Content Extraction ( ACE ) corpus provided by the National Institute for Standards and Technology ( NIST ) . \n\t', '\n\t\t The follows : K(T1,T2) = ^ ^^ ^^ Figure 2 : Two instances of the NEAR relation . \n\t', '\n\t\t data consists of about 800 annotated text documents gathered from various newspapers and broadcasts . \n\t', '\n\t\t Five entities have been annotated ( PERSON , ORGANIZATION , GEO-POLITICAL ENTITY , LOCATION , FACILITY ) , along with 24 types of relations ( Table 2 ) . \n\t', '\n\t\t As noted from the distribution of relationship types in the training data ( Figure 3 ) , data imbalance and sparsity are potential problems . \n\t', '\n\t\t In addition to the contiguous and sparse tree kernels , we also implement a bag-of-words kernel , which treats the tree as a vector of features over nodes , disregarding any structural information . \n\t', '\n\t\t We also create composite kernels by combining the sparse and contiguous kernels with the bagof-words kernel . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t We find that this composite kernel improves performance when the Gram matrix G is sparse ( i.e. our instances are far apart in the kernel space ) . \n\t', '\n\t\t The features used to represent each node are shown in Table 3 . \n\t', '\n\t\t After initial experimentation , the set of features we use in the matching func- tion is ^m(ti) = { general-pos , entity-type , relation- argument } , and the similarity function examines the Figure 3 : Distribution over relation types in training data . \n\t', '\n\t\t remaining features . \n\t', '\n\t\t In our experiments we tested the following five kernels : K0 = K1 = K2 = K3 = K4 = We also experimented with the function C(vQ , vr ) , the compatibility function between two feature values . \n\t', '\n\t\t For example , we can increase the importance of two nodes having the same Wordnet hypernym2 . \n\t', '\n\t\t If vQ , vr are hypernym features , then we can define ~ a if vQ = vr C(vQ , vr ) = 0 otherwise When a > 1 , we increase the similarity of nodes that share a hypernym . \n\t', '\n\t\t We tested a number of weighting schemes , but did not obtain a set of weights that produced consistent significant improvements . \n\t', '\n\t\t See Section 8 for alternate approaches to setting C. 2http://www.cogsci.princeton.edu/\x98wn/ ARG _A person forces noun ARG A person troops noun t 1 u 1 NA NA moved verb quickly adverb NA NA NA NA advanced verb t t2 t3 0 u 0 Baghdad noun geopolitical ARG B geopolitical ARG B toward prep NA NA Tikrit noun near prep NA NA t 4 u 2 u 3 sparse kernel contiguous kernel bag-of-words kernel K0 + K2 K1 + K2 Avg . \n\t', '\n\t\t Prec . \n\t', '\n\t\t Avg . \n\t', '\n\t\t Rec . \n\t', '\n\t\t Avg . \n\t', '\n\t\t F 1 K1 69.6 25.3 36.8 K2 47.0 10.0 14.2 K3 68.9 24.3 35.5 K4 70.3 26.3 38.0 Table 4 : Kernel performance comparison . \n\t', '\n\t\t Table 4 shows the results of each kernel within an SVM . \n\t', '\n\t\t ( We augment the LibSVM3 implementation to include our dependency tree kernel . \n\t', '\n\t\t ) Note that , although training was done over all 24 relation subtypes , we evaluate only over the 5 high-level relation types . \n\t', '\n\t\t Thus , classifying a RESIDENCE relation as a LOCATED relation is deemed correct4 . \n\t', '\n\t\t Note also that K0 is not included in Table 4 because of burdensome computational time . \n\t', '\n\t\t Table 4 shows that precision is adequate , but recall is low . \n\t', '\n\t\t This is a result of the aforementioned class imbalance \x96 very few of the training examples are relations , so the classifier is less likely to identify a testing instances as a relation . \n\t', '\n\t\t Because we treat every pair of mentions in a sentence as a possible relation , our training set contains fewer than 15 % positive relation instances . \n\t', '\n\t\t To remedy this , we retrain each SVMs for a binary classification task . \n\t', '\n\t\t Here , we detect , but do not classify , relations . \n\t', '\n\t\t This allows us to combine all positive relation instances into one class , which provides us more training samples to estimate the class boundary . \n\t', '\n\t\t We then threshold our output to achieve an optimal operating point . \n\t', '\n\t\t As seen in Table 5 , this method of relation detection outperforms that of the multi-class classifier . \n\t', '\n\t\t We then use these binary classifiers in a cascading scheme as follows : First , we use the binary SVM to detect possible relations . \n\t', '\n\t\t Then , we use the SVM trained only on positive relation instances to classify each predicted relation . \n\t', '\n\t\t These results are shown in Table 6 . \n\t', '\n\t\t The first result of interest is that the sparse tree kernel , K0 , does not perform as well as the contiguous tree kernel , K1 . \n\t', '\n\t\t Suspecting that noise was introduced by the non-matching nodes allowed in the sparse tree kernel , we performed the experiment with different values for the decay factor A = { .9,.5 ,. 1 } , but obtained no improvement . \n\t', '\n\t\t The second result of interest is that all tree kernels outperform the bag-of-words kernel , K2 , most noticeably in recall performance , implying that the 3http://www.csie.ntu.edu.tw/\x98cj lin/libsvm/ 4This is to compensate for the small amount of training data for many classes . \n\t', '\n\t\t Prec . \n\t', '\n\t\t Rec . \n\t', '\n\t\t F 1 K0 \x96 \x96 \x96 K0 ( B ) 83.4 45.5 58.8 K1 91.4 37.1 52.8 K1 ( B ) 84.7 49.3 62.3 K2 92.7 10.6 19.0 K2 ( B ) 72.5 40.2 51.7 K3 91.3 35.1 50.8 K3 ( B ) 80.1 49.9 61.5 K4 91.8 37.5 53.3 K4 ( B ) 81.2 51.8 63.2 Table 5 : Relation detection performance . \n\t', '\n\t\t ( B ) denotes binary classification . \n\t', '\n\t\t D C Avg . \n\t', '\n\t\t Prec . \n\t', '\n\t\t Avg . \n\t', '\n\t\t Rec . \n\t', '\n\t\t Avg . \n\t', '\n\t\t F1 K0 K0 66.0 29.0 40.1 K1 K1 66.6 32.4 43.5 K2 K2 62.5 27.7 38.1 K3 K3 67.5 34.3 45.3 K4 K4 67.1 35.0 45.8 K1 K4 67.4 33.9 45.0 K4 K1 65.3 32.5 43.3 Table 6 : Results on the cascading classification . \n\t', '\n\t\t D and C denote the kernel used for relation detection and classification , respectively . \n\t', '\n\t\t structural information the tree kernel provides is extremely useful for relation detection . \n\t', '\n\t\t Note that the average results reported here are representative of the performance per relation , except for the NEAR relation , which had slightly lower results overall due to its infrequency in training . \n\t', '\n\t\t 7 Conclusions We have shown that using a dependency tree kernel for relation extraction provides a vast improvement over a bag-of-words kernel . \n\t', '\n\t\t While the dependency tree kernel appears to perform well at the task of classifying relations , recall is still relatively low . \n\t', '\n\t\t Detecting relations is a difficult task for a kernel method because the set of all non-relation instances is extremely heterogeneous , and is therefore difficult to characterize with a similarity metric . \n\t', '\n\t\t An improved system might use a different method to detect candidate relations and then use this kernel method to classify the relations . \n\t', '\n\t\t 8 Future Work The most immediate extension is to automatically learn the feature compatibility function C(vq , vr ) . \n\t', '\n\t\t A first approach might use tf-idf to weight each feature . \n\t', '\n\t\t Another approach might be to calculate the information gain for each feature and use that as its weight . \n\t', '\n\t\t A more complex system might learn a weight for each pair of features ; however this seems computationally infeasible for large numbers of features . \n\t', '\n\t\t One could also perform latent semantic indexing to collapse feature values into similar \x93categories\x94 \x97 for example , the words \x93football\x94 and \x93baseball\x94 might fall into the same category . \n\t', '\n\t\t Here , C(vQ , vr ) might return a1 if vQ = vr , and a2 if vQ and vr are in the same category , where a1 > a2 > 0 . \n\t', '\n\t\t Any method that provides a \x93soft\x94 match between feature values will sharpen the granularity of the kernel and enhance its modeling power . \n\t', '\n\t\t Further investigation is also needed to understand why the sparse kernel performs worse than the contiguous kernel . \n\t', '\n\t\t These results contradict those given in \n\t\t']",Positive
"['\n\t\t It is worthwhile to characterize relation types that are better captured by the sparse kernel , and to determine when using the sparse kernel is worth the increased computational burden . \n\t', '\n\t\t References Eugene Agichtein and Luis Gravano . \n\t', '\n\t\t 2000. Snowball : Extracting relations from large plain-text collections . \n\t', '\n\t\t In Proceedings of the Fifth ACMInternational Conference on Digital Libraries . \n\t', '\n\t\t Sergey Brin . \n\t', '\n\t\t 1998. Extracting patterns and relations from the world wide web. . \n\t', '\n\t\t In WebDB Workshop at 6th International Conference on Extending Database Technology , EDBT\x9298 . \n\t', '\n\t\t M. Collins and N. Duffy . \n\t', '\n\t\t 2002. Convolution kernels for natural language . \n\t', '\n\t\t In T. G. Dietterich , S. Becker , and Z. Ghahramani , editors , Advances in Neural Information Processing Systems 14 , Cambridge , MA . \n\t', '\n\t\t MIT Press . \n\t', '\n\t\t Corinna Cortes and Vladimir Vapnik . \n\t', '\n\t\t 1995. Support-vector networks . \n\t', '\n\t\t Machine Learning , 20(3):273\x96297 . \n\t', '\n\t\t N. Cristianini and J. Shawe-Taylor . \n\t', '\n\t\t 2000. An introduction to support vector machines . \n\t', '\n\t\t Cambridge University Press . \n\t', '\n\t\t Chad M. Cumby and Dan Roth . \n\t', '\n\t\t 2003. On kernel methods for relational learning . \n\t', '\n\t\t In Tom Fawcett and Nina Mishra , editors , Machine Learning , Proceedings of the Twentieth International Conference ( ICML 2003 ) , August 21-24 , 2003 , Washington , DC , USA . \n\t', '\n\t\t AAAI Press . \n\t', '\n\t\t K. Fukunaga . \n\t', '\n\t\t 1990. Introduction to Statistical Pat- tern Recognition . \n\t', '\n\t\t Academic Press , second edition . \n\t', '\n\t\t D. Haussler . \n\t', '\n\t\t 1999. Convolution kernels on discrete structures . \n\t', '\n\t\t Technical Report UCS-CRL-99- 10 , University of California , Santa Cruz . \n\t', '\n\t\t Thorsten Joachims , Nello Cristianini , and John Shawe-Taylor . \n\t', '\n\t\t 2001. Composite kernels for hypertext categorisation . \n\t', '\n\t\t In Carla Brodley and Andrea Danyluk , editors , Proceedings of ICML01 , 18th International Conference on Machine Learning , pages 250\x96257 , Williams College , US . \n\t', '\n\t\t Morgan Kaufmann Publishers , San Francisco , US . \n\t', '\n\t\t Huma Lodhi , John Shawe-Taylor , Nello Cristianini , and Christopher J. C. H. Watkins . \n\t', '\n\t\t 2000. Text classification using string kernels . \n\t', '\n\t\t In NIPS , pages 563\x96569 . \n\t', '\n\t\t A. McCallum and B. Wellner . \n\t', '\n\t\t 2003. Toward conditional models of identity uncertainty with application to proper noun coreference . \n\t', '\n\t\t In IJCAI Workshop on Information Integration on the Web. S. Miller , H. Fox , L. Ramshaw , and R. Weischedel . \n\t', '\n\t\t 2000. A novel use of statistical parsing to extract information from text . \n\t', '\n\t\t In 6th Applied Natural Language Processing Conference . \n\t', '\n\t\t H. Pasula , B. Marthi , B. Milch , S. Russell , and I. Shpitser . \n\t', '\n\t\t 2002. Identity uncertainty and citation matching . \n\t', '\n\t\t Dan Roth and Wen-tau Yih . \n\t', '\n\t\t 2002. Probabilistic reasoning for entity and relation recognition . \n\t', '\n\t\t In 19th International Conference on Computational Linguistics . \n\t', '\n\t\t Sam Scott and Stan Matwin . \n\t', '\n\t\t 1999. Feature engineering for text classification . \n\t', '\n\t\t In Proceedings of ICML-99 , 16th International Conference on Machine Learning . \n\t', '\n\t\t Erik F. Tjong Kim Sang and Fien De Meulder . \n\t', '\n\t\t 2003. Introduction to the CoNLL-2003 shared task : Language-independent named entity recognition . \n\t', '\n\t\t In Walter Daelemans and Miles Osborne , editors , Proceedings of CoNLL-2003 , pages 142\x96 147 . \n\t', '\n\t\t Edmonton , Canada . \n\t', '\n\t\t Vladimir Vapnik . \n\t', '\n\t\t 1998. Statistical Learning Theory . \n\t', '\n\t\t Whiley , Chichester , GB . \n\t', '\n\t\t D. Zelenko , C. Aone , and A. Richardella. 2003 . \n\t', '\n\t\t Kernel methods for relation extraction . \n\t', '\n\t\t Journal ofMachine Learning Research , pages 1083\x96 1106. \n\t', '\n\t\t Classifying Semantic Relations in Bioscience Texts Barbara Rosario SIMS UC Berkeley Berkeley , CA 94720 rosario@sims.berkeley.edu Marti A. Hearst SIMS UC Berkeley Berkeley , CA 94720 hearst@sims.berkeley.edu Abstract A crucial step toward the goal of automatic extraction of propositional information from natural language text is the identification of semantic relations between constituents in sentences . \n\t', '\n\t\t We examine the problem of distinguishing among seven relation types that can occur between the entities \x93treatment\x94 and \x93disease\x94 in bioscience text , and the problem of identifying such entities . \n\t', '\n\t\t We compare five generative graphical models and a neural network , using lexical , syntactic , and semantic features , finding that the latter help achieve high classification accuracy . \n\t', '\n\t\t 1 Introduction The biosciences literature is rich , complex and continually growing . \n\t', '\n\t\t The National Library of Medicine\x92s MEDLINE database1 contains bibliographic citations and abstracts from more than 4,600 biomedical journals , and an estimated half a million new articles are added every year . \n\t', '\n\t\t Much of the important , late-breaking bioscience information is found only in textual form , and so methods are needed to automatically extract semantic entities and the relations between them from this text . \n\t', '\n\t\t For example , in the following sentences , hepatitis and its variants , which are DISEASES , are found in different semantic relationships with various TREATMENTs : 1http://www.nlm.nih.gov/pubs/factsheets/medline.html ( 1 ) Effect of interferon on hepatitis B ( 2 ) A two-dose combined hepatitis A and B vac- cine wouldfacilitate immunization programs ( 3 ) These results suggest that con A-induced hep- atitis was ameliorated by pretreatment with TJ-135 . \n\t', '\n\t\t In ( 1 ) there is an unspecified effect of the treatment interferon on hepatitis B . \n\t', '\n\t\t In ( 2 ) the vaccine prevents hepatitis A and B while in ( 3 ) hepatitis is cured by the treatment TJ-135 . \n\t', '\n\t\t We refer to this problem as Relation Classification . \n\t', '\n\t\t A related task is Role Extraction ( also called , in the literature , \x93information extraction\x94 or \x93named entity recognition\x94 ) , defined as : given a sentence such as \x93The fluoroquinolones for urinary tract infections : a review\x94 , extract all and only the strings of text that correspond to the roles TREATMENT ( fluoroquinolones ) and DISEASE ( urinary tract infections ) . \n\t', '\n\t\t To make inferences about the facts in the text we need a system that accomplishes both these tasks : the extraction of the semantic roles and the recognition of the relationship that holds between them . \n\t', '\n\t\t In this paper we compare five generative graphical models and a discriminative model ( a multi- layer neural network ) on these tasks . \n\t', '\n\t\t Recognizing subtle differences among relations is a difficult task ; nevertheless the results achieved by our models are quite promising : when the roles are not given , the neural network achieves 79.6 % accuracy and the best graphical model achieves 74.9 % . \n\t', '\n\t\t When the roles are given , the neural net reaches 96.9 % accuracy while the best graphical model gets 91.6 % accuracy . \n\t', '\n\t\t Part of the reason for the Relationship Definition and Example Cure TREAT cures DIS 810 ( 648 , 162 ) Intravenous immune globulin for recurrent spontaneous abortion Only DIS TREAT not mentioned 616 ( 492 , 124 ) Social ties and susceptibility to the common cold Only TREAT DIS not mentioned 166 ( 132 , 34 ) Flucticasone propionate is safe in recommended doses Prevent TREAT prevents the DIS Statinsforprevention ofstroke 63 ( 50 , 13 ) Vague Very unclear relationship Phenylbutazone and leukemia 36(28,8) Side Effect DIS is a result of a TREAT Malignant mesodermal mixed tumor ofthe uterus following irradiation 29(24,5) NO Cure TREAT does not cure DIS Evidence for double resistance to permethrin and malathion in head lice 4(3,1) Total relevant : 1724 ( 1377 , 347 ) Irrelevant TREAT and DIS not present 1771 ( 1416 , 355 ) Patients were followed up for 6 months Total : 3495 ( 2793 , 702 ) Table 1 : Candidate semantic relationships between treatments and diseases . \n\t', '\n\t\t In parentheses are shown the numbers of sentences used for training and testing , respectively . \n\t', '\n\t\t success of the algorithms is the use of a large domain-specific lexical hierarchy for generalization across classes of nouns . \n\t', '\n\t\t In the remainder of this paper we discuss related work , describe the annotated dataset , describe the models , present and discuss the results of running the models on the relation classification and entity extraction tasks and analyze the relative importance of the features used . \n\t', '\n\t\t 2 Related work While there is much work on role extraction , very little work has been done for relationship recognition . \n\t', '\n\t\t Moreover , many papers that claim to be doing relationship recognition in reality address the task of role extraction : ( usually two ) entities are extracted and the relationship is implied by the co- occurrence of these entities or by the presence of some linguistic expression . \n\t', '\n\t\t These linguistic patterns could in principle distinguish between differ- ent relations , but instead are usually used to identify examples of one relation . \n\t', '\n\t\t In the related work for statistical models there has been , to the best of our knowledge , no attempt to distinguish between different relations that can occur between the same semantic entities . \n\t', '\n\t\t In \n\t\t']",Negative
"['\n\t\t Their technique generates and evaluates lexical patterns that are indicative of the relation . \n\t', '\n\t\t Only the relation location of is tackled and the entities are assumed given . \n\t', '\n\t\t In \n\t\t']",Positive
"['\n\t\t The classification ( done with Support Vector Machine and Voted Perceptron algorithms ) is between positive and negative sentences , where the positive sentences contain the two entities . \n\t', '\n\t\t In the bioscience NLP literature there are also efforts to extract entities and relations . \n\t', '\n\t\t In \n\t\t']",Negative
"['\n\t\t The authors acknowledge that the task of extracting relations is different from the task of extracting entities . \n\t', '\n\t\t Nevertheless , they consider positive examples to be all the sentences that simply contain the entities , rather than analyzing which relations hold between these entities . \n\t', '\n\t\t In \n\t\t']",Negative
"['\n\t\t The authors treat it as a text classification problem and propose and compare two classifiers : a Naive Bayes classifier and a relational learning algorithm . \n\t', '\n\t\t This is a two-way classification , and again there is no mention of whether the co-occurrence of the entities actually represents the target relation . \n\t', '\n\t\t \n\t\t']",Negative
"['\n\t\t Their experiments use sentences that contain verbal and nominal forms of the stem inhibit . \n\t', '\n\t\t Thus the actual task performed is the extraction of entities that are connected by some form of the stem in- hibit , which by requiring occurrence of this word explicitly , is not the same as finding all sentences that talk about inhibiting actions . \n\t', '\n\t\t Similarly , \n\t\t']",Positive
['\n\t\t In \n\t\t'],Positive
"['\n\t\t In the bioscience domain the work on relation classification is primary done through hand-built rules . \n\t', '\n\t\t \n\t\t']",Positive
['\n\t\t The GENIES system \n\t\t'],Positive
"['\n\t\t 3 Data and Features For our experiments , the text was obtained from MEDLINE 20012 . \n\t', '\n\t\t An annotator with biology expertise considered the titles and abstracts separately and labeled the sentences ( both roles and relations ) based solely on the content of the individual sentences . \n\t', '\n\t\t Seven possible types of relationships between TREATMENT and DISEASE were identified . \n\t', '\n\t\t Table 1 shows , for each relation , its definition , one example sentence and the number of sentences found containing it . \n\t', '\n\t\t We used a large domain-specific lexical hierarchy ( MeSH , Medical Subject Headings3 ) to map words into semantic categories . \n\t', '\n\t\t There are about 19,000 unique terms in MeSH and 15 main sub-hierarchies , each corresponding to a major branch of medical ontology ; e.g. , tree A corresponds to Anatomy , tree C to Disease , and so on . \n\t', '\n\t\t As an example , the word migraine maps to the term C 10.228 , that is , C ( a disease ) , C10 ( Nervous System Diseases ) , C10.228 ( Central Ner- 2We used the first 100 titles and the first 40 abstracts from each of the 59 files medline01n*.xml in Medline 2001 ; the labeled data is available at biotext.berkeley.edu 3http://www.nlm.nih . \n\t', '\n\t\t gov/mesh/meshhome.html vous System Diseases ) . \n\t', '\n\t\t When there are multiple MeSH terms for one word , we simply choose the first one . \n\t', '\n\t\t These semantic features are shown to be very useful for our tasks ( see Section 4.3 ) . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t The results reported in this paper were obtained with the following features : the word itself , its part of speech from the Brill tagger \n\t\t']",Positive
"['\n\t\t In addition , we identified the sub-hierarchies of MeSH that tend to correspond to treatments and diseases , and convert these into a tri-valued attribute indicating one of : disease , treatment or neither . \n\t', '\n\t\t Finally , we included orthographic features such as \x91is the word a number\x92 , \x91only part of the word is a number\x92 , \x91first letter is capitalized\x92 , \x91all letters are capitalized\x92 . \n\t', '\n\t\t In Section 4.3 we analyze the impact of these features . \n\t', '\n\t\t 4 Models and Results This section describes the models and their performance on both entity extraction and relation classification . \n\t', '\n\t\t Generative models learn the prior probability of the class and the probability of the features given the class ; they are the natural choice in cases with hidden variables ( partially observed or missing data ) . \n\t', '\n\t\t Since labeled data is expensive to collect , these models may be useful when no labels are available . \n\t', '\n\t\t However , in this paper we test the generative models on fully observed data and show that , although not as accurate as the discriminative model , their performance is promising enough to encourage their use for the case of partially observed data . \n\t', '\n\t\t Discriminative models learn the probability of the class given the features . \n\t', '\n\t\t When we have fully observed data and we just need to learn the mapping from features to classes ( classification ) , a discriminative approach may be more appropriate , as shown in \n\t\t']",Negative
"['\n\t\t For the evaluation of the role extraction task , we calculate the usual metrics of precision , recall and F-measure . \n\t', '\n\t\t Precision is a measure of how many of the roles extracted by the system are correct and recall is the measure of how many of the true roles were extracted by the system . \n\t', '\n\t\t The F-measure is a weighted combination of precision and recall4 . \n\t', '\n\t\t Our role evaluation is very strict : every token is assessed and we do not assign partial credit for constituents for which only some of the words are correctly labeled . \n\t', '\n\t\t We report results for two cases : ( i ) considering only the relevant sentences and ( ii ) including also irrelevant sentences . \n\t', '\n\t\t For the relation classification task , we report results in terms of classification accuracy , choosing one out of seven choices for ( i ) and one out of eight choices for ( ii ) . \n\t', '\n\t\t ( Most papers report the results for only the relevant sentences , while some papers assign credit to their algorithms if their system extracts only one instance of a given relation from the collection . \n\t', '\n\t\t By contrast , in our experiments we expect the system to extract all instances of every relation type . \n\t', '\n\t\t ) For both tasks , 75 % of the data were used for training and the rest for testing . \n\t', '\n\t\t 4.1 Generative Models In Figure 1 we show two static and three dynamic models . \n\t', '\n\t\t The nodes labeled \x93Role\x94 represent the entities ( in this case the choices are DISEASE , TREATMENT and NULL ) and the node labeled \x93Relation\x94 represents the relationship present in the sentence . \n\t', '\n\t\t We assume here that there is a single relation for each sentence between the entities5 . \n\t', '\n\t\t The children of the role nodes are the words and their features , thus there are as many role states as there are words in the sentence ; for the static models , this is depicted by the box ( or \x93plate\x94 ) which is the standard graphical model notation for replication . \n\t', '\n\t\t For each state , the features are those mentioned in Section 3 . \n\t', '\n\t\t The simpler static models S1 and S2 do not assume an ordering in the role sequence . \n\t', '\n\t\t The dynamic models were inspired by prior work on HMM-like graphical models for role extraction \n\t\t']",Positive
"['\n\t\t These models consist of a 4In this paper , precision and recall are given equal weight , that is , F-measure = . \n\t', '\n\t\t 5We found 75 sentences which contain more than one relationship , often with multiple entities or the same entities taking part in several interconnected relationships ; we did not include these in the study . \n\t', '\n\t\t dynamic model ( D1 ) dynamic model ( D2 ) dynamic model ( D3 ) Figure 1 : Models for role and relation extraction . \n\t', '\n\t\t Markov sequence of states ( usually corresponding to semantic roles ) where each state generates one or multiple observations . \n\t', '\n\t\t Model D1 in Figure 1 is typical of these models , but we have augmented it with the Relation node . \n\t', '\n\t\t The task is to recover the sequence of Role states , given the observed features . \n\t', '\n\t\t These models assume that there is an ordering in the semantic roles that can be captured with the Markov assumption and that the role generates the observations ( the words , for example ) . \n\t', '\n\t\t All our models make the additional assumption that there is a relation that generates the role sequence ; thus , these static model ( S1 ) static model ( S2 ) f1 a f2 . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t ^ fna f1h RolO f2 . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t ^ f/n f ROlati on RolO f2 ...^ fn RolO f1 f2 . \n\t', '\n\t\t . \n\t', '\n\t\t .^ fn ROlati RolO on f1 f2 .. . \n\t', '\n\t\t ^ fn ROlati on RolO ^ T f1 f2 . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t ^ fn f1~ RolO f2 . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t ^ fn f1~ ROlati on RolO f2...^fn RolO f1 f2 . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t ^ fn f1~ RolO f2 . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t ^ fn f1~ ROlati on RolO f2...^fn RolO Sentences Static Dynamic S1 S2 D1 D2 D3 No Smoothing Only rel . \n\t', '\n\t\t 0.67 0.68 0.71 0.52 0.55 Rel . \n\t', '\n\t\t + irrel . \n\t', '\n\t\t 0.61 0.62 0.66 0.35 0.37 Absolute discounting Only rel . \n\t', '\n\t\t 0.67 0.68 0.72 0.73 0.73 Rel . \n\t', '\n\t\t + irrel . \n\t', '\n\t\t 0.60 0.62 0.67 0.71 0.69 Table 2 : F-measures for the models of Figure 1 for role extraction . \n\t', '\n\t\t models have the appealing property that they can simultaneously perform role extraction and relationship recognition , given the sequence of observations . \n\t', '\n\t\t In S 1 and D 1 the observations are independent from the relation ( given the roles ) . \n\t', '\n\t\t In S2 and D2 , the observations are dependent on both the relation and the role ( or in other words , the relation generates not only the sequence of roles but also the observations ) . \n\t', '\n\t\t D2 encodes the fact that even when the roles are given , the observations depend on the relation . \n\t', '\n\t\t For example , sentences containing the word prevent are more likely to represent a \x93prevent\x94 kind of relationship . \n\t', '\n\t\t Finally , in D3 only one observation per state is dependent on both the relation and the role , the motivation being that some observations ( such as the words ) depend on the relation while others might not ( like for example , the parts of speech ) . \n\t', '\n\t\t In the experiments reported here , the observations which have edges from both the role and the relation nodes are the words . \n\t', '\n\t\t ( We ran an experiment in which this observation node was the MeSH term , obtaining similar results . \n\t', '\n\t\t ) Model D1 defines the following joint probabil- ity distribution over relations , roles , words and word features , assuming the leftmost Role node is , and is the number of words in the sen- tence : Model D1 is similar to the model in \n\t\t']",Positive
"['\n\t\t Structurally , the differences are ( i ) \n\t\t']",Negative
"['\n\t\t The joint probability distributions for D2 and D3 are similar to Equation ( 1 ) where we substitute the term with for D2 and for D3 . \n\t', '\n\t\t The parameters and of Equation ( 1 ) are constrained to be equal . \n\t', '\n\t\t The parameters were estimated using maximum likelihood on the training set ; we also implemented a simple absolute discounting smoothing method \n\t\t']",Positive
"['\n\t\t Table 2 shows the results ( F-measures ) for the problem of finding the most likely sequence of roles given the features observed . \n\t', '\n\t\t In this case , the relation is hidden and we marginalize over it6 . \n\t', '\n\t\t We experimented with different values for the smoothing factor ranging from a minimum of 0.0000005 to a maximum of 10 ; the results shown fix the smoothing factor at its minimum value . \n\t', '\n\t\t We found that for the dynamic models , for a wide range of smoothing factors , we achieved almost identical results ; nevertheless , in future work , we plan to implement cross-validation to find the optimal smoothing factor . \n\t', '\n\t\t By contrast , the static models were more sensitive to the value of the smoothing factor . \n\t', '\n\t\t Using maximum likelihood with no smoothing , model D1 performs better than D2 and D3 . \n\t', '\n\t\t This was expected , since the parameters for models D2 and D3 are more sparse than D1 . \n\t', '\n\t\t However , when smoothing is applied , the three dynamic models achieve similar results . \n\t', '\n\t\t Although the additional edges in models D2 and D3 did not help much for the task of role extraction , they did help for relation classification , discussed next . \n\t', '\n\t\t Model D2 6To perform inference for the dynamic model , we used the junction tree algorithm . \n\t', '\n\t\t We used Kevin Murphy\x92s BNT package , found at http://www.ai.mit.edu/ murphyk/Bayes/bnintro.html . \n\t', '\n\t\t ( 1 ) achieves the best F-measures : 0.73 for \x93only relevant\x94 and 0.71 for \x93rel . \n\t', '\n\t\t + irrel.\x94 . \n\t', '\n\t\t It is difficult to compare results with the related work since the data , the semantic roles and the evaluation are different ; in \n\t\t']",Negative
"['\n\t\t They report approximately an F-measure of 32 % for the extraction of the entities PROTEINS and LOCATIONS , and an F-measure of 50 % for GENE and DISORDER . \n\t', '\n\t\t The second target task is to find the most likely relation , i.e. , to classify a sentence into one of the possible relations . \n\t', '\n\t\t Two types of experiments were conducted . \n\t', '\n\t\t In the first , the true roles are hidden and we classify the relations given only the observable features , marginalizing over the hidden roles . \n\t', '\n\t\t In the second , the roles are given and only the relations need to be inferred . \n\t', '\n\t\t Table 3 reports the results for both conditions , both with absolute discounting smoothing and without . \n\t', '\n\t\t Again model D1 outperforms the other dynamic models when no smoothing is applied ; with smoothing and when the true roles are hidden , D2 achieves the best classification accuracies . \n\t', '\n\t\t When the roles are given D1 is the best model ; D1 does well in the cases when both roles are not present . \n\t', '\n\t\t By contrast , D2 does better than D1 when the presence of specific words strongly determines the outcome ( e.g. , the presence \x93prevention\x94 or \x93prevent\x94 helps identify the Prevent relation ) . \n\t', '\n\t\t The percentage improvements of D2 and D3 versus D1 are , respectively , 10 % and 6.5 % for relation classification and 1.4 % for role extraction ( in the \x93only relevant\x94 , \x93only features\x94 case ) . \n\t', '\n\t\t This suggests that there is a dependency between the observations and the relation that is captured by the additional edges in D2 and D3 , but that this dependency is more helpful in relation classification than in role extraction . \n\t', '\n\t\t For relation classification the static models perform worse than for role extraction ; the decreases in performance from D1 to S 1 and from D2 to S2 are , respectively ( in the \x93only relevant\x94 , \x93only features\x94 case ) , 7.4 % and 7.3 % for role extraction and 27.1 % and 44 % for relation classification . \n\t', '\n\t\t This suggests the importance of modeling the sequence of roles for relation classification . \n\t', '\n\t\t To provide an idea of where the errors occur , Table 4 shows the confusion matrix for model D2 for the most realistic and difficult case of \x93rel + irrel.\x94 , \x93only features\x94 . \n\t', '\n\t\t This indicates that the algorithm performs poorly primarily for the cases for which there is little training data , with the exception of the ONLY DISEASE case , which is often mistaken for CURE . \n\t', '\n\t\t 4.2 Neural Network To compare the results of the generative models of the previous section with a discriminative method , we use a neural network , using the Matlab package to train a feed-forward network with conjugate gradient descent . \n\t', '\n\t\t The features are the same as those used for the models in Section 4 . \n\t', '\n\t\t 1 , but are represented with indicator variables . \n\t', '\n\t\t That is , for each feature we calculated the number of possible values and then represented an observation of the feature as a sequence of binary values in which one value is set to and the remaining values are set to . \n\t', '\n\t\t The input layer of the NN is the concatenation of this representation for all features . \n\t', '\n\t\t The network has one hidden layer , with a hyperbolic tangent function . \n\t', '\n\t\t The output layer uses a logistic sigmoid function . \n\t', '\n\t\t The number of units of the output layer is fixed to be the number of relations ( seven or eight ) for the relation classification task and the number of roles ( three ) for the role extraction task . \n\t', '\n\t\t The network was trained for several choices of numbers of hidden units ; we chose the best- performing networks based on training set error . \n\t', '\n\t\t We then tested these networks on held-out testing data . \n\t', '\n\t\t The results for the neural network are reported in Table 3 in the column labeled NN. . \n\t', '\n\t\t These results are quite strong , achieving 79.6 % accuracy in the relation classification task when the entities are hidden and 96.9 % when the entities are given , outperforming the graphical models . \n\t', '\n\t\t Two possible reasons for this are : as already mentioned , the discriminative approach may be the most appropriate for fully labeled data ; or the graphical models we proposed may not be the right ones , i.e. , the independence assumptions they make may misrepresent underlying dependencies . \n\t', '\n\t\t It must be pointed out that the neural network Sentences Input B Static Dynamic NN S1 S2 D1 D2 D3 No Smoothing Only rel . \n\t', '\n\t\t only feat . \n\t', '\n\t\t 46.7 51.9 50.4 65.4 58.2 61.4 79.8 roles given 51.3 52.9 66.6 43.8 49.3 92.5 Rel . \n\t', '\n\t\t + irrel . \n\t', '\n\t\t only feat . \n\t', '\n\t\t 50.6 51.2 50.2 68.9 58.7 61.4 79.6 roles given 55.7 54.4 82.3 55.2 58.8 96.6 Absolute discounting Only rel . \n\t', '\n\t\t only feat . \n\t', '\n\t\t 46.7 51.9 50.4 66.0 72.6 70.3 roles given 51.9 53.6 83.0 76.6 76.6 Rel . \n\t', '\n\t\t + irrel . \n\t', '\n\t\t only feat . \n\t', '\n\t\t 50.6 51.1 50.2 68.9 74.9 74.6 roles given 56.1 54.8 91.6 82.0 82.3 Table 3 : Accuracies of relationship classification for the models in Figure 1 and for the neural network ( NN ) . \n\t', '\n\t\t For absolute discounting , the smoothing factor was fixed at the minimum value . \n\t', '\n\t\t B is the baseline of always choosing the most frequent relation . \n\t', '\n\t\t The best results are indicated in boldface . \n\t', '\n\t\t is much slower than the graphical models , and requires a great deal of memory ; we were not able to run the neural network package on our machines for the role extraction task , when the feature vectors are very large . \n\t', '\n\t\t The graphical models can perform both tasks simultaneously ; the percentage decrease in relation classification of model D2 with respect to the NN is of 8.9 % for \x93only relevant\x94 and 5.8 % for \x93relevant + irrelevant\x94 . \n\t', '\n\t\t 4.3 Features In order to analyze the relative importance of the different features , we performed both tasks using the dynamic model D1 of Figure 1 , leaving out single features and sets of features ( grouping all of the features related to the MeSH hierarchy , meaning both the classification of words into MeSH IDs and the domain knowledge as defined in Section 3 ) . \n\t', '\n\t\t The results reported here were found with maximum likelihood ( no smoothing ) and are for the \x93relevant only\x94 case ; results for \x93relevant + irrelevant\x94 were similar . \n\t', '\n\t\t For the role extraction task , the most important feature was the word : not using it , the GM achieved only 0.65 F-measure ( a decrease of 9.7 % from 0.72 F-measure using all the features ) . \n\t', '\n\t\t Leaving out the features related to MeSH the F- measure obtained was 0.69 % ( a 4.1 % decrease ) and the next most important feature was the partof-speech ( 0.70 F-measure not using this feature ) . \n\t', '\n\t\t For all the other features , the F-measure ranged between 0.71 and 0.73 . \n\t', '\n\t\t For the task of relation classification , the MeSH-based features seem to be the most important . \n\t', '\n\t\t Leaving out the word again lead to the biggest decrease in the classification accuracy for a single feature but not so dramatically as in the role extraction task ( 62.2 % accuracy , for a decrease of 4 % from the original value ) , but leaving out all the MeSH features caused the accuracy to decrease the most ( a decrease of 13.2 % for 56.2 % accuracy ) . \n\t', '\n\t\t For both tasks , the impact of the domain knowledge alone was negligible . \n\t', '\n\t\t As described in Section 3 , words can be mapped to different levels of the MeSH hierarchy . \n\t', '\n\t\t Currently , we use the \x93second\x94 level , so that , for example , surgery is mapped to G02.403 ( when the whole MeSH ID is G02.403.810.762 ) . \n\t', '\n\t\t This is somewhat arbitrary ( and mainly chosen with the sparsity issue in mind ) , but in light of the importance of the MeSH features it may be worthwhile investigating the issue of finding the optimal level of description . \n\t', '\n\t\t ( This can be seen as another form of smoothing . \n\t', '\n\t\t ) 5 Conclusions We have addressed the problem of distinguishing between several different relations that can hold between two semantic entities , a difficult and important task in natural language understanding . \n\t', '\n\t\t We have presented five graphical models and a neural network for the tasks of semantic relation classification and role extraction from bioscience text . \n\t', '\n\t\t The methods proposed yield quite promising results . \n\t', '\n\t\t We also discussed the strengths and weaknesses of the discriminative and generative Prediction Num . \n\t', '\n\t\t Sent . \n\t', '\n\t\t ( Train , Test ) Relation accuracy Truth Vague OD NC Cure Prev. OT SE Irr . \n\t', '\n\t\t Vague 0 3 0 4 0 0 0 1 28,8 0 Only DIS ( OD ) 2 69 0 27 1 1 0 24 492,124 55.6 No Cure ( NC ) 0 0 0 1 0 0 0 0 3,1 0 Cure 2 5 0 150 1 1 0 3 648,162 92.6 Prevent 0 1 0 2 5 0 0 5 50,13 38.5 Only TREAT ( OT ) 0 0 0 16 0 6 1 11 132,34 17.6 Side effect ( SE ) 0 0 0 3 1 0 0 1 24,5 20 Irrelevant 1 32 1 16 2 7 0 296 1416,355 83.4 Table 4 : Confusion matrix for the dynamic model D2 for \x93rel + irrel.\x94 , \x93only features\x94 . \n\t', '\n\t\t In column \x93Num . \n\t', '\n\t\t Sent.\x94 the numbers of sentences used for training and testing and in the last column the classification accuracies for each relation . \n\t', '\n\t\t The total accuracy for this case is 74.9 % . \n\t', '\n\t\t approaches and the use of a lexical hierarchy . \n\t', '\n\t\t Because there is no existing gold-standard for this problem , we have developed the relation definitions of Table 1 ; this however may not be an exhaustive list . \n\t', '\n\t\t In the future we plan to assess additional relation types . \n\t', '\n\t\t It is unclear at this time if this approach will work on other types of text ; the technical nature of bioscience text may lend itself well to this type of analysis . \n\t', '\n\t\t Acknowledgements We thank Kaichi Sung for her work on the relation labeling and Chris Manning for helpful suggestions . \n\t', '\n\t\t This research was supported by a grant from the ARDA AQUAINT program , NSF DBI-0317510 , and a gift from Genentech . \n\t', '\n\t\t References E. Agichtein and L. Gravano . \n\t', '\n\t\t 2000. Snowball : Extracting relations from large plain-text collections . \n\t', '\n\t\t Proceedings ofDL \x9200 . \n\t', '\n\t\t D. Bikel , R. Schwartz , and R. Weischedel . \n\t', '\n\t\t 1999. An algorithm that learns what\x92s in a name . \n\t', '\n\t\t Machine Learning , 34(1-3):211\x9623 1 . \n\t', '\n\t\t E. Brill . \n\t', '\n\t\t 1995. Transformation-based error-driven learning and natural language processing : A case study in part-of-speech tagging . \n\t', '\n\t\t Computational Lin- guistics , 21(4):543\x96565 . \n\t', '\n\t\t M. Collins . \n\t', '\n\t\t 1996. A new statistical parser based on bigram lexical dependencies . \n\t', '\n\t\t Proc . \n\t', '\n\t\t ofACL \x9296 . \n\t', '\n\t\t M. Craven . \n\t', '\n\t\t 1999. Learning to extract relations from Medline . \n\t', '\n\t\t AAAI-99 Workshop on Machine Learning for Information Extraction . \n\t', '\n\t\t R. Feldman , Y. Regev , M. Finkelstein-Landau , E. Hurvitz , and B. Kogan . \n\t', '\n\t\t 2002. Mining biomed- ical literature using information extraction . \n\t', '\n\t\t Current Drug Discovery , Oct. . \n\t', '\n\t\t D. Freitag and A. McCallum . \n\t', '\n\t\t 2000. Information extraction with HMM structures learned by stochastic optimization . \n\t', '\n\t\t AAAI/IAAI , pages 584\x96589 . \n\t', '\n\t\t C. Friedman , P. Kra , H. Yu , M. Krauthammer , and A. Rzhetzky . \n\t', '\n\t\t 2001. Genies : a natural-language processing system for the extraction of molecular pathways from journal articles . \n\t', '\n\t\t Bioinformatics , 17(1) . \n\t', '\n\t\t A. Ng and M. Jordan . \n\t', '\n\t\t 2002. On discriminative vs. generative classifiers : A comparison of logistic regression and Naive Bayes . \n\t', '\n\t\t NIPS 14 . \n\t', '\n\t\t J. Pustejovsky , J. Castano , and J. Zhang . \n\t', '\n\t\t 2002 . \n\t', '\n\t\t Robust relational parsing over biomedical literature : Extracting inhibit relations . \n\t', '\n\t\t PSB 2002 . \n\t', '\n\t\t S. Ray and M. Craven . \n\t', '\n\t\t 2001. Representing sentence structure in Hidden Markov Models for information extraction . \n\t', '\n\t\t Proceedings ofIJCAI-2001 . \n\t', '\n\t\t T. Rindflesch , L. Hunter , and L. Aronson . \n\t', '\n\t\t 1999. Mining molecular binding terminology from biomedical text . \n\t', '\n\t\t Proceedings of the AMIA Symposium . \n\t', '\n\t\t B. Rosario , M. Hearst , and C. Fillmore . \n\t', '\n\t\t 2002. The descent of hierarchy , and selection in relational semantics . \n\t', '\n\t\t Proceedings ofACL-02 . \n\t', '\n\t\t P. Srinivasan and T. Rindflesch . \n\t', '\n\t\t 2002. Exploring text mining from Medline . \n\t', '\n\t\t Proceedings of the AMIA Symposium . \n\t', '\n\t\t C. Thompson , R. Levy , and C. Manning . \n\t', '\n\t\t 2003. A generative model for semantic role labeling . \n\t', '\n\t\t Proceedings ofEMCL \x9203 . \n\t', '\n\t\t D. Zelenko , C. Aone , and A. Richardella. 2002 . \n\t', '\n\t\t Kernel methods for relation extraction . \n\t', '\n\t\t Proceedings of EMNLP 2002 . \n\t', '\n\t\t C. Zhai and J. Lafferty . \n\t', '\n\t\t 2001. A study of smoothing methods for language models applied to ad hoc information retrieval . \n\t', '\n\t\t In Proceedings of SIGIR \x9201 . \n\t', '\n\t\t Collective Information Extraction with Relational Markov Networks Razvan Bunescu Department of Computer Sciences University of Texas at Austin 1 University Station U0500 Austin , TX 78712 razvan@cs.utexas.edu Raymond J. Mooney Department of Computer Sciences University of Texas at Austin 1 University Station U0500 Austin , TX 78712 mooney@cs.utexas.edu Abstract Most information extraction ( IE ) systems treat separate potential extractions as independent . \n\t', '\n\t\t However , in many cases , considering influences between different potential extractions could improve overall accuracy . \n\t', '\n\t\t Statistical methods based on undirected graphical models , such as conditional random fields ( CRFs ) , have been shown to be an effective approach to learning accurate IE systems . \n\t', '\n\t\t We present a new IE method that employs Relational Markov Networks ( a generalization of CRFs ) , which can represent arbitrary dependencies between extractions . \n\t', '\n\t\t This allows for "" collective information extraction "" that exploits the mutual influence between possible extractions . \n\t', '\n\t\t Experiments on learning to extract protein names from biomedical text demonstrate the advantages of this approach . \n\t', '\n\t\t 1 Introduction Information extraction ( IE ) , locating references to specific types of items in natural-language documents , is an important task with many practical applications . \n\t', '\n\t\t Since IE systems are difficult and time-consuming to construct , most recent research has focused on empirical techniques that automatically construct information extractors by training on supervised corpora \n\t\t']",Positive
"[""\n\t\t One of the current best empirical approaches to IE is conditional random fields ( CRF 's ) \n\t\t""]",Positive
"[""\n\t\t CRF 's are a restricted class of undirected graphical models \n\t\t""]",Positive
"['\n\t\t In a recent follow-up to previously published experiments comparing a large variety of IE-learning methods ( including HMM , SVM , MaxEnt , and rule-based methods ) on the task of tagging references to human proteins in Medline abstracts \n\t\t']",Positive
"[""\n\t\t As typically applied , CRF 's , like almost all IE methods , assume separate extractions are independent and treat each potential extraction in isolation . \n\t"", '\n\t\t However , in many cases , considering influences between extractions can be very useful . \n\t', '\n\t\t For example , in our protein-tagging task , repeated references to the same protein are common . \n\t', '\n\t\t If the context surrounding one occurrence of a phrase is very indicative of it being a protein , then this should also influence the tagging of another occurrence of the same phrase in a different context which is not indicative of protein references . \n\t', ""\n\t\t Relational Markov Networks ( RMN 's ) \n\t\t""]",Positive
['\n\t\t Results on classifying connected sets of web pages have verified the advantage of this approach \n\t\t'],Positive
"[""\n\t\t In this paper , we present an approach to collective information extraction using RMN 's that simultaneously extracts all of the information from a document by exploiting the textual content and context of each relevant substring as well as the document relationships between them . \n\t"", '\n\t\t Experiments on human protein tagging demonstrate the advantages of collective extraction on several annotated corpora of Medline abstracts . \n\t', '\n\t\t 2 The RMN Framework for Entity Recognition Given a collection of documents D , we associate with each document d E D a set of candidate entities d.E , in our case a restricted set of token sequences from the document . \n\t', '\n\t\t Each entity e E d.E is characterized by a predefined set of boolean features e.F. . \n\t', '\n\t\t This set of features is the same for all candidate entities , and it can be assimilated with the relational database definition of a table . \n\t', '\n\t\t One particular feature is e.label which is set to 1 if e is considered a valid extraction , and 0 otherwise . \n\t', '\n\t\t In this document model , labels are the only hidden features , and the inference procedure will try to find a most probable assignment of values to labels , given the current model parameters . \n\t', '\n\t\t Each document is associated with an undirected graphical model , with nodes corresponding directly to entity features , one node for each feature of each candidate entity in the docu- ment . \n\t', '\n\t\t The set of edges is created by matching clique templates against the entire set of entities d.E. . \n\t', '\n\t\t A clique template is a procedure that finds all subsets of entities satisfying a given constraint , after which , for each entity subset , it connects a selected set of feature nodes so that they form a clique . \n\t', '\n\t\t Formally , there is a set of clique templates C , with each template c E C specified by : 1 . \n\t', '\n\t\t A matching operator M , for selecting subsets of entities . \n\t', '\n\t\t 2. A selected set of features S , _ ~~,~ Y , ) for entities returned by the matching operator . \n\t', '\n\t\t X , denotes the observed features , while Y , refers to the hidden labels . \n\t', '\n\t\t 3. A clique potential 0 , that gives the com- patibility of each possible configuration of values for the features in S , , s.t. 0,(s) > 0 , Vs E S , . \n\t', '\n\t\t Given a set , E , of nodes , M,(E) C_ 2E con- sists of subsets of entities whose feature nodes S , are to be connected in a clique . \n\t', '\n\t\t In previous applications of RMNs , the selected subsets of entities for a given template have the same size ; however , our clique templates may match a variable number of entities . \n\t', '\n\t\t The set S , may contain the same feature from different entities . \n\t', '\n\t\t Usually , for each entity in the matching set , its label is included in S , . \n\t', '\n\t\t All these will be illustrated with examples in Sections 4 and 5 where the clique templates used in our model are de- scribed in detail . \n\t', '\n\t\t Depending on the number of hidden labels in Y , we define two categories of clique templates : \x95 Local Templates are all templates c E C for which JY,I = ~ 1 . \n\t', ""\n\t\t They model the correlations between an entity 's observed features and its label . \n\t"", '\n\t\t \x95 Global Templates are all templates c E C for which IY , > 1 . \n\t', '\n\t\t They capture influences between multiple entities from the same document . \n\t', '\n\t\t After the graph model for a document d has been completed with cliques from all templates , the probability distribution over the random field of hidden entity labels d.Y given the ob- served features d.X is computed as : P(d.Yld.X) = ~(~~.~) H H Oc(G.X,7,G.Y,7) cGC GGM,(d.E) ( 1 ) where Z(d.X) is the normalizing partition func- tion : Z(d.X) _ Oc ( G.X , , G.Y , ) ( 2 ) Y CGCGEM,(d.E) The above distribution presents the RMN as a Markov random field ( MRF ) with the clique templates as a method for tying potential values across different cliques in the graphical model . \n\t', '\n\t\t 3 Candidate Entities and Entity Features Like most entity names , almost all proteins in our data are base noun phrases or parts of them . \n\t', '\n\t\t Therefore , such substrings are used to determine candidate entities . \n\t', '\n\t\t To avoid missing op- tions , we adopt a very broad definition of base noun phrase . \n\t', '\n\t\t Definition 1 : A base noun phrase is a maximal contiguous sequence of tokens whose POS tags are from { "" JJ "" , "" VBN "" , "" VBG "" , "" POS "" , "" NN "" , "" NNS "" , "" NNP "" , "" NNPS "" , "" CD "" , ""-""J , and whose last word ( the head ) is tagged either as a noun , or a number . \n\t', '\n\t\t Candidate extractions consist of base NPs , augmented with all their contiguous subsequences headed by a noun or number . \n\t', '\n\t\t The set of features associated with each candidate is based on the feature templates introduced in \n\t\t']",Positive
"['\n\t\t Many of these features use the concept of word type , which allows a different form of token generalization than POS tags . \n\t', ""\n\t\t The short type of a word is created by replacing any maximal contiguous sequences of capital letters with ' A ' , of lower- case letters with ' a ' , and of digits with '0 ' . \n\t"", '\n\t\t For example , the word TGF-1 would be mapped to type A-0 . \n\t', '\n\t\t Consequently , each token position i in a candidate extraction provides three types of information : the word itself wi , its POS tag ti , and its short type si . \n\t', '\n\t\t The full set of features types is listed in Table 1 , where we consider a generic e label ^HD=enzyme e label ^PF=A0_a ... ^SF=A0_a ... ^SF=a ^PF=A0 fCv ; ef2=v ; efh=v ; 1 2 h e Note that the factor graph above has an equivalent RMN graph consisting of a one-node clique only , on which it is hard to visualize the various potentials involved . \n\t', '\n\t\t There are cases where different factor graphs may yield the same underlying RMN graph , which makes the factor graph representation preferable . \n\t', '\n\t\t 5 Global Clique Templates Global clique templates enable us to model hypothesized influences between entities from the same document . \n\t', '\n\t\t They connect the label nodes of two or more entities , which , in the factor graph , translates into potential nodes connected to at least two label nodes . \n\t', '\n\t\t In our experiments we use three global templates : Overlap Template ( OT ) : No two entity names overlap in the text i.e if the span of one entity is [ sl , el ] and the span of another entity is [ s2 , e2 ] , and sl < s2 , then el < s2 . \n\t', '\n\t\t Repeat Template ( RT ) : If multiple entities in the same document are repetitions of the same name , their labels tend to have the same value ( i.e. most of them are protein names , or most of them are not protein names ) . \n\t', '\n\t\t Later we discuss situations in which repetitions of the same protein name are not tagged as proteins , and design an approach to handle this . \n\t', '\n\t\t Acronym Template ( AT ) : It is common convention that a protein is first introduced by its long name , immediately followed by its short-form ( acronym ) in parentheses . \n\t', '\n\t\t 5.1 The Overlap Template The definition of a candidate extraction from Section 3 leads to many overlapping entities . \n\t', ""\n\t\t For example , ' glutathione S - transferase ' is a base NP , and it generates five candidate extractions : ' glutathione ' , ' glutathione S ' , ' glutathione S - transferase ' , ' S - transferase ' , and ' transferase ' . \n\t"", ""\n\t\t If ' glutathione S - transferase ' has label-value 1 , because the other four entities overlap with it , they should all have label-value 0 . \n\t"", '\n\t\t This type of constraint is enforced by the overlap template whose M operator matches any two overlapping candidate entities , and which connects their label nodes ( specified in S ) through a potential node with a potential function O that allows at most one of them to have label-value 1 , as illustrated in Table 2 . \n\t', ""\n\t\t Contin- uing with the previous example , because ' glutathione S ' and ' S - transferase ' are two overlap- ping entities , the factor graph model will contain an overlap potential node connected to the label nodes of these two entities . \n\t"", '\n\t\t An alternative solution for the overlap template is to create a potential node for each token position that is covered by at least two candidate entities in the document , and connect it to their label nodes . \n\t', '\n\t\t The difference in this case is that the potential node will be connected to a variable number of entity label nodes . \n\t', '\n\t\t However this second approach has the advantage of creating fewer potential nodes in the document factor graph , which results in faster inference . \n\t', '\n\t\t OoT el.label = 0 el.label = 1 e2.label = 0 1 1 e2.label = 1 1 0 Table 2 : Overlap Potential . \n\t', '\n\t\t 5.2 The Repeat Template We could specify the potential for the repeat template in a similar 2-by-2 table , this time leaving the table entries to be learned , given that it is not a hard constraint . \n\t', '\n\t\t However we can do better by noting that the vast majority of cases where a repeated protein name is not also tagged as a protein happens when it is part of a larger phrase that is tagged . \n\t', ""\n\t\t For example , ' HDAC1 enzyme ' is a protein name , there- fore ' HDAC1 ' is not tagged in this phrase , even though it may have been tagged previously in the abstract where it was not followed by ' enzyme ' . \n\t"", '\n\t\t We need a potential that allows two en- tities with the same text to have different labels if the entity with label-value 0 is inside another entity with label-value 1 . \n\t', '\n\t\t But a candidate entity may be inside more than one "" including "" entity , and the number of including entities may vary from one candidate extraction to another . \n\t', ""\n\t\t Using the example from Section 5.1 , the candidate entity ' glutathione ' is included in two other entities : ' glutathione S ' and ' glutathione S - transferase ' . \n\t"", '\n\t\t In order to instantiate potentials over variable number of label nodes , we introduce a logical OR clique template that matches a vari- able number of entities . \n\t', '\n\t\t When this template matches a subset of entities el , e2 , ... , en , it will create an auxiliary OR entity eor , with a single feature eor.label . \n\t', '\n\t\t The potential function is set so that it assigns a non-zero potential only when eor.label = el.label V e2.label V ... \n\t', '\n\t\t V en.label . \n\t', '\n\t\t The cliques are only created as needed , e.g. when the auxiliary OR variable is required by repeat and acronym clique templates . \n\t', '\n\t\t Figure 3 shows the factor graph for a sam- ~AT uor v ^or ~RT u uor v vor u ... un 1 u2 Cr Cr ... ... u1 u2 un v1 v2 vm verges , it gives a good approximation to the correct marginals . \n\t', '\n\t\t The algorithm works by altering the belief at each label node by repeatedly passing messages between the node and all potential nodes connected to it \n\t\t']",Positive
"['\n\t\t As many of the label nodes are indirectly connected through potential nodes instantiated by global templates , their belief values will propagate in the graph and mutually influence each other , leading in the end to a collective labeling decision . \n\t', '\n\t\t The time complexity of computing messages from a potential node to a label node is exponential in the number of label nodes attached to the potential . \n\t', '\n\t\t Since this "" fan-in "" can be large for OR potential nodes , this step required optimization . \n\t', '\n\t\t Fortunately , due to the special form of the OR potential , and the normalization be- fore each message-passing step , we were able to develop a linear-time algorithm for this special case . \n\t', '\n\t\t Details are omitted due to limited space . \n\t', '\n\t\t 7 Learning Potentials in Factor Graphs Following a maximum likelihood estimation , we shall use the log-linear representation of poten- tials : Oc ( G.X7 , G.Y , , ) =exp{w.f~(G.X , G.Y , ) } ( 4 ) where f , is a vector of binary features , one for each configuration of values for X , and Y , . \n\t', '\n\t\t Let w be the concatenated vector of all potential parameters w , . \n\t', '\n\t\t One approach to finding the maximum-likelihood solution for w is to use a gradient-based method , which requires computing the gradient of the log-likelihood with respect to potential parameters w , . \n\t', '\n\t\t It can be shown that this gradient is equal with the difference between the empirical counts of f , and their expectation under the current set of parameters w . \n\t', '\n\t\t This expectation is expensive to compute , since it requires summing over all possible configurations of candidate entity labels from a given document . \n\t', ""\n\t\t To circumvent this complexity , we use Collins ' voted perceptron approach \n\t\t""]",Positive
"['\n\t\t In all our experiments , the perceptron was run for 50 epochs , with a learning rate set at 0.01 . \n\t', '\n\t\t 8 Experimental Results We have tested the RMN approach on two datasets that have been hand-tagged for hu- man protein names . \n\t', '\n\t\t The first dataset is Yapexl which consists of 200 Medline abstracts . \n\t', '\n\t\t Of these , 147 have been randomly selected by posing a query containing the ( Mesh ) terms protein binding , interaction , and molecular to Medline , while the rest of 53 have been extracted randomly from the GENIA corpus \n\t\t']",Positive
"['\n\t\t It contains a total of 3713 protein references . \n\t', '\n\t\t The second dataset is Aimed which has been previously used for training the protein interaction extraction systems in \n\t\t']",Positive
"['\n\t\t It consists of 225 Medline abstracts , of which 200 are known to describe interactions between human proteins , while the other 25 do not refer to any interaction . \n\t', '\n\t\t There are 4084 pro- tein references in this dataset . \n\t', '\n\t\t We compared the performance of three systems : LT-RMN is the RMN approach using local templates and the overlap template , GLT-RMN is the full RMN approach , using both local and global templates , and CRF , which uses a CRF for labeling token sequences . \n\t', '\n\t\t We used the CRF implementation from \n\t\t']",Positive
"[""\n\t\t All Medline abstracts were tokenized and then POS tagged using Brill 's tagger \n\t\t""]",Positive
"['\n\t\t Each extracted protein name in the test data was compared to the human-tagged data , with the positions taken into account . \n\t', '\n\t\t Two extractions are considered a match if they consist of the same character sequence in the same position in the text . \n\t', '\n\t\t Results are shown in Tables 3 and 4 which give average precision , recall , and F-measure using 10-fold cross validation . \n\t', '\n\t\t Method Precision Recall F-measure LT-RMN 70.79 53.81 61.14 GLT-RMN 69.71 65.76 67.68 CRF 72.45 58.64 64.81 Table 3 : Extraction Performance on Yapex . \n\t', '\n\t\t Method Precision Recall F-measure LT-RMN 81.33 72.79 76.82 GLT-RMN 82.79 80.04 81.39 CRF 85.37 75.90 80.36 Table 4 : Extraction Performance on Aimed . \n\t', '\n\t\t These tables show that , in terms of F- measure , the use of global templates for mod- IURL : www.sics.se/humle/projects/prothalt/ 2URL : ftp.cs.utexas.edu/mooney/bio-data/ 100 90 GLT-RMN LT-RMN 80 70 60 50 0 20 40 60 80 100 Recall ( % ) 100 90 80 70 GLT-RMN LT-RMN 60 50 0 20 40 60 80 100 Recall ( % ) to improve a Maximum-Entropy tagger ; however , these features do not fully capture the mutual influence between the labels of acronyms and their long forms , or between entity repetitions . \n\t', '\n\t\t In particular , they only allow earlier extractions in a document to influence later ones and not vice-versa . \n\t', '\n\t\t The RMN approach handles these and potentially other mutual influences between entities in a more complete , probabilistically sound manner . \n\t', '\n\t\t 10 ^onc^usions and Future ^ork We have presented an approach to collective information extraction that uses Relational Markov Networks to reason about the mutual influences between multiple extractions . \n\t', '\n\t\t A new type of clique template \x97 the logical OR template \x97 was introduced , allowing a variable num- ber of relevant entities to be used by other clique templates . \n\t', '\n\t\t Soft correlations between repetitions and acronyms and their long form in the same document have been captured by global clique templates , allowing for local extraction decisions to propagate and mutually influence each other . \n\t', '\n\t\t Regarding future work , a richer set of features for the local templates would likely improve performance . \n\t', ""\n\t\t Currently , LT-RMN 's accuracy is still significantly less than CRF 's , which limits the performance of the full system . \n\t"", '\n\t\t Another limitation is the approximate inference used by both RMN methods . \n\t', '\n\t\t The number of factor graphs for which the sum-product algorithm did not converge was non-negligible , and our approach stopped after a fix number of iterations . \n\t', '\n\t\t Besides exploring improvements to loopy belief propagation that increase computational cost \n\t\t']",Positive
"['\n\t\t 11 Acknowledgements This work was partially supported by grants IIS-0117308 and IIS-0325116 from the NSF . \n\t', '\n\t\t References Eric Brill . \n\t', '\n\t\t 1995. Transformation-based error-driven learning and natural language processing : A case study in part-of-speech tagging . \n\t', '\n\t\t Computational Lin- guistics , 21(4):543-565 . \n\t', '\n\t\t Razvan Bunescu , Ruifang Ge , Rohit J. Kate , Edward M. Marcotte , Raymond J. Mooney , Arun Kumar Ra- mani , and Yuk Wah Wong . \n\t', '\n\t\t 2004. Comparative exper- iments on learning information extractors for proteins and their interactions . \n\t', '\n\t\t Special Issue in the Journal Artificial Intelligence in Medicine on Summarization and Information Extraction from Medical Documents . \n\t', '\n\t\t To appear . \n\t', '\n\t\t Mary Elaine Califf , editor . \n\t', '\n\t\t 1999. Papers from the AAAI1999 Workshop on Machine Learning for Information Extraction , Orlando , FL . \n\t', '\n\t\t AAAI Press . \n\t', '\n\t\t Claire Cardie . \n\t', '\n\t\t 1997. Empirical methods in information extraction . \n\t', '\n\t\t AI Magazine , 18(4):65-79 . \n\t', '\n\t\t Hai Leong Chieu and Hwee Tou Ng. 2003 . \n\t', '\n\t\t Named entity recognition with a maximum entropy approach . \n\t', '\n\t\t In Proceedings of the Seventh Conference on Natural Language Learning ( CoNLL-2003 ) , pages 160-163 , Edmonton , Canada . \n\t', '\n\t\t N. Collier , H. Park , N. Ogata , Y. Tateisi , C. Nobata , T.Ohta , T. Sekimizu , H. Imai , K. Ibushi , and J. Tsu- jii . \n\t', '\n\t\t 1999. The GENIA project : Corpus-based knowledge acquisition and information extraction from genome research papers . \n\t', '\n\t\t In Ninth Conference of the European Chapter of the Association for Computational Linguistics ( EACL-99 ) , pages 271-272 , Bergen . \n\t', '\n\t\t Michael Collins . \n\t', '\n\t\t 2002. Ranking algorithms for named- entity extraction : Boosting and the voted perceptron . \n\t', '\n\t\t In Proceedings of the Annual Meeting of the Association for Computational Linguistics ( A CL-02 ) , pages 489-496 , Philadelphia , PA . \n\t', '\n\t\t Michael I. Jordan , editor . \n\t', '\n\t\t 1999. Learning in Graphical Models . \n\t', '\n\t\t MIT Press , Cambridge , MA . \n\t', '\n\t\t F. R. Kschischang , B. Frey , and H.-A. Loeliger . \n\t', '\n\t\t 2001. Factor graphs and the sum-product algorithm . \n\t', '\n\t\t IEEE Transactions on Information Theory , 47(2):498-519 . \n\t', '\n\t\t John Lafferty , Andrew McCallum , and Fernando Pereira . \n\t', '\n\t\t 2001. Conditional random fields : Probabilistic models for segmenting and labeling sequence data . \n\t', '\n\t\t In Proceedings of 18th International Conference on Machine Learning ( ICML-2001 ) , pages 282-289 , Williams College , MA . \n\t', '\n\t\t Andrew Kachites McCallum . \n\t', '\n\t\t 2002. Mallet : A machine learning for language toolkit . \n\t', '\n\t\t http://mallet.cs.umass.edu . \n\t', '\n\t\t Judea Pearl . \n\t', '\n\t\t 1988. Probabilistic Reasoning in Intelligent Systems : Networks of Plausible Inference . \n\t', '\n\t\t Morgan Kaufmann , San Mateo,CA . \n\t', '\n\t\t Ariel S. Schwartz and Marti A. Hearst . \n\t', '\n\t\t 2003. A sim- ple algorithm for identifying abbreviation definitions in biomedical text . \n\t', '\n\t\t In Proceedings of the 8th Pacific Symposium on Biocomputing , pages 451-462 , Lihue , HI , January . \n\t', '\n\t\t Fei Sha and Fernando Pereira . \n\t', '\n\t\t 2003. Shallow parsing with conditional random fields . \n\t', '\n\t\t In Proceedings of Hu- man Language Technology and the Meeting of the North American Association for Computational Linguistics , pages 134-141 , Edmonton , Canada . \n\t', '\n\t\t Benjamin Taskar , Pieter Abbeel , and D. Koller . \n\t', '\n\t\t 2002. Discriminative probabilistic models for relational data . \n\t', '\n\t\t In Proceedings of 18th Conference on Uncer- tainty in Artificial Intelligence ( UAI-02 ) , pages 485- 492 , Edmonton , Canada . \n\t', '\n\t\t Erik F. Tjong Kim Sang and Fien De Meulder . \n\t', '\n\t\t 2003. Introduction to the CoNLL-2003 shared task : Language-independent named entity recognition . \n\t', '\n\t\t In Proceedings of CoNLL-2003 , pages 142-147 . \n\t', '\n\t\t Edmonton , Canada . \n\t', '\n\t\t Jonathan S. Yedidia , William T. Freeman , and Yair Weiss . \n\t', '\n\t\t 2000. Generalized belief propagation . \n\t', '\n\t\t In Advances in Neural Information Processing Systems 12 , pages 689-695 , Denver , CO . \n\t', '\n\t\t Error Mining for Wide-Coverage Grammar Engineering Gertj an van Noord Alfa-informatica University of Groningen POBox 716 9700 AS Groningen The Netherlands vannoord@let.rug.nl Abstract Parsing systems which rely on hand-coded linguistic descriptions can only perform adequately in as far as these descriptions are correct and complete . \n\t', '\n\t\t The paper describes an error mining technique to discover problems in hand-coded linguistic descriptions for parsing such as grammars and lexicons . \n\t', '\n\t\t By analysing parse results for very large unannotated corpora , the technique discovers missing , incorrect or incomplete linguistic descriptions . \n\t', '\n\t\t The technique uses the frequency of n-grams of words for arbitrary values of n . \n\t', '\n\t\t It is shown how a new combination of suffix arrays and perfect hash finite automata allows an efficient implementation . \n\t', '\n\t\t 1 Introduction As we all know , hand-crafted linguistic descriptions such as wide-coverage grammars and large scale dictionaries contain mistakes , and are incomplete . \n\t', '\n\t\t In the context of parsing , people often construct sets of example sentences that the system should be able to parse correctly . \n\t', '\n\t\t If a sentence cannot be parsed , it is a clear sign that something is wrong . \n\t', '\n\t\t This technique only works in as far as the problems that might occur have been anticipated . \n\t', '\n\t\t More recently , tree-banks have become available , and we can apply the parser to the sentences of the tree-bank and compare the resulting parse trees with the gold standard . \n\t', '\n\t\t Such techniques are limited , however , because tree- banks are relatively small . \n\t', '\n\t\t This is a serious problem , because the distribution of words is Zipfian ( there are very many words that occur very infrequently ) , and the same appears to hold for syntactic constructions . \n\t', '\n\t\t In this paper , an error mining technique is described which is very effective at automatically discovering systematic mistakes in a parser by using very large ( but unannotated ) corpora . \n\t', '\n\t\t The idea is very simple . \n\t', '\n\t\t We run the parser on a large set of sentences , and then analyze those sentences the parser cannot parse successfully . \n\t', '\n\t\t Depending on the nature of the parser , we define the notion \x91success ful parse\x92 in different ways . \n\t', '\n\t\t In the experiments described here , we use the Alpino wide-coverage parser for Dutch ( Bouma et al. , 2001 ; van der Beek et al. , 2002b ) . \n\t', '\n\t\t This parser is based on a large constructionalist HPSG for Dutch as well as a very large electronic dictionary ( partly derived from CELEX , Parole , and CGN ) . \n\t', '\n\t\t The parser is robust in the sense that it essentially always produces a parse . \n\t', '\n\t\t If a full parse is not possible for a given sentence , then the parser returns a ( minimal ) number of parsed non- overlapping sentence parts . \n\t', '\n\t\t In the context of the present paper , a parse is called successful only if the parser finds an analysis spanning the full sentence . \n\t', '\n\t\t The basic idea is to compare the frequency of words and word sequences in sentences that cannot be parsed successfully with the frequency of the same words and word sequences in unproblematic sentences . \n\t', '\n\t\t As we illustrate in section 3 , this technique obtains very good results if it is applied to large sets of sentences . \n\t', '\n\t\t To compute the frequency of word sequences of arbitrary length for very large corpora , we use a new combination of suffix arrays and perfect hash finite automata . \n\t', '\n\t\t This implementation is described in section 4 . \n\t', '\n\t\t The error mining technique is able to discover systematic problems which lead to parsing failure . \n\t', '\n\t\t This includes missing , incomplete and incorrect lexical entries and grammar rules . \n\t', '\n\t\t Problems which cause the parser to assign complete but incorrect parses cannot be discovered . \n\t', '\n\t\t Therefore , tree-banks and hand-crafted sets of example sentences remain important to discover problems of the latter type . \n\t', '\n\t\t 2 A parsability metric for word sequences The error mining technique assumes we have available a large corpus of sentences . \n\t', '\n\t\t Each sentence is a sequence of words ( of course , words might include tokens such as punctuation marks , etc. ) . \n\t', '\n\t\t We run the parser on all sentences , and we note for which sentences the parser is successful . \n\t', '\n\t\t We define the parsability of a word R(w) as the ratio of the num- ber of times the word occurs in a sentence with a successful parse ( C(wIOK)) and the total number of sentences that this word occurs in ( C(w)) : R(w) = Qw ~ ) Thus , if a word only occurs in sentences that cannot be parsed successfully , the parsability of that word is 0 . \n\t', '\n\t\t On the other hand , if a word only occurs in sentences with a successful parse , its parsability is 1 . \n\t', '\n\t\t If we have no reason to believe that a word is particularly easy or difficult , then we expect its parsability to be equal to the coverage of the parser ( the proportion of sentences with a successful parse ) . \n\t', '\n\t\t If its parsability is ( much ) lower , then this indicates that something is wrong . \n\t', '\n\t\t For the experiments described below , the coverage of the parser lies between 91 % and 95 % . \n\t', '\n\t\t Yet , for many words we found parsability values that were much lower than that , including quite a number of words with parsability 0 . \n\t', '\n\t\t Below we show some typical examples , and discuss the types of problem that are discovered in this way . \n\t', '\n\t\t If a word has a parsability of 0 , but its frequency is very low ( say 1 or 2 ) then this might easily be due to chance . \n\t', '\n\t\t We therefore use a frequency cut-off ( e.g. 5 ) , and we ignore words which occur less often in sentences without a successful parse . \n\t', '\n\t\t In many cases , the parsability of a word depends on its context . \n\t', '\n\t\t For instance , the Dutch word via is a preposition . \n\t', '\n\t\t Its parsability in a certain experiment was more than 90 % . \n\t', '\n\t\t Yet , the parser was unable to parse sentences with the phrase via via which is an adverbial expression which means via some complicated route . \n\t', '\n\t\t For this reason , we generalize the parsability of a word to word sequences in a straightforward way . \n\t', '\n\t\t We write C(wz ... wj ) for the number of sentences in which the sequence wz ... wj occurs . \n\t', '\n\t\t Furthermore , C(wz ... wj IOK ) , is the number of sentences with a successful parse which contain the sequence wz ... wj . \n\t', '\n\t\t The parsability of a sequence is defined as : R(wz ... wj ) = Cc(wz . \n\t', '\n\t\t wj I OK ) . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t wj ) If a word sequence wz ... wj has a low parsability , then this might be because it is part of a difficult phrase . \n\t', '\n\t\t It might also be that part of the sequence is the culprit . \n\t', '\n\t\t In order that we focus on the relevant sequence , we consider a longer sequence wh ... wz ... wj ... wk only if its parsability is lower than the parsability of each of its sub- strings : R(wh ... wz ... wj ... wk ) < R(wz ... wj ) This is computed efficiently by considering the parsability of sequences in order of length ( shorter sequences before longer ones ) . \n\t', '\n\t\t We construct a parsability table , which is a list of n-grams sorted with respect to parsability . \n\t', '\n\t\t An n- gram is included in the parsability table , provided : \x95 its frequency in problematic parses is larger than the frequency cut-off \x95 its parsability is lower than the parsability of all of its sub-strings The claim in this paper is that a parsability table provides a wealth of information about systematic problems in the grammar and lexicon , which is otherwise hard to obtain . \n\t', '\n\t\t 3 Experiments and results 3.1 First experiment Data . \n\t', '\n\t\t For our experiments , we used the Twente Nieuws Corpus , version pre-release 0.1.1 This corpus contains among others a large collection of news articles from various Dutch newspapers in the period 1994-2001 . \n\t', '\n\t\t In addition , we used all news articles from the Volkskrant 1997 ( available on CD- ROM ) . \n\t', '\n\t\t In order that this material can be parsed relatively quickly , we discarded all sentences of more than 20 words . \n\t', '\n\t\t Furthermore , a time-out per sentence of twenty CPU-seconds was enforced . \n\t', '\n\t\t The Alpino parser normally exploits a part-of-speech tag filter for efficient parsing \n\t\t']",Positive
"['\n\t\t In table 1 we list some basic quantitative facts about this material . \n\t', '\n\t\t We exploited a cluster of Linux PCs for parsing . \n\t', '\n\t\t If only a single PC had been available , it would have taken in the order of 100 CPU days , to construct the material described in table 1 . \n\t', '\n\t\t These experiments were performed in the autumn of 2002 , with the Alpino parser available then . \n\t', '\n\t\t Below , we report on more recent experiments with the latest version of the Alpino parser , which has been improved quite a lot on the basis of the results of the experiments described here . \n\t', '\n\t\t Results . \n\t', '\n\t\t For the data described above , we computed the parsability table , using a frequency cutoff of 5 . \n\t', '\n\t\t In figure 1 the frequencies of parsability scores in the parsability table are presented . \n\t', '\n\t\t From the figure , it is immediately clear that the relatively high number of word sequences with a parsability of ( almost ) zero cannot be due to chance . \n\t', '\n\t\t Indeed , the 1http://wwwhome.cs.utwente.nl/\x98druid/ TwNC/TwNC-main.html newspaper sents coverage % NRC 1994 582K 91.2 NRC 1995 588K 91.5 Volkskrant 1997 596K 91.6 AD 2000 631K 91.5 PAROOL 2001 529K 91.3 total 2,927K 91.4 Table 1 : Overview of corpus material ; first experiment \n\t\t']",Positive
"['\n\t\t 0.0 0.2 0.4 0.6 0.8 1.0 Parsability Figure 1 : Histogram of the frequencies of parsability scores occurring in parsability table . \n\t', '\n\t\t Frequency cut-off=5 ; first experiment \n\t\t', '\n\t\t parsability table starts with word sequences which constitute systematic problems for the parser . \n\t', '\n\t\t In quite a lot of cases , these word sequences originate from particular types of newspaper text with idiosyncratic syntax , such as announcements of new books , movies , events , television programs etc. ; as well as checkers , bridge and chess diagrams . \n\t', '\n\t\t Another category consists of ( parts of ) English , French and German phrases . \n\t', '\n\t\t We also find frequent spelling mistakes such as de de where only a single de ( the definite article ) is expected , and heben for hebben ( to have ) , indentiek for identiek ( identical ) , koninging for koningin ( queen ) , etc. . \n\t', '\n\t\t Other examples include wordt ik ( becomes I ) , vindt ik ( finds I ) , vind hij ( find he ) etc. . \n\t', '\n\t\t We now describe a number of categories of examples which have been used to improve the parser . \n\t', '\n\t\t Tokenization . \n\t', '\n\t\t A number of n-grams with low parsability scores point towards systematic mistakes during tokenization . \n\t', '\n\t\t Here are a number of examples:2 2The @ symbol indicates a sentence boundary . \n\t', '\n\t\t R C n-gram 0.00 1884 @ . \n\t', '\n\t\t 0.00 385 @ ! \n\t', '\n\t\t 0.00 22 \x92s advocaat \x92s lawyer 0.11 8 H. \x92s H. \x92s 0.00 98 @ , roept @ , yells 0.00 20 @ , schreeuwt @ , screams 0.00 469 @ , vraagt @ , asks The first and second n-gram indicate sentences which start with a full stop or an exclamation mark , due to a mistake in the tokenizer . \n\t', '\n\t\t The third and fourth n-grams indicate a problem the tokenizer had with a sequence of a single capital letter with a dot , followed by the genitive marker . \n\t', '\n\t\t The grammar assumes that the genitive marking is attached to the proper name . \n\t', '\n\t\t Such phrases occur frequently in reports on criminals , which are indicated in news paper only with their initials . \n\t', '\n\t\t Another systematic mistake is reflected by the last n-grams . \n\t', '\n\t\t In reported speech such as ( 1 ) Je bent gek ! \n\t', '\n\t\t , roept Franca . \n\t', '\n\t\t You are crazy ! \n\t', '\n\t\t , yells Franca . \n\t', '\n\t\t Franca yells : You are crazy ! \n\t', '\n\t\t the tokenizer mistakenly introduced a sentence boundary between the exclamation mark and the comma . \n\t', '\n\t\t On the basis of examples such as these , the tokenizer has been improved . \n\t', '\n\t\t Mistakes in the lexicon . \n\t', '\n\t\t Another reason an n- gram receives a low parsability score is a mistake in the lexicon . \n\t', '\n\t\t The following table lists two typical examples : R C n-gram 0.27 18 de kaft the cover 0.30 7 heeft opgetreden has performed In Dutch , there is a distinction between neuter and non-neuter common nouns . \n\t', '\n\t\t The definite article de combines with non-neuter nouns , whereas neuter nouns select het . \n\t', '\n\t\t The common noun kaft , for example , combines with the definite article de . \n\t', '\n\t\t However , according to the dictionary , it is a neuter common noun ( and thus would be expected to combine only with the definite article het ) . \n\t', '\n\t\t Many similar errors were discovered . \n\t', '\n\t\t Another syntactic distinction that is listed in the dictionary is the distinction between verbs which take the auxiliary hebben ( to have ) to construct a perfect tense clause vs. those that take the auxiliary zijn ( to be ) . \n\t', '\n\t\t Some verbs allow both possibilities . \n\t', '\n\t\t The last example illustrates an error in the dictionary with respect to this syntactic feature . \n\t', '\n\t\t Incomplete lexical descriptions . \n\t', '\n\t\t The majority of problems that the parsability scores indicate reflect incomplete lexical entries . \n\t', '\n\t\t A number of examples is provided in the following table : R C n-gram 0.00 11 begunstigden favoured ( N/V ) 0.23 10 zich eraan dat self there-on that 0.08 12 aan te klikken on to click 0.08 12 doodzonde dat mortal sin that 0.15 11 zwarts black\x92s 0.00 16 dupe van victim of 0.00 13 het Turks . \n\t', '\n\t\t the Turkish The word begunstigden is ambiguous between on the one hand the past tense of the verb begunstigen ( to favour ) and on the other hand the plural nominalization begunstigden ( beneficiaries ) . \n\t', '\n\t\t The dictionary contained only the first reading . \n\t', '\n\t\t The sequence zich eraan dat illustrates a missing valency frame for verbs such as ergeren ( to irritate ) . \n\t', '\n\t\t In Dutch , verbs which take a prepositional complement sometimes also allow the object of the prepositional complement to be realized by a subordinate ( finite or infinite ) clause . \n\t', '\n\t\t In that case , the prepositional complement is R-pronominalized . \n\t', '\n\t\t Examples : ( 2 ) a. Hij ergert zich aan zijn aanwezigheid He is-irritated self on his presence He is irritated by his presence b. Hij ergert zich er niet aan dat ... \n\t', '\n\t\t He is-irritated self there not on that ... \n\t', '\n\t\t He is not irritated by the fact that ... \n\t', '\n\t\t The sequence aan te klikken is an example of a verb-particle combination which is not licensed in the dictionary . \n\t', '\n\t\t This is a relatively new verb which is used for click in the context of buttons and hyper- links . \n\t', '\n\t\t The sequence doodzonde dat illustrates a syntactic construction where a copula combines with a predicative complement and a sentential subject , if that predicative complement is of the appropriate type . \n\t', '\n\t\t This type is specified in the dictionary , but was missing in the case of doodzonde . \n\t', '\n\t\t Example : ( 3 ) Het is doodzonde dat hij slaapt It is mortal-sin that he sleeps That he is sleeping is a pity The word zwarts should have been analyzed as a genitive noun , as in ( typically sentences about chess or checkers ) : ( 4 ) Hij keek naar zwarts toren He looked at black\x92s rook whereas the dictionary only assigned the inflected adjectival reading . \n\t', '\n\t\t The sequence dupe van illustrates an example of an R-pronominalization of a PP modifier . \n\t', '\n\t\t This is generally not possible , except for ( quite a large ) number of contexts which are determined by the verb and the object : ( 5 ) a. Hij is de dupe van jouw vergissing He is the victim of your mistake He has to suffer for your mistake b. Hij is daar nu de dupe van He is there now the victim of He has to suffer for it The word Turks can be both an adjective ( Turkish ) or a noun the Turkish language . \n\t', '\n\t\t The dictionary contained only the first reading . \n\t', '\n\t\t Very many other examples of incomplete lexical entries were found . \n\t', '\n\t\t Frozen expressions with idiosyncratic syntax . \n\t', '\n\t\t Dutch has many frozen expressions and idioms with archaic inflection and/or word order which breaks the parser . \n\t', '\n\t\t Examples include : R C n-gram 0.00 13 dan schaadt het then harms it 0.00 13 @ God zij @ God be[I] 0.22 25 God zij God be[I] 0.00 19 Het zij zo It be[I] so 0.45 12 goeden huize good house[I] 0.09 11 berge mountain[I] 0.00 10 hele gedwaald whole[I] dwelled 0.00 14 te weeg The sequence dan schaadt het is part of the idiom Baat het niet , dan schaadt het niet ( meaning : it might be unsure whether something is helpful , but in any case it won\x92t do any harm ) . \n\t', '\n\t\t The sequence God zij is part of a number of archaic formulas such as God zij dank ( Thank God ) . \n\t', '\n\t\t In such examples , the form zij is the ( archaic ) subjunctive form of the Dutch verb zijn ( to be ) . \n\t', '\n\t\t The sequence Het zij zo is another fixed formula ( English : So be it ) , containing the same subjunctive . \n\t', '\n\t\t The phrase van goeden huize ( of good family ) is a frozen expression with archaic inflection . \n\t', '\n\t\t The word berge exhibits archaic inflection on the word berg ( mountain ) , which only occurs in the idiomatic expression de haren rijzen mij te berge ( my hair rises to the mountain ) which expresses a great deal of surprise . \n\t', '\n\t\t The n-gram hele gedwaald only occurs in the idiom Beter ten halve gekeerd dan ten hele gedwaald : it is better to turn halfway , then to go all the way in the wrong direc- tion . \n\t', '\n\t\t Many other ( parts of ) idiomatic expressions were found in the parsability table . \n\t', '\n\t\t The sequence te weeg only occurs as part of the phrasal verb te weeg brengen ( to cause ) . \n\t', '\n\t\t Incomplete grammatical descriptions . \n\t', '\n\t\t Although the technique strictly operates at the level of words and word sequences , it is capable of indicating grammatical constructions that are not treated , or not properly treated , in the grammar . \n\t', '\n\t\t R C n-gram 0.06 34 Wij Nederlanders We Dutch 0.08 23 Geeft niet Matters not 0.00 15 de alles the everything 0.10 17 Het laten The letting 0.00 10 tenzij . \n\t', '\n\t\t unless . \n\t', '\n\t\t The sequence Wij Nederlanders constitutes an example of a pronoun modified by means of an apposition ( not allowed in the grammar ) as in ( 6 ) Wij Nederlanders eten vaak aardappels We Dutch eat often potatoes We , the Dutch , often eat potatoes The sequence Geeft niet illustrates the syntactic phenomenon of topic-drop ( not treated in the grammar ) : verb initial sentences in which the topic ( typically the subject ) is not spelled out . \n\t', '\n\t\t The sequence de alles occurs with present participles ( used as prenominal modifiers ) such as overheersende as in de alles overheersende paniek ( literally : the all dominating panic , i.e. , the panic that dominated everything ) . \n\t', '\n\t\t The grammar did not allow prenominal modifiers to select an NP complement . \n\t', '\n\t\t The sequence Het laten often occurs in nominalizations with multiple verbs . \n\t', '\n\t\t These were not treated in the grammar . \n\t', '\n\t\t Example : ( 7 ) Het laten zien van problemen The letting see of problems Showing problems The word sequence tenzij . \n\t', '\n\t\t is due to sentences in which a subordinate coordinator occurs without a complement clause : ( 8 ) Gij zult niet doden , tenzij . \n\t', '\n\t\t Thou shallt not kill , unless . \n\t', '\n\t\t A large number of n-grams also indicate elliptical structures , not treated in that version of the grammar . \n\t', '\n\t\t Another fairly large source of errors are irregular named entities ( Gil y Gil , Osama bin Laden . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t newspaper # sentences coverage % NRC 1994 552,833 95.0 Volkskrant 1997 569,314 95,2 AD 2000 662,380 95,7 Trouw 1999 406,339 95,5 Volkskrant 2001 782,645 95,1 Table 2 : Overview of corpus material used for the experiments ; second experiment \n\t\t', '\n\t\t 3.2 Later experiment Many of the errors and omissions that were found on the basis of the parsability table have been corrected . \n\t', '\n\t\t As can be seen in table 2 , the coverage obtained by the improved parser increased substantially . \n\t', '\n\t\t In this experiment , we also measured the coverage on additional sets of sentences ( all sentences from the Trouw 1999 and Volkskrant 2001 newspaper , available in the TwNC corpus ) . \n\t', '\n\t\t The results show that coverage is similar on these unseen test- sets . \n\t', '\n\t\t Obviously , coverage only indicates how often the parser found a full parse , but it does not indicate whether that parse actually was the correct parse . \n\t', '\n\t\t For this reason , we also closely monitored the performance of the parser on the Alpino tree-bank3 ( van der Beek et al. , 2002a ) , both in terms of parsing accuracy and in terms of average number of parses per sentence . \n\t', '\n\t\t The average number of parses increased , which is to be expected if the grammar and lexicon are extended . \n\t', '\n\t\t Accuracy has been steadily increasing on the Alpino tree-bank . \n\t', '\n\t\t Accuracy is defined as the proportion of correct named dependency relations of the first parse returned by Alpino . \n\t', '\n\t\t Alpino employs a maximum entropy disambiguation component ; the first parse is the most promising parse according to this statistical model . \n\t', '\n\t\t The maximum entropy disambiguation component of Alpino assigns a score S(x) to each parse x : S(x) = X BZfZ(x) ( 1 ) Z where fZ ( x ) is the frequency of a particular feature i in parse x and BZ is the corresponding weight of that feature . \n\t', '\n\t\t The probability of a parse x for sentence w is then defined as follows , where Y(w) are all the parses of w : exp ( S(x)) p(x| w ) = Py^Y(w) exp ( S(y)) ( 2 ) The disambiguation component is described in detail in Malouf and van \n\t\t']",Positive
"['\n\t\t 3http://www.let.rug.nl/\x98vannoord/trees/ Time ( days ) Figure 2 : Development of Accuracy of the Alpino parser on the Alpino Tree-bank Figure 2 displays the accuracy from May 2003- May 2004 . \n\t', '\n\t\t During this period many of the problems described earlier were solved , but other parts of the system were improved too ( in particular , the disambiguation component was improved considerably ) . \n\t', '\n\t\t The point of the graph is that apparently the increase in coverage has not been obtained at the cost of decreasing accuracy . \n\t', '\n\t\t 4 A note on the implementation The most demanding part of the implementation consists of the computation of the frequency of n- grams . \n\t', '\n\t\t If the corpus is large , or n increases , simple techniques break down . \n\t', '\n\t\t For example , an approach in which a hash data-structure is used to maintain the counts of each n-gram , and which increments the counts of each n-gram that is encountered , requires excessive amounts of memory for large n and/or for large corpora . \n\t', '\n\t\t On the other hand , if a more compact data-structure is used , speed becomes an issue . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t If the corpus size increases , the memory required for the suffix array may become problematic . \n\t', '\n\t\t We propose a new combination of suffix arrays with perfect hash finite automata , which reduces typical memory requirements by a factor of five , in combination with a modest increase in processing efficiency . \n\t', '\n\t\t 4.1 Suffix arrays Suffix arrays \n\t\t']",Positive
"['\n\t\t A corpus is a sequence of characters . \n\t', '\n\t\t A suffix array s is an array consisting of all suffixes of the corpus , sorted alphabetically . \n\t', '\n\t\t For example , if the corpus is the string abba , the suffix array is ( a , abba , ba , bba ) . \n\t', '\n\t\t Rather than writing out each suffix , we use integers i to refer to the suffix starting at position i in the corpus . \n\t', '\n\t\t Thus , in this case the suffix array consists of the integers ( 3 , 0 , 2 , 1 ) . \n\t', '\n\t\t It is straightforward to compute the suffix array . \n\t', '\n\t\t For a corpus of k + 1 characters , we initialize the suffix array by the integers 0 ... k . \n\t', '\n\t\t The suffix array is sorted , using a specialized comparison routine which takes integers i and j , and alphabetically compares the strings starting at i and j in the corpus.4 Once we have the suffix array , it is simple to compute the frequency of n-grams . \n\t', '\n\t\t Suppose we are interested in the frequency of all n-grams for n = 10 . \n\t', '\n\t\t We simply iterate over the elements of the suffix array : for each element , we print the first ten words of the corresponding suffix . \n\t', '\n\t\t This gives us all occurrences of all 10-grams in the corpus , sorted alphabetically . \n\t', '\n\t\t We now count each 10-gram , e.g. by piping the result to the Unix uniq -c command . \n\t', '\n\t\t 4.2 Perfect hash finite automata Suffix arrays can be used more efficiently to compute frequencies of n-grams for larger n , with the help of an additional data-structure , known as the perfect hash finite automaton \n\t\t']",Positive
"['\n\t\t The perfect hash automaton for an alphabetically sorted finite set of words wo ... wn is a weighted minimal deterministic finite automaton which maps wi \x97* i for each w0<i<n . \n\t', '\n\t\t We call i the word code of wi . \n\t', '\n\t\t An example is given in figure 3 . \n\t', '\n\t\t Note that perfect hash automata implement an order preserving , minimal perfect hash function . \n\t', '\n\t\t The function is minimal , in the sense that n keys are mapped into the range 0 ... n \x97 1 , and the function is order preserving , in the sense that the alphabetic order of words is reflected in the numeric order of word codes . \n\t', '\n\t\t 4.3 Suffix arrays with words In the approach of \n\t\t']",Positive
"['\n\t\t A more space- efficient approach takes the corpus as a sequence of words , represented by word codes reflecting the alphabetic order . \n\t', '\n\t\t To compute frequencies of n-grams for larger n , we first compute the perfect hash finite automaton for all words which occur in the corpus,5 and map 4The suffix sort algorithm of Peter M. McIlroy and M. Douglas McIlroy is used , available as http : / /www. cs. dartmouth.edu/\x98doug/ssort.c ; This algorithm is robust against long repeated substrings in the corpus . \n\t', '\n\t\t 5 We use an implementation by Jan Daciuk freely available from http://www.eti.pg.gda.pl/\x98jandac/ fsa.html . \n\t', '\n\t\t 0 50 100 150 200 250 300 350 Figure 3 : Example of a perfect hash finite automaton for the words clock , dock , dog , duck , dust , rock , rocker , stock . \n\t', '\n\t\t Summing the weights along an accepting path in the automaton yields the rank of the word in alphabetic ordering . \n\t', '\n\t\t the corpus to a sequence of integers , by mapping each word to its word code . \n\t', '\n\t\t Suffix array construction then proceeds on the basis of word codes , rather than character codes . \n\t', '\n\t\t This approach has several advantages . \n\t', '\n\t\t The representation of both the corpus and the suffix array is more compact . \n\t', '\n\t\t If the average word length is k , then the corresponding arrays are k times smaller ( but we need some additional space for the perfect hash automaton ) . \n\t', '\n\t\t In Dutch , the average word length k is about 5 , and we obtained space savings in that order . \n\t', '\n\t\t If the suffix array is shorter , sorting should be faster too ( but we need some additional time to compute the perfect hash automaton ) . \n\t', '\n\t\t In our experience , sorting is about twice as fast for word codes . \n\t', '\n\t\t 4.4 Computing parsability table To compute parsability scores , we assume there are two corpora cm and ca , where the first is a sub- corpus of the second . \n\t', '\n\t\t cm contains all sentences for which parsing was not successful . \n\t', '\n\t\t ca contains all sentences overall . \n\t', '\n\t\t For both corpora , we compute the frequency of all n-grams for all n ; n-grams with a frequency below a specified frequency cutoff are ignored . \n\t', '\n\t\t Note that we need not impose an a priori maximum value for n ; since there is a frequency cut-off , for some n there simply aren\x92t any sequences which occur more frequently than this cut-off . \n\t', '\n\t\t The two n-gram frequency files are organized in such a way that shorter n-grams precede longer n-grams . \n\t', '\n\t\t The two frequency files are then combined as follows . \n\t', '\n\t\t Since the frequency file corresponding to cm is ( much ) smaller than the file corresponding to ca , we read the first file into memory ( into a hash data structure ) . \n\t', '\n\t\t We then iteratively read an n-gram frequency from the second file , and com pute the parsability of that n-gram . \n\t', '\n\t\t In doing so , we keep track of the parsability scores assigned to previous ( hence shorter ) n-grams , in order to ensure that larger n-grams are only reported in case the parsability scores decrease . \n\t', '\n\t\t The final step consists in sorting all remaining n-grams with respect to their parsability . \n\t', '\n\t\t To give an idea of the practicality of the approach , consider the following data for one of the experiments described above . \n\t', '\n\t\t For a corpus of 2,927,016 sentences ( 38,846,604 words , 209Mb ) , it takes about 150 seconds to construct the perfect hash automaton ( mostly sorting ) . \n\t', '\n\t\t The automaton is about 5Mb in size , to represent 677,488 distinct words . \n\t', '\n\t\t To compute the suffix array and frequencies of all n-grams ( cut-off=5 ) , about 15 minutes of CPU-time are required . \n\t', '\n\t\t Maximum runtime memory requirements are about 400Mb . \n\t', '\n\t\t The result contains frequencies for 1,641,608 distinct n- grams . \n\t', '\n\t\t Constructing the parsability scores on the basis of the n-gram files only takes 10 seconds CPU-time , resulting in parsability scores for 64,998 n-grams ( since there are much fewer n-grams which actually occur in problematic sentences ) . \n\t', '\n\t\t The experiment was performed on a Intel Pentium III , 1266MHz machine running Linux . \n\t', '\n\t\t The software is freely available from http : / /www. let . \n\t', '\n\t\t rug . \n\t', '\n\t\t nl/\x98vannoord/software.html . \n\t', '\n\t\t 5 Discussion An error mining technique has been presented which is very helpful in identifying problems in hand-coded grammars and lexicons for parsing . \n\t', '\n\t\t An important ingredient of the technique consists of the computation of the frequency of n-grams of words for arbitrary values of n . \n\t', '\n\t\t It was shown how a new combination of suffix arrays and perfect hash finite automata allows an efficient implementation . \n\t', '\n\t\t A number of potential improvements can be envisioned . \n\t', '\n\t\t In the definition of R(w) , the absolute frequency of w is ignored . \n\t', '\n\t\t Yet , if w is very frequent , R(w) is more reliable than if w is not frequent . \n\t', '\n\t\t Therefore , as an alternative , we also experimented with a set-up in which an exact binomial test is applied to compute a confidence interval for R(w) . \n\t', '\n\t\t Results can then be ordered with respect to the maximum of these confidence intervals . \n\t', '\n\t\t This procedure seemed to improve results somewhat , but is computationally much more expensive . \n\t', '\n\t\t For the first experiment described above , this alternative set-up results in a parsability table of 42K word tuples , whereas the original method produces a table of 65K word tuples . \n\t', '\n\t\t r u::2 r::5 d::1 t o c s::1 c g::1 c e::1 k c o k l s::7 c o t R C n-gram 0.00 8 0.20 12 0.15 11 0.00 8 0.09 10 0.69 15 0.17 10 0.00 10 0.00 8 0.20 10 Table 3 : Multiple n-grams indicating same error The parsability table only contains longer n- grams if these have a lower parsability than the corresponding shorter n-grams . \n\t', '\n\t\t Although this heuristic appears to be useful , it is still possible that a single problem is reflected multiple times in the parsability table . \n\t', '\n\t\t For longer problematic sequences , the parsability table typically contains partially overlapping parts of that sequence . \n\t', '\n\t\t This phenomenon is illustrated in table 3 for the idiom Beter ten halve gekeerd dan ten hele gedwaald discussed earlier . \n\t', '\n\t\t This suggests that it would be useful to consider other heuristics to eliminate such redundancy , perhaps by considering statistical feature selection methods . \n\t', '\n\t\t The definition used in this paper to identify a successful parse is a rather crude one . \n\t', '\n\t\t Given that grammars of the type assumed here typically assign very many analyses to a given sentence , it is often the case that a specific problem in the grammar or lexicon rules out the intended parse for a given sentence , but alternative ( wrong ) parses are still possible . \n\t', '\n\t\t What appears to be required is a ( statistical ) model which is capable of judging the plausibility of a parse . \n\t', '\n\t\t We investigated whether the maximum entropy score S(x) ( equation 1 ) can be used to indi- cate parse plausibility . \n\t', '\n\t\t In this set-up , we considered a parse successful only if S(x) of the best parse is above a certain threshold . \n\t', '\n\t\t However , the resulting parsability table did not appear to indicate problematic word sequences , but rather word sequences typically found in elliptical sentences were returned . \n\t', '\n\t\t Apparently , the grammatical rules used for ellipsis are heavily punished by the maximum entropy model in order that these rules are used only if other rules are not applicable . \n\t', '\n\t\t Acknowledgments This research was supported by the PIONIER project Algorithms for Linguistic Processing funded by NWO . \n\t', '\n\t\t References Gosse Bouma , Gertjan van Noord , and Robert Malouf . \n\t', '\n\t\t 2001. Wide coverage computational analysis of Dutch . \n\t', '\n\t\t In W. Daelemans , K. Sima\x92an , J. Veenstra , and J. Zavrel , editors , Computational Linguistics in the Netherlands 2000 . \n\t', '\n\t\t Kenneth Ward Church . \n\t', '\n\t\t 1995. Ngrams . \n\t', '\n\t\t ACL 1995 , MIT Cambridge MA , June 16 . \n\t', '\n\t\t ACL Tutorial . \n\t', '\n\t\t Claudio Lucchiesi and Tomasz Kowaltowski . \n\t', '\n\t\t 1993. Applications of finite automata representing large vocabularies . \n\t', '\n\t\t Software Practice and Experience , 23(1):15\x9630 , Jan. Robert Malouf and Gertjan van Noord . \n\t', '\n\t\t 2004. Wide coverage parsing with stochastic attribute value grammars . \n\t', '\n\t\t In Beyond shallow analyses . \n\t', '\n\t\t Formalisms and statistical modeling for deep analysis , Sanya City , Hainan , China . \n\t', '\n\t\t IJCNLP-04 Workshop . \n\t', '\n\t\t Udi Manber and Gene Myers . \n\t', '\n\t\t 1990. Suffix arrays : A new method for on-line string searching . \n\t', '\n\t\t In Proceedings of the First Annual AC-SIAM Symposium on Discrete Algorithms , pages 319\x96327. http : / /manber . \n\t', '\n\t\t com/publications.html . \n\t', '\n\t\t Robbert Prins and Gertjan van Noord . \n\t', '\n\t\t 2003. Reinforcing parser preferences through tagging . \n\t', '\n\t\t TraitementAutomatique des Langues , 44(3):121\x96 139. in press . \n\t', '\n\t\t Dominique Revuz . \n\t', '\n\t\t 1991. Dictionnaires et lexiques : m´ethodes et algorithmes . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , Institut Blaise Pascal , Paris , France . \n\t', '\n\t\t LITP 91.44 . \n\t', '\n\t\t Emmanuel Roche . \n\t', '\n\t\t 1995. Finite-state tools for language processing . \n\t', '\n\t\t ACL 1995 , MIT Cambridge MA , June 16 . \n\t', '\n\t\t ACL Tutorial . \n\t', '\n\t\t Leonoor van der Beek , Gosse Bouma , Robert Malouf , and Gertjan van Noord . \n\t', '\n\t\t 2002a . \n\t', '\n\t\t The Alpino dependency treebank . \n\t', '\n\t\t In Mari¨et Theune , Anton Nijholt , and Hendri Hondorp , editors , Computational Linguistics in the Netherlands 2001 . \n\t', '\n\t\t Selected Papers from the Twelfth CLIN Meeting , pages 8\x9622 . \n\t', '\n\t\t Rodopi . \n\t', '\n\t\t Leonoor van der Beek , Gosse Bouma , and Gertjan van Noord . \n\t', '\n\t\t 2002b . \n\t', '\n\t\t Een brede computationele grammatica voor het Nederlands . \n\t', '\n\t\t Nederlandse Taalkunde , 7(4):353\x96374. in Dutch . \n\t', '\n\t\t Mikio Yamamoto and Kenneth W. Church . \n\t', '\n\t\t 2001. Using suffix arrays to compute term frequency and document frequency for all substrings in a corpus . \n\t', '\n\t\t Computational Linguistics , 27(1):1\x9630 . \n\t', '\n\t\t Beter ten ten halve halve gekeerd gekeerd dan dan ten hele dan ten ten hele hele gedwaald gedwaald . \n\t', '\n\t\t gedwaald \n\t', '\n\t\t Alternative Approaches for Generating Bodies of Grammar Rules Gabriel Infante-Lopez and Maarten de Rijke Informatics Institute , University of Amsterdam {infante,mdr}@science.uva.nl Abstract We compare two approaches for describing and generating bodies of rules used for natural language parsing . \n\t', '\n\t\t In today\x92s parsers rule bodies do not exist a priori but are generated on the fly , usually with methods based on n-grams , which are one particular way of inducing probabilistic regular languages . \n\t', '\n\t\t We compare two approaches for inducing such languages . \n\t', '\n\t\t One is based on n-grams , the other on minimization of the Kullback-Leibler divergence . \n\t', '\n\t\t The inferred regular languages are used for generating bodies of rules inside a parsing procedure . \n\t', '\n\t\t We compare the two approaches along two dimensions : the quality of the probabilistic regular language they produce , and the performance of the parser they were used to build . \n\t', '\n\t\t The second approach outperforms the first one along both dimensions . \n\t', '\n\t\t 1 Introduction N-grams have had a big impact on the state of the art in natural language parsing . \n\t', '\n\t\t They are central to many parsing models ( Charniak , 1997 ; Collins , 1997 , 2000 ; Eisner , 1996 ) , and despite their simplicity n-gram models have been very successful . \n\t', '\n\t\t Modeling with n-grams is an induction task \n\t\t']",Positive
"['\n\t\t Given a sample set of strings , the task is to guess the grammar that produced that sample . \n\t', '\n\t\t Usually , the grammar is not be chosen from an arbitrary set of possible grammars , but from a given class . \n\t', '\n\t\t Hence , grammar induction consists of two parts : choosing the class of languages amongst which to search and designing the procedure for performing the search . \n\t', '\n\t\t By using n-grams for grammar induction one addresses the two parts in one go . \n\t', '\n\t\t In particular , the use of n-grams implies that the solution will be searched for in the class of probabilistic regular languages , since n-grams induce probabilistic automata and , consequently , probabilistic regular languages . \n\t', '\n\t\t However , the class of probabilistic regular languages induced using n-grams is a proper subclass of the class of all probabilistic regular languages ; n-grams are incapable of capturing long-distance relations between words . \n\t', '\n\t\t At the technical level the restricted nature of n-grams is witnessed by the special structure of the automata induced from them , as we will see in Section 4.2 . \n\t', '\n\t\t N-grams are not the only way to induce regular languages , and not the most powerful way to do so . \n\t', '\n\t\t There is a variety of general methods capable of inducing all regular languages \n\t\t']",Positive
"['\n\t\t What is their relevance for natural language parsing ? \n\t', '\n\t\t Recall that regular languages are used for describing the bodies of rules in a grammar . \n\t', '\n\t\t Consequently , the quality and expressive power of the resulting grammar is tied to the quality and expressive power of the regular languages used to describe them . \n\t', '\n\t\t And the quality and expressive power of the latter , in turn , are influenced directly by the method used to induce them . \n\t', '\n\t\t These observations give rise to a natural question : can we gain anything in parsing from using general methods for inducing regular languages instead of methods based on n-grams ? \n\t', '\n\t\t Specifically , can we describe the bodies of grammatical rules more accurately and more concisely by using general methods for inducing regular languages ? \n\t', '\n\t\t In the context of natural language parsing we present an empirical comparison between algorithms for inducing regular languages using n- grams on the one hand , and more general algorithms for learning the general class of regular language on the other hand . \n\t', '\n\t\t We proceed as follows . \n\t', '\n\t\t We generate our training data from the Wall Street Journal Section of the Penn Tree Bank ( PTB ) , by transforming it to projective dependency structures , following \n\t\t']",Positive
"['\n\t\t These rules are used as training material for the rule induction algorithms we consider . \n\t', '\n\t\t The automata produced this way are then used to build grammars which , in turn , are used for parsing . \n\t', '\n\t\t We are interested in two different aspects of the use of probabilistic regular languages for natural language parsing : the quality of the induced automata and the performance of the resulting parsers . \n\t', '\n\t\t For evaluation purposes , we use two different metrics : perplexity for the first aspect and percentage of correct attachments for the second . \n\t', '\n\t\t The main results of the paper are that , measured in terms of perplexity , the automata induced by algorithms other than n-grams describe the rule bodies better than automata induced using n-gram-based algorithms , and that , moreover , the gain in automata quality is reflected by an improvement in parsing performance . \n\t', '\n\t\t We also find that the parsing performance of both methods ( n-grams vs. general automata ) can be substantially improved by splitting the training material into POS categories . \n\t', '\n\t\t As a side product , we find empirical evidence to suggest that the effectiveness of rule lexicalization techniques ( Collins , 1997 ; Sima\x92an , 2000 ) and parent annotation techniques \n\t\t']",Positive
"['\n\t\t Section 2 surveys our experiments , and later sections provide details of the various aspects . \n\t', '\n\t\t Section 3 offers details on our grammatical framework , PCW-grammars , on transforming automata to PCW-grammars , and on parsing with PCWgrammars . \n\t', '\n\t\t Section 4 explains the starting point of this process : learning automata , and Section 5 reports on parsing experiments . \n\t', '\n\t\t We discuss related work in Section 6 and conclude in Section 7 . \n\t', '\n\t\t 2 Overview We want to build grammars using different algorithms for inducing their rules . \n\t', '\n\t\t Our main question is aimed at understanding how different algorithms for inducing regular languages impact the parsing performance with those grammars . \n\t', '\n\t\t A second issue that we want to explore is how the grammars perform when the quality of the training material is improved , that is , when the training material is separated into part of speech ( POS ) categories before the regular language learning algorithms are run . \n\t', '\n\t\t We first transform the PTB into projective dependencies structures following \n\t\t']",Positive
"['\n\t\t From the resulting tree bank we delete all lexical information except POS tags . \n\t', '\n\t\t Every POS in a tree belonging to the tree-bank has associated to it two different , possibly empty , sequences of right and left dependents , respectively . \n\t', '\n\t\t We extract all these sequences for all trees , producing two different sets containing right and left sequences of dependents respectively . \n\t', '\n\t\t These two sets form the training material used for building four different grammars . \n\t', '\n\t\t The four grammars differ along two dimensions : the number of automata used for building them and the algorithm used for inducing the automata . \n\t', '\n\t\t As to the latter dimension , in Section 4 we use two algorithms : the Minimum Discriminative Information ( MDI ) algorithm , and a bigram-based algorithm . \n\t', '\n\t\t As to the former dimension , two of the grammars are built using only two different automata , each of which is built using the two sample set generated from the PTB . \n\t', '\n\t\t The other two grammars were built using two automata per POS , exploiting a split of the train ing samples into multiple samples , two samples per POS , to be precise , each containing only those samples where the POS appeared as the head . \n\t', '\n\t\t The grammars built from the induced automata are so-called PCW-grammars ( see Section 3 ) , a formalism based on probabilistic context free grammars ( PCFGs ) ; as we will see in Section 3 , inferring them from automata is almost immediate . \n\t', '\n\t\t 3 Grammatical Framework We briefly detail the grammars we work with ( PCW-grammars ) , how automata give rise to these grammars , and how we parse using them . \n\t', '\n\t\t 3.1 PCW-Grammars We need a grammatical framework that models rule bodies as instances of a regular language and that allows us to transform automata to grammars as directly as possible . \n\t', '\n\t\t We decided to embed them in the general grammatical framework of CW-grammars \n\t\t']",Positive
"['\n\t\t A probabilistic constrained W-grammar ( PCWgrammar ) consists of two different sets of PCF-like rules called pseudo-rules and meta-rules respectively and three pairwise disjoint sets of symbols : variables , non-terminals and terminals . \n\t', '\n\t\t Pseudo- rules and meta-rules provide mechanisms for building \x91real\x92 rewrite rules . \n\t', '\n\t\t We use a w==.> Q to indicate that a should be rewritten as Q . \n\t', '\n\t\t In the case of PCWgrammars , rewrite rules are built by first selecting a pseudo-rule , and then using meta-rules for instantiating all the variables in the body of the pseudo-rule . \n\t', '\n\t\t To illustrate these concepts , we provide an example . \n\t', ""\n\t\t Let W = ( V , NT , T , 5 , m^ ) , '9^ ) ) be a CWgrammar such that the set of variable , non-terminals pseudo-rules 5 '9^ )1 Adj Noun Adj '9^ )0.1 big Noun '9^ )1 ball ... and terminals are defined as follows : V = { Adj } , NT = { 5 , Adj , Noun } , T = { ball , big , fat , red , green , ... } . \n\t"", '\n\t\t As usual , the numbers attached to the arrows indicate the probabilities of the rules . \n\t', '\n\t\t The rules defined by W have the following shape : 5 =w=.> Adj* Noun . \n\t', '\n\t\t Suppose now that we want to build the rule 5 =w=.> Adj Adj Noun . \n\t', ""\n\t\t We take the pseudo-rule 5 '9^ ) 1 Adj Noun and instantiate the Adj m^ )0.5 Adj Adj Adj m^ )0.5 Adj meta-rules variable Adj with Adj Adj to get the desired rule . \n\t"", '\n\t\t The probability for it is 1 x 0.5 x 0.5 , that is , the probability of the derivation for Adj Adj times the probability of the pseudo-rule used . \n\t', '\n\t\t Trees for this particular grammar are flat , with a main node 5 and all the adjectives in it as daughters . \n\t', '\n\t\t An example derivation is given in Figure 1(a) . \n\t', '\n\t\t 3.2 From Automata to Grammars Now that we have introduced PCW-grammars , we describe how we build them from the automata that we are going to induce in Section 4 . \n\t', '\n\t\t Since we will induce two families of automata ( \x93Many- Automata\x94 where we use two automata per POS , and \x93One-Automaton\x94 where we use only two automata to fit every POS ) , we need to describe two automata-to-grammar transformations . \n\t', '\n\t\t Let\x92s start with the case where we build two automata per POS . \n\t', '\n\t\t Let w be a POS in the PTB ; let AwL and AwR be the two automata associated to it . \n\t', '\n\t\t Let GwL and GwR be the PCFGs equivalent to AwLandAwR , respectively , following \n\t\t']",Positive
"[""\n\t\t We build our final grammar G with starting symbol 5 , by defining its meta-rules as the disjoint union of all rules in GwL and GwR ( for all POS w ) , its set of pseudo-rules as the union of the sets { W '9^ )1 5wLw5wR and 5 '9^ )1 5wLw5wR } , where W is a unique new variable symbol associated to w . \n\t"", '\n\t\t When we use two automata for all parts of speech , the grammar is defined as follows . \n\t', '\n\t\t Let AL and AR be the two automata learned . \n\t', '\n\t\t Let GL and GR be the PCFGs equivalent to AL and AR , and let 5L and 5R be the starting symbols of GL and GR , respectively . \n\t', '\n\t\t Fix a POS w in the PTB . \n\t', '\n\t\t Since the automata are deterministic , there exist states 5wL and 5wR that are reachable from 5L and 5R , respectively , by following the arc labeled with w. Define a grammar as in the previous case . \n\t', ""\n\t\t Its starting symbol is 5 , its set of meta-rules is the disjoint union of all rules in GwL and GwR ( for all POS w ) , its set of pseudo- rules is { W '9^ )1 5wLw5wR,5 '9^ )1 5wLw5wR : w is a POS in the PTB and W is a unique new variable symbol associated to w } . \n\t"", '\n\t\t 3.3 Parsing PCW-Grammars Parsing PCW-grammars requires two steps : a generation-rule step followed by a tree-building step . \n\t', '\n\t\t We now explain how these two steps can be carried out in one go . \n\t', '\n\t\t Parsing with PCW-grammars can be viewed as parsing with PCF grammars . \n\t', '\n\t\t The main difference is that in PCW-parsing derivations for variables remain hidden in the final tree . \n\t', '\n\t\t To clarify this , consider the trees depicted in Figure 1 ; the tree in part ( a ) is the CW-tree corresponding to the word red big green ball , and the tree in part ( b ) is the same tree but now the instantiations of the meta- rules that were used have been made visible . \n\t', '\n\t\t ( a ) ( b ) Figure 1 : ( a ) A tree generated by W. ( b ) The same tree with meta-rule derivations made visible . \n\t', '\n\t\t To adapt a PCFG to parse CW-grammars , we need to define a PCF grammar for a given PCWgrammar by adding the two sets of rules while making sure that all meta-rules have been marked somehow . \n\t', '\n\t\t In Figure 1(b) the head symbols of meta-rules have been marked with the superscript 1 . \n\t', '\n\t\t After parsing the sentence with the PCF parser , all marked rules should be collapsed as shown in part ( a ) . \n\t', '\n\t\t 4 Building Automata The four grammars we intend to induce are completely defined once the underlying automata have been built . \n\t', '\n\t\t We now explain how we build those automata from the training material . \n\t', '\n\t\t We start by detailing how the material is generated . \n\t', '\n\t\t 4.1 Building the Sample Sets We transform the PTB , sections 2\x9622 , to dependency structures , as suggested by \n\t\t']",Positive
"['\n\t\t All sentences containing CC tags are filtered out , following \n\t\t']",Positive
"['\n\t\t We also eliminate all word information , leaving only POS tags . \n\t', '\n\t\t For each resulting dependency tree we extract a sample set of right and left sequences of dependents as shown in Figure 2 . \n\t', '\n\t\t From the tree we generate a sample set with all right sequences of dependents { E , E , E } , and another with all left sequences { E , E , red big green } . \n\t', '\n\t\t The sample set used for automata induction is the union of all individual tree sample sets . \n\t', '\n\t\t 4.2 Learning Probabilistic Automata Probabilistic deterministic finite state automata ( PDFA ) inference is the problem of inducing a stochastic regular grammar from a sample set of strings belonging to an unknown regular language . \n\t', '\n\t\t The most direct approach for solving the task is by Adj 1 Adj 1 Adj 1 Adj red S Noun ball Adj green Adj big Adj red Adj big S Adj green Noun ball left right left right left right e e e e red big green e ( c ) Figure 2 : ( a ) , ( b ) Dependency representations of Figure 1. ( c ) Sample instances extracted from this tree . \n\t', '\n\t\t using n-grams . \n\t', '\n\t\t The n-gram induction algorithm adds a state to the resulting automaton for each sequence of symbols of length n it has seen in the training material ; it also adds an arc between states aQ and Qb labeled b , if the sequence aQb appears in the training set . \n\t', '\n\t\t The probability assigned to the arc ( aQ , Qb ) is proportional to the number of times the sequence aQb appears in the training set . \n\t', '\n\t\t For the remainder , we take n-grams to be bigrams . \n\t', '\n\t\t There are other approaches to inducing regular grammars besides ones based on n-grams . \n\t', '\n\t\t The first algorithm to learn PDFAs was ALERGIA \n\t\t']",Positive
['\n\t\t The Minimum Discrimination Information ( MDI ) algorithm \n\t\t'],Positive
"['\n\t\t We opted for the MDI algorithm as an alternative to n-gram based induction algorithms , mainly because their working principles are radically different from the n-gram-based algorithm . \n\t', '\n\t\t The MDI algorithm first builds an automaton that only accepts the strings in the sample set by merging common prefixes , thus producing a tree-shaped automaton in which each transition has a probability proportional to the number of times it is used while generating the positive sample . \n\t', '\n\t\t The MDI algorithm traverses the lattice of all possible partitions for this general automaton , attempting to merge states that satisfy a trade-off that can be specified by the user . \n\t', '\n\t\t Specifically , assume that A1 is a temporary solution of the algorithm and that A2 is a tentative new solution derived from A1 . \n\t', '\n\t\t A(A1,A2) = D(A0IIA2) \x97 D(A0IIA1) de- notes the divergence increment while going from A1 to A2 , where D(A0I I AZ ) is the Kullback-Leibler divergence or relative entropy between the two distributions generated by the corresponding au- tomata \n\t\t']",Positive
"['\n\t\t The new solution A2 is compatible with the training data if the divergence increment relative to the size reduction , that is , the reduction of the number of states , is small enough . \n\t', '\n\t\t Formally , let alpha denote a compatibility threshold ; then the compatibility is satisfied if A(A1,A2) < alpha . \n\t', '\n\t\t For this learning algorithm , IA11^1A21 alpha is the unique parameter ; we tuned it to get better quality automata . \n\t', '\n\t\t 4.3 Optimizing Automata We use three measures to evaluate the quality of a probabilistic automaton ( and set the value of alpha optimally ) . \n\t', '\n\t\t The first , called test sample perplexity ( PP ) , is based on the per symbol log- likelihood of strings x belonging to a test sample according to the distribution defined by the au- tomaton . \n\t', '\n\t\t Formally , LL = \x97 Is1 Px^S log ( P( where P(x) is the probability assigned to the string x by the automata . \n\t', '\n\t\t The perplexity PP is defined as PP = 2LL . \n\t', '\n\t\t The minimal perplexity PP = 1 is reached when the next symbol is always predicted with probability 1 from the current state , while PP = I E I corresponds to uniformly guessing from an alphabet of size I E I. The second measure we used to evaluate the quality of an automaton is the number of missed samples ( MS ) . \n\t', '\n\t\t A missed sample is a string in the test sample that the automaton failed to accept . \n\t', '\n\t\t One such instance suffices to have PP undefined ( LL infinite ) . \n\t', '\n\t\t Since an undefined value of PP only witnesses the presence of at least one MS we decided to count the number of MS separately , and compute PP without taking MS into account . \n\t', '\n\t\t This choice leads to a more accurate value of PP , while , moreover , the value of MS provides us with information about the generalization capacity of automata : the lower the value of MS , the larger the generalization capacities of the automaton . \n\t', '\n\t\t The usual way to circumvent undefined perplexity is to smooth the resulting automaton with unigrams , thus increasing the generalization capacity of the automaton , which is usually paid for with an increase in perplexity . \n\t', '\n\t\t We decided not to use any smoothing techniques as we want to compare bigram-based automata with MDI-based automata in the cleanest possible way . \n\t', '\n\t\t The PP and MS measures are relative to a test sample ; we transformed section 00 of the PTB to obtain one.1 1If smoothing techniques are used for optimizing automata based on n-grams , they should also be used for optimizing MDI-based automata . \n\t', '\n\t\t A fair experiment for comparing the two automata-learning algorithms using smoothing techniques would consist of first building two pairs of automata . \n\t', '\n\t\t The first pair would consist of the unigram-based automaton together jj jj nn S nn red big green ball JJ JJ JJ jj red jj big jj green ball ( a ) ( b ) x ) ) , The third measure we used to evaluate the quality of automata concerns the size of the automata . \n\t', '\n\t\t We compute NumEdges and NumStates ( the number of edges and the number of states of the automaton ) . \n\t', '\n\t\t We used PP , US , NumEdges , and NumStates to compare automata . \n\t', '\n\t\t We say that one automaton is of a better quality than another if the values of the 4 indicators are lower for the first than for the second . \n\t', '\n\t\t Our aim is to find a value of alpha that produces an automaton of better quality than the bigram-based counterpart . \n\t', '\n\t\t By exhaustive search , using all training data , we determined the optimal value of alpha . \n\t', '\n\t\t We selected the value of alpha for which the MDI-based automaton outperforms the bigram-based one.2 We exemplify our procedure by considering automata for the \x93One-Automaton\x94 setting ( where we used the same automata for all parts of speech ) . \n\t', '\n\t\t In Figure 3 we plot all values of PP and MS computed for different values of alpha , for each training set ( i.e. , left and right ) . \n\t', '\n\t\t From the plots we can identify values of alpha that produce automata having better values of PP and MS than the bigram-based ones . \n\t', '\n\t\t All such alphas are the ones inside the marked areas ; automata induced using those alphas possess a lower value of PP as well as a smaller number of MS , as required . \n\t', '\n\t\t Based on these explorations MDI Bigrams Right Left Right Left NumEdges NumStates 268 328 20519 16473 12 15 844 755 Table 1 : Automata sizes for the \x93One-Automaton\x94 case , with alpha = 0.0001. we selected alpha = 0.0001 for building the au tomata used for grammar induction in the \x93One- Automaton\x94 case . \n\t', '\n\t\t Besides having lower values of PP and MS , the resulting automata are smaller than the bigram based automata ( Table 1 ) . \n\t', '\n\t\t MDI com- presses information better ; the values in the tables with an MDI-based automaton outperforming the unigrambased one . \n\t', '\n\t\t The second one , a bigram-based automata together with an MDI-based automata outperforming the bigram-based one . \n\t', '\n\t\t Second , the two n-gram based automata smoothed into a single automaton have to be compared against the two MDIbased automata smoothed into a single automaton . \n\t', '\n\t\t It would be hard to determine whether the differences between the final automata are due to smoothing procedure or to the algorithms used for creating the initial automata . \n\t', '\n\t\t By leaving smoothing out of the picture , we obtain a clearer understanding of the differences between the two automata induction algorithms . \n\t', '\n\t\t 2An equivalent value of alpha can be obtained independently of the performance of the bigram-based automata by defining a measure that combines PP and MS . \n\t', '\n\t\t This measure should reach its maximum when PP and MS reach their minimums . \n\t', '\n\t\t suggest that MDI finds more regularities in the sample set than the bigram-based algorithm . \n\t', '\n\t\t To determine optimal values for the \x93Many- Automata\x94 case ( where we learned two automata for each POS ) we used the same procedure as for the \x93One-Automaton\x94 case , but now for every individual POS . \n\t', '\n\t\t Because of space constraints we are not able to reproduce analogues of Figure 3 and Table 1 for all parts of speech . \n\t', '\n\t\t Figure 4 contains representative plots ; the remaining plots are available online at http : //www. science . \n\t', '\n\t\t uva.nl/\x98infante/POS . \n\t', '\n\t\t Besides allowing us to find the optimal alphas , the plots provide us with a great deal of information . \n\t', '\n\t\t For instance , there are two remarkable things in the plots for VBP ( Figure 4 , second row ) . \n\t', '\n\t\t First , it is one of the few examples where the bigrambased algorithm performs better than the MDI algorithm . \n\t', '\n\t\t Second , the values of PP in this plot are relatively high and unstable compared to other POS plots . \n\t', '\n\t\t Lower perplexity usually implies better quality automata , and as we will see in the next section , better automata produce better parsers . \n\t', '\n\t\t How can we obtain lower PP values for the VBP automata ? \n\t', '\n\t\t The class of words tagged with VBP harbors many different behaviors , which is not surprising , given that verbs can differ widely in terms of , e.g. , their sub- categorization frames . \n\t', '\n\t\t One way to decrease the PP values is to split the class of words tagged with VBP into multiple , more homogeneous classes . \n\t', '\n\t\t Note from Figures 3 and 4 that splitting the original sample sets into POS-dependent sets produces a huge decrease on PP . \n\t', '\n\t\t One attempt to implement this idea is lexicalization : increasing the information in the POS tag by adding the lemma to it ( Collins , 1997 ; Sima\x92an , 2000 ) . \n\t', '\n\t\t Lexicalization splits the class of verbs into a family of singletons producing more homogeneous classes , as desired . \n\t', '\n\t\t A different approach \n\t\t']",Positive
"['\n\t\t Some POS present very high perplexities , but tags such as DT present a PP close to 1 ( and 0 MS ) for all values of alpha . \n\t', '\n\t\t Hence , there is no need to introduce further distinctions in DT , doing so will not increase the quality of the automata but will increase their number ; splitting techniques are bound to add noise to the resulting grammars . \n\t', '\n\t\t The plots also indicate that the bigram-based algorithm captures them as well as the MDI algorithm . \n\t', '\n\t\t In Figure 4 , third row , we see that the MDI-based automata and the bigram-based automata achieve the same value of PP ( close to 5 ) for NN , but Unique Automaton - Left Side 5e-05 0.0001 0.00015 0.0002 0.00025 0.0003 0.00035 0.0004 Alpha Unique Automaton - Right Side 5e-05 0.0001 0.00015 0.0002 0.00025 0.0003 0.00035 0.0004 Alpha 25 20 15 10 5 0 MDI Perplex . \n\t', '\n\t\t ( PP ) Bigram Perplex . \n\t', '\n\t\t ( PP ) MDI Missed Samples ( MS ) Bigram Missed Samples ( MS ) 30 25 20 15 10 5 0 MDI Perplex . \n\t', '\n\t\t ( PP ) Bigram Perplex . \n\t', '\n\t\t ( PP ) MDI Missed Samples ( MS ) Bigram Missed Samples ( MS ) Figure 3 : Values of PP and MS for automata used in building One-Automaton grammars . \n\t', '\n\t\t ( X-axis ) : alpha . \n\t', '\n\t\t ( Y-axis ) : missed samples ( MS ) and perplexity ( PP ) . \n\t', '\n\t\t The two constant lines represent the values of PP and MS for the bigram-based automata . \n\t', '\n\t\t VBP - LeftSide 9 8 7 6 5 4 3 Alpha VBP - LeftSide 9 8 7 6 5 4 3 Alpha MDI Perplex . \n\t', '\n\t\t ( PP ) Bigram Perplex . \n\t', '\n\t\t ( PP ) MDI Missed Samples ( MS ) Bigram Missed Samples ( MS ) MDI Perplex . \n\t', '\n\t\t ( PP ) Bigram Perplex . \n\t', '\n\t\t ( PP ) MDI Missed Samples ( MS ) Bigram Missed Samples ( MS ) NN - LeftSide NN - RightSide 30 25 20 15 10 5 30 25 20 15 10 5 0 0 Alpha Alpha MDI Perplex . \n\t', '\n\t\t ( PP ) Bigram Perplex . \n\t', '\n\t\t ( PP ) MDI Missed Samples ( MS ) Bigram Missed Samples ( MS ) MDI Perplex . \n\t', '\n\t\t ( PP ) Bigram Perplex . \n\t', '\n\t\t ( PP ) MDI Missed Samples ( MS ) Bigram Missed Samples ( MS ) Figure 4 : Values of PP and MS for automata for ad-hoc automata the MDI misses fewer examples for alphas big ger than 1.4e \x97 04 . \n\t', '\n\t\t As pointed out , we built the One-Automaton-MDI using alpha = 0.0001 and even though the method allows us to fine-tune each alpha in the Many-Automata-MDI grammar , we used a fixed alpha = 0.0002 for all parts of speech , which , for most parts of speech , produces better au- tomata than bigrams . \n\t', '\n\t\t Table 2 lists the sizes of the automata . \n\t', '\n\t\t The differences between MDI-based and bigram-based automata are not as dramatic as in the \x93One-Automaton\x94 case ( Table 1 ) , but the former again have consistently lower NumEdges and NumStates values , for all parts of speech , even where bigram-based automata have a lower perplexity . \n\t', '\n\t\t POS MDI Bigrams Right Left Right Left DT NumEdges 21 14 35 39 NumStates 4 3 25 17 VBP NumEdges 300 204 2596 1311 NumStates 50 45 250 149 NN NumEdges 104 111 3827 4709 NumStates 6 4 284 326 Table 2 : Automata sizes for the three parts of speech in the \x93Many-Automata\x94 case , with alpha = 0.0002 for parts of speech . \n\t', '\n\t\t 5 Parsing the PTB We have observed remarkable differences in quality between MDI-based and bigram-based automata . \n\t', '\n\t\t Next , we present the parsing scores , and discuss the meaning of the measures observed for automata in the context of the grammars they produce . \n\t', '\n\t\t The measure that translates directly from automata to grammars is automaton size . \n\t', '\n\t\t Since each automaton is transformed into a PCFG , the number of rules in the resulting grammar is proportional to the number of arcs in the automaton , and the number of non- terminals is proportional to the number of states . \n\t', '\n\t\t From Table 3 we see that MDI compresses information better : the sizes of the grammars produced by the MDI-based automata are an order of magnitude smaller that those produced using bigram-based automata . \n\t', '\n\t\t Moreover , the \x93One-Automaton\x94 versions substantially reduce the size of the resulting grammars ; this is obviously due to the fact that all POS share the same underlying automaton so that information does not need to be duplicated across parts of speech . \n\t', '\n\t\t To understand the meaning of PP and One Automaton Many Automata MDI Bigram MDI Bigram 702 38670 5316 68394 Table 3 : Number of rules in the grammars built . \n\t', '\n\t\t MS in the context of grammars it helps to think of PCW-parsing as a two-phase procedure . \n\t', '\n\t\t The first phase consists of creating the rules that will be used in the second phase . \n\t', '\n\t\t And the second phase consists in using the rules created in the first phase as a PCFG and parsing the sentence using a PCF parser . \n\t', '\n\t\t Since regular expressions are used to build rules , the values of PP and MS quantify the quality of the set of rules built for the second phase : MS gives us a measure of the number rule bodies that should be created but that will not be created , and , hence , it gives us a measure of the number of \x93correct\x94 trees that will not be produced . \n\t', '\n\t\t PP tells us how uncertain the first phase is about producing rules . \n\t', '\n\t\t Finally , we report on the parsing accuracy . \n\t', '\n\t\t We use two measures , the first one ( %Words ) was proposed by \n\t\t']",Positive
"['\n\t\t Lin\x92s measure computes the fraction of words that have been attached to the right word . \n\t', '\n\t\t The second one ( %POS ) marks as correct a word attachment if , and only if , the POS tag of the head is the same as that of the right head , i.e. , the word was attached to the correct word-class , even though the word is not the correct one in the sentence . \n\t', '\n\t\t Clearly , the second measure is always higher than the first one . \n\t', '\n\t\t The two measures try to capture the performance of the PCW-parser in the two phases described above : ( %POS ) tries to capture the performance in the first phase , and ( %Words ) in the second phase . \n\t', '\n\t\t The measures reported in Table 4 are the mean values of ( %POS ) and ( %Words ) computed over all sentences in section 23 having length at most 20 . \n\t', '\n\t\t We parsed only those sentences because the resulting grammars for bigrams are too big : parsing all sentences without any serious pruning techniques was simply not feasible . \n\t', '\n\t\t From Table 4 MDI Bigrams %Words %POS %Words %POS One-Aut . \n\t', '\n\t\t Many-Aut . \n\t', '\n\t\t 0.69 0.73 0.59 0.63 0.85 0.88 0.73 0.76 Table 4 : Parsing results for the PTB we see that the grammars induced with MDI outperform the grammars created with bigrams . \n\t', '\n\t\t Moreover , the grammar using different automata per POS outperforms the ones built using only a single automaton per side ( left or right ) . \n\t', '\n\t\t The results suggest that an increase in quality of the automata has a direct impact on the parsing performance . \n\t', '\n\t\t 6 Related Work and Discussion Modeling rule bodies is a key component of parsers . \n\t', '\n\t\t N-grams have been used extensively for this purpose ( Collins 1996 , 1997 ; Eisner , 1996 ) . \n\t', '\n\t\t In these formalisms the generative process is not considered in terms of probabilistic regular languages . \n\t', '\n\t\t Considering them as such ( like we do ) has two advantages . \n\t', '\n\t\t First , a vast area of research for inducing regular languages \n\t\t']",Positive
"['\n\t\t Second , the parsing device itself can be viewed under a unifying grammatical paradigm like PCW-grammars \n\t\t']",Positive
"['\n\t\t As PCWgrammars are PCFGs plus post tree transformations , properties of PCFGs hold for them too \n\t\t']",Positive
"['\n\t\t In our comparison we optimized the value of alpha , but we did not optimize the n-grams , as doing so would mean two different things . \n\t', '\n\t\t First , smoothing techniques would have to be used to combine different order n-grams . \n\t', '\n\t\t To be fair , we would also have to smooth different MDI-based automata , which would leave us in the same point . \n\t', '\n\t\t Second , the degree of the n-gram . \n\t', '\n\t\t We opted for n = 2 as it seems the right balance of informativeness and generalization . \n\t', '\n\t\t N-grams are used to model sequences of arguments , and these hardly ever have length > 3 , making higher degrees useless . \n\t', '\n\t\t To make a fair comparison for the Many-Automata grammars we did not tune the MDI-based automata individually , but we picked a unique alpha . \n\t', '\n\t\t MDI presents a way to compact rule information on the PTB ; of course , other approaches exists . \n\t', '\n\t\t In particular , \n\t\t']",Positive
"['\n\t\t The attempt to use algorithms other than n-grams-based for inducing of regular languages in the context of grammar induction is not new ; for example , Kruijff(2003) uses profile hidden models in an attempt to quantify free order variations across languages ; we are not aware of evaluations of his grammars as parsing devices . \n\t', '\n\t\t 7 Conclusions and Future Work Our experiments support two kinds of conclusions . \n\t', '\n\t\t First , modeling rules with algorithms other than n-grams not only produces smaller grammars but also better performing ones . \n\t', '\n\t\t Second , the procedure used for optimizing alpha reveals that some POS behave almost deterministically for selecting their arguments , while others do not . \n\t', '\n\t\t These findings suggests that splitting classes that behave non- deterministically into homogeneous ones could improve the quality of the inferred automata . \n\t', '\n\t\t We saw that lexicalization and head-annotation seem to attack this problem . \n\t', '\n\t\t Obvious questions for future work arise : Are these two techniques the best way to split non-homogeneous classes into homogeneous ones ? \n\t', '\n\t\t Is there an optimal splitting ? \n\t', '\n\t\t Acknowledgments We thank our referees for valuable comments . \n\t', '\n\t\t Both authors were supported by the Netherlands Organization for Scientific Research ( NWO ) under project number 220-80-001 . \n\t', '\n\t\t De Rijke was also supported by grants from NWO , under project numbers 365- 20-005 , 612.069.006 , 612.000.106 , 612.000.207 , and 612.066.302 . \n\t', '\n\t\t References S. Abney , D. McAllester , and F. Pereira . \n\t', '\n\t\t 1999. Relating probabilistic grammars and automata . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 37th Annual Meeting of the ACL , pages 542\x96549 . \n\t', '\n\t\t T. Booth and R. Thompson . \n\t', '\n\t\t 1973. Applying probability measures to abstract languages . \n\t', '\n\t\t IEEE Transaction on Computers , C-33(5):442\x96450 . \n\t', '\n\t\t R. Carrasco and J. Oncina . \n\t', '\n\t\t 1994. Learning stochastic regular grammars by means of state merging method . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ICGI-94 , Springer , pages 139\x96150 . \n\t', '\n\t\t E. Charniak . \n\t', '\n\t\t 1997. Statistical parsing with a context- free grammar and word statistics . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 14th Nat . \n\t', '\n\t\t Conf . \n\t', '\n\t\t on Artificial Intelligence , pages 598\x96603 . \n\t', '\n\t\t G. Chastellier and A. Colmerauer . \n\t', '\n\t\t 1969. W-grammar . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 1969 24th National Conf. , pages 511\x96518 . \n\t', '\n\t\t M. Collins . \n\t', '\n\t\t 1996. A new statistical parser based on bigram lexical dependencies . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 34th Annual Meeting of the ACL , pages 184\x96191 . \n\t', '\n\t\t M. Collins . \n\t', '\n\t\t 1997. Three generative , lexicalized models for statistical parsing . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 35th Annual Meeting of the ACL and 8th Conf . \n\t', '\n\t\t of the EACL , pages 16\x9623 . \n\t', '\n\t\t M. Collins . \n\t', '\n\t\t 1999. Head-Driven Statistical Models for Natural Language Parsing . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , University of Pennsylvania , PA . \n\t', '\n\t\t M. Collins . \n\t', '\n\t\t 2000. Discriminative reranking for natural language parsing . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ICML-2000 , Stanford , Ca. T. . \n\t', '\n\t\t Cover and J. Thomas . \n\t', '\n\t\t 1991. Elements ofInformation Theory . \n\t', '\n\t\t Jonh Wiley and Sons , New York . \n\t', '\n\t\t F. Denis . \n\t', '\n\t\t 2001. Learning regular languages from simple positive examples . \n\t', '\n\t\t Machine Learning , 44(1/2):37\x9666 . \n\t', '\n\t\t P. Dupont and L. Chase . \n\t', '\n\t\t 1998. Using symbol clustering to improve probabilistic automaton inference . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ICGI-98 , pages 232\x96243 . \n\t', '\n\t\t J. Eisner . \n\t', '\n\t\t 1996. Three new probabilistic models for dependency parsing : An exploration . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t COLING96 , pages 340\x96245 , Copenhagen , Denmark . \n\t', '\n\t\t J. Eisner . \n\t', '\n\t\t 2000. Bilexical grammars and their cubic-time parsing algorithms . \n\t', '\n\t\t In Advances in Probabilistic and Other Parsing Technologies , pages 29\x9662 . \n\t', '\n\t\t Kluwer . \n\t', '\n\t\t E. M. . \n\t', '\n\t\t Gold . \n\t', '\n\t\t 1967. Language identification in the limit . \n\t', '\n\t\t Information and Control , 10:447\x96474 . \n\t', '\n\t\t G. Infante-Lopez and M. de Rijke . \n\t', '\n\t\t 2003. Natural language parsing with W-grammars . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t CLIN 2003 . \n\t', '\n\t\t D. Klein and C. Manning . \n\t', '\n\t\t 2003 . \n\t', '\n\t\t Accurate unlexicalized parsing . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 41st Annual Meeting of the ACL . \n\t', '\n\t\t A. Krotov , M. Hepple , R.J. Gaizauskas , and Y. Wilks . \n\t', '\n\t\t 1998. Compacting the Penn Treebank grammar . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t COLING-ACL , pages 699\x96703 . \n\t', '\n\t\t G. Kruijff . \n\t', '\n\t\t 2003. 3-phase grammar learning . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t Workshop on Ideas and Strategies for Multilingual Grammar Development . \n\t', '\n\t\t D. Lin . \n\t', '\n\t\t 1995. A dependency-based method for evaluating broad-coverage parsers . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t IJCAI-95 . \n\t', '\n\t\t K. Sima\x92an . \n\t', '\n\t\t 2000. Tree-gram Parsing : Lexical Dependencies and Structual Relations . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 38th Annual Meeting of the ACL , pages 53\x9660 , Hong Kong , China . \n\t', '\n\t\t F. Thollard , P. Dupont , and C. de la Higuera . \n\t', '\n\t\t 2000. Probabilistic DFA inference using kullback-leibler divergence and minimality . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ICML 2000. \n\t', '\n\t\t Adaptive Chinese Word Segmentation1 Jianfeng Gao* , Andi Wu* , Mu Li* , Chang-Ning Huang* , Hongqiao Li** , Xinsong Xia $ , Haowei Qin& * Microsoft Research . \n\t', '\n\t\t { jfgao , andiwu , muli , cnhuang}@microsoft.com ** Beijing Institute of Technology , Beijing . \n\t', '\n\t\t lhqtxm@bit.edu.cn $ Peking University , Beijing . \n\t', '\n\t\t xia_xinsong@founder.com & Shanghai Jiaotong university , Shanghai . \n\t', '\n\t\t haoweiqin@sjtu.edu.cn Abstract This paper presents a Chinese word segmentation system which can adapt to different domains and standards . \n\t', '\n\t\t We first present a statistical framework where domain-specific words are identified in a unified approach to word segmentation based on linear models . \n\t', '\n\t\t We explore several features and describe how to create training data by sampling . \n\t', '\n\t\t We then describe a transformation-based learning method used to adapt our system to different word segmentation standards . \n\t', '\n\t\t Evaluation of the proposed system on five test sets with different standards shows that the system achieves state- of-the-art performance on all of them . \n\t', '\n\t\t 1 Introduction Chinese word segmentation has been a long- standing research topic in Chinese language processing . \n\t', '\n\t\t Recent development in this field shows that , in addition to ambiguity resolution and unknown word detection , the usefulness of a Chinese word segmenter also depends crucially on its ability to adapt to different domains of texts and different segmentation standards . \n\t', '\n\t\t The need of adaptation involves two research issues that we will address in this paper . \n\t', '\n\t\t The first is new word detection . \n\t', '\n\t\t Different domains/applications may have different vocabularies which contain new words/terms that are not available in a general dictionary . \n\t', '\n\t\t In this paper , new words refer to OOV words other than named entities , factoids and morphologically derived words . \n\t', '\n\t\t These words are mostly domain specific terms ( e.g. OnA \x91cellular\x92 ) and time-sensitive political , social or cultural terms ( e.g. _~\x91Three Links\x92 , 4-A \x91SARS\x92 ) . \n\t', '\n\t\t The second issue concerns the customizable display of word segmentation . \n\t', '\n\t\t Different Chinese NLP-enabled applications may have different requirements that call for different granularities of word segmentation . \n\t', '\n\t\t For example , speech recognition systems prefer \x93longer words\x94 to achieve higher accuracy whereas information retrieval systems prefer \x93shorter words\x94 to obtain higher recall rates , etc. \n\t\t']",Positive
"['\n\t\t Given a word segmentation specification ( or standard ) and/or some application data used as training data , a segmenter with customizable display should be able to provide alternative segmentation units according to the specification which is either pre-defined or implied in the data . \n\t', '\n\t\t In this paper , we first present a statistical framework for Chinese word segmentation , where various problems of word segmentation are solved simultaneously in a unified approach . \n\t', '\n\t\t Our approach is based on linear models where component models are inspired by the source-channel models of Chinese sentence generation . \n\t', '\n\t\t We then describe in detail how the new word identification ( NWI ) problem is handled in this framework . \n\t', '\n\t\t We explore several features and describe how to create training data by sampling . \n\t', '\n\t\t We evaluate the performance of our segmentation system using an annotated test set , where new words are simulated by sampling . \n\t', '\n\t\t We then describe a transformation-based learning ( TBL , Brill , 1995 ) method that is used to adapt our system to different segmentation standards . \n\t', '\n\t\t We compare the adaptive system to other state-of-the-art systems using four test sets in the SIGHAN\x92s First International Chinese Word Segmentation Bakeoff , each of which is constructed according to a different segmentation standard . \n\t', '\n\t\t The performance of our system is comparable to the best systems reported on all four test sets . \n\t', '\n\t\t It demonstrates the possibility of having a single adaptive Chinese word segmenter that is capable of supporting multiple user applications . \n\t', '\n\t\t 1 This work was done while Hongqiao Li , Xinsong Xia and Haowei Qin were visiting Microsoft Research ( MSR ) Asia . \n\t', '\n\t\t We thank Xiaodan Zhu for his early contribution , and the three reviewers , one of whom alerted us the related work of \n\t\t']",Positive
"['\n\t\t Word Class2 Model Feature Functions , f(S , W ) Context Model Word class based trigram , P(W) . \n\t', '\n\t\t -log(P(W)) Lexical Word ( LW ) --- 1 if S forms a word lexicon entry , 0 otherwise . \n\t', '\n\t\t Morphological Word ( MW ) --- 1 if S forms a morph lexicon entry , 0 otherwise . \n\t', '\n\t\t Named Entity ( NE ) Character/word bigram , P(S|NE) . \n\t', '\n\t\t -log(P(S|NE)) Factoid ( FT ) --- 1 if S can be parsed using a factoid grammar , 0 otherwise New Word ( NW ) --- Score of SVM classifier Figure 1 : Context model , word classes , and class models , and feature functions . \n\t', '\n\t\t 2 Chinese Word Segmentation with Linear Models Let S be a Chinese sentence which is a character string . \n\t', '\n\t\t For all possible word segmentations W , we will choose the most likely one W* which achieves the highest conditional probability P(W|S) : W* argmaxw P(W|S) . \n\t', '\n\t\t According to Bayes\x92 decision rule and dropping the constant denominator , we can equivalently perform the following maximization : W Equation ( 1 ) represents a source-channel approach to Chinese word segmentation . \n\t', '\n\t\t This approach models the generation process of a Chinese sentence : first , the speaker selects a sequence of concepts W to output , according to the probability distribution P(W) ; then he attempts to express each concept by choosing a sequence of characters , according to the probability distribution P(S|W) . \n\t', '\n\t\t We define word class as a group of words that are supposed to be generated according to the same distribution ( or in the same manner ) . \n\t', '\n\t\t For instance , all Chinese person names form a word class . \n\t', '\n\t\t We then have multiple channel models , each for one word class . \n\t', '\n\t\t Since a channel model estimates the likelihood that a character string is generated given a word class , it is also referred to as class model . \n\t', '\n\t\t Similarly , source model is referred to as context model because it indicates the likelihood that a word class occurs in a context . \n\t', '\n\t\t We have only one context model which is a word-class-based trigram model . \n\t', '\n\t\t Figure 1 shows word classes and class models that we used in our system . \n\t', '\n\t\t We notice that different class models are constructed in different ways ( e.g. name entity models are n-gram models trained on corpora whereas factoid models use derivation rules and have binary values ) . \n\t', '\n\t\t The dynamic value ranges of different class models can be so different that it is improper to combine all models through simple multiplication as Equation ( 1 ) . \n\t', '\n\t\t In this study we use linear models . \n\t', '\n\t\t The method is derived from linear discriminant functions widely used for pattern classification \n\t\t']",Positive
['\n\t\t It is also related to log- linear models for machine translation \n\t\t'],Positive
"['\n\t\t In this framework , we have a set of M+1 feature functions fi(S , W ) , i = 0,...,M . \n\t', '\n\t\t They are derived from the context model ( i.e. f0(W)) and M class models , each for one word class , as shown in Figure 1 : For probabilistic models such as the context model or person name model , the feature functions are defined as the negative logarithm of the corresponding probabilistic models . \n\t', '\n\t\t For each feature function , there is a model parameter ^i . \n\t', '\n\t\t The best word segmentation W* is determined by the decision rule as M W* = arg max Score(4 , S , W ) = arg max ^ Ai f , . \n\t', '\n\t\t ( S , W ) ( 2 ) W W i=0 Below we describe how to optimize As . \n\t', '\n\t\t Our method is a discriminative approach inspired by the Minimum Error Rate Training method proposed in \n\t\t']",Positive
"['\n\t\t Assume that we can measure the number of segmentation errors in W by comparing it with a reference segmentation R using a function Er(R,W) . \n\t', '\n\t\t The training criterion is to minimize the count of errors over the training data as where W is detected by Equation ( 2 ) . \n\t', '\n\t\t However , we cannot apply standard gradient descent to optimize W*=arg maxPWPS | W ) . \n\t', '\n\t\t ( 1 ) M ^ arg min ~M , ( 3 ) ^ ( ErR,WS , ^ 1 = ^1 = S,W,R 2 In our system , we define three types of named entity : person name ( PN ) , location name ( LN ) , organization ( ON ) and transliteration name ( TN ) ; ten types of factoid : date , time ( TIME ) , percentage , money , number ( NUM ) , measure , e-mail , phone number , and WWW ; and five types of morphologically derived words ( MDW ) : affixation , reduplication , merging , head particle and split . \n\t', '\n\t\t Initialization : A0=a , Ai=1 , i = 1,...,M . \n\t', '\n\t\t For t = 1 ... \n\t', '\n\t\t T , j = 1 ... \n\t', '\n\t\t N Wj = argmax Y_ Ai fi(Sj , W ) For i = 1 ... \n\t', '\n\t\t M Ai = Ai + ^(Score(^,S,W)-Score(^,S,R))(fi(R) - fi(W)) , where ^= { A0 , A1,...,AM } and Y7 =0.001 . \n\t', '\n\t\t Figure 2 : The training algorithm for model parameters model parameters according to Equation ( 3 ) because the gradient cannot be computed explicitly ( i.e. , Er is not differentiable ) , and there are many local minima in the error surface . \n\t', '\n\t\t We then use a variation called stochastic gradient descent ( or unthresholded perceptron , Mitchell , 1997 ) . \n\t', '\n\t\t As shown in Figure 2 , the algorithm takes T passes over the training set ( i.e. N sentences ) . \n\t', '\n\t\t All parameters are initially set to be 1 , except for the context model parameter A0 which is set to be a constant a during training , and is estimated separately on held-out data . \n\t', '\n\t\t Class model parameters are updated in a simple additive fashion . \n\t', '\n\t\t Notice that Score(^ , S , W ) is not less than Score(~,,S,R) . \n\t', '\n\t\t Intuitively the updated rule increases the parameter values for word classes whose models were \x93underestimated\x94 ( i.e. expected feature value f(W) is less than observed feature value f(R)) , and decreases the parameter values whose models were \x93overestimated\x94 ( i.e. f(W) is larger than f(R)) . \n\t', '\n\t\t Although the method cannot guarantee a global optimal solution , it is chosen for our modeling because of its efficiency and the best results achieved in our experiments . \n\t', '\n\t\t Given the linear models , the procedure of word segmentation in our system is as follows : First , all word candidates ( lexical words and OOV words of certain types ) are generated , each with its word class tag and class model score . \n\t', '\n\t\t Second , Viterbi search is used to select the best W according to Equation ( 2 ) . \n\t', '\n\t\t Since the resulting W* is a sequence of segmented words that are either lexical words or OOV words with certain types ( e.g. person name , morphological words , new words ) we then have a system that can perform word segmentation and OOV word detection simultaneously in a unified approach . \n\t', '\n\t\t Most previous works treat OOV word detection as a separate step after word segmentation . \n\t', '\n\t\t Compared to these approaches , our method avoids the error propagation problem and can incorporate a variety of knowledge to achieve a globally optimal solution . \n\t', '\n\t\t The superiority of the unified approach has been demonstrated empirically in \n\t\t']",Positive
"['\n\t\t 3 New Word Identification New words in this section refer to OOV words that are neither recognized as named entities or factoids nor derived by morphological rules . \n\t', '\n\t\t These words are mostly domain specific and/or time-sensitive . \n\t', '\n\t\t The identification of such new words has not been studied extensively before . \n\t', '\n\t\t It is an important issue that would have substantial impact on the performance of word segmentation . \n\t', '\n\t\t For example , approximately 30 % of OOV words in the SIGHAN\x92s PK corpus ( see Table 1 ) are new words of this type . \n\t', '\n\t\t There has been previous work on detecting Chinese new words from a large corpus in an off-line manner and updating the dictionary before word segmentation . \n\t', '\n\t\t However , our approach is able to detect new words on-line , i.e. to spot new words in a sentence on the fly during the process of word segmentation where widely-used statistical features such as mutual information or term frequency are not available . \n\t', '\n\t\t For brevity of discussion , we will focus on the identification of 2-character new words , denoted as NW_11 . \n\t', '\n\t\t Other types of new words such as NW_21 ( a 2-character word followed with a character ) and NW_12 can be detected similarly ( e.g. by viewing the 2-character word as an inseparable unit , like a character ) . \n\t', '\n\t\t Below , we shall describe the class model and context model for NWI , and the creation of training data by sampling . \n\t', '\n\t\t 3.1 Class Model We use a classifier ( SVM in our experiments ) to estimate the likelihood of two adjacent characters to form a new word . \n\t', '\n\t\t Of the great number of features we experimented , three linguistically-motivated features are chosen due to their effectiveness and availability for on-line detection . \n\t', '\n\t\t They are Independent Word Probability ( IWP ) , Anti-Word Pair ( AWP ) , and Word Formation Analogy ( WFA ) . \n\t', '\n\t\t Below we describe each feature in turn . \n\t', '\n\t\t In Section 3.2 , we shall describe the way the training data ( new word list ) for the classifier is created by sampling . \n\t', '\n\t\t IWP is a real valued feature . \n\t', '\n\t\t Most Chinese characters can be used either as independent words or component parts of multi-character words , or both . \n\t', '\n\t\t The IWP of a single character is the likelihood for this character to appear as an independent word in texts \n\t\t']",Positive
"['\n\t\t We assume that the IWP of a character string is the product of the IWPs of the component characters . \n\t', '\n\t\t Intuitively , the lower the IWP value , the more likely the character string forms a new word . \n\t', '\n\t\t In our implementation , the training data is word-segmented . \n\t', '\n\t\t AWP is a binary feature derived from IWP . \n\t', '\n\t\t For example , the value of AWP of an NW_11 candidate ab is defined as : AWP(ab)=1 if IWP(a)>^ or IWP(b) >^ , 0 otherwise . \n\t', '\n\t\t ^ ^ [ 0 , 1 ] is a pre-set threshold . \n\t', '\n\t\t Intuitively , if one of the component characters is very likely to be an independent word , it is unlikely to be able to form a word with any other characters . \n\t', '\n\t\t While IWP considers all component characters in a new word candidate , AWP only considers the one with the maximal IWP value . \n\t', '\n\t\t WFA is a binary feature . \n\t', '\n\t\t Given a character pair ( x , y ) , a character ( or a multi-character string ) z is called the common stem of ( x , y ) if at least one of the following two conditions hold : ( 1 ) character strings xz and yz are lexical words ( i.e. x and y as prefixes ) ; and ( 2 ) character strings zx and zy are lexical words ( i.e. x and y as suffixes ) . \n\t', '\n\t\t We then collect a list of such character pairs , called affix pairs , of which the number of common stems is larger than a pre-set threshold . \n\t', '\n\t\t The value of WFA for a given NW_1 1 candidate ab is defined as : WFA(ab) = 1 if there exist an affix pair ( a , x ) ( or ( b , x ) ) and the string xb ( or ax ) is a lexical word , 0 otherwise . \n\t', '\n\t\t For example , given an NW_1 1 candidate ^ ^ ( xia4-gang3 , \x91out of work\x92 ) , we have WFA(^ ^ ) = 1 because ( ^ , ^ ) is an affix pair ( they have 32 common stems such as _ ^ , ^ , ^ , ^ , ^ , ^ , ^ ) and ^ ^ ( shang4-gang3 , \x91take over a shift\x92 ) is a lexical word . \n\t', '\n\t\t 3.2 Context Model The motivations of using context model for NWI are two-fold . \n\t', '\n\t\t The first is to capture useful contextual information . \n\t', '\n\t\t For example , new words are more likely to be nouns than pronouns , and the POS tagging is context-sensitive . \n\t', '\n\t\t The second is more important . \n\t', '\n\t\t As described in Section 2 , with a context model , NWI can be performed simultaneously with other word segmentation tasks ( e.g. : word break , named entity recognition and morphological analysis ) in a unified approach . \n\t', '\n\t\t However , it is difficult to develop a training corpus where new words are annotated because \x93we usually do not know what we don\x92t know\x94 . \n\t', '\n\t\t Our solution is Monte Carlo simulation . \n\t', '\n\t\t We sample a set of new words from our dictionary according to the distribution \x96 the probability that any lexical word w would be a new word P(NW|w) . \n\t', '\n\t\t We then generate a new-word-annotated corpus from a word-segmented text corpus . \n\t', '\n\t\t Now we describe the way P(NW|w) is estimated . \n\t', '\n\t\t It is reasonable to assume that new words are those words whose probability to appear in a new document is lower than general lexical words . \n\t', '\n\t\t Let Pi(k) be the probability of word wi that occurs k times in a document . \n\t', '\n\t\t In our experiments , we assume that P(NW|wi) can be approximated by the probability of wi occurring less than K times in a new document : K ^1 P ( NW | wi ) ^ ^ Pi ( k ) , ( 5 ) k where the constant K is dependent on the size of the document : The larger the document , the larger the value . \n\t', '\n\t\t Pi(k) can be estimated using several term distribution models ( see Chapter 15.3 in Manning and Schütze , 1999 ) . \n\t', '\n\t\t Following the empirical study in \n\t\t']",Positive
"['\n\t\t ^ and ^ are pa- rameters that can be fit using the observed mean ^ and the observed inverse document frequency IDF as follow : =c N , IDF=log f , ^=^×2IDF ^1= cfa~f , and ^= ^ ^ where cf is the total number of occurrence of word wi in training data , df is the number of documents in training data that wi occurs in , and N is the total number of documents . \n\t', '\n\t\t In our implementation , the training data contain approximately 40 thousand documents that have been balanced among domain , style and time . \n\t', '\n\t\t 4 Adaptation to Different Standards The word segmentation standard ( or standard for brevity ) varies from system to system because there is no commonly accepted definition of Chinese 0 = PersonName Affixation Year Mon Day FamilyName GivenName Prefix Stem Suffix Date Dig _Y Pre_Y Dig_M Pre_M Dig _D Pre _D Condition : \x91Affixation\x92 Condition : \x91Date\x92 Condition : \x91PersonName\x92 Actions : Insert a boundary Actions : Insert a boundary between Actions : Insert a boundary be- between \x91Prefix\x92 and \x91Stem\x92 ... \x91Year\x92 and \x91Mon\x92 ... tween \x91FamilyName\x92 and \x91Given- Name\x92 ... \n\t', '\n\t\t Figure 3 : Word internal structure and class-type transformation templates . \n\t', '\n\t\t words and different applications may have different requirements that call for different granularities of word segmentation . \n\t', '\n\t\t It is ideal to develop a single word segmentation system that is able to adapt to different standards . \n\t', '\n\t\t We consider the following standard adaptation paradigm . \n\t', '\n\t\t Suppose we have a \x91general\x92 standard pre-defined by ourselves . \n\t', '\n\t\t We have also created a large amount of training data which are segmented according to this general standard . \n\t', '\n\t\t We then develop a generic word segmenter , i.e. the system described in Sections 2 and 3 . \n\t', '\n\t\t Whenever we deploy the segmenter for any application , we need to customize the output of the segmenter according to an application-specific standard , which is not always explicitly defined . \n\t', '\n\t\t However , it is often implicitly defined in a given amount of application data ( called adaptation data ) from which the specific standard can be partially learned . \n\t', '\n\t\t In our system , the standard adaptation is conducted by a postprocessor which performs an ordered list of transformations on the output of the generic segmenter \x96 removing extraneous word boundaries , and inserting new boundaries \x96 to obtain a word segmentation that meets a different standard . \n\t', '\n\t\t The method we use is transformation-based learning \n\t\t']",Positive
"['\n\t\t Under the abovementioned adaptation paradigm , the initial segmentation is the output of the generic segmenter . \n\t', '\n\t\t The goal segmentation is adaptation data . \n\t', '\n\t\t The transformation templates can make reference to words ( i.e. lexicalized templates ) as well as some pre-defined types ( i.e. class-type based templates ) , as described below . \n\t', '\n\t\t We notice that most variability in word segmentation across different standards comes from those words that are not typically stored in the dictionary . \n\t', '\n\t\t Those words are dynamic in nature and are usually formed through productive morphological processes . \n\t', '\n\t\t In this study , we focus on three categories : morphologically derived words ( MDW ) , named entities ( NE ) and factoids . \n\t', '\n\t\t For each word class that belongs to these categories2 , we define an internal structure similar to \n\t\t']",Positive
"['\n\t\t The structure is a tree with \x91word class\x92 as the root , and \x91component types\x92 as the other nodes . \n\t', '\n\t\t There are 30 component types . \n\t', '\n\t\t As shown in Figure 3 , the word class Affixation has three component types : Prefix , Stem and Suffix . \n\t', '\n\t\t Similarly , PersonName has two component types and Date has nine \x96 3 as non-terminals and 6 as terminals . \n\t', '\n\t\t These internal structures are assigned to words by the generic segmenter at run time . \n\t', '\n\t\t The transformation templates for words of the above three categories are of the form : Condition : word class Actions : \x95 Insert \x96 place a new boundary between two component types . \n\t', '\n\t\t \x95 Delete \x96 remove an existing boundary between two component types . \n\t', '\n\t\t Since the application of the transformations derived from the above templates are conditioned on word class and make reference to component types , we call the templates class-type transformation templates . \n\t', '\n\t\t Some examples are shown in Figure 3 . \n\t', '\n\t\t In addition , we also use lexicalized transformation templates as : \x95 Insert \x96 place a new boundary between two lemmas . \n\t', '\n\t\t \x95 Delete \x96 remove an existing boundary between two lemmas . \n\t', '\n\t\t Here , lemmas refer to those basic lexical words that cannot be formed by any productive morphological process . \n\t', '\n\t\t They are mostly single characters , bi-character words , and 4-character idioms . \n\t', '\n\t\t In short , our adaptive Chinese word segmenter consists of two components : ( 1 ) a generic segmenter that is capable of adapting to the vocabularies of different domains and ( 2 ) a set of output adaptors , learned from application data , for adapting to different \x93application-specific\x94 standards 5 Evaluation We evaluated the proposed adaptive word segmentation system ( henceforth AWS ) using five different standards . \n\t', '\n\t\t The training and test corpora of these standards are detailed in Table 1 , where MSR is defined by ourselves , and the other four are standards used in SIGHAN\x92s First International Chinese Word Segmentation Bakeoff ( Bakeoff test sets for brevity , see \n\t\t']",Positive
"['\n\t\t Corpus Abbrev . \n\t', '\n\t\t # Tr. . \n\t', '\n\t\t Word # Te . \n\t', '\n\t\t Word \x91General\x92 standard MSR 20M 226K Beijing University PK 1.1M 17K U. Penn Chinese CTB 250K 40K Treebank Hong Kong City U. HK 240K 35K Academia Sinica AS 5.8M 12K Table 1 : standards and corpora . \n\t', '\n\t\t MSR is used as the general standard in our experiments , on the basis of which the generic segmenter has been developed . \n\t', '\n\t\t The training and test corpora were annotated manually , where there is only one allowable word segmentation for each sentence . \n\t', '\n\t\t The training corpus contains approximately 35 million Chinese characters from various domains of text such as newspapers , novels , magazines etc. 90 % of the training corpus are used for context model training , and 10 % are held-out data for model parameter training as shown in Figure 2 . \n\t', '\n\t\t The NE class models , as shown in Figure 1 , were trained on the corresponding NE lists that were collected separately . \n\t', '\n\t\t The test set contains a total of 225,734 tokens , including 205,162 lexicon/morph-lexicon words , 3,703 PNs , 5,287 LNs , 3,822 ONs , and 4,152 factoids . \n\t', '\n\t\t In Section 5 . \n\t', '\n\t\t 1 , we will describe some simulated test sets that are de rived from the MSR test set by sampling NWs from a 98,686-entry dictionary . \n\t', '\n\t\t The four Bakeoff standards are used as \x91specific\x92 standards into which we wish to adapt the general standard . \n\t', '\n\t\t We notice in Table 1 that the sizes of adaptation data sets ( i.e. training corpora of the four Bakeoff standards ) are much smaller than that of the MSR training set . \n\t', '\n\t\t The experimental setting turns out to be a good simulation of the adaptation paradigm described in Section 4 . \n\t', '\n\t\t The performance of word segmentation is measured through test precision ( P ) , test recall ( R ) , F score ( which is defined as 2PR/(P+R)) , the OOV rate for the test corpus ( on Bakeoff corpora , OOV is defined as the set of words in the test corpus not occurring in the training corpus . \n\t', '\n\t\t ) , the recall on OOV words ( Roov ) , and the recall on in-vocabulary ( Riv ) words . \n\t', '\n\t\t We also tested the statistical significance of results , using the criterion proposed by \n\t\t']",Positive
"['\n\t\t 5.1 NWI Results This section discusses two factors that we believe have the most impact on the performance of NWI . \n\t', '\n\t\t First , we compare methods where we use the NWI component ( i.e. an SVM classifier ) as a post- processor versus as a feature function in the linear models of Equation ( 2 ) . \n\t', '\n\t\t Second , we compare different sampling methods of creating simulated training data for context model . \n\t', '\n\t\t Which sampling method is best depends on the nature of P(NW|w) . \n\t', '\n\t\t As described in Section 3.2 , P(NW|w) is unknown and has to be approximated by Pi(k) in our study , so it is expected that the closer P(NW|w) and Pi(k) are , the better the resulting context model . \n\t', '\n\t\t We compare three estimates of Pi(k) in Equation ( 5 ) using term models based on Uniform , Possion , and K- Mixture distributions , respectively . \n\t', '\n\t\t Table 2 shows the results of the generic segmenter on three test sets that are derived from the MSR test set using the above three different sampling methods , respectively . \n\t', '\n\t\t For all three distributions , unified approaches ( i.e. using NWI component as a feature function ) outperform consecutive approaches ( i.e. using NWI component as a post- processor ) . \n\t', '\n\t\t This demonstrates empirically the benefits of using context model for NWI and the unified approach to Chinese word segmentation , as described in 3.2 . \n\t', '\n\t\t We also perform NWI on Bakeoff AWS w/o NW AWS w/ NW ( post-processor ) AWS w/ NW ( unified approach ) # of NW word segmentation word segmentation NW word segmentation NW P % R % P % R % P % R % P % R % P % R % Uniform 5,682 92.6 94.5 94.7 95.2 64.1 66.8 95.1 95.5 68.1 78.4 Poisson 3,862 93.4 95.6 94.5 95.9 61.4 45.6 95.0 95.7 57.2 60.6 K-Mixture 2,915 94.7 96.4 95.1 96.2 44.1 41.5 95.6 96.2 46.2 60.4 Table 2 : NWI results on MSR test set , NWI as post-processor versus unified approach PK CTB P R F OOV Roov Riv P R F OOV Roov Riv 1 . \n\t', '\n\t\t AWS w/o adaptation .824 .854 .839 .069 .320 .861 .799 .818 .809 .181 .624 .861 2 . \n\t', '\n\t\t AWS .952 .959 .955 .069 .781 .972 .895 .914 .904 .181 .746 .950 3 . \n\t', '\n\t\t AWS w/o NWI .949 .963 .956 .069 .741 .980 .875 .910 .892 .181 .690 .959 4 . \n\t', '\n\t\t FMM w/ adaptation .913 .946 .929 .069 .524 .977 .805 .874 .838 .181 .521 .952 5 . \n\t', '\n\t\t Rank 1 in Bakeoff .956 .963 .959 .069 .799 .975 .907 .916 .912 .181 .766 .949 6 . \n\t', '\n\t\t Rank 2 in Bakeoff .943 .963 .953 .069 .743 .980 .891 .911 .901 .181 .736 .949 Table 3 : Comparison scores for PK open and CTB open . \n\t', '\n\t\t HK AS P R F OOV Roov Riv P R F OOV Roov Riv 1 . \n\t', '\n\t\t AWS w/o adaptation .819 .822 .820 .071 .593 .840 .832 .838 .835 .021 .405 .847 2 . \n\t', '\n\t\t AWS .948 .960 .954 .071 .746 .977 .955 .961 .958 .021 .584 .969 3 . \n\t', '\n\t\t AWS w/o NWI .937 .958 .947 .071 .694 .978 .958 .943 .951 .021 .436 .969 4 . \n\t', '\n\t\t FMM w/ adaptation .818 .823 .821 .071 .591 .841 .930 .947 .939 .021 .160 .964 5 . \n\t', '\n\t\t Rank 1 in Bakeoff .954 .958 .956 .071 .788 .971 .894 .915 .904 .021 .426 .926 6 . \n\t', '\n\t\t Rank 2 in Bakeoff .863 .909 .886 .071 .579 .935 .853 .892 .872 .021 .236 .906 Table 4 : Comparison scores for HK open and AS open . \n\t', '\n\t\t test sets . \n\t', '\n\t\t As shown in Tables 3 and 4 ( Rows 2 and 3 ) , the use of NW functions ( via the unified approach ) substantially improves the word segmentation performance . \n\t', '\n\t\t We find in our experiments that NWs sampled by Possion and K-Mixture are mostly specific and time-sensitive terms , in agreement with our intuition , while NWs sampled by Uniform include more common words and lemmas that are easier to detect . \n\t', '\n\t\t Consequently , by Uniform sampling , the P/R of NWI is the highest but the P/R of the overall word segmentation is the lowest , as shown in Table 2 . \n\t', '\n\t\t Notice that the three sampling methods are not comparable in terms of P/R of NWI in Table 2 because of different sampling result in different sets of new words in the test set . \n\t', '\n\t\t We then perform NWI on Bakeoff test sets where the sets of new words are less dependent on specific sampling methods . \n\t', '\n\t\t The results however do not give a clear indication which sampling method is the best because the test sets are too small to show the difference . \n\t', '\n\t\t We then leave it to future work a thorough empirical comparison among different sampling methods . \n\t', '\n\t\t 5.2 Standard Adaptation Results The results of standard adaptation on four Bakeoff test sets are shown in Tables 3 and 4 . \n\t', '\n\t\t A set of transformations for each standard is learnt using TBL from the corresponding Bakeoff training set . \n\t', '\n\t\t For each test set , we report results using our system with and without standard adaptation ( Rows 1 and 2 ) . \n\t', '\n\t\t It turns out that performance improves dramatically across the board in all four test sets . \n\t', '\n\t\t For comparison , we also include in each table the results of using the forward maximum matching ( FMM ) greedy segmenter as a generic segmenter ( Row 4 ) , and the top 2 scores ( sorted by F ) that are reported in SIGHAN\x92s First International Chinese Word Segmentation Bakeoff ( Rows 5 and 6 ) . \n\t', '\n\t\t We can see that with adaptation , our generic segmenter can achieve state-of-the-art performance on different standards , showing its superiority over other systems . \n\t', '\n\t\t For example , there is no single segmenter in SIGHAN\x92s Bakeoff , which achieved top-2 ranks in all four test sets \n\t\t']",Positive
"['\n\t\t We notice in Table 3 and 4 that the quality of adaptation seems to depend largely upon the size of adaptation data : we outperformed the best bakeoff systems in the AS set because the size of the adaptation data is big while we are worse in the CTB set because of the small size of the adaptation data . \n\t', '\n\t\t To verify our speculation , we evaluated the adaptation results using subsets of the AS training set of different sizes , and observed the same trend . \n\t', '\n\t\t However , even with a much smaller adaptation data set ( e.g. 250K ) , we still outperform the best bakeoff results . \n\t', '\n\t\t 6 Related Work Many methods of Chinese word segmentation have been proposed ( See Wu and Tseng , 1993 ; Sproat and Shih , 2001 for reviews ) . \n\t', '\n\t\t However , it is difficult to compare systems due to the fact that there is no widely accepted standard . \n\t', '\n\t\t There has been less work on dealing with NWI and standard adaptation . \n\t', '\n\t\t All feature functions in Figure 1 , except the NW function , are derived from models presented in \n\t\t']",Positive
['\n\t\t The linear models are similar to what was presented in \n\t\t'],Positive
['\n\t\t An alternative to linear models is the log-linear models suggested by \n\t\t'],Positive
['\n\t\t See \n\t\t'],Positive
['\n\t\t The features for NWI were studied in Wu & \n\t\t'],Positive
['\n\t\t The use of sampling was proposed in Della \n\t\t'],Positive
['\n\t\t There is also a related work on this line in Japanese \n\t\t'],Positive
['\n\t\t A detailed discussion on differences among the four Bakeoff standards is presented in \n\t\t'],Positive
"['\n\t\t The method described in Section 4 can be viewed as an improved version in that the transformations are learnt automatically from adaptation data . \n\t', '\n\t\t The use of TBL for Chinese word segmentation was first suggested in \n\t\t']",Positive
"['\n\t\t 7 Conclusion This paper presents a statistical approach to adaptive Chinese word segmentation based on linear models and TBL . \n\t', '\n\t\t The system has two components : A generic segmenter that can adapt to the vocabularies of different domains , and a set of output adaptors , learned from application data , for adapting to different \x93application-specific\x94 standards . \n\t', '\n\t\t We evaluate our system on five test sets , each corresponding to a different standard . \n\t', '\n\t\t We achieve state-of-the-art performance on all test sets . \n\t', '\n\t\t References Brill , Eric . \n\t', '\n\t\t 1995. Transformation-based error-driven learning and natural language processing : a case study in Part-of-Speech tagging . \n\t', '\n\t\t In : Computational Linguistics , 21(4) . \n\t', '\n\t\t Collins , Michael and Nigel Duffy . \n\t', '\n\t\t 2001. Convolution kernels for natural language . \n\t', '\n\t\t In : Advances in Neural Information Processing Systems ( NLPS 14 ) . \n\t', '\n\t\t Collins , Michael . \n\t', '\n\t\t 2002. Parameter estimation for statistical parsing models : theory and practice of distribution-free methods . \n\t', '\n\t\t To appear . \n\t', '\n\t\t Della Pietra , S. , Della Pietra , V. , and Lafferty , J. 1997 . \n\t', '\n\t\t Inducing features of random fields . \n\t', '\n\t\t In : IEEE Transactions on Pattern Analysis and Machine Intelligence , 19 , 380-393 . \n\t', '\n\t\t Duda , Richard O , Hart , Peter E. and Stork , David G. 2001 . \n\t', '\n\t\t Pattern classification . \n\t', '\n\t\t John Wiley & Sons , Inc. . \n\t', '\n\t\t Gao , Jianfeng and Kai-Fu Lee . \n\t', '\n\t\t 2000. Distribution based pruning of backoff language models . \n\t', '\n\t\t In : ACL2000 . \n\t', '\n\t\t Gao , Jianfeng , Mu Li and Chang-Ning Huang . \n\t', '\n\t\t 2003. Improved source-channel model for Chinese word segmentation . \n\t', '\n\t\t In : ACL2003 . \n\t', '\n\t\t Katz , S. M. 1996 . \n\t', '\n\t\t Distribution of content words and phrases in text and language modeling , In : Natural Language Engineering , 1996(2) : 15-59 Li , Hongqiao , Chang-Ning Huang , Jianfeng Gao and Xiaozhong Fan . \n\t', '\n\t\t 2004. The use of SVM for Chinese new word identification . \n\t', '\n\t\t In : IJCNLP2004 . \n\t', '\n\t\t Manning , C. D. and H. Schütze , 1999 . \n\t', '\n\t\t Foundations of Statistical Natural Language Processing . \n\t', '\n\t\t The MIT Press . \n\t', '\n\t\t Mitchell , Tom M. 1997 . \n\t', '\n\t\t Machine learning . \n\t', '\n\t\t The McGraw-Hill Companies , Inc. . \n\t', '\n\t\t Och , Franz . \n\t', '\n\t\t 2003. Minimum error rate training in statistical machine translation . \n\t', '\n\t\t In : ACL2003 . \n\t', '\n\t\t Palmer , D. 1997 . \n\t', '\n\t\t A trainable rule-based algorithm for word segmentation . \n\t', ""\n\t\t In : ACL '97 . \n\t"", '\n\t\t Rosenfeld , R. , S. F. Chen and X. Zhu . \n\t', '\n\t\t 2001. Whole sentence exponential language models : a vehicle for linguistic statistical integration . \n\t', '\n\t\t In : Computer Speech and Language , 15 ( 1 ) . \n\t', '\n\t\t Sproat , Richard and Chilin Shih . \n\t', '\n\t\t 2002. Corpus-based methods in Chinese morphology and phonology . \n\t', '\n\t\t In : COLING 2002 . \n\t', '\n\t\t Sproat , Richard and Tom Emerson . \n\t', '\n\t\t 2003. The first international Chinese word segmentation bakeoff . \n\t', '\n\t\t In : SIGHAN 2003 . \n\t', '\n\t\t Uchimoto , K. , S. Sekine and H. Isahara . \n\t', '\n\t\t 2001. The unknown word problem : a morphological analysis of Japanese using maximum entropy aided by a dictionary . \n\t', '\n\t\t In : EMNLP2001 . \n\t', '\n\t\t Wu , Andi and Zixin Jiang . \n\t', '\n\t\t 2000. Statistically-enhanced new word identification in a rule-based Chinese system . \n\t', '\n\t\t In : Proc of the 2rd ACL Chinese Processing Workshop . \n\t', '\n\t\t Wu , Andi . \n\t', '\n\t\t 2003. Customizable segmentation of morphologically derived words in Chinese . \n\t', '\n\t\t In : International Journal of Computational Linguistics and Chinese Language Processing , 8(1) : 1-27 . \n\t', '\n\t\t Wu , Zimin and Gwyneth Tseng . \n\t', '\n\t\t 1993. Chinese text segmentation for text retrieval achievements and problems . \n\t', '\n\t\t In : JASIS , 44(9) : 532-542 . \n\t', '\n\t\t Experiments in Parallel-Text Based Grammar Induction Jonas Kuhn Department of Linguistics The University of Texas at Austin Austin , TX 78712 jonak@mail.utexas.edu Abstract This paper discusses the use of statistical word alignment over multiple parallel texts for the identification of string spans that cannot be constituents in one of the languages . \n\t', '\n\t\t This information is exploited in monolingual PCFG grammar induction for that language , within an augmented version of the inside-outside algorithm . \n\t', '\n\t\t Besides the aligned corpus , no other resources are required . \n\t', '\n\t\t We discuss an implemented system and present experimental results with an evaluation against the Penn Tree- bank . \n\t', '\n\t\t 1 Introduction There have been a number of recent studies exploiting parallel corpora in bootstrapping of monolingual analysis tools . \n\t', '\n\t\t In the \x93information projection\x94 approach ( e.g. , \n\t\t']",Positive
"['\n\t\t A high-quality analysis tool is applied to the English text , and the statistical word alignment is used to project a ( noisy ) target annotation to the version of the text . \n\t', '\n\t\t Robust learning techniques are then applied to bootstrap an analysis tool for , using the annotations projected with high confidence as the initial training data . \n\t', '\n\t\t ( Confidence of both the English analysis tool and the statistical word alignment is taken into account . \n\t', '\n\t\t ) The results that have been achieved by this method are very encouraging . \n\t', '\n\t\t Will the information projection approach also work for less shallow analysis tools , in particular full syntactic parsers ? \n\t', '\n\t\t An obvious issue is that one does not expect the phrase structure representation of English ( as produced by state-of-the-art tree- bank parsers ) to carry over to less configurational languages . \n\t', '\n\t\t Therefore , \n\t\t']",Positive
"['\n\t\t From the resulting ( noisy ) dependency treebank , a dependency parser is trained using the techniques of \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t Our hypothesis is that the quality of the resulting parser/grammar for language can be significantly improved if the training method for the parser is changed to accomodate for training data which are in part unreliable . \n\t', '\n\t\t The experiments we report in this paper focus on a specific part of the problem : we replace standard treebank training with an Expectation-Maximization ( EM ) algorithm for PCFGs , augmented by weighting factors for the reliability of training data , following the approach of \n\t\t']",Positive
['\n\t\t The factors are only sensitive to the constituent/distituent ( C/D ) status of each span of the string in ( cp. \n\t\t'],Positive
"['\n\t\t The C/D status is derived from an aligned parallel corpus in a way discussed in section 2 . \n\t', '\n\t\t We use the Europarl corpus \n\t\t']",Positive
"['\n\t\t While better absolute results could be expected using one or more parsers for the languages involved , we think that it is important to isolate the usefulness of exploiting just crosslinguistic word order divergences in order to obtain partial prior knowledge about the constituent structure of a language , which is then exploited in an EM learning approach ( section 3 ) . \n\t', '\n\t\t Not using a parser for some languages also makes it possible to compare various language pairs at the same level , and specifically , we can experiment with grammar induction for English exploiting various 1 The software is available at http://www.isi.edu/\x98och/GIZA++.html At that moment the voting will commence . \n\t', '\n\t\t Le vote aura lieu à ce moment -la. . \n\t', '\n\t\t Figure 1 : Alignment example other languages . \n\t', '\n\t\t Indeed the focus of our initial experiments has been on English ( section 4 ) , which facilitates evaluation against a treebank ( section 5 ) . \n\t', '\n\t\t 2 Cross-language order divergences The English-French example in figure 1 gives a simple illustration of the partial information about constituency that a word-aligned parallel corpus may provide . \n\t', '\n\t\t The en bloc reversal of subsequences of words provides strong evidence that , for instance , [ moment the voting ] or [ aura lieu à ce ] do not form constituents . \n\t', '\n\t\t At first sight it appears as if there is also clear evidence for [ at that moment ] forming a constituent , since it fully covers a substring that appears in a different position in French . \n\t', '\n\t\t Similarly for [ Le vote aura lieu ] . \n\t', '\n\t\t However , from the distribution of contiguous substrings alone we cannot distinguish between two the types of situations sketched in ( 1 ) and ( 2 ) : ( 1 ) ( 2 ) A string that is contiguous under projection , like ( 1 ) may be a true constituent , but it may also be a non-constituent part of a larger constituent as in in ( 2 ) . \n\t', '\n\t\t Word blocks . \n\t', '\n\t\t Let us define the notion of a word block ( as opposed to a phrase or constituent ) induced by a word alignment to capture the relevant property of contiguousness under translation.2 The alignments induced by GIZA++ ( following the IBM models ) are asymmetrical in that several words from may be aligned with one word in , but not vice versa . \n\t', '\n\t\t So we can view a word alignment as a function that maps each word in an -sentence to a ( possibly empty ) subset of words from its translation in . \n\t', '\n\t\t For example , in figure 1 , voting ={vote } , and that = { ce -la. . \n\t', '\n\t\t Note that for . \n\t', '\n\t\t The -images of a sentence need not exhaust the words of the translation in ; however it is common to assume a special empty word NULL in each -sentence , for which by definition NULL is the set of -words not contained in any -image of the overt words . \n\t', '\n\t\t We now define an -induced block ( or -block for short ) as a substring of a sentence in , such that the union over all -images ( or are -induced blocks . \n\t', '\n\t\t Let us define a maximal -block as an -block , such that adding at the beginning or at the end is either ( i ) impossible ( because it would lead to a non-block , or or do not exist as we are at the beginning or end of the string ) , or ( ii ) it would introduce a new crossing alignment 2The block notion we are defining in this section is indirectly related to the concept of a \x93phrase\x94 in recent work in Statistical Machine Translation . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t In our context , we are interested in inducing syntactic constituents based on alignment information ; given the observations from Statistical MT , it does not come as a surprise that there is no direct link from blocks to constituents . \n\t', '\n\t\t Our work can be seen as an attempt to zero in on the distinction between the concepts ; we find that it is most useful to keep track of the boundaries between blocks . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t ) forms a contiguous substring in , modulo the words from NULL . \n\t', '\n\t\t For example , in ( 1 ) ( or ( 2 ) ) is not an -block since the union over its -images is which do not form a contiguous string in . \n\t', '\n\t\t The sequences to the block.3 String in ( 1 ) is not a maximal -block , be- cause is an -block ; but is maxi- mal since is the final word of the sentence and is a non-block . \n\t', '\n\t\t We can now make the initial observation precise that ( 1 ) and ( 2 ) have the same block structure , but the constituent structures are different ( and this is not due to an incorrect alignment ) . \n\t', '\n\t\t is a maximal block in both cases , but while it is a constituent in ( 1 ) , it isn\x92t in ( 2 ) . \n\t', '\n\t\t We may call maximal blocks that contain only non-maximal blocks as substrings first-order maximal -blocks . \n\t', '\n\t\t A maximal block that contains other maximal blocks as substrings is a higher-order maximal -block . \n\t', '\n\t\t In ( 1 ) and ( 2 ) , the complete string is a higher-order maximal block . \n\t', '\n\t\t Note that a higher-order maximal block may contain substrings which are non-blocks . \n\t', '\n\t\t Higher-order maximal blocks may still be non- constituents as the following simple English-French example shows : ( 3 ) He gave Mary a book Il a donné un livre à Mary The three first-order maximal blocks in English are [ He gave ] , [ Mary ] , and [ a book ] . \n\t', '\n\t\t [ Mary a book ] is a higher-order maximal block , since its \x93projection\x94 to French is contiguous , but it is not a constituent . \n\t', '\n\t\t ( Note that the VP constituent gave Mary a book on the other hand is not a maximal block here . \n\t', '\n\t\t ) Block boundaries . \n\t', '\n\t\t Let us call the string position between two maximal blocks an -block boundary.4 In (1)/(2) , the position between and is a block boundary . \n\t', '\n\t\t We can now formulate the ( 4 ) Distituent hypothesis If a substring of a sentence in language crosses a first-order -block boundary ( zones ) , then it can only be a constituent of if it contains at least one of the two maximal -blocks separated by that boundary in full . \n\t', '\n\t\t This hypothesis makes it precise under which conditions we assume to have reliable negative evidence against a constituent . \n\t', '\n\t\t Even examples of complicated structural divergence from the classical MT 3I.e. , an element of ( or ) continues the - string at the other end . \n\t', '\n\t\t 4 We will come back to the situation where a block boundary may not be unique below . \n\t', '\n\t\t 5This will be explained below . \n\t', '\n\t\t literature tend not to pose counterexamples to the hypothesis , since it is so conservative . \n\t', '\n\t\t Projecting phrasal constituents from one language to another is problematic in cases of divergence , but projecting information about distituents is generally safe . \n\t', '\n\t\t Mild divergences are best . \n\t', '\n\t\t As should be clear , the -block-based approach relies on the occurrence of reorderings of constituents in translation . \n\t', '\n\t\t If two languages have the exact same structure ( and no paraphrases whatsoever are used in translation ) , the approach does not gain any information from a parallel text . \n\t', '\n\t\t However , this situation does not occur realistically . \n\t', '\n\t\t If on the other hand , massive reordering occurs without preserving any contiguous sub- blocks , the approach cannot gain information either . \n\t', '\n\t\t The ideal situation is in the middleground , with a number of mid-sized blocks in most sentences . \n\t', '\n\t\t The table in figure 2 shows the distribution of sentences with -block boundaries based on the alignment of English and 7 other languages , for a sample of c. 3,000 sentences from the Europarl corpus . \n\t', '\n\t\t We can see that the occurrence of boundaries is in a range that should make it indeed useful.6 : de el es fi fr it sv 1 82.3 % 76.7 % 80.9 % 70.2 % 83.3 % 82.9 % 67.4 % 2 73.5 % 64.2 % 74.0 % 55.7 % 76.0 % 74.6 % 58.0 % 3 57.7 % 50.4 % 57.5 % 39.3 % 60.5 % 60.7 % 38.4 % 4 47.9 % 40.1 % 50.9 % 29.7 % 53.3 % 52.1 % 31.3 % 5 38.0 % 30.6 % 42.5 % 21.5 % 45.9 % 42.0 % 23.0 % 6 28.7 % 23.2 % 33.4 % 15.2 % 36.1 % 33.4 % 15.2 % 7 22.6 % 17.9 % 28.0 % 10.2 % 30.2 % 26.6 % 11.0 % 8 17.0 % 13.6 % 22.4 % 7.6 % 24.4 % 21.8 % 8.0 % 9 12.3 % 10.3 % 17.4 % 5.4 % 19.7 % 17.3 % 5.6 % 10 9.5 % 7.8 % 13.7 % 3.4 % 16.3 % 13.1 % 4.1 % de : German ; el : Greek ; es : Spanish ; fi : Finnish ; fr : French ; it : Italian ; sv : Swedish . \n\t', '\n\t\t Figure 2 : Proportion of sentences with -block boundaries for : English Zero fertility words . \n\t', '\n\t\t So far we have not addressed the effect of finding zero fertility words , i.e. , words from with . \n\t', '\n\t\t Statistical word alignment makes frequent use of this mechanism . \n\t', '\n\t\t An actual example from our alignment is shown in figure 3 . \n\t', '\n\t\t The English word has is treated as a zero fertility word . \n\t', '\n\t\t While we can tell from the block structure that there is a maximal block boundary somewhere between Baringdorf and the , it is 6The average sentence length for the English sentence is 26.5 words . \n\t', '\n\t\t ( Not too suprisingly , Swedish gives rise to the fewest divergences against English . \n\t', '\n\t\t Note also that the Romance languages shown here behave very similarly . \n\t', '\n\t\t ) Mr. Graefe zu Baringdorf has the floor to explain this request . \n\t', '\n\t\t La parole est à M. Graefe zu Baringdorf pour motiver la demande . \n\t', '\n\t\t Figure 3 : Alignment example with zero-fertility word in English unclear on which side has should be located.7 The definitions of the various types of word blocks cover zero fertility words in principle , but they are somewhat awkward in that the same word may belong to two maximal -blocks , on its left and on its right . \n\t', '\n\t\t It is not clear where the exact block boundary is located . \n\t', '\n\t\t So we redefine the notion of - block boundaries . \n\t', '\n\t\t We call the ( possibly empty ) sub- string between the rightmost non-zero-fertility word of one maximal -block and the leftmost non-zerofertility word of its right neighbor block the -block boundary zone . \n\t', '\n\t\t The distituent hypothesis is sensitive to crossing a boundary zone , i.e. , if a constituent-candidate ends somewhere in the middle of a non-empty boundary zone , this does not count as a crossing . \n\t', '\n\t\t This reflects the intuition of uncertainty and keeps the exclusion of clear distituents intact . \n\t', '\n\t\t 3 EM grammar induction with weighting factors The distituent identification scheme introduced in the previous section can be used to hypothesize a fairly reliable exclusion of constituency for many spans of strings from a parallel corpus . \n\t', '\n\t\t Besides a statistical word alignment , no further resources are required . \n\t', '\n\t\t In order to make use of this scattered ( non- ) constituency information , a semi-supervised approach is needed that can fill in the ( potentially large ) areas for which no prior information is available . \n\t', '\n\t\t For the present experiments we decided to choose a conceptually simple such approach , with which we can build on substantial existing work in grammar induction : we construe the learning problem as PCFG induction , using the inside-outside algorithm , with the addition of weighting factors based on the ( non- )constituency information . \n\t', '\n\t\t This use of weighting factors in EM learning follows the approach discussed in \n\t\t']",Positive
"['\n\t\t Since we are mainly interested in comparative experiments at this stage , the conceptual simplicity , and the availability of efficient implemented open- 7Since zero-fertility words are often function words , there is probably a rightward-tendency that one might be able to exploit ; however in the present study we didn\x92t want to build such high-level linguistic assumptions into the system . \n\t', '\n\t\t source systems of a PCFG induction approach outweighs the disadvantage of potentially poorer overall performance than one might expect from some other approaches . \n\t', '\n\t\t The PCFG topology we use is a binary , entirely unrestricted X-bar-style grammar based on the Penn Treebank POS-tagset ( expanded as in the TreeTagger by \n\t\t']",Positive
"['\n\t\t All possible combinations of projections of POS-categories X and Y are included following the schemata in ( 5 ) . \n\t', '\n\t\t This gives rise to 13,110 rules . \n\t', '\n\t\t ( 5 ) a. XP X b. XP XP YP c. XP YP XP d. XP YP X e. XP X YP We tagged the English version of our training section of the Europarl corpus with the TreeTagger and used the strings of POS-tags as the training corpus for the inside-outside algorithm ; however , it is straightforward to apply our approach to a language for which no taggers are available if an unsupervised word clustering technique is applied first . \n\t', '\n\t\t We based our EM training algorithm on Mark Johnson\x92s implementation of the inside-outside algorithm . \n\t', '\n\t\t $ The initial parameters on the PCFG rules are set to be uniform . \n\t', '\n\t\t In the iterative induction process of parameter reestimation , the current rule parameters are used to compute the expectations of how often each rule occurred in the parses of the training corpus , and these expectations are used to adjust the rule parameters , so that the likelihood of the training data is increased . \n\t', '\n\t\t When the probablity of a given rule drops below a certain threshold , the rule is excluded from the grammar . \n\t', '\n\t\t The iteration is continued until the increase in likelihood of the training corpus is very small . \n\t', '\n\t\t Weight factors . \n\t', '\n\t\t The inside-outside algorithm is a dynamic programming algorithm that uses a chart in order to compute the rule expectations for each sentence . \n\t', '\n\t\t We use the information obtained from the parallel corpus as discussed in section 2 as prior information ( in a Bayesian framework ) to adjust the 8http://cog.brown.edu/\x98mj/ you can table questions under rule 28 , and you no longer have the floor . \n\t', '\n\t\t vous pouvez poser les questions au moyen de l\x92 article 28 du réglement . \n\t', '\n\t\t je ne vous donne pas la parole . \n\t', '\n\t\t Figure 4 : Alignment example with higher-fertility words in English expectations that the inside-outside algorithm determines based on its current rule parameters . \n\t', '\n\t\t Note that the this prior information is information about string spans of (non-)constituents \x96 it does not tell us anything about the categories of the potential constituents affected . \n\t', '\n\t\t It is combined with the PCFG expectations as the chart is constructed . \n\t', '\n\t\t For each span in the chart , we get a weight factor that is multiplied with the parameter-based expectations.9 4 Experiments We applied GIZA++ \n\t\t']",Positive
"['\n\t\t For the experiments we report in this paper , we only used the 1999 debates , with the language pairs of English combined with Finnish , French , German , Greek , Italian , Spanish , and Swedish . \n\t', '\n\t\t For computing the weight factors we used a two- step process implemented in Perl , which first determines the maximal -block boundaries ( by detecting discontinuities in the sequence of the - projected words ) . \n\t', '\n\t\t Words with fertility whose - correspondents were non-adjacent ( modulo NULL- projections ) were treated like zero fertility words , i.e. , we viewed them as unreliable indicators of block status ( compare figure 4 ) . \n\t', '\n\t\t ( 7 ) shows the internal representation of the block structure for ( 6 ) ( compare figure 3 ) . \n\t', '\n\t\t L and R are used for the beginning and end of blocks , when the adjacent boundary zone is empty ; l and r are used next to non-empty boundary zones . \n\t', '\n\t\t Words that have correspondents in 9In the simplest model , we use the factor 0 for spans satisfying the distituent condition underlying hypothesis ( 4 ) , and factor 1 for all other spans ; in other words , parses involving a distituent are cancelled out . \n\t', '\n\t\t We also experimented with various levels of weight factors : for instance , distituents were assigned factor 0.0 1 , likely distituents factor 0 . \n\t', '\n\t\t 1 , neutral spans 1 , and likely constituents factor 2 . \n\t', '\n\t\t Likely constituents are defined as spans for which one end is adjacent to an empty block boundary zone ( i.e. , there is no zero fertility word in the block boundary zone which could be the actual boundary of constituents in which the block is involved ) . \n\t', '\n\t\t Most variations in the weighting scheme did not have a significant effect , but they caused differences in coverage because rules with a probability below a certain threshold were dropped in training . \n\t', '\n\t\t Below , we report the results of the 0.01\x960.1\x961\x962 scheme , which had a reasonably high coverage on the test data . \n\t', '\n\t\t the normal sequence are encoded as * , zero fertility words as - ; A and B are used for the first block in a sentence instead of L and R , unless it arises from \x93relocation\x94 , which increases likelihood for constituent status ( likewise for the last block : Y and Z ) . \n\t', '\n\t\t Since we are interested only in first-order blocks here , the compact string-based representation is sufficient . \n\t', '\n\t\t ( 6 ) la parole est à m. graefe zu baringdorf pour motiver la demande NULL ( { 3 4 11 } ) mr ( { 5 } ) graefe ( { 6 } ) zu ( { 7 } ) baringdorf ( { 8 } ) has ( { } ) the ( { 1 } ) floor ( { 2 } ) to ( { 9 } ) explain ( { 10 } ) this ( { } ) request ( { 12 } ) ( 7 ) [ L**r-lRY*-*Z ] The second step for computing the weight factors creates a chart of all string spans over the given sentence and marks for each span whether it is a distituent , possible constituent or likely distituent , based on the location of boundary symbols . \n\t', '\n\t\t ( For instance zu Baringdorf has the is marked as a distituent ; the floor and has the floor are marked as likely constituents . \n\t', '\n\t\t ) The tests are implemented as simple regular expressions . \n\t', '\n\t\t The chart of weight factors is represented as an array which is stored in the training corpus file along with the sentences . \n\t', '\n\t\t We combine the weight factors from various languages , since each of them may contribute distinct ( non- )constituent information . \n\t', '\n\t\t The inside-outside algorithm reads in the weight factor array and uses it in the computation of expected rule counts . \n\t', '\n\t\t We used the probability of the statistical word alignment as a confidence measure to filter out unreliable training sentences . \n\t', '\n\t\t Due to the conservative nature of the information we extract from the alignment , the results indicate however that filtering is not necessary . \n\t', '\n\t\t 5 Evaluation For evaluation , we ran the PCFG resulting from training with the Viterbi algorithm10 on parts of the Wall Street Journal ( WSJ ) section of the Penn Tree- bank and compared the tree structure for the most 10We used the LoPar parser \n\t\t']",Positive
"['\n\t\t System Unlab . \n\t', '\n\t\t Prec . \n\t', '\n\t\t Unlab . \n\t', '\n\t\t Recall F -Score Crossing Brack . \n\t', '\n\t\t Left-branching 30.4 35.8 32.9 3.06 Right-branching 36.2 42.6 39.2 2.48 Standard PCFG induction 42.4 64.9 51.3 2.2 PCFG trained with C/D weight factors from Europarl corpus 47.8 72.1 57.5 1.7 Upper limit 66.08 100.0 79.6 0.0 Figure 5 : Scores for test sentences from WSJ section 23 , up to length 10. probable parse for the test sentences against the gold standard treebank annotation . \n\t', '\n\t\t ( Note that one does not necessarily expect that an induced grammar will match a treebank annotation , but it may at least serve as a basis for comparison . \n\t', '\n\t\t ) The evaluation criteria we apply are unlabeled bracketing precision and recall ( and crossing brackets ) . \n\t', '\n\t\t We follow an evaluation criterion that ( Klein and Manning , 2002 , footnote 3 ) discuss for the evaluation of a not fully supervised grammar induction approach based on a binary grammar topology : bracket multiplicity ( i.e. , non-branching projections ) is collapsed into a single set of brackets ( since what is relevant is the constituent structure that was induced).11 For comparison , we provide baseline results that a uniform left-branching structure and a uniform right-branching structure ( which encodes some nontrivial information about English syntax ) would give rise to . \n\t', '\n\t\t As an upper boundary for the performance a binary grammar can achieve on the WSJ , we present the scores for a minimal binarized extension of the gold-standard annotation . \n\t', '\n\t\t The results we can report at this point are based on a comparatively small training set . \n\t', '\n\t\t 12 So , it may be too early for conclusive results . \n\t', '\n\t\t ( An issue that arises with the small training set is that smoothing techniques would be required to avoid overtraining , but these tend to dominate the test application , so the effect of the parallel-corpus based information cannot be seen so clearly . \n\t', '\n\t\t ) But we think that the results are rather encouraging . \n\t', '\n\t\t As the table in figure 5 shows , the PCFG we induced based on the parallel-text derived weight factors reaches 57.5 as the F -score of unlabeled precision and recall on sentences up to length 10.13 We 11Note that we removed null elements from the WSJ , but we left punctuation in place . \n\t', '\n\t\t We used the EVALB program for obtaining the measures , however we preprocessed the bracketings to reflect the criteria we discuss here . \n\t', '\n\t\t 12This is not due to scalability issues of the system ; we expect to be able to run experiments on rather large training sets . \n\t', '\n\t\t Since no manual annotation is required , the available resources are practically indefinite . \n\t', '\n\t\t 13For sentences up to length 30 , the F -score drops to 28.7 show the scores for an experiment without smoothing , trained on c. 3,000 sentences . \n\t', '\n\t\t Since no smoothing was applied , the resulting coverage ( with low- probability rules removed ) on the test set is about 80 % . \n\t', '\n\t\t It took 74 iterations of the inside-outside algorithm to train the weight-factor-trained grammar ; the final version has 1005 rules . \n\t', '\n\t\t For comparison we induced another PCFG based on the same X-bar topology without using the weight factor mechanism . \n\t', '\n\t\t This grammar ended up with 1145 rules after 115 iterations . \n\t', '\n\t\t The F -score is only 51.3 ( while the coverage is the same as for the weight-factor-trained grammar ) . \n\t', '\n\t\t Figure 6 shows the complete set of ( singular ) \x93NP rules\x94 emerging from the weight-factor-trained grammar , which are remarkably well-behaved , in particular when we compare them to the corresponding rules from the PCFG induced in the standard way ( figure 7 ) . \n\t', '\n\t\t ( XP categories are written as POS-TAG -P , X head categories are written as POS-TAG -0 \x96 so the most probable NP productions in figure 6 are NP N PP , NP N , NP ADJP N , NP NP PP , NP N PropNP . \n\t', '\n\t\t ) Of course we are comparing an unsupervised technique with a mildly supervised technique ; but the results indicate that the relatively subtle information discussed in section 2 seems to be indeed very useful . \n\t', '\n\t\t 6 Discussion This paper presented a novel approach of using parallel corpora as the only resource in the creation of a monolingual analysis tools . \n\t', '\n\t\t We believe that in order to induce high-quality tools based on statistical word alignment , the training approach for the target language tool has to be able to exploit islands of reliable information in a stream of potentially rather noisy data . \n\t', '\n\t\t We experimented with an initial idea to address this task , which is conceptually simple and can be implemented building on existing technology : using the notion of word blocks projected ( as compared to 23.5 for the standard PCFG ) . \n\t', '\n\t\t 0.300467 NN-P --> NN-0 IN-P 0.25727 NN-P --> NN-0 0.222335 NN-P --> JJ-P NN-0 0.0612312 NN-P --> NN-P IN-P 0.0462079 NN-P --> NN-0 NP-P 0.0216048 NN-P --> NN-0 , -P 0.0173518 NN-P --> NN-P NN-0 0.0114746 NN-P --> NN-0 NNS-P 0.00975112 NN-P --> NN-0 MD-P 0.00719605 NN-P --> NN-0 VBZ-P 0.00556762 NN-P --> NN-0 NN-P 0.00511326 NN-P --> NN-0 VVD-P 0.00438077 NN-P --> NN-P VBD-P 0.00423814 NN-P --> NN-P , -P 0.00409675 NN-P --> NN-0 CD-P 0.00286634 NN-P --> NN-0 VHZ-P 0.00258022 NN-P --> VVG-P NN-0 0.0018237 NN-P --> NN-0 TO-P 0.00162601 NN-P --> NN-P VVN-P 0.00157752 NN-P --> NN-P VB-P 0.00125101 NN-P --> NN-0 VVN-P 0.00106749 NN-P --> NN-P VBZ-P 0.00105866 NN-P --> NN-0 VBD-P 0.000975359 NN-P --> VVN-P NN-0 0.000957702 NN-P --> NN-0 SENT-P 0.000931056 NN-P --> NN-0 CC-P 0.000902116 NN-P --> NN-P SENT-P 0.000717542 NN-P --> NN-0 VBP-P 0.000620843 NN-P --> RB-P NN-0 0.00059608 NN-P --> NN-0 WP-P 0.000550255 NN-P --> NN-0 PDT-P 0.000539155 NN-P --> NN-P CC-P 0.000341498 NN-P --> WP$-P NN-0 0.000330967 NN-P --> WRB-P NN-0 0.000186441 NN-P --> , -P NN-0 0.000135449 NN-P --> CD-P NN-0 7.16819e-05 NN-P --> NN-0 POS-P Figure 6 : Full set of rules based on the NN tag in the C/D-trained PCFG by word alignment as an indication for ( mainly ) impossible string spans . \n\t', '\n\t\t Applying this information in order to impose weighting factors on the EM algorithm for PCFG induction gives us a first , simple instance of the \x93island-exploiting\x94 system we think is needed . \n\t', '\n\t\t More sophisticated models may make use some of the experience gathered in these experiments . \n\t', '\n\t\t The conservative way in which cross-linguistic relations between phrase structure is exploited has the advantage that we don\x92t have to make unwarranted assumptions about direct correspondences among the majority of constituent spans , or even direct correspondences of phrasal categories . \n\t', '\n\t\t The technique is particularly well-suited for the exploitation of parallel corpora involving multiple lan- 0.429157 NN-P --> DT-P NN-0 0.0816385 NN-P --> IN-P NN-0 0.0630426 NN-P --> NN-0 0.0489261 NN-P --> PP$-P NN-0 0.0487434 NN-P --> JJ-P NN-0 0.0451819 NN-P --> NN-P , -P 0.0389741 NN-P --> NN-P VBZ-P 0.0330732 NN-P --> NN-P NN-0 0.0215872 NN-P --> NN-P MD-P 0.0201612 NN-P --> NN-P TO-P 0.0199536 NN-P --> CC-P NN-0 0.015509 NN-P --> NN-P VVZ-P 0.0112734 NN-P --> NN-P RB-P 0.00977683 NN-P --> NP-P NN-0 0.00943218 NN-P --> CD-P NN-0 0.00922132 NN-P --> NN-P WDT-P 0.00896826 NN-P --> POS-P NN-0 0.00749452 NN-P --> NN-P VHZ-P 0.00621328 NN-P --> NN-0 , -P 0.00520734 NN-P --> NN-P VBD-P 0.004674 NN-P --> JJR-P NN-0 0.00407644 NN-P --> NN-P VVD-P 0.00394681 NN-P --> NN-P VVN-P 0.00354741 NN-P --> NN-0 MD-P 0.00335451 NN-P --> NN-0 NN-P 0.0030748 NN-P --> EX-P NN-0 0.0026483 NN-P --> WRB-P NN-0 0.00262025 NN-P --> NN-0 TO-P [ ... ] 0.000403279 NN-P --> NN-0 VBP-P 0.000378414 NN-P --> NN-0 PDT-P 0.000318026 NN-P --> NN-0 VHZ-P 2.27821e-05 NN-P --> NN-P PP-P Figure 7 : Standard induced PCFG : Excerpt of rules based on the NN tag guages like the Europarl corpus . \n\t', '\n\t\t Note that nothing in our methodology made any language particular assumptions ; future research has to show whether there are language pairs that are particularly effective , but in general the technique should be applicable for whatever parallel corpus is at hand . \n\t', '\n\t\t A number of studies are related to the work we presented , most specifically work on parallel-text based \x93information projection\x94 for parsing \n\t\t']",Positive
"['\n\t\t However to our knowledge the specific way of bringing these aspects together is new . \n\t', '\n\t\t References Yaser Al-Onaizan , Jan Curin , Michael Jahr , Kevin Knight , John Lafferty , Dan Melamed , Franz- Josef Och , David Purdy , Noah A. Smith , and David Yarowsky . \n\t', '\n\t\t 1999. Statistical machine translation . \n\t', '\n\t\t Final report , JHU Workshop . \n\t', '\n\t\t Michael Collins . \n\t', '\n\t\t 1999. A statistical parser for Czech . \n\t', '\n\t\t In Proceedings ofA CL . \n\t', '\n\t\t Rebecca Hwa , Philip Resnik , and Amy Weinberg . \n\t', '\n\t\t 2002. Breaking the resource bottleneck for multilingual parsing . \n\t', '\n\t\t In Proceedings ofLREC . \n\t', '\n\t\t Dan Klein and Christopher Manning . \n\t', '\n\t\t 2002. A generative constituent-context model for improved grammar induction . \n\t', '\n\t\t In Proceedings ofACL . \n\t', '\n\t\t Philipp Koehn , Franz Josef Och , and Daniel Marcu . \n\t', '\n\t\t 2003. Statistical phrase-based translation . \n\t', '\n\t\t In Proceedings of the Human Language Technology Conference 2003 ( HLT-NAACL 2003 ) , Edmonton , Canada . \n\t', '\n\t\t Philipp Koehn . \n\t', '\n\t\t 2002. Europarl : A multilingual corpus for evaluation of machine translation . \n\t', '\n\t\t Ms. , University of Southern California . \n\t', '\n\t\t Kamal Nigam , Andrew Kachites McCallum , Sebastian Thrun , and Tom M. Mitchell . \n\t', '\n\t\t 2000. Text classification from labeled and unlabeled documents using EM . \n\t', '\n\t\t Machine Learning , 39(2/3):103\x96134 . \n\t', '\n\t\t Franz Josef Och and Hermann Ney . \n\t', '\n\t\t 2003. A systematic comparison of various statistical alignment models . \n\t', '\n\t\t Computational Linguistics , 29(1):19\x9651 . \n\t', '\n\t\t Helmut Schmid . \n\t', '\n\t\t 1994. Probabilistic part-of-speech tagging using decision trees . \n\t', '\n\t\t In International Conference on New Methods in Language Processing , Manchester , UK . \n\t', '\n\t\t Helmut Schmid . \n\t', '\n\t\t 2000. Lopar : Design and implementation . \n\t', '\n\t\t Arbeitspapiere des Sonderforschungsbereiches 340 , No. 149 , IMS Stuttgart . \n\t', '\n\t\t Menno van Zaanen . \n\t', '\n\t\t 2000. ABL : Alignment-based learning . \n\t', '\n\t\t In COLING 2000 - Proceedings of the 18th International Conference on Computational Linguistics , pages 961\x96967 . \n\t', '\n\t\t Dekai Wu . \n\t', '\n\t\t 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora . \n\t', '\n\t\t Computational Linguistics , 23(3):377\x96403 . \n\t', '\n\t\t David Yarowsky and Grace Ngai . \n\t', '\n\t\t 2001. Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora . \n\t', '\n\t\t In Proceedings ofNAACL . \n\t', '\n\t\t Corpus-Based Induction of Syntactic Structure : Models of Dependency and Constituency Dan Klein Computer Science Department Stanford University Stanford , CA 94305-9040 klein@cs.stanford.edu Christopher D. Manning Computer Science Department Stanford University Stanford , CA 94305-9040 manning@cs.stanford.edu Abstract We present a generative model for the unsupervised learning of dependency structures . \n\t', '\n\t\t We also describe the multiplicative combination of this dependency model with a model of linear constituency . \n\t', '\n\t\t The product model outperforms both components on their respective evaluation metrics , giving the best published figures for unsupervised dependency parsing and unsupervised constituency parsing . \n\t', '\n\t\t We also demonstrate that the combined model works and is robust cross-linguistically , being able to exploit either attachment or distributional regularities that are salient in the data . \n\t', '\n\t\t 1 Introduction The task of statistically inducing hierarchical syntactic structure over unannotated sentences of natural language has received a great deal of attention \n\t\t']",Positive
['\n\t\t Researchers have explored this problem for a variety of reasons : to argue empirically against the poverty of the stimulus \n\t\t'],Positive
"['\n\t\t An important distinction should be drawn between work primarily interested in the weak generative capacity of models , where modeling hierarchical structure is only useful insofar as it leads to improved models over observed structures \n\t\t']",Positive
"['\n\t\t This paper falls into the latter category ; we will be inducing models of linguistic constituency and dependency with the goal of recovering linguistically plausible structures . \n\t', '\n\t\t We make no claims as to the cognitive plausibility of the induction mechanisms we present here ; however , the ability of these systems to recover substantial linguistic patterns from surface yields alone does speak to the strength of support for these patterns in the data , and hence undermines arguments based on \x93the poverty of the stimulus\x94 \n\t\t']",Positive
['\n\t\t 2 Unsupervised Dependency Parsing Most recent progress in unsupervised parsing has come from tree or phrase-structure grammar based models \n\t\t'],Negative
"['\n\t\t First , most state-of- the-art supervised parsers make use of specific lexical information in addition to word-class level information \x96perhaps lexical information could be a useful source of information for unsupervised methods . \n\t', '\n\t\t Second , a central motivation for using tree structures in computational linguistics is to enable the extraction of dependencies \x96 function-argument and modification structures \x96 and it might be more advantageous to induce such structures directly . \n\t', '\n\t\t Third , as we show below , for languages such as Chinese , which have few function words , and for which the definition of lexical categories is much less clear , dependency structures may be easier to detect . \n\t', ""\n\t\t 2.1 Representation and Evaluation An example dependency representation of a short sentence is shown in figure 1(a) , where , following the traditional dependency grammar notation , the regent or head of a dependency is marked with the tail of the dependency arrow , and the dependent is marked with the arrowhead ( Mel'^cuk , 1988 ) . \n\t"", '\n\t\t It will be important in what follows to see that such a representation is isomorphic ( in terms of strong generative capacity ) to a restricted form of phrase structure grammar , where the set of terminals and nonterminals is identical , and every rule is of the form X--* X Y or X--* Y X \n\t\t']",Positive
"['\n\t\t NN NNS VBD IN NN ROOT Factory NN NNS payrolls NNS VBD fell VBD IN IN NN Factory NN NP lls ell IN payro fell VP NNS VBD PP NN speech categories may be included in the dependency representation , as shown here , or dependencies may be directly between words . \n\t', '\n\t\t Below , we will assume an additonal reserved nonterminal ROOT , whose sole dependent is the head of the sentence . \n\t', '\n\t\t This simplifies the notation , math , and the evaluation metric . \n\t', '\n\t\t A dependency analysis will always consist of exactly as many dependencies as there are words in the sentence . \n\t', '\n\t\t For example , in the dependency structure of figure 1(b) , the dependencies are { ( ROOT , fell ) , ( fell , payrolls ) , ( fell , in ) , ( in , September ) , ( payrolls , Factory ) } . \n\t', '\n\t\t The quality of a hypothesized depen- dency structure can hence be evaluated by accuracy as compared to a gold-standard dependency structure , by reporting the percentage of dependencies shared between the two analyses . \n\t', '\n\t\t In the next section , we discuss several models of dependency structure , and throughout this paper we report the accuracy of various methods at recovering gold-standard dependency parses from various corpora , detailed here . \n\t', '\n\t\t WSJ is the entire Penn English Treebank WSJ portion . \n\t', '\n\t\t WSJ10 is the subset of sentences which contained 10 words or less after the removal of punctuation . \n\t', '\n\t\t CTB 10 is the sentences of the same length from the Penn Chinese treebank ( v3 ) . \n\t', '\n\t\t NEGRA10 is the same , for the German NEGRA corpus , based on the supplied conversion of the NEGRA corpus into Penn treebank format . \n\t', '\n\t\t In most of the present experiments , the provided partsof-speech were used as the input alphabet , though we also present limited experimentation with synthetic parts-of-speech . \n\t', '\n\t\t It is important to note that the Penn treebanks do not include dependency annotations ; however , the automatic dependency rules from \n\t\t']",Positive
"['\n\t\t Similar head-finding rules were used for Chinese experiments . \n\t', '\n\t\t The NEGRA corpus , however , does supply hand-annotated dependency structures . \n\t', '\n\t\t structures which specify orders of attachment among multiple dependents which share a common head . \n\t', '\n\t\t \x95 \x95 \x95 \x95 \x95ROOT Figure 2 : Dependency graph with skeleton chosen , but words not populated . \n\t', '\n\t\t Where possible , we report an accuracy figure for both directed and undirected dependencies . \n\t', '\n\t\t Reporting undirected numbers has two advantages : first , it facilitates comparison with earlier work , and , more importantly , it allows one to partially obscure the effects of alternate analyses , such as the systematic choice between a modal and a main verb for the head of a sentence ( in either case , the two verbs would be linked , but the direction would vary ) . \n\t', '\n\t\t 2.2 Dependency Models The dependency induction task has received relatively little attention ; the best known work is \n\t\t']",Positive
"['\n\t\t All systems that we are aware of operate under the assumption that the probability of a dependency structure is the product of the scores of the dependencies ( attachments ) in that structure . \n\t', '\n\t\t Dependencies are seen as ordered ( head , dependent ) pairs of words , but the score of a dependency can optionally condition on other characteristics of the structure , most often the direction of the dependency ( whether the arrow points left or right ) . \n\t', '\n\t\t Some notation before we present specific mod- els : a dependency d is a pair ( h , a ) of a head and argument , which are words in a sentence s , in a cor- pus S . \n\t', '\n\t\t For uniformity of notation with section 4 , words in s are specified as size-one spans of s : for example the first word would be 0s1 . \n\t', '\n\t\t A dependency structure D over a sentence is a set of dependencies ( arcs ) which form a planar , acyclic graph rooted at the special symbol ROOT , and in which each word in s appears as an argument exactly once . \n\t', '\n\t\t For a de- pendency structure D , there is an associated graph G which represents the number of words and arrows between them , without specifying the words them- selves ( see figure 2 ) . \n\t', '\n\t\t A graph G and sentence s to- gether thus determine a dependency structure . \n\t', '\n\t\t The Model Dir . \n\t', '\n\t\t Undir . \n\t', '\n\t\t English ( WSJ ) Paskin 01 39.7 RANDOM 41.7 Charniak and Carroll 92-inspired 44.7 ADJACENT 53.2 DMV 54.4 English ( WSJ10 ) RANDOM 30.1 45.6 ADJACENT 33.6 56.7 DMV 43.2 63.7 German ( NEGRA10 ) RANDOM 21.8 41.5 ADJACENT 32.6 51.2 DMV 36.3 55.8 Chinese ( CTB10 ) RANDOM 35.9 47.3 ADJACENT 30.2 47.3 DMV 42.5 54.2 Figure 3 : Parsing performance ( directed and undirected dependency accuracy ) of various dependency models on various treebanks , along with baselines . \n\t', '\n\t\t dependency structure is the object generated by all of the models that follow ; the steps in the derivations vary from model to model . \n\t', '\n\t\t Existing generative dependency models intended for unsupervised learning have chosen to first generate a word-free graph G , then populate the sentence s conditioned on G . \n\t', '\n\t\t For instance , the model of \n\t\t']",Negative
"['\n\t\t The corresponding probabilistic model is P(D) = P(s , G ) = P(G)P(sIG) = P(G) Y P(i\x971si I j\x971sj , dir ) . \n\t', '\n\t\t ( i , j,dir)EG In \n\t\t']",Negative
"['\n\t\t The parameters for left and right arguments of a single head are completely independent , while the parameters for first and subsequent arguments in the same direction are identified . \n\t', '\n\t\t In those experiments , the model above was trained on over 30M words of raw newswire , using EM in an entirely unsupervised fashion , and at great computational cost . \n\t', '\n\t\t However , as shown in figure 3 , the resulting parser predicted dependencies at below chance level ( measured by choosing a random dependency structure ) . \n\t', '\n\t\t This below-random performance seems to be because the model links word pairs which have high mutual information ( such as occurrences of congress and bill ) regardless of whether they are plausibly syntactically related . \n\t', '\n\t\t In practice , high mutual information between words is often stronger between two topically similar nouns than between , say , a preposition and its object . \n\t', '\n\t\t One might hope that the problem with this model is that the actual lexical items are too semantically charged to represent workable units of syntactic structure . \n\t', '\n\t\t If one were to apply the \n\t\t']",Positive
"['\n\t\t In these models , Carroll and Charniak considered PCFGs with precisely the productions ( discussed above ) that make them isomorphic to dependency grammars , with the terminal alphabet being simply partsof-speech . \n\t', '\n\t\t Here , the rule probabilities are equiva- lent to P(YIX , right ) and P(YIX , left ) respectively.2 The actual experiments in \n\t\t']",Negative
"['\n\t\t With hindsight , however , the main issue in their experiments appears to be not their model , but that they randomly initialized the production ( attachment ) probabilities . \n\t', '\n\t\t As a result , their learned grammars were of very poor quality and had high variance . \n\t', '\n\t\t However , one nice property of their structural constraint , which all dependency models share , is that the symbols in the grammar are not symmetric . \n\t', '\n\t\t Even with a grammar in which the productions are initially uniform , a symbol X can only possibly have non-zero posterior likelihood over spans which contain a matching terminal X . \n\t', '\n\t\t Therefore , one can start with uniform rewrites and let the interaction between the data and the model structure break the initial symmetry . \n\t', '\n\t\t If one recasts their experiments in this way , they achieve an accuracy of 44.7 % on the Penn treebank , which is higher than choosing a random dependency structure , but lower than simply linking all adjacent words into a left-headed ( and right-branching ) structure ( 53.2 % ) . \n\t', '\n\t\t A huge limitation of both of the above models is that they are incapable of encoding even first-order valence facts . \n\t', '\n\t\t For example , the latter model learns that nouns to the left of the verb ( usually subjects ) 2There is another , subtle distinction : in the Paskin work , a canonical ordering of multiple attachments was fixed , while in the Carroll and Charniak work all attachment orders are considered , giving a numerical bias towards structures where heads take more than one argument . \n\t', '\n\t\t h h fal j k i i fal hl j hl k i j hl h STOP STOP i fhl hl j ( a ) ( b ) ( c ) ( d ) Figure 4 : Dependency configurations in a lexicalized tree : ( a ) right attachment , ( b ) left attachment , ( c ) right stop , ( d ) left stop . \n\t', '\n\t\t h and a are head and argument words , respectively , while i , j , and k are positions between words . \n\t', '\n\t\t attach to the verb . \n\t', '\n\t\t But then , given a NOUN NOUN VERB sequence , both nouns will attach to the verb \x96 there is no way that the model can learn that verbs have exactly one subject . \n\t', '\n\t\t We now turn to an improved dependency model that addresses this problem . \n\t', '\n\t\t 3 An Improved Dependency Model The dependency models discussed above are distinct from dependency models used inside high- performance supervised probabilistic parsers in several ways . \n\t', '\n\t\t First , in supervised models , a head outward process is modeled \n\t\t']",Positive
"['\n\t\t In such processes , heads generate a sequence of arguments outward to the left or right , conditioning on not only the identity of the head and direction of the attachment , but also on some notion of distance or valence . \n\t', '\n\t\t Moreover , in a head-outward model , it is natural to model stop steps , where the final argument on each side of a head is always the special symbol STOP . \n\t', '\n\t\t Models like \n\t\t']",Positive
['\n\t\t Previous work \n\t\t'],Positive
"['\n\t\t We propose a simple head-outward dependency model over word classes which includes a model of valence , which we call DMV ( for dependency model with valence ) . \n\t', '\n\t\t We begin at the ROOT . \n\t', '\n\t\t In the standard way , each head generates a series of non- STOP arguments to one side , then a STOP argument to that side , then non-STOP arguments to the other side , then a second STOP . \n\t', '\n\t\t For example , in the dependency structure in figure 1 , we first generate a single child of ROOT , here fell . \n\t', '\n\t\t Then we recurse to the subtree under fell . \n\t', '\n\t\t This subtree begins with generating the right argument in . \n\t', '\n\t\t We then recurse to the subtree under in ( generating September to the right , a right STOP , and a left STOP ) . \n\t', '\n\t\t Since there are no more right arguments after in , its right STOP is generated , and the process moves on to the left arguments offell . \n\t', '\n\t\t In this process , there are two kinds of derivation events , whose local probability factors constitute the model\x92s parameters . \n\t', '\n\t\t First , there is the decision at any point whether to terminate ( generate STOP ) or not : PSTOP(STOP1h , dir , adj ) . \n\t', '\n\t\t This is abi- nary decision conditioned on three things : the head h , the direction ( generating to the left or right of the head ) , and the adjacency ( whether or not an argument has been generated yet in the current direction , a binary variable ) . \n\t', '\n\t\t The stopping decision is estimated directly , with no smoothing . \n\t', '\n\t\t If a stop is generated , no more arguments are generated for the current head to the current side . \n\t', '\n\t\t If the current head\x92s argument generation does not stop , another argument is chosen using : PCHOOSE(a~h , dir ) . \n\t', '\n\t\t Here , the argument is picked conditionally on the identity of the head ( which , recall , is a word class ) and the direction . \n\t', '\n\t\t This term , also , is not smoothed in any way . \n\t', '\n\t\t Adjacency has no effect on the identity of the argument , only on the likelihood of termination . \n\t', '\n\t\t After an argument is generated , its subtree in the dependency structure is recursively generated . \n\t', '\n\t\t Formally , for a dependency structure D , let each word h have left dependents depsD(h,l) and right dependents depsD(h , r ) . \n\t', '\n\t\t The follow- ing recursion defines the probability of the frag- ment D ( h ) of the dependency tree rooted at h : P(D(h)) = Y H PSTOP(\x97STOPIh , dir , adj ) dire~l,r } aedepsD(h,dir) PCHOOSE(a~h,dir)P(D(a)) PSTOP(STOPlh , dir , adj ) One can view a structure generated by this derivational process as a \x93lexicalized\x94 tree composed of the local binary and unary context-free configurations shown in figure 4.3 Each configuration equivalently represents either a head-outward derivation step or a context-free rewrite rule . \n\t', '\n\t\t There are four such configurations . \n\t', '\n\t\t Figure 4(a) shows a head h 3It is lexicalized in the sense that the labels in the tree are derived from terminal symbols , but in our experiments the terminals were word classes , not individual lexical items . \n\t', '\n\t\t taking a right argument a . \n\t', '\n\t\t The tree headed by h contains h itself , possibly some right arguments of h , but no left arguments of h ( they attach after all the right arguments ) . \n\t', '\n\t\t The tree headed by a contains a itself , along with all of its left and right children . \n\t', '\n\t\t Figure 4(b) shows a head h taking a left argument a \x96 the tree headed by h must have already generated its right stop to do so . \n\t', '\n\t\t Figure 4(c) and figure 4(d) show the sealing operations , where STOP derivation steps are generated . \n\t', '\n\t\t The left and right marks on node labels represent left and right STOPs that have been generated.4 The basic inside-outside algorithm \n\t\t']",Positive
"['\n\t\t For each sentence s E S , it gives us cs ( x : i , j ) , the expected fraction of parses of s with a node labeled x extending from position i to position j . \n\t', '\n\t\t The model can be re-estimated from these counts . \n\t', '\n\t\t For example , to re-estimate an entry of PSTOP(STOP|h , left , non-adj ) according to a current model O , we calculate two quantities.5 The first is the ( expected ) number of trees headed by h ] whose rightmost edge i is strictly left of h . \n\t', '\n\t\t The second is the number of trees headed by Fh ] with rightmost edge i strictly left of h . \n\t', '\n\t\t The ratio is the MLE of that local probability factor : PSTOP(STOP|h , left , non-adj ) = EsES K<loc(h) Ek c(h ] : i , k ) EsES K<loc(h) Ekc(Fh ] : i , k ) This can be intuitively thought of as the relative number of times a tree headed by h had already taken at least one argument to the left , had an opportunity to take another , but didn\x92t.6 Initialization is important to the success of any local search procedure . \n\t', '\n\t\t We chose to initialize EM not with an initial model , but with an initial guess at posterior distributions over dependency structures ( completions ) . \n\t', '\n\t\t For the first-round , we constructed a somewhat ad-hoc \x93harmonic\x94 completion where all non-ROOT words took the same number of arguments , and each took other words as arguments in inverse proportion to ( a constant plus ) the distance between them . \n\t', '\n\t\t The ROOT always had a single 4Note that the asymmetry of the attachment rules enforces the right-before-left attachment convention . \n\t', '\n\t\t This is harmless and arbitrary as far as dependency evaluations go , but imposes an x-bar-like structure on the constituency assertions made by this model . \n\t', '\n\t\t This bias/constraint is dealt with in section 5 . \n\t', '\n\t\t 5To simplify notation , we assume each word h occurs at most one time in a given sentence , between indexes loc(h) and loc(h) + 1 ) . \n\t', '\n\t\t 6As a final note , in addition to enforcing the right-argumentfirst convention , we constrained ROOT to have at most a single dependent , by a similar device . \n\t', '\n\t\t argument and took each word with equal probability . \n\t', '\n\t\t This structure had two advantages : first , when testing multiple models , it is easier to start them all off in a common way by beginning with an M-step , and , second , it allowed us to point the model in the vague general direction of what linguistic dependency structures should look like . \n\t', '\n\t\t On the WSJ10 corpus , the DMV model recovers a substantial fraction of the broad dependency trends : 43.2 % of guessed directed dependencies were correct ( 63.7 % ignoring direction ) . \n\t', '\n\t\t To our knowledge , this is the first published result to break the adjacent-word heuristic ( at 33.6 % for this corpus ) . \n\t', '\n\t\t Verbs are the sentence heads , prepositions take following noun phrases as arguments , adverbs attach to verbs , and so on . \n\t', '\n\t\t The most common source of discrepancy between the test dependencies and the model\x92s guesses is a result of the model systematically choosing determiners as the heads of noun phrases , while the test trees have the rightmost noun as the head . \n\t', '\n\t\t The model\x92s choice is supported by a good deal of linguistic research \n\t\t']",Positive
"['\n\t\t On this adjusted metric , the score jumps hugely to 55.7 % directed ( and 67.9 % undirected ) . \n\t', '\n\t\t This model also works on German and Chinese at above-baseline levels ( 55.8 % and 54.2 % undirected , respectively ) , with no modifications whatsoever . \n\t', '\n\t\t In German , the largest source of errors is also the systematic postulation of determiner-headed noun- phrases . \n\t', '\n\t\t In Chinese , the primary mismatch is that subjects are considered to be the heads of sentences rather than verbs . \n\t', '\n\t\t This dependency induction model is reasonably successful . \n\t', '\n\t\t However , our intuition is still that the model can be improved by paying more attention to syntactic constituency . \n\t', '\n\t\t To this end , after briefly recapping the model of \n\t\t']",Positive
"['\n\t\t As we will see , this combined model finds correct dependencies more successfully than the model above , and finds constituents more successfully than the model of \n\t\t']",Negative
"['\n\t\t 4 Distributional Constituency Induction In linear distributional clustering , items ( e.g. , words or word sequences ) are represented by characteristic distributions over their linear contexts ( e.g. , multinomial models over the preceding and following words , see figure 5 ) . \n\t', '\n\t\t These context distributions are then clustered in some way , often using standard Span Label Constituent Context ( 0,5 ) S NN NNS VBD IN NN O\x96O ( 0,2 ) NP NNNNS O\x96VBD ( 2,5 ) VP VBD IN NN NNS\x96O ( 3,5 ) PP IN NN VBD\x96O ( 0,1 ) NN NN O\x96NNS ( 1,2 ) NNS NNS NN\x96VBD ( 2,3 ) VBD VBD NNS\x96IN ( 3,4 ) IN IN VBD \x96 NN ( 4,5 ) NN NNS IN\x96O ( a ) ( b ) Figure 5 : The CCM model\x92s generative process for the sentence in figure 1. ( a ) A binary tree-equivalent bracketing is chosen at random . \n\t', '\n\t\t ( b ) Each span generates its yield and context ( empty spans not shown here ) . \n\t', '\n\t\t Derivations which are not coherent are given mass zero . \n\t', '\n\t\t i-1si \x97 jsj+1 ( see figure 5 ) . \n\t', '\n\t\t The model generates all constituent-context pairs , span by span . \n\t', '\n\t\t The first stage is to choose a bracketing B for the sentence , which is a maximal non-crossing subset of the spans ( equivalent to a binary tree ) . \n\t', '\n\t\t In the basic model , P(B) is uniform over binary trees . \n\t', '\n\t\t Then , for each ( i , j ) , the subspan and context pair ( isj , i-1si \x97 jsj+1 ) is generated via a class- conditional independence model : P(s , B ) = P(B) F1 P(isj 1 bij)P(i-1si \x97 jsj+11 bij ) ( i,j ) data clustering methods . \n\t', '\n\t\t In the most common case , the items are words , and one uses distributions over adjacent words to induce word classes . \n\t', '\n\t\t Previous work has shown that even this quite simple representation allows the induction of quite high quality word classes , largely corresponding to traditional parts of speech ( Finch , 1993 ; Sch¨utze , 1995 ; Clark , 2000 ) . \n\t', '\n\t\t A typical pattern would be that stocks and treasuries both frequently occur before the words fell and rose , and might therefore be put into the same class . \n\t', '\n\t\t \n\t\t']",Negative
"['\n\t\t However , as one might expect , it is easier to cluster word sequences ( or word class sequences ) than to tell how to put them together into trees . \n\t', '\n\t\t In particular , if one is given all contiguous subsequences ( subspans ) from a corpus of sentences , most natural clusters will not represent valid constituents ( to the extent that constituency of a non-situated sequence is even a well-formed notion ) . \n\t', '\n\t\t For example , it is easy enough to discover that DET N and DET ADJ N are similar and that V PREP DET and V PREP DET ADJ are similar , but it is much less clear how to discover that the former pair are generally constituents while the latter pair are generally not . \n\t', '\n\t\t In \n\t\t']",Positive
"['\n\t\t During the calculation of cluster assignments , only a non-crossing subset of the observed word sequences can be assigned to other , constituent clusters . \n\t', '\n\t\t This integrated approach is empirically successful . \n\t', '\n\t\t The CCM works as follows . \n\t', '\n\t\t Sentences are given as sequences s of word classes ( parts-of-speech or otherwise ) . \n\t', '\n\t\t One imagines each sentence as a list of the O(n2) index pairs ( i , j ) , each followed by the corresponding subspan isj and linear context That is , all spans guess their sequences and contexts given only a constituency decision b.7 This is a model P(s , B ) over hidden bracketings and observed sentences , and it is estimated via EM to maximize the sentence likelihoods P(s) over the training corpus . \n\t', '\n\t\t Figure 6 shows the accuracy of the CCM model not only on English but for the Chinese and German corpora discussed above.8 Results are reported at convergence ; for the English case , F1 is monotonic during training , while for the others , there is an earlier peak . \n\t', '\n\t\t Also shown is an upper bound ( the target trees are not all binary and so any all-binary system will over- propose constituents ) . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t While absolute numbers are hard to compare across corpora , all the systems compared to in \n\t\t']",Positive
"['\n\t\t 5 A Combined Model The two models described above have some common ground . \n\t', '\n\t\t Both can be seen as models over lexicalized trees composed of the configurations in figure 4 . \n\t', '\n\t\t For the DMV , it is already a model over these structures . \n\t', '\n\t\t At the \x93attachment\x94 rewrite for the CCM 7As is typical of distributional clustering , positions in the corpus can get generated multiple times . \n\t', '\n\t\t Since derivations need not be consistent , the entire model is mass deficient when viewed as a model over sentences . \n\t', '\n\t\t 8In \n\t\t']",Negative
"['\n\t\t The numbers here hew more closely to the standard methods used for evaluating supervised parsers , by being micro-averaged and including full-span brackets . \n\t', '\n\t\t However , the scores are , overall , approximately the same . \n\t', '\n\t\t in ( a/b ) , we assign the quantity : P(isk1true)P(i_1si - ksk+11true ) P(isk1false)P(i_1si - ksk+11false ) which is the odds ratio of generating the subse- quence and context for span ( i , k ) as a constituent as opposed to a non-constituent . \n\t', '\n\t\t If we multiply all trees\x92 attachment scores by Y(i,j) P(isj 1false)P(i_1si - jsj+1 1false ) the denominators of the odds ratios cancel , and we are left with each tree being assigned the probability it would have received under the CCM.9 In this way , both models can be seen as generating either constituency or dependency structures . \n\t', '\n\t\t Of course , the CCM will generate fairly random dependency structures ( constrained only by bracketings ) . \n\t', '\n\t\t Getting constituency structures from the DMV is also problematic , because the choice of which side to first attach arguments on has ramifications on constituency \x96 it forces x-bar-like structures \x96 even though it is an arbitrary convention as far as dependency evaluations are concerned . \n\t', '\n\t\t For example , if we attach right arguments first , then a verb with a left subject and a right object will attach the object first , giving traditional VPs , while the other attachment order gives subject-verb groups . \n\t', '\n\t\t To avoid this bias , we alter the DMV in the following ways . \n\t', '\n\t\t When using the dependency model alone , we allow each word to have even probability for either generation order ( but in each actual head derivation , only one order occurs ) . \n\t', '\n\t\t When using the models together , better performance was obtained by releasing the one-side-attaching-first requirement entirely . \n\t', '\n\t\t In figure 6 , we give the behavior of the CCM constituency model and the DMV dependency model on both constituency and dependency induction . \n\t', '\n\t\t Unsurprisingly , their strengths are complementary . \n\t', '\n\t\t The CCM is better at recovering constituency , and the dependency model is better at recovering dependency structures . \n\t', '\n\t\t It is reasonable to hope that a combination model might exhibit the best of both . \n\t', '\n\t\t In the supervised parsing domain , for example , scoring a lexicalized tree with the product of a simple lexical dependency model and a PCFG model can outperform each factor on its respective metric \n\t\t']",Positive
"['\n\t\t 9This scoring function as described is not a generative model over lexicalized trees , because it has no generation step at which nodes\x92 lexical heads are chosen . \n\t', '\n\t\t This can be corrected by multiplying in a \x93head choice\x94 factor of 1 /(k _ j ) at each final \x93sealing\x94 configuration ( d ) . \n\t', '\n\t\t In practice , this correction factor was harmful for the model combination , since it duplicated a strength of the dependency model , badly . \n\t', '\n\t\t Model UP UR UF1 Dir Undir English ( WSJ 10 \x96 7422 Sentences ) LBRANCH/RHEAD 25.6 32.6 28.7 33.6 56.7 RANDOM 31.0 39.4 34.7 30.1 45.6 RBRANCH/LHEAD 55.1 70.0 61.7 24.0 55.9 DMV 46.6 59.2 52.1 43.2 62.7 CCM 64.2 81.6 71.9 23.8 43.3 DMV+CCM ( POS ) 69.3 88.0 77.6 47.5 64.5 DMV+CCM ( DISTR . \n\t', '\n\t\t ) 65.2 82.8 72.9 42.3 60.4 UBOUND 78.8 100.0 88.1 100.0 100.0 German ( NEGRA 10 \x96 2175 Sentences ) LBRANCH/RHEAD 27.4 48.8 35.1 32.6 51.2 RANDOM 27.9 49.6 35.7 21.8 41.5 RBRANCH/LHEAD 33.8 60.1 43.3 21.0 49.9 DMV 38.4 69.5 49.5 40.0 57.8 CCM 48.1 85.5 61.6 25.5 44.9 DMV+CCM 49.6 89.7 63.9 50.6 64.7 UBOUND 56.3 100.0 72.1 100.0 100.0 Chinese ( CTB 10 \x96 2437 Sentences ) LBRANCH/RHEAD 26.3 48.8 34.2 30.2 43.9 RANDOM 27.3 50.7 35.5 35.9 47.3 RBRANCH/LHEAD 29.0 53.9 37.8 14.2 41.5 DMV 35.9 66.7 46.7 42.5 54.2 CCM 34.6 64.3 45.0 23.8 40.5 DMV+CCM 33.3 62.0 43.3 55.2 60.3 UBOUND 53.9 100.0 70.1 100.0 100.0 Figure 6 : Parsing performance of the combined model on various treebanks , along with baselines . \n\t', '\n\t\t In the combined model , we score each tree with the product of the probabilities from the individual models above . \n\t', '\n\t\t We use the inside-outside algorithm to sum over all lexicalized trees , similar to the situation in section 3 . \n\t', '\n\t\t The tree configurations are shown in figure 4 . \n\t', '\n\t\t For each configuration , the relevant scores from each model are multiplied together . \n\t', '\n\t\t For example , consider figure 4(a) . \n\t', '\n\t\t From the CCM we must generate isk as a constituent and its corresponding context . \n\t', '\n\t\t From the dependency model , we pay the cost of h taking a as a right argument ( P CHOOSE ) , as well as the cost of choosing not to stop ( PSTOP ) . \n\t', '\n\t\t We then running the inside-outside algorithm over this product model . \n\t', '\n\t\t For the results , we can extract the sufficient statistics needed to re- estimate both individual models . \n\t', '\n\t\t 10 The models in combination were intitialized in the same way as when they were run individually . \n\t', '\n\t\t Sufficient statistics were separately taken off these individual completions . \n\t', '\n\t\t From then on , the resulting models were used together during re-estimation . \n\t', '\n\t\t Figure 6 summarizes the results . \n\t', '\n\t\t The combined model beats the CCM on English F1 : 77.6 vs. 71.9 . \n\t', '\n\t\t The figure also shows the combination model\x92s score when using word classes which were induced entirely automatically , using the simplest distributional clustering method of Sch¨utze ( 1995 ) . \n\t', '\n\t\t These classes show some degradation , e.g. 72.9 F1 , but it 10 The product , like the CCM itself , is mass-deficient . \n\t', '\n\t\t is worth noting that these totally unsupervised numbers are better than the performance of the CCM model of \n\t\t']",Negative
"['\n\t\t Again , if we modify the gold standard so as to make determiners the head of NPs , then this model with distributional tags scores 50.6 % on directed and 64.8 % on undirected dependency accuracy . \n\t', '\n\t\t On the German data , the combination again outperforms each factor alone , though while the combination was most helpful at boosting constituency quality for English , for German it provided a larger boost to the dependency structures . \n\t', '\n\t\t Finally , on the Chinese data , the combination did substantially boost dependency accuracy over either single factor , but actually suffered a small drop in constituency.1 1 Overall , the combination is able to combine the individual factors in an effective way . \n\t', '\n\t\t 6 Conclusion We have presented a successful new dependency- based model for the unsupervised induction of syntactic structure , which picks up the key ideas that have made dependency models successful in supervised statistical parsing work . \n\t', '\n\t\t We proceeded to show that it works cross-linguistically . \n\t', '\n\t\t We then demonstrated how this model could be combined with the previous best constituent-induction model to produce a combination which , in general , substantially outperforms either individual model , on either metric . \n\t', '\n\t\t A key reason that these models are capable of recovering structure more accurately than previous work is that they minimize the amount of hidden structure that must be induced . \n\t', '\n\t\t In particular , neither model attempts to learn intermediate , recursive categories with no direct connection to surface statistics . \n\t', '\n\t\t Our results here are just on the ungrounded induction of syntactic structure . \n\t', '\n\t\t Nonetheless , we see the investigation of what patterns can be recovered from corpora as important , both from a computational perspective and from a philosophical one . \n\t', '\n\t\t It demonstrates that the broad constituent and dependency structure of a language can be recovered quite successfully ( individually or , more effectively , jointly ) from a very modest amount of training data . \n\t', '\n\t\t 7 Acknowledgements This work was supported by a Microsoft Graduate Research Fellowship to the first author and by 11 This seems to be partially due to the large number of unanalyzed fragments in the Chinese gold standard , which leave a very large fraction of the posited bracketings completely unjudged . \n\t', '\n\t\t the Advanced Research and Development Activity (ARDA)\x92s Advanced Question Answering for Intelligence ( AQUAINT ) Program . \n\t', '\n\t\t This work also benefited from an enormous amount of useful feedback , from many audiences and individuals . \n\t', '\n\t\t References Stephen P. Abney . \n\t', '\n\t\t 1987. The English Noun Phrase in its Sentential Aspect . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , MIT . \n\t', '\n\t\t James K. Baker . \n\t', '\n\t\t 1979. Trainable grammars for speech recognition . \n\t', '\n\t\t In D. H. Klatt and J. J. Wolf , editors , Speech Communication Papers for the 97th Meeting of the Acoustical Society of America , pages 547\x96550 . \n\t', '\n\t\t Eric Brill . \n\t', '\n\t\t 1993. Automatic grammar induction and parsing free text : A transformation-based approach . \n\t', '\n\t\t In ACL 31 , pages 259\x96265 . \n\t', '\n\t\t Glenn Carroll and Eugene Charniak . \n\t', '\n\t\t 1992. Two experiments on learning probabilistic dependency grammars from corpora . \n\t', '\n\t\t In Carl Weir , Stephen Abney , Ralph Grishman , and Ralph Weischedel , editors , Working Notes of the Workshop Statistically-Based NLP Techniques , pages 1\x9613 . \n\t', '\n\t\t AAAI Press , Menlo Park , CA . \n\t', '\n\t\t Stanley F. Chen . \n\t', '\n\t\t 1995. Bayesian grammar induction for language modeling . \n\t', '\n\t\t In ACL 33 , pages 228\x96235 . \n\t', '\n\t\t Noam Chomsky . \n\t', '\n\t\t 1965. Aspects of the Theory of Syntax . \n\t', '\n\t\t MIT Press , Cambridge , MA . \n\t', '\n\t\t Alexander Clark . \n\t', '\n\t\t 2000. Inducing syntactic categories by context distribution clustering . \n\t', '\n\t\t In The Fourth Conference on Natural Language Learning . \n\t', '\n\t\t Alexander Clark . \n\t', '\n\t\t 2001. Unsupervised induction of stochastic context- free grammars using distributional clustering . \n\t', '\n\t\t In The Fifth Conference on Natural Language Learning . \n\t', '\n\t\t Michael Collins . \n\t', '\n\t\t 1999. Head-Driven Statistical Models for Natural Language Parsing . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , University of Pennsylvania . \n\t', '\n\t\t Jason Eisner . \n\t', '\n\t\t 1996. Three new probabilistic models for dependency parsing : An exploration . \n\t', '\n\t\t In COLING 16 , pages 340\x96345 . \n\t', '\n\t\t Steven Paul Finch . \n\t', '\n\t\t 1993. Finding Structure in Language . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , University of Edinburgh . \n\t', '\n\t\t Dan Klein and Christopher D. Manning . \n\t', '\n\t\t 2002. A generative constituent-context model for improved grammar induction . \n\t', '\n\t\t In ACL 40 , pages 128\x96135 . \n\t', '\n\t\t Dan Klein and Christopher D. Manning . \n\t', '\n\t\t 2003. Fast exact inference with a factored model for natural language parsing . \n\t', '\n\t\t In Suzanna Becker , Sebastian Thrun , and Klaus Obermayer , editors , Advances in Neural Information Processing Systems 15 , Cambridge , MA . \n\t', '\n\t\t MIT Press . \n\t', '\n\t\t Igor Aleksandrovich Mel~^cuk . \n\t', '\n\t\t 1988. Dependency Syntax : theory and practice . \n\t', '\n\t\t State University of New York Press , Albany , NY . \n\t', '\n\t\t Philip H. Miller . \n\t', '\n\t\t 1999. Strong Generative Capacity . \n\t', '\n\t\t CSLI Publications , Stanford , CA . \n\t', '\n\t\t Mark A. Paskin . \n\t', '\n\t\t 2002. Grammatical bigrams . \n\t', '\n\t\t In T. G. Dietterich , S. Becker , and Z. Ghahramani , editors , Advances in Neural Information Processing Systems 14 , Cambridge , MA . \n\t', '\n\t\t MIT Press . \n\t', '\n\t\t Fernando Pereira and Yves Schabes . \n\t', '\n\t\t 1992. Inside-outside reestimation from partially bracketed corpora . \n\t', '\n\t\t In ACL 30 , pages 128\x96135 . \n\t', '\n\t\t Hinrich Sch¨utze . \n\t', '\n\t\t 1995. Distributional part-of-speech tagging . \n\t', '\n\t\t In EACL 7 , pages 141\x96148 . \n\t', '\n\t\t Zach Solan , Eytan Ruppin , David Horn , and Shimon Edelman . \n\t', '\n\t\t 2003. Automatic acquisition and efficient representation of syntactic structures . \n\t', '\n\t\t In Suzanna Becker , Sebastian Thrun , and Klaus Obermayer , editors , Advances in Neural Information Processing Systems 15 , Cambridge , MA . \n\t', '\n\t\t MIT Press . \n\t', '\n\t\t Andreas Stolcke and Stephen M. Omohundro . \n\t', '\n\t\t 1994. Inducing probabilistic grammars by Bayesian model merging . \n\t', '\n\t\t In Grammatical Inference and Applications : Proceedings of the Second International Colloquium on Grammatical Inference . \n\t', '\n\t\t Springer Verlag . \n\t', '\n\t\t Menno van Zaanen . \n\t', '\n\t\t 2000. ABL : Alignment-based learning . \n\t', '\n\t\t In COLING 18 , pages 961\x96967 . \n\t', '\n\t\t Deniz Yuret . \n\t', '\n\t\t 1998. Discovery of Linguistic Relations Using Lexical Attraction . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , MIT . \n\t', '\n\t\t Annealing Techniques for Unsupervised Statistical Language Learning Noah A. Smith and Jason Eisner Department of Computer Science / Center for Language and Speech Processing Johns Hopkins University , Baltimore , MD 21218 USA {nasmith,jason}@cs.jhu.edu Abstract Exploiting unannotated natural language data is hard largely because unsupervised parameter estimation is hard . \n\t', '\n\t\t We describe deterministic annealing \n\t\t']",Positive
"['\n\t\t Seeking to avoid search error , DA begins by globally maximizing an easy concave function and maintains a local maximum as it gradually morphs the function into the desired non-concave likelihood function . \n\t', '\n\t\t Applying DA to parsing and tagging models is shown to be straightforward ; significant improvements over EM are shown on a part-of-speech tagging task . \n\t', '\n\t\t We describe a variant , skewed DA , which can incorporate a good initializer when it is available , and show significant improvements over EM on a grammar induction task . \n\t', '\n\t\t 1 Introduction Unlabeled data remains a tantalizing potential resource for NLP researchers . \n\t', '\n\t\t Some tasks can thrive on a nearly pure diet of unlabeled data \n\t\t']",Positive
"['\n\t\t But for other tasks , such as machine translation \n\t\t']",Positive
['\n\t\t The standard starting point is the Expectation- Maximization ( EM ) algorithm \n\t\t'],Positive
"['\n\t\t EM iteratively adjusts a model\x92s parameters from an initial guess until it converges to a local maximum . \n\t', '\n\t\t Unfortunately , likelihood functions in practice are riddled with suboptimal local maxima ( e.g. , Charniak , 1993 , ch. 7 ) . \n\t', '\n\t\t Moreover , maximizing likelihood is not equivalent to maximizing task-defined accuracy ( e.g. , Merialdo , 1994 ) . \n\t', '\n\t\t Here we focus on the search error problem . \n\t', '\n\t\t Assume that one has a model for which improving likelihood really will improve accuracy ( e.g. , at predicting hidden part-of-speech ( POS ) tags or parse trees ) . \n\t', '\n\t\t Hence , we seek methods that tend to locate mountaintops rather than hilltops of the likelihood function . \n\t', '\n\t\t Alternatively , we might want methods that find hilltops with other desirable properties.1 1\n\t\t']",Positive
"['\n\t\t § 3 shows how DA can be used for parameter estimation for models of language structure that use dynamic programming to compute posteriors over hidden structure , such as hidden Markov models ( HMMs ) and stochastic context-free grammars ( SCFGs ) . \n\t', '\n\t\t In § 4 we apply DA to the problem of learning a trigram POS tagger without labeled data . \n\t', '\n\t\t We then describe how one of the received strengths of DA\x97 its robustness to the initializing model parameters\x97 can be a shortcoming in situations where the initial parameters carry a helpful bias . \n\t', '\n\t\t We present a solution to this problem in the form of a new algorithm , skewed deterministic annealing ( SDA ; § 5 ) . \n\t', '\n\t\t Finally we apply SDA to a grammar induction model and demonstrate significantly improved performance over EM ( § 6 ) . \n\t', '\n\t\t § 7 highlights future directions for this work . \n\t', '\n\t\t 2 Deterministic annealing Suppose our data consist of a pairs of random variables X and Y , where the value of X is observed and Y is hidden . \n\t', '\n\t\t For example , X might range over sentences in English and Y over POS tag sequences . \n\t', '\n\t\t We use X and Y to denote the sets of possible values of X and Y , respectively . \n\t', '\n\t\t We seek to build a model that assigns probabilities to each ( x , y ) E X x Y . \n\t', '\n\t\t Let x~ = { x1 , x2 , ... , xn } be a corpus of unlabeled examples . \n\t', '\n\t\t Assume the class of models is fixed ( for example , we might consider only first- order HMMs with s states , corresponding notionally to POS tags ) . \n\t', '\n\t\t Then the task is to find good parameters B~ E RN for the model . \n\t', '\n\t\t The criterion most commonly used in building such models from unlabeled data is maximum likelihood ( ML ) ; we seek the parameters ~B* : argmax Pr(~xI ~B ) = argmax ^~ ^~ entropy hilltop . \n\t', '\n\t\t They argue that to account for partially- observed ( unlabeled ) data , one should choose the distribution with the highest Shannon entropy , subject to certain data-driven constraints . \n\t', '\n\t\t They show that this desirable distribution is one of the local maxima of likelihood . \n\t', '\n\t\t Whether high-entropy local maxima really predict test data better is an empirical question . \n\t', ""\n\t\t ~n 11 Pr(xi , y I ~B ) ( 1 ) i=1 yEld Input : ' x , ' B(0) Output : ' B* i 0 do : ( E ) \x98p('y)Pr(x,y|6(*)) ~y~EYn Pr(~x,~y~|~6(*)) , 8'y ( M ) [ B(i+1) +\x97 argmaxe E\x98p(Y) log Pr('x , I ' 0~ ] i+\x97i+1 until ' B(i) ' B(i_1) ' B* ' B(i) Fig . \n\t"", '\n\t\t 1 : The EM algorithm . \n\t', '\n\t\t Each parameter Bj corresponds to the conditional probability of a single model event , e.g. , a state transition in an HMM or a rewrite in a PCFG . \n\t', '\n\t\t Many NLP models make it easy to maximize the likelihood of supervised training data : simply count the model events in the observed ( xi , yi ) pairs , and set the conditional probabilities Bi to be proportional to the counts . \n\t', '\n\t\t In our unsupervised setting , the yi are unknown , but solving ( 1 ) is almost as easy provided that we can obtain the posterior distribution of Y given each xi ( that is , Pr(y xi ) for each y E ~ and each xi ) . \n\t', '\n\t\t The only difference is that we must now count the model events fractionally , using the expected number of occurrences of each ( xi , y ) pair . \n\t', '\n\t\t This intuition leads to the EM algorithm in Fig . \n\t', ""\n\t\t 1. It is guaranteed that Pr('x ' B(i+1)) > Pr('x ' B(i)) . \n\t"", '\n\t\t For language-structure models like HMMs and SCFGs , efficient dynamic programming algorithms ( forward-backward , inside-outside ) are available to compute the distribution p\x98 at the E step of Fig . \n\t', '\n\t\t 1 and use it at the M step . \n\t', '\n\t\t These algorithms run in polynomial time and space by structure-sharing the possible y ( tag sequences or parse trees ) for each xi , of which there may be exponentially many in the length of xi . \n\t', '\n\t\t Even so , the majority of time spent by EM for such models is on the E steps . \n\t', '\n\t\t In this paper , we can fairly compare the runtime of EM and other training procedures by counting the number of E steps they take on a given training set and model . \n\t', '\n\t\t 2.1 Generalizing EM Figure 2 shows the deterministic annealing ( DA ) algorithm derived from the framework of \n\t\t']",Positive
"['\n\t\t It is quite similar to EM.2 However , DA adds an outer loop that iteratively increases a value Q , and computation of the posterior in the E step is modified to involve this Q. 2Other expositions of DA abound ; we have couched ours in data-modeling language . \n\t', '\n\t\t Readers interested in the Lagrangianbased derivations and analogies to statistical physics ( including phase transitions and the role of Q as the inverse of temperature in free-energy minimization ) are referred to \n\t\t']",Positive
"[""\n\t\t Input : ' x , ' B(0) , Qmax > Qmin > 0 , a > 1 Output : ' B* i 0;Q Qmin while Q < Qmax : do : ( E ) \x98p('y)Pr(:F,Y16(*))^ P~y~ EYn Pr(~x,~y~ I6(*) ^,y B(i+1) +\x97 argmaxe E\x98p(V) [ log Pr ( ' x , Y ' ' B ) i+\x97i+1 until ' B(i) , : ' B(i_1) Q a Q end while B* ' ' B(i) Fig . \n\t"", '\n\t\t 2 : The DA algorithm : a generalization of EM . \n\t', '\n\t\t When Q = 1 , DA\x92s inner loop will behave exactly like EM , computing p\x98 at the E step by the same formula that EM uses . \n\t', ""\n\t\t When Q Pz~ 0 , p\x98 will be close to a uniform distribution over the hidden variable ' y , since each numerator Pr('x , y ' ' B)^ pz~ 1 . \n\t"", '\n\t\t At such Q-values , DA effectively ignores the current parameters B when choosing the posterior p\x98 and the new parameters . \n\t', ""\n\t\t Finally , as Q \x97* +oo , p\x98 tends to place nearly all of the probability mass on the single most likely ' y . \n\t"", '\n\t\t This winner-take-all situation is equivalent to the \x93Viterbi\x94 variant of the EM algorithm . \n\t', ""\n\t\t 2.2 Gradated difficulty In both the EM and DA algorithms , the E step selects a posterior p\x98 over the hidden variable Y ' and the M step selects parameters ' B. \n\t\t""]",Positive
"[""\n\t\t DA can also be seen this way ; DA\x92s objective function at a given Q is ( B , p , Q ) = QH(\x98p) + E\x98p(V) [ log Pr('x , I ' ' B ) ] ( 2 ) The EM version simply sets Q = 1 . \n\t"", '\n\t\t A complete derivation is not difficult but is too lengthy to give here ; it is a straightforward extension of that given by Neal and Hinton for EM . \n\t', '\n\t\t It is clear that the value of Q allows us to manipulate the relative importance of the two terms when maximizing T . \n\t', '\n\t\t When Q is close to 0 , only the H term matters . \n\t', '\n\t\t The H term is the Shannon entropy of the posterior distribution \x98p , which is known to be concave in \x98p . \n\t', '\n\t\t Maximizing it is simple : set all x to be equiprobable ( the uniform distribution ) . \n\t', '\n\t\t Therefore a sufficiently small Q drives up the importance of H relative to the other term , and the entire problem becomes concave with a single global maximum to which we expect to converge . \n\t', '\n\t\t In gradually increasing Q from near 0 to 1 , we start out by solving an easy concave maximization problem and use the result to initialize the next max- imization problem , which is slightly more difficult ( i.e. , less concave ) . \n\t', '\n\t\t This continues , with the solution to each problem in the series being used to initialize the subsequent problem . \n\t', '\n\t\t When Q reaches 1 , DA behaves just like EM . \n\t', '\n\t\t Since the objective function is continuous in Q where Q > 0 , we can visualize DA as gradually morphing the easy concave objective function into the one we really care about ( likelihood ) ; we hope to \x93ride the maximum\x94 as Q moves toward 1 . \n\t', '\n\t\t DA guarantees iterative improvement of the objective function ( see \n\t\t']",Positive
"['\n\t\t But it does not guarantee convergence to a global maximum , or even to a better local maximum than EM will find , even with extremely slow Q-raising . \n\t', '\n\t\t A new mountain on the surface of the objective function could arise at any stage that is preferable to the one that we will ultimately find . \n\t', '\n\t\t To run DA , we must choose a few control parameters . \n\t', '\n\t\t In this paper we set Qmax = 1 so that DA will approach EM and finish at a local maximum of likelihood . \n\t', '\n\t\t Qmin and the Q-increase factor a can be set high for speed , but at a risk of introducing local maxima too quickly for DA to work as intended . \n\t', '\n\t\t ( Note that a \x93fast\x94 schedule that tries only a few Q values is not as fast as one might expect , since it will generally take longer to converge at each Q value . \n\t', '\n\t\t ) To conclude the theoretical discussion of DA , we review its desirable properties . \n\t', '\n\t\t DA is robust to initial parameters , since when Q is close to 0 the objective hardly depends on ~~ . \n\t', '\n\t\t DA gradually increases the difficulty of search , which may lead to the avoidance of some local optima . \n\t', '\n\t\t By modifying the annealing schedule , we can change the runtime of the DA algorithm . \n\t', '\n\t\t DA is almost exactly like EM in implementation , requiring only a slight modification to the E step ( see § 3 ) and an additional outer loop . \n\t', '\n\t\t 2.3 Prior work DA was originally described as an algorithm for clustering data in RN \n\t\t']",Positive
"['\n\t\t Its predecessor , simulated annealing , modifies the objective function during search by applying random perturbations of gradually decreasing size \n\t\t']",Positive
"['\n\t\t Deterministic annealing moves the randomness \x93inside\x94 the objective function by taking expectations . \n\t', '\n\t\t DA has since been applied to many problems \n\t\t']",Positive
"['\n\t\t Pereira , Tishby , and \n\t\t']",Positive
"['\n\t\t In their case , when Q is close to 0 , each noun is fuzzily placed in each cluster so that Pr(cluster I noun ) is nearly uniform . \n\t', '\n\t\t On the M step , this results in clusters that are almost exactly identical ; there is one effective cluster . \n\t', '\n\t\t As Q is increased , it becomes increasingly attractive for the cluster centroids to move apart , or \x93split\x94 into two groups ( two effective clusters ) , and eventually they do so . \n\t', '\n\t\t Continuing to increase Q yields a hierarchical clustering through repeated splits . \n\t', '\n\t\t Pereira et al . \n\t', '\n\t\t describe the tradeoff given through Q as a control on the locality of influence of each noun on the cluster centroids , so that as Q is raised , each noun exerts less influence on more distant centroids and more on the nearest centroids . \n\t', '\n\t\t DA has also been applied in speech recognition . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t Their goal was to optimize not likelihood but classification error rate , a difficult objective function that is piecewiseconstant ( hence not differentiable everywhere ) and riddled with shallow local minima . \n\t', '\n\t\t Rao and Rose applied DA,3 moving from training a nearly uniform classifier with a concave cost surface ( Q ^ 0 ) toward the desired deterministic classifier ( Q ^ +oc ) . \n\t', '\n\t\t They reported substantial gains in spoken letter recognition accuracy over both a ML-trained classifier and a localized error-rate optimizer . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t Their training algorithm began by running an EM approximation on the simplest model , then used the result to initialize the next , more complex model ( which had greater predictive power and many more parameters ) , and so on . \n\t', '\n\t\t Whereas DA provides gradated difficulty in parameter search , their learning method involves gradated difficulty among classes of models . \n\t', '\n\t\t The two are orthogonal and could be used together . \n\t', '\n\t\t 3 DA with dynamic programming We turn now to the practical use of deterministic annealing in NLP . \n\t', '\n\t\t Readers familiar with the EM algorithm will note that , for typical stochastic models of language structure ( e.g. , HMMs and SCFGs ) , the bulk of the computational effort is required by the E step , which is accomplished by a two-pass dynamic programming ( DP ) algorithm ( like the forward-backward algorithm ) . \n\t', '\n\t\t The M step for these models normalizes the posterior expected counts from the E step to get probabilities .4 3 With an M step modified for their objective function : it improved expected accuracy under P , not expected log-likelihood . \n\t', '\n\t\t 4That is , assuming the usual generative parameterization of such models ; if we generalize to Markov random fields ( also known as log-linear or maximum entropy models ) the M step , while still concave , might entail an auxiliary optimization routine such as iterative scaling or a gradient-based method . \n\t', '\n\t\t Running DA for such models is quite simple and requires no modifications to the usual DP algorithms . \n\t', '\n\t\t The only change to make is in the values of the parameters passed to the DP algorithm : simply replace each ^j by ^,3j . \n\t', '\n\t\t For a given x , the forward pass of the DP computes ( in a dense representation ) Pr(y I x , ~^ ) for all y . \n\t', '\n\t\t Each Pr(y I x , ~^ ) is a product of some of the ^j ( each ^j is multiplied in once for each time its corresponding model event is present in ( x , y ) ) . \n\t', '\n\t\t Raising the ^j to a power will also raise their product to that power , so the forward pass will compute Pr(y I x , ~^),3 when given ~^,3 as parameter values . \n\t', '\n\t\t The backward pass normalizes to the sum ; in this case it is the sum of the Pr(y I x , ~^),3 , and we have the E step described in Figure 2 . \n\t', '\n\t\t We therefore expect an EM iteration of DA to take the same amount of time as a normal EM iteration.5 4 Part-of-speech tagging We turn now to the task of inducing a trigram POS tagging model ( second-order HMM ) from an unlabeled corpus . \n\t', '\n\t\t This experiment is inspired by the experiments in \n\t\t']",Positive
"['\n\t\t As in that work , complete knowledge of the tagging dictionary is assumed . \n\t', '\n\t\t The task is to find the trigram transition probabilities Pr(tagi I tagi-1 , tagi-2 ) and emis- sion probabilities Pr(wordi I tagi ) . \n\t', '\n\t\t Merialdo\x92s key result:6 If some labeled data were used to initialize the parameters ( by taking the ML estimate ) , then it was not helpful to improve the model\x92s likelihood through EM iterations , because this almost always hurt the accuracy of the model\x92s Viterbi tagging on a held-out test set . \n\t', '\n\t\t If only a small amount of labeled data was used ( 200 sentences ) , then some accuracy improvement was possible using EM , but only for a few iterations . \n\t', '\n\t\t When no labeled data were used , EM was able to improve the accuracy of the tagger , and this improvement continued in the long term . \n\t', '\n\t\t Our replication of Merialdo\x92s experiment used the Wall Street Journal portion of the Penn Tree- bank corpus , reserving a randomly selected 2,000 sentences ( 48,526 words ) for testing . \n\t', '\n\t\t The remaining 47,208 sentences ( 1,125,240 words ) were used in training , without any tags . \n\t', '\n\t\t The tagging dictionary was constructed using the entire corpus ( as done by Merialdo ) . \n\t', '\n\t\t To initialize , the conditional transition and emission distributions in the HMM were set to uniform with slight perturbation . \n\t', '\n\t\t Every distribution was smoothed using add-0.1 smoothing ( at every M 5 With one caveat : less pruning may be appropriate because probability mass is spread more uniformly over different recon- structions of the hidden data . \n\t', '\n\t\t This paper uses no pruning . \n\t', '\n\t\t 6Similar results were found by \n\t\t']",Positive
"['\n\t\t 0 200 400 600 800 1000 1200 EM iterations step ) . \n\t', '\n\t\t The criterion for convergence is that the relative increase in the objective function between two iterations fall below 10-9 . \n\t', '\n\t\t 4.1 Experiment In the DA condition , we set Qmin = 0.0001 , Qmax = 1 , and a = 1.2 . \n\t', '\n\t\t Results for the completely unsupervised condition ( no labeled data ) are shown in Figure 3 and Table 1 . \n\t', '\n\t\t Accuracy was nearly monotonic : the final model is approximately the most accurate . \n\t', '\n\t\t DA happily obtained a 10 % reduction in tag error rate on training data , and an 11 % reduction on test data . \n\t', '\n\t\t On the other hand , it did not manage to improve likelihood over EM . \n\t', '\n\t\t So was the accuracy gain mere luck ? \n\t', '\n\t\t Perhaps not . \n\t', '\n\t\t DA may be more resistant to overfitting , because it may favor models whose posteriors p\x98 have high entropy . \n\t', '\n\t\t At least in this experiment , its initial bias toward such models carried over to the final learned model.7 In other words , the higher-entropy local maximum found by DA , in this case , explained the observed data almost as well without overcommitting to particular tag sequences . \n\t', '\n\t\t The maximum entropy and latent maximum entropy principles ( Wang et al. , 2003 , discussed in footnote 1 ) are best justified as ways to avoid overfitting . \n\t', '\n\t\t For a supervised tagger , the maximum entropy principle prefers a conditional model Pr(~yI ~x ) that is maximally unsure about what tag sequence y~ to apply to the training word sequence x~ ( but expects the same feature counts as the true y- ) . \n\t', '\n\t\t Such a model is hoped to generalize better to unsupervised data . \n\t', '\n\t\t We can make the same argument . \n\t', '\n\t\t But in our case , the split between supervised/unsupervised data is not the split between training/test data . \n\t', '\n\t\t Our supervised data are , roughly , the fragments of the training corpus that are unambiguously tagged thanks to the tag dictionary.8 The EM model may overfit some 7 We computed the entropy over possible tags for each word in the test corpus , given the sentence the word occurs in . \n\t', '\n\t\t On average , the DA model had 0.082 bits per tag , while EM had only 0.057 bits per tag , a statistically significant difference ( p < 10-6 ) under a binomial sign test on word tokens . \n\t', '\n\t\t 8 Without the tag dictionary , our learners would treat the tag 70 65 60 55 50 45 40 75 Fig . \n\t', '\n\t\t 3 : Learning curves for EM and DA . \n\t', '\n\t\t Steps in DA\x92s curve correspond to ( -changes . \n\t', '\n\t\t The shape of the DA curve is partly a function of the an nealing schedule , which only gradually ( and EM in steps ) allows the parameters to move away from the uniform distribution . \n\t', '\n\t\t DA E steps final training cross- entropy ( bits/word ) final test cross- entropy ( bits/word ) % correct training tags ( all ) ( ambiguous ) % correct test tags ( all ) ( ambiguous ) EM DA 279 9.136 9.321 82.04 66.61 82.08 66.63 1200 9.138 9.325 83.85 70.02 84.00 70.25 Table 1 : EM vs. DA on unsupervised trigram POS tagging , using a tag dictionary . \n\t', '\n\t\t Each of the accuracy results is significant when accuracy is compared at either the word-level or sentence-level . \n\t', '\n\t\t ( Significance at p < 10-6 under a binomial sign test in each case . \n\t', '\n\t\t E.g. , on the test set , the DA model correctly tagged 1,652 words that EM\x92s model missed while EM correctly tagged 726 words that DA missed . \n\t', '\n\t\t Similarly , the DA model had higher accuracy on 850 sentences , while EM had higher accuracy on only 287 . \n\t', '\n\t\t These differences are extremely unlikely to occur due to chance . \n\t', '\n\t\t ) The differences in cross-entropy , compared by sentence , were significant in the training set but not the test set ( p < 0.01 under a binomial sign test ) . \n\t', '\n\t\t Recall that lower cross entropy means higher likelihood . \n\t', '\n\t\t parameters to these fragments . \n\t', '\n\t\t The higher-entropy DA model may be less likely to overfit , allowing it to do better on the unsupervised data\x97i.e. , the rest of the training corpus and the entire test corpus . \n\t', '\n\t\t We conclude that DA has settled on a local maximum of the likelihood function that ( unsurprisingly ) corresponds well with the entropy criterion , and perhaps as a result , does better on accuracy . \n\t', '\n\t\t 4.2 Significance Seeking to determine how well this result generalized , we randomly split the corpus into ten equally- sized , nonoverlapping parts . \n\t', '\n\t\t EM and DA were run on each portion;9 the results were inconclusive . \n\t', '\n\t\t DA achieved better test accuracy than EM on three of ten trials , better training likelihood on five trials , and better test likelihood on all ten trials.10 Certainly decreasing the amount of data by an order of magnitude results in increased variance of the performance of any algorithm\x97so ten small corpora were not enough to determine whether to expect an improvement from DA more often than not . \n\t', '\n\t\t 4.3 Mixing labeled and unlabeled data ( I ) In the other conditions described by Merialdo , varying amounts of labeled data ( ranging from 100 sentences to nearly half of the corpus ) were used to initialize the parameters ~^ , which were then trained using EM on the remaining unlabeled data . \n\t', '\n\t\t Only in the case where 100 labeled examples were used , and only for a few iterations , did EM improve the names as interchangeable and could not reasonably be evaluated on gold-standard accuracy . \n\t', '\n\t\t 9The smoothing parameters were scaled down so as to be proportional to the corpus size . \n\t', '\n\t\t 10It is also worth noting that runtimes were longer with the 10%-sized corpora than the full corpus ( EM took 1.5 times as many E steps ; DA , 1.3 times ) . \n\t', '\n\t\t Perhaps the algorithms traveled farther to find a local maximum . \n\t', '\n\t\t We know of no study of the effect of unlabeled training set size on the likelihood surface , but suggest two issues for future exploration . \n\t', '\n\t\t Larger datasets contain more idiosyncrasies but provide a stronger overall signal . \n\t', '\n\t\t Hence , we might expect them to yield a bumpier likelihood surface whose local maxima are more numerous but also differ more noticeably in height . \n\t', '\n\t\t Both these tendencies of larger datasets would in theory increase DA\x92s advantage over EM . \n\t', '\n\t\t accuracy of this model . \n\t', '\n\t\t We replicated these experiments and compared EM with DA ; DA damaged the models even more than EM . \n\t', '\n\t\t This is unsurprising ; as noted before , DA effectively ignores the initial parameters ~^(0) . \n\t', '\n\t\t Therefore , even if initializing with a model trained on small amounts of labeled data had helped EM , DA would have missed out on this benefit . \n\t', '\n\t\t In the next section we address this issue . \n\t', '\n\t\t 5 Skewed deterministic annealing The EM algorithm is quite sensitive to the initial parameters ~^(0) . \n\t', '\n\t\t We touted DA\x92s insensitivity to those parameters as an advantage , but in scenarios where well-chosen initial parameters can be provided ( as in § 4.3 ) , we wish for DA to be able exploit them . \n\t', '\n\t\t In particular , there are at least two cases where \x93good\x94 initializers might be known . \n\t', '\n\t\t One is the case explored by Merialdo , where some labeled data were available to build an initial model . \n\t', '\n\t\t The other is a situation where a good distribution is known over the labels y ; we will see an example of this in § 6 . \n\t', '\n\t\t We wish to find a way to incorporate an initializer into DA and still reap the benefit of gradated difficulty . \n\t', ""\n\t\t To see how this will come about , consider again the E step for DA , which for all y : where u is the uniform distribution over ~ and Z'(~^ , 0 ) and Z(~^ , 0 ) = Z'(~^ , 0 ) · u(y)1-3 are nor malizing terms . \n\t"", '\n\t\t ( Note that Z(~^ , 0 ) does not depend on y because u(y) is constant with respect to y . \n\t', '\n\t\t ) Of course , when 0 is close to 0 , DA chooses the uniform posterior because it has the highest entropy . \n\t', '\n\t\t Seen this way , DA is interpolating in the log domain between two posteriors : the one given by y and ^~ and the uniform one u ; the interpolation coefficient is 0 . \n\t', '\n\t\t To generalize DA , we will replace the uniform u with another posterior , the \x93skew\x94 posterior ´p , which is an input to the algorithm . \n\t', '\n\t\t This posterior might be specified directly , as it will be in § 6 , or it might be computed using an M step from some good initial ~^(0) . \n\t', '\n\t\t = Pr(x , y ~^)^u(y)1 \x97^ ~^ , 0 ) Z( \x98p(y) ^ Pr(x , y ~^ , 0 ) ~^)^ Z/( The skewed DA ( SDA ) E step is given by : \x98p(y) ^ Z ) Pr(x , y I B)^´p(y)1^^ ( 3 ) When Q is close to 0 , the E step will choose p\x98 to be very close to ´p . \n\t', '\n\t\t With small Q , SDA is a \x93cautious\x94 EM variant that is wary of moving too far from the initializing posterior p´ ( or , equivalently , the initial parameters ~B(0)) . \n\t', '\n\t\t As Q approaches 1 , the effect of p´ will diminish , and when Q = 1 , the algorithm becomes identical to EM . \n\t', '\n\t\t The overall objective ( matching ( 2 ) except for the boxed term ) is : F~(B , \x98p , Q ) = 1 H(\x98p) + E\x98p(V) [ log Pr ( x , Y I B ) ] + 1QE\x98p(v) [ log p´ ( Y ) ] Mixing labeled and unlabeled data ( II ) Returning to Merialdo\x92s mixed conditions ( § 4.3 ) , we found that SDA repaired the damage done by DA but did not offer any benefit over EM . \n\t', '\n\t\t Its behavior in the 100-labeled sentence condition was similar to that of EM\x92s , with a slightly but not significantly higher peak in training set accuracy . \n\t', '\n\t\t In the other conditions , SDA behaved like EM , with steady degradation of accuracy as training proceeded . \n\t', '\n\t\t It ultimately damaged performance only as much as EM did or did slightly better than EM ( but still hurt ) . \n\t', '\n\t\t This is unsurprising : Merialdo\x92s result demonstrated that ML and maximizing accuracy are generally not the same ; the EM algorithm consistently degraded the accuracy of his supervised models . \n\t', '\n\t\t SDA is simply another search algorithm with the same criterion as EM . \n\t', '\n\t\t SDA did do what it was expected to do\x97it used the initializer , repairing DA damage . \n\t', '\n\t\t 6 Grammar induction We turn next to the problem of statistical grammar induction : inducing parse trees over unlabeled text . \n\t', '\n\t\t An excellent recent result is by \n\t\t']",Positive
"['\n\t\t The constituent-context model ( CCM ) they present is a generative , deficient channel model of POS tag strings given binary tree bracketings . \n\t', '\n\t\t We first review the model and describe a small modification that reduces the deficiency , then compare both models under EM and DA . \n\t', '\n\t\t 6.1 Constituent-context model Let ( x , y ) be a ( tag sequence , binary tree ) pair . \n\t', '\n\t\t xji denotes the subsequence of x from the ith to the jth word . \n\t', '\n\t\t Let yi,j be 1 if the yield from i to j is a constituent in the tree y and 0 if it is not . \n\t', '\n\t\t The CCM gives to a pair ( x , y ) the following probability : Pr(x , y ) = Pr(y) \x95 H [ ( xi yi,j ) 1<i<j<|x| ^(xi-1,xj+1I yi,j ) ] where ^ is a conditional distribution over possi- ble tag-sequence yields ( given whether the yield is a constituent or not ) and ^ is a conditional distribution over possible contexts of one tag on either side of the yield ( given whether the yield is a constituent or not ) . \n\t', '\n\t\t There are therefore four distributions to be estimated ; Pr(y) is taken to be uniform . \n\t', '\n\t\t The model is initialized using expected counts of the constituent and context features given that all the trees are generated according to a random-split model.11 The CCM generates each tag not once but O(n2) times , once by every constituent or non-constituent span that dominates it . \n\t', '\n\t\t We suggest the following modification to alleviate some of the deficiency : [ ^ ( xj yi,j,^i+1 ) i\x95^ ( xi-1 , xj+1I yi,j ) ] The change is to condition the yield feature ^ on the length of the yield . \n\t', '\n\t\t This decreases deficiency by disallowing , for example , a constituent over a four- tag yield to generate a seven-tag sequence . \n\t', '\n\t\t It also decreases inter-parameter dependence by breaking the constituent ( and non-constituent ) distributions into a separate bin for each possible constituent length . \n\t', '\n\t\t We will refer to Klein and Manning\x92s CCM and our version as models 1 and 2 , respectively . \n\t', '\n\t\t 6.2 Experiment We ran experiments using both CCM models on the tag sequences of length ten or less in the Wall Street Journal Penn Treebank corpus , after extracting punctuation . \n\t', '\n\t\t This corpus consists of 7,519 sentences ( 52,837 tag tokens , 38 types ) . \n\t', '\n\t\t We report PARSEVAL scores averaged by constituent ( rather than by sentence ) , and do not give the learner credit for getting full sentences or single tags as constituents.12 Because the E step for this model is computationally intensive , we set the DA parameters at Qmin = 0.01 , a = 1.5 so that fewer E steps would be necessary . \n\t', '\n\t\t 13 The convergence criterion was relative improvement < 10-9 in the objective . \n\t', '\n\t\t The results are shown in Table 2 . \n\t', '\n\t\t The first point to notice is that a uniform initializer is a bad idea , as Klein and Manning predicted . \n\t', '\n\t\t All conditions but 11We refer readers to \n\t\t']",Positive
"['\n\t\t Klein and Manning\x92s argument for this initialization step is that it is less biased toward balanced trees than the uniform model used during learning ; we also found that it works far better in practice . \n\t', '\n\t\t 12 This is why the CCM 1 performance reported here differs from Klein and Manning\x92s ; our implementation of the EM condition gave virtually identical results under either evaluation scheme ( D. Klein , personal communication ) . \n\t', '\n\t\t 13A pilot study got very similar results for , 3min = 10-6 . \n\t', '\n\t\t Pr(x , y ) = Pr(y) \x95 H 1<i<j<|x E steps cross-entropy ( bits/tag ) UR UP F CB CCM 1 EM ( uniform ) 146 103.1654 61.20 45.62 52.27 1.69 DA 403 103.1542 55.13 41.10 47.09 1.91 EM ( split ) 124 103.1951 78.14 58.24 66.74 0.98 SDA ( split ) 339 103.1651 62.71 46.75 53.57 1.62 CCM 2 EM ( uniform ) 26 84.8106 57.60 42.94 49.20 1.86 DA 331 84.7899 40.81 30.42 34.86 2.66 EM ( split ) 44 84.8049 78.56 58.56 67.10 0.98 SDA ( split ) 290 84.7940 79.64 59.37 68.03 0.93 Table 2 : The two CCM models , trained with two unsupervised algorithms , each with two initializers . \n\t', '\n\t\t Note that DA is equivalent to SDA initialized with a uniform distribution . \n\t', '\n\t\t The third line corresponds to the setup reported by \n\t\t']",Positive
"['\n\t\t UR is unlabeled recall , UP is unlabeled precision , F is their harmonic mean , and CB is the average number of crossing brackets per sentence . \n\t', '\n\t\t All evaluation is on the same data used for unsupervised learning ( i.e. , there is no training/test split ) . \n\t', '\n\t\t The high cross-entropy values arise from the deficiency of models 1 and 2 , and are not comparable across models . \n\t', '\n\t\t one find better structure when initialized with Klein and Manning\x92s random-split model . \n\t', '\n\t\t ( The exception is SDA on model 1 ; possibly the high deficiency of model 1 interacts poorly with SDA\x92s search in some way . \n\t', '\n\t\t ) Next we note that with the random-split initializer , our model 2 is a bit better than model 1 on PARSEVAL measures and converges more quickly . \n\t', '\n\t\t Every instance of DA or SDA achieved higher log-likelihood than the corresponding EM condition . \n\t', '\n\t\t This is what we hoped to gain from annealing : better local maxima . \n\t', '\n\t\t In the case of model 2 with the random-split initializer , SDA significantly outperformed EM ( comparing both matches and crossing brackets per sentence under a binomial sign test , p < 10-6 ) ; we see a > 5 % reduction in average crossing brackets per sentence . \n\t', '\n\t\t Thus , our strategy of using DA but modifying it to accept an initializer worked as desired in this case , yielding our best overall performance . \n\t', '\n\t\t The systematic results we describe next suggest that these patterns persist across different training sets in this domain . \n\t', '\n\t\t 6.3 Significance The difficulty we experienced in finding generalization to small datasets , discussed in § 4.2 , was apparent here as well . \n\t', '\n\t\t For 10-way and 3-way random , nonoverlapping splits of the dataset , we did not have consistent results in favor of either EM or SDA . \n\t', '\n\t\t Interestingly , we found that training model 2 ( using EM or SDA ) on 10 % of the corpus resulted on average in models that performed nearly as well on their respective training sets as the full corpus condition did on its training set ; see Table 3 . \n\t', '\n\t\t In addition , SDA sometimes performed as well as EM under model 1 . \n\t', '\n\t\t For a random two-way split , EM and SDA converged to almost identical solutions on one of the sub-corpora , and SDA outperformed EM significantly on the other ( on model 2 ) . \n\t', '\n\t\t In order to get multiple points of comparison of EM and SDA on this task with a larger amount of data , we jack-knifed the WSJ-10 corpus by splitting it randomly into ten equally-sized nonoverlapping parts then training models on the corpus with each of the ten sub-corpora excluded . \n\t', '\n\t\t 14 These trials are not independent of each other ; any two of the sub-corpora have 89 of their training data in common . \n\t', '\n\t\t Aggregate results are shown in Table 3 . \n\t', '\n\t\t Using model 2 , SDA always outperformed EM , and in 8 of 10 cases the difference was significant when comparing matching constituents per sentence ( 7 of 10 when comparing crossing constituents ) . \n\t', '\n\t\t 15 The variance of SDA was far less than that of EM ; SDA not only always performed better with model 2 , but its performance was more consistent over the trials . \n\t', '\n\t\t We conclude this experimental discussion by cautioning that both CCM models are highly deficient models , and it is unknown how well they generalize to corpora of longer sentences , other languages , or corpora of words ( rather than POS tags ) . \n\t', '\n\t\t 7 Future work There are a number of interesting directions for future work . \n\t', '\n\t\t Noting the simplicity of the DA algorithm , we hope that current devotees of EM will run comparisons of their models with DA ( or SDA ) . \n\t', '\n\t\t Not only might this improve performance of exist- 14 Note that this is not a cross-validation experiment ; results are reported on the unlabeled training set , and the excluded sub- corpus remains unused . \n\t', '\n\t\t 15Binomial sign test , with significance defined as p < 0.05 , though all significant results had p < 0.001 . \n\t', '\n\t\t 10 % corpus 90 % corpus µF QF µF QF CCM 1 EM 65.00 1.091 66.12 0.6643 SDA 63.00 4.689 53.53 0.2135 CCM 2 EM 66.74 1.402 67.24 0.7077 SDA 66.77 1.034 68.07 0.1193 Table 3 : The mean p and standard deviation v of F-measure performance for 10 trials using 10 % of the corpus and 10 jackknifed trials using 90 % of the corpus . \n\t', '\n\t\t ing systems , it will contribute to the general understanding of the likelihood surface for a variety of problems ( e.g. , this paper has raised the question of how factors like dataset size and model deficiency affect the likelihood surface ) . \n\t', '\n\t\t DA provides a very natural way to gradually introduce complexity to clustering models \n\t\t']",Positive
"['\n\t\t This comes about by manipulating the Q parameter ; as it rises , the number of effective clusters is allowed to increase . \n\t', '\n\t\t An open question is whether the analogues of \x93clusters\x94 in tagging and parsing models\x97tag symbols and grammatical categories , respectively\x97might be treated in a similar manner under DA . \n\t', '\n\t\t For instance , we might begin with the CCM , the original formulation of which posits only one distinction about constituency ( whether a span is a constituent or not ) and gradually allow splits in constituent-label space , resulting in multiple grammatical categories that , we hope , arise naturally from the data . \n\t', '\n\t\t In this paper , we used Qmax = 1 . \n\t', '\n\t\t It would be interesting to explore the effect on accuracy of \x93quenching,\x94 a phase at the end of optimization that rapidly raises Q from 1 to the winner-take-all ( Viterbi ) variant at Q = +^ . \n\t', '\n\t\t Finally , certain practical speedups may be possible . \n\t', '\n\t\t For instance , increasing Qmin and a , as noted in § 2.2 , will vary the number of E steps required for convergence . \n\t', '\n\t\t We suggested that the change might result in slower or faster convergence ; optimizing the schedule using an online algorithm ( or determining precisely how these parameters affect the schedule in practice ) may prove beneficial . \n\t', '\n\t\t Another possibility is to relax the convergence criterion for earlier Q values , requiring fewer E steps before increasing Q , or even raising Q slightly after every E step ( collapsing the outer and inner loops ) . \n\t', '\n\t\t 8 Conclusion We have reviewed the DA algorithm , describing it as a generalization of EM with certain desirable properties , most notably the gradual increase of difficulty of learning and the ease of implementation for NLP models . \n\t', '\n\t\t We have shown how DA can be used to improve the accuracy of a trigram POS tagger learned from an unlabeled corpus . \n\t', '\n\t\t We described a potential shortcoming of DA for NLP applications\x97its failure to exploit good initializers\x97and then described a novel algorithm , skewed DA , that solves this problem . \n\t', '\n\t\t Finally , we reported significant improvements to a state-of-the-art grammar induction model using SDA and a slight modification to the parameterization of that model . \n\t', '\n\t\t These results support the case that annealing tech- niques in some cases offer performance gains over the standard EM approach to learning from unlabeled corpora , particularly with large corpora . \n\t', '\n\t\t Acknowledgements This work was supported by a fellowship to the first author from the Fannie and John Hertz Foundation , and by an NSF ITR grant to the second author . \n\t', '\n\t\t The views expressed are not necessarily endorsed by the sponsors . \n\t', '\n\t\t The authors thank Shankar Kumar , Charles Schafer , David Smith , and Roy Tromble for helpful comments and discussions ; three ACL reviewers for advice that improved the paper ; Eric Goldlust for keeping the Dyna compiler \n\t\t']",Positive
"['\n\t\t References P. F. Brown , J. Cocke , S. A. Della Pietra , V. J. Della Pietra , F. Jelinek , J. D. Lafferty , R. L. Mercer , and P. S. Roossin . \n\t', '\n\t\t 1990. A statistical approach to machine translation . \n\t', '\n\t\t Computational Linguistics , 16(2):79\x9685 . \n\t', '\n\t\t E. Charniak . \n\t', '\n\t\t 1993. Statistical Language Learning . \n\t', '\n\t\t MIT Press . \n\t', '\n\t\t M. Collins and Y . \n\t', '\n\t\t Singer . \n\t', '\n\t\t 1999. Unsupervised models for named-entity classification . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ofEMNLP . \n\t', '\n\t\t T. M. . \n\t', '\n\t\t Cover and J. A. Thomas . \n\t', '\n\t\t 1991. Elements ofInformation Theory . \n\t', '\n\t\t John Wiley and Sons . \n\t', '\n\t\t S. Cucerzan and D. Yarowsky . \n\t', '\n\t\t 2003. Minimally supervised induction of grammatical gender . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ofHLT/NAACL . \n\t', '\n\t\t A. Dempster , N. Laird , and D. Rubin . \n\t', '\n\t\t 1977. Maximum likelihood estimation from incomplete data via the EM algorithm . \n\t', '\n\t\t Journal of the Royal Statistical Society B , 39:1\x9638 . \n\t', '\n\t\t J. Eisner , E. Goldlust , and N. A. Smith . \n\t', '\n\t\t 2004. Dyna : A declarative language for implementing dynamic programs . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ofACL ( companion volume ) . \n\t', '\n\t\t D. Elworthy . \n\t', '\n\t\t 1994. Does Baum-Welch re-estimation help taggers ? \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ofANLP . \n\t', '\n\t\t S. Kirkpatrick , C. D. Gelatt , and M. P. Vecchi . \n\t', '\n\t\t 1983. Optimiza- tion by simulated annealing . \n\t', '\n\t\t Science , 220:671\x96680 . \n\t', '\n\t\t D. Klein and C. D. Manning . \n\t', '\n\t\t 2002. A generative constituent- context model for grammar induction . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ofACL . \n\t', '\n\t\t B. Merialdo . \n\t', '\n\t\t 1994. Tagging English text with a probabilistic model . \n\t', '\n\t\t Computational Linguistics , 20(2):155\x9672 . \n\t', '\n\t\t R. Neal and G. Hinton . \n\t', '\n\t\t 1998. A view of the EM algorithm that justifies incremental , sparse , and other variants . \n\t', '\n\t\t In M. I. Jordan , editor , Learning in Graphical Models . \n\t', '\n\t\t Kluwer . \n\t', '\n\t\t F. C. N. Pereira , N. Tishby , and L. Lee . \n\t', '\n\t\t 1993. Distributional clustering of English words . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ofACL . \n\t', '\n\t\t A. Rao and K. Rose . \n\t', '\n\t\t 2001. Deterministically annealed design of Hidden Markov Model speech recognizers . \n\t', '\n\t\t IEEE Transactions on Speech and Audio Processing , 9(2):111\x96126 . \n\t', '\n\t\t K. Rose , E. Gurewitz , and G. C. Fox. 1990 . \n\t', '\n\t\t Statistical mechanics and phase transitions in clustering . \n\t', '\n\t\t Physical Review Letters , 65(8):945\x96948 . \n\t', '\n\t\t K. Rose . \n\t', '\n\t\t 1998. Deterministic annealing for clustering , compression , classification , regression , and related optimization problems . \n\t', '\n\t\t Proc . \n\t', '\n\t\t of the IEEE , 86(11):2210\x962239 . \n\t', '\n\t\t N. Ueda and R. Nakano . \n\t', '\n\t\t 1998. Deterministic annealing EM algorithm . \n\t', '\n\t\t Neural Networks , 11(2):271\x96282 . \n\t', '\n\t\t S. Wang , D. Schuurmans , and Y. Zhao . \n\t', '\n\t\t 2003. The latent maximum entropy principle . \n\t', '\n\t\t In review . \n\t', '\n\t\t D. Yarowsky . \n\t', '\n\t\t 1995. Unsupervised word sense disambiguation rivaling supervised methods . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ofACL . \n\t', '\n\t\t Multi-Engine Machine Translation with Voted Language Model Tadashi Nomoto National Institute of Japanese Literature 1-16-10 Yutaka Shinagawa Tokyo 142-8585 Japan nomoto@acm.org Abstract The paper describes a particular approach to multi- engine machine translation ( MEMT ) , where we make use of voted language models to selectively combine translation outputs from multiple off-theshelf MT systems . \n\t', '\n\t\t Experiments are done using large corpora from three distinct domains . \n\t', '\n\t\t The study found that the use of voted language models leads to an improved performance of MEMT systems . \n\t', '\n\t\t 1 Introduction As the Internet grows , an increasing number of commercial MT systems are getting on line ready to serve anyone anywhere on the earth . \n\t', '\n\t\t An interesting question we might ponder is whether it is not possible to aggregate the vast number of MT systems available on the Internet into one super MT which surpasses in performance any of those MTs that comprise the system . \n\t', '\n\t\t And this is what we will be concerned with in the paper , with somewhat watered-down settings . \n\t', '\n\t\t People in the speech community pursued the idea of combining off-the-shelf ASRs ( automatic speech recognizers ) into a super ASR for some time , and found that the idea works \n\t\t']",Positive
"['\n\t\t In IR ( information retrieval ) , we find some efforts going ( under the name of distributed IR or meta-search ) to selectively fuse outputs from multiple search engines on the Internet \n\t\t']",Positive
"['\n\t\t So it would be curious to see whether we could do the same with MTs . \n\t', '\n\t\t Now back in machine translation , we do find some work addressing such concern : \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
['\n\t\t \n\t\t'],Positive
['\n\t\t Similar efforts are also found in \n\t\t'],Positive
['\n\t\t The present paper builds on the prior work by \n\t\t'],Positive
"['\n\t\t We start by reviewing his approach , and go on to demonstrate that it could be improved by capitalizing on dependence of the MEMT model there on language model . \n\t', '\n\t\t Throughout the paper , we refer to commercial black box MT systems as OTS ( off-the-shelf ) systems , or more simply , OTSs . \n\t', '\n\t\t 2 Confidence Models We take it here that the business of MEMT is about choosing among translation outputs from multiple MT systems , whether black box or not , for each input text . \n\t', '\n\t\t Therefore the question we want to address is , how do we go about choosing among MT outputs so that we end up with a best one ? \n\t', '\n\t\t What we propose to do is to use some confidence models for translations generated by OTSs , and let them decide which one we should pick . \n\t', '\n\t\t We essentially work along the lines of \n\t\t']",Positive
"['\n\t\t We review below some of the models proposed there , together with some motivation behind them . \n\t', '\n\t\t Confidence models he proposes come in two varieties : Fluency based model ( FLM ) and Alignment based model ( ALM ) , which is actually an extension of FLM . \n\t', '\n\t\t Now suppose we have an English sentence e and its Japanese translation j generated by some OTS . \n\t', '\n\t\t ( One note here : throughout the paper we work on English to Japanese translation . \n\t', '\n\t\t ) FLM dictates that the quality of j as a translation of e be deter- mined by : FLM(e , j ) = logPl(j) ( 1 ) Pl ( j ) is the probability of j under a particular language model ( LM ) l.1 What FLM says is that the quality of a translation essentially depends on its log likelihood ( or fluency ) and has nothing to do with what it is a translation of . \n\t', '\n\t\t ALM extends FLM to include some information on fidelity . \n\t', '\n\t\t That is , it pays some attention to how faithful a translation is to its source text . \n\t', '\n\t\t ALM does this by using alignment models from the statistical machine translation literature \n\t\t']",Positive
"['\n\t\t Here is what ALM looks like . \n\t', '\n\t\t ALM ( e , j ) = log Pl(j)Q(e I j ) Q(e I j ) is the probability estimated using IBM Model 1 . \n\t', '\n\t\t ALM takes into account the fluency of a translation output ( given by Pl ( j ) ) and the degree of association between e and j ( given by Q(e I j ) ) , which are in fact two features generally agreed in the MT literature to be most relevant for assessing the quality of translations \n\t\t']",Positive
"['\n\t\t One problem with FLM and ALM is that they fail to take into account the reliability of an OTS system . \n\t', '\n\t\t As \n\t\t']",Positive
"['\n\t\t ALM and FLM work solely on statistical information that can be gathered from source and target sentences , dismissing any operational bias that an OTS might have on a particular task . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t What SVR is intended to do is to modify confidence scores FLM and ALM produce for MT outputs in such a way that they may more accurately reflect their independent evaluation involving human translations or judgments . \n\t', '\n\t\t SVR is a multi-dimensional regressor , and works pretty much like its enormously popular counterpart , Support Vector classification , except that we are going to work with real numbers for target values and construct the margin , using Vapnik\x92s c-insensitive loss function ( Sch¨olkopf et al. , 1998 ) . \n\t', '\n\t\t 1Note that Pl ( j ) = P(l) Qmi P(wi wi_2 , wi_1 , l ) where j = w1 \x95 \x95 \x95 wm . \n\t', '\n\t\t Assume a uniform prior for l. SVR looks something like this . \n\t', '\n\t\t h(~x) = w~\x95 x~ + b , with input data x~ = ( x 1 , ... , xm ) and the corresponding weights w~ = ( w1 , ... , wm ) . \n\t', '\n\t\t \x91x \x95 y\x92 denotes the inner product of x and y. x~ could be a set of features associated with e and j. Parameters w~ and b are something determined by SVR . \n\t', '\n\t\t It is straightforward to extend the ALM and FLM with SVR , which merely consists of plugging in either model as an input variable in the regressor . \n\t', '\n\t\t This would give us the following two SVR models with m = 1 . \n\t', '\n\t\t Regressive FLM ( rFLM ) h(FLM(e , j ) ) = w1 \x95 FLM(e , j ) + b Regressive ALM ( rALM ) h(ALM(e , j ) ) = w1 \x95 ALM(e , j ) + b Notice that h(\x95) here is supposed to relate FLM or ALM to some independent evaluation metric such as BLEU \n\t\t']",Positive
"['\n\t\t With confidence models in place , define a MEMT model IF by : IF(e , J,l ) = arg maxj ^J(0(e , jI l ) ) Here e represents a source sentence , J a set of translations for e generated by OTSs , and 0 denotes some confidence model under an LM l . \n\t', '\n\t\t Throughout the rest of the paper , we let FLMP and ALMP denote MEMT systems based on FLM and ALM , respectively , and similarly for others . \n\t', '\n\t\t 3 Notes on Evaluation We assume here that the MEMT works on a sentence-by-sentence basis . \n\t', '\n\t\t That is , it takes as input a source sentence , gets it translated by several OTSs , and picks up the best among translations it gets . \n\t', '\n\t\t Now a problem with using BLEU in this setup is that translations often end up with zero because model translations they refer to do not contain n- grams of a particular length.2 This would make impossible a comparison and selection among possible translations . \n\t', '\n\t\t 2In their validity study of BLEU , \n\t\t']",Positive
['\n\t\t Also \n\t\t'],Positive
"['\n\t\t One way out of this , \n\t\t']",Positive
"['\n\t\t Imagine a system which operates by choosing , among candidates , a translation that gives a best m-precision . \n\t', '\n\t\t We would reasonably expect the system to outperform any of its component OTSs . \n\t', '\n\t\t Indeed \n\t\t']",Positive
"['\n\t\t Moreover , since rFLMP and rALMP work on a sentence , not on a block of them , what h(.) relates to is not BLEU , but m-precision . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t The rationale for this is that it is often the case that the efficacy of MEMT systems does not translate into performance of outputs that they generate . \n\t', '\n\t\t We recall that with BLEU , one measures performance of translations , not how often a given MEMT system picks the best translation among candidates . \n\t', '\n\t\t The problem is , even if a MEMT is right about its choices more often than a best component engine , BLEU may not show it . \n\t', '\n\t\t This happens because a best translation may not always get a high score in BLEU . \n\t', '\n\t\t Indeed , differences in BLEU among candidate translations could be very small . \n\t', '\n\t\t Now what \n\t\t']",Positive
"['\n\t\t where S(i , j ) is the Kronecker delta function , which gives 1 if i = j and 0 otherwise . \n\t', '\n\t\t Here 0m rep- resents some MEMT system , 0m(e) denotes a par- ticular translation 0m chooses for sentence e , i.e. , 0m(e) = ^(e , J , l ) . \n\t', '\n\t\t ^e1 ... ^eM E J denotes a set of candidate translations . \n\t', '\n\t\t max here gives a transla- tion with the highest score in m-precision . \n\t', '\n\t\t N is the number of source sentences . \n\t', '\n\t\t S(.) says that you get 1 if a particular translation the MEMT chooses for a given sentences happens to rank highest among can- 3 For a reference translation r and a machine-generated translation t , m -precision is defined as : P vE:Si t C(v , r ) PvE:Sti C(v , t ) which is nothing more than Papineni et al . \n\t', '\n\t\t (2002)\x92s modified n-gram precision applied to a pair of a single reference and the associated translation . \n\t', '\n\t\t Sit here denotes a set of i-grams in t , v an i-gram . \n\t', '\n\t\t C(v , t ) indicates the count of v in t. \n\t\t']",Positive
"['\n\t\t didates . \n\t', '\n\t\t d(0m) gives the average ratio of the times 0m hits a right translation . \n\t', '\n\t\t Let us call d(0m) HF accuracy ( HFA ) for the rest of the paper . \n\t', '\n\t\t 4 LM perplexity and MEMT performance Now the question we are interested in asking is whether the choice of LM really matters . \n\t', '\n\t\t That is , does a particular choice of LM gives a better performing FLMP or ALMP than something else , and if it does , do we have a systematic way of choosing one LM over another ? \n\t', '\n\t\t Let us start with the first question . \n\t', '\n\t\t As a way of shedding some light on the issue , we ran FLMP and ALMP using a variety of LMs , derived from various domains with varying amount of training data . \n\t', '\n\t\t We worked with 24 LMs from various genres , with vocabulary of size ranging from somewhere near 10K to 20K in words ( see below and also Appendix A for details on train sets ) . \n\t', '\n\t\t LMs here are trigram based and created using an open source speech recognition tool called JULIUS.4 Now train data for LMs are collected from five corpora , which we refer to as CPC , EJP , PAT , LIT , NIKMAI for the sake of convenience . \n\t', '\n\t\t CPC is a huge set of semi-automatically aligned pairs of English and Japanese texts from a Japanese news paper which contains as many as 150,000 sentences \n\t\t']",Positive
"['\n\t\t Both LIT and NIKMAI are monolingual . \n\t', '\n\t\t Fig.1 gives a plot of HF accuracy by perplexity for FLMP\x92s on test sets pulled out of PAT , EJP and CPC.6 Each dot there represents an FLMP with a particular LM plugged into it . \n\t', '\n\t\t The HFA of each FLMP in Fig.1 represents a 10-fold cross validated HFA score , namely an HFA averaged over evenly- 4http://julius.sourceforge.jp 5A bibliographic note . \n\t', '\n\t\t NTCIR-3 PATENT : NII Test Collection for Information Retrieval Systems distributed through National Institute of Informatics ( www.nii.ac.jp ) . \n\t', '\n\t\t 6A test set from EJP and CPC each contains 7,500 bilingual sentences , that from PAT contains 4,600 bilingual abstracts ( approximately 9,200 sentences ) . \n\t', '\n\t\t None of them overlaps with the remaining part of the corresponding data set . \n\t', '\n\t\t Relevant LMs are built on Japanese data drawn from the data sets . \n\t', '\n\t\t We took care not to train LMs on test sets . \n\t', '\n\t\t ( See Section 6 for further details . \n\t', '\n\t\t ) d(0m) = ~i S(0~e),max{^e1 ...^eM } ) N m -precision = XN i Figure 1 : HF accuracy-by-perplexity plots for FLMP with four OTSs , Ai , Lo , At , Ib , on PAT ( left ) , CPC ( center ) and EJP ( right ) . \n\t', '\n\t\t Dots represent FLMP\x92s with various LMs . \n\t', '\n\t\t split 10 blocks of a test set . \n\t', '\n\t\t The perplexity is that of Pl ( j ) averaged over blocks , with a particular LM plugged in for l ( see Equation 1 ) . \n\t', '\n\t\t We can see there an apparent tendency for an LM with lower perplexity to give rise to an FLMP with higher HFA , indicating that the choice of LM does indeed influence the performance of FLMP . \n\t', '\n\t\t Which is somewhat surprising given that the perplexity of a machine generated translation should be independent of how similar it is to a model translation , which dictates the HFA.7 Now let us turn to the question of whether there is any systematic way of choosing an LM so that it gives rise to a FLMP with high HFA . \n\t', '\n\t\t Since we are working with multiple OTS systems here , we get multiple outputs for a source text . \n\t', '\n\t\t Our idea is to let them vote for an LM to plug into FLMP or for that matter , any other forms of MEMT discussed earlier . \n\t', '\n\t\t Note that we could take an alternate approach of letting a model ( or human ) translation ( associated with a source text ) pick an LM by alone . \n\t', '\n\t\t An obvious problem with this approach , however , is that a mandatory reference to model translations would compromise the robustness of the approach . \n\t', '\n\t\t We would want the LM to work for MEMT regardless of whether model translations are available . \n\t', '\n\t\t So our concern here is more with choosing an LM in the absence of model translations , to which we will return below . \n\t', '\n\t\t 5 Voting Language Model We consider here a simple voting scheme a ` la ROVER \n\t\t']",Positive
"['\n\t\t Table 1 : A MEMT algorithm implementing V-byM . \n\t', '\n\t\t 5 represents a set of OTS systems , L a set of language models . \n\t', '\n\t\t 0 is some confidence model such (r)FLM or (r)ALM . \n\t', '\n\t\t V-by-M chooses a most-votedfor LM among those in L , given the set J of translations for e. MEMT(e,5,L) begin J= { j I j is a translation of e generated by s E 5 . \n\t', '\n\t\t } l = V-by-M(J , L ) jk = arg maxJEJ(0(e , j Il ) ) return jk end up an LM voted for by the majority . \n\t', '\n\t\t More specifically , for each output translation for a given input , we first pick up an LM which gives it the smallest perplexity , and out of those LMs , one picked by the majority of translations will be plugged into MEMT . \n\t', '\n\t\t We call the selection scheme voting-by-majority or simply V-by-M . \n\t', '\n\t\t The V-by-M scheme is motivated by the results in Fig . \n\t', '\n\t\t 1 , where perplexity is found to be a reasonably good predictor of HFA . \n\t', '\n\t\t Formally , we could put the V-by-M scheme as follows . \n\t', '\n\t\t For each of the translation outputs j~1 ... j~n associated with a given input sentence e , we want to find some LM M from a set L of LMs such that : Mi = arg minmELPP(jiI m ) , where PP(j I m ) is the perplexity of j under m . \n\t', '\n\t\t Now assume M1 ... \n\t', '\n\t\t Mn are such LMs for j~1 . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t j~n . \n\t', '\n\t\t Then we pick up an M with the largest frequency and plug it into 0 such as FLM.8 Suppose , for instance , that Ma , Mb , Ma and M , are lowest perplexity LMs found for translations ji,j2,j3 and j4 , respectively . \n\t', '\n\t\t Then we choose Ma as an LM most voted for , because it gets two votes from ji and j3 , meaning that Ma is nominated as an LM with lowest perplexity by ji and j3 , while Mb and M , each collect only one vote . \n\t', '\n\t\t In case of ties , we randomly choose one of the LMs with the largest count of votes . \n\t', '\n\t\t 6 Experiment Setup and Procedure Let us describe the setup of experiments we have conducted . \n\t', '\n\t\t The goal here is to learn how the Vby-M affects the overall MEMT performance . \n\t', '\n\t\t For test sets , we carry over those from the perplexity experiments ( see Footnote 6 , Section 4 ) , which are derived from CPC , EJP , and PAT . \n\t', '\n\t\t ( Call them tCPC , tEJP , and tPAT hereafter . \n\t', '\n\t\t ) In experiments , we begin by splitting a test set into equal-sized blocks , each containing 500 sentences for tEJP and tCPC , and 100 abstracts ( approximately 200 sentences ) for tPAT.9 We had the total of 15 blocks for tCPC and tEJP , and 46 blocks for tPAT . \n\t', '\n\t\t We leave one for evaluation and use the rest for training alignment models , i.e. , Q(e I j ) , SV regressors and some inside-data LMs . \n\t', '\n\t\t ( Again we took care not to inadvertently train LMs on test sets . \n\t', '\n\t\t ) We send a test block to OTSs Ai , Lo , At , and Ib , for translation and combine their outputs using the V-by-M scheme , which may or may not be coupled with regression SVMs . \n\t', '\n\t\t Recall that the MEMT operates on a sentence by sentence basis . \n\t', '\n\t\t So what happens here is that for each of the sentences in a block , the MEMT works the four MT systems to get translations and picks one that produces the best score under 0 . \n\t', '\n\t\t We evaluate the MEMT performance by running HFA and BLEU on MEMT selected translations block by block,10 and giving average performance over the blocks . \n\t', '\n\t\t Table 1 provides algorithmic details on how the MEMT actually operates . \n\t', '\n\t\t 8It is worth noting that the voted language model readily lends itself to a mixture model : P(j) = Pm^m AmP(j I m ) where Am = 1 if m is most voted for and 0 otherwise . \n\t', '\n\t\t 9tCPC had the average of 15,478 words per block , whereas tEJP had about 11,964 words on the average in each block . \n\t', '\n\t\t With tPAT , however , the average per block word length grew to 16,150 . \n\t', '\n\t\t 10We evaluate performance by block , because of some reports in the MT literature that warn that BLEU behaves erratically on a small set of sentences \n\t\t']",Positive
"['\n\t\t See also Section 3 and Footnote 2 for the relevant discussion . \n\t', '\n\t\t Table 2 : HF accuracy of MEMT models with V-byM . \n\t', '\n\t\t Model tCPC tEJP tPAT avg . \n\t', '\n\t\t rFLMO 0.4230 0.4510 0.8066 0.5602 rALMO 0.4194 0.4346 0.8093 0.5544 FLMO 0.4277 0.4452 0.7342 0.5357 ALMO 0.4453 0.4485 0.7702 0.5547 Table 3 : HF accuracy of MEMT models with randomly chosen LMs . \n\t', '\n\t\t Note how FLMO and ALMO drop in performance . \n\t', '\n\t\t Model tCPC tEJP tPAT avg . \n\t', '\n\t\t rFLMO 0.4207 0.4186 0.8011 0.5468 rALMO 0.4194 0.4321 0.8095 0.5537 FLMO 0.4126 0.3520 0.6350 0.4665 ALMO 0.4362 0.3597 0.6878 0.4946 7 Results and Discussion Now let us see what we found from the experiments . \n\t', '\n\t\t We ran the MEMT on a test set with (r)FLM or (r)ALM embedded in it . \n\t', '\n\t\t Recall that our goal here is to find how the V-by-M affects performance of MEMT on tCPC , tEJP , and tPAT . \n\t', '\n\t\t First , we look at whether the V-by-M affects in any way , the HFA of the MEMT , and if it does , then how much . \n\t', '\n\t\t Table 2 and Table 3 give summaries of results on HFA versus V-by-M . \n\t', '\n\t\t Table 2 shows how things are with V-by-M on , and Table 3 shows what happens to HFA when we turn off V-by-M , that is , when we randomly choose an LM from the same set that the V-by-M chooses from . \n\t', '\n\t\t The results indicate a clear drop in performance of FLMO and ALMO when one chooses an LM randomly.11 Curiously , however , rFLMO and rALMO are affected less . \n\t', '\n\t\t They remain roughly at the same level of HFA over Table 2 and Table 3 . \n\t', '\n\t\t What this means 11Another interesting question to ask at this point is , how does one huge LM trained across domains compare to the Vby-M here ? \n\t', '\n\t\t By definition of perplexity , the increase in size of the training data leads to an increase in perplexity of the LM . \n\t', '\n\t\t So if general observations in Fig.1 hold , then we would expect the \x93one-huge-LM\x94 approach to perform poorly compared to the V-by-M , which is indeed demonstrated by the following results . \n\t', '\n\t\t HFLMO below denotes a FLMO based on a composite LM trained over CPC , LIT , PAT , NIKMAI , and EJP . \n\t', '\n\t\t The testing procedure is same as that described in Sec.6 Model tCPC tEJP tPAT avg . \n\t', '\n\t\t HFLMO ( HFA ) 0.4182 0.4081 0.6927 0.5063 HFLMO ( BLEU ) 0.1710 0.2619 0.1874 0.2067 Table 4 : Performance in BLEU of MEMT models with V-by-M . \n\t', '\n\t\t Model tCPC tEJP tPAT avg . \n\t', '\n\t\t rFLMO 0.1743 0.2861 0.1954 0.2186 rALMO 0.1735 0.2869 0.1954 0.2186 FLMO 0.1736 0.2677 0.1907 0.2107 ALMO 0.1763 0.2622 0.1934 0.2106 Table 7 : Performance of OTS systems in BLEU . \n\t', '\n\t\t Model tCPC tEJP tPAT avg . \n\t', '\n\t\t Ai 0.1495 0.2874 0.1385 0.1918 Lo 0.1440 0.1711 0.1402 0.1518 At 0.1738 0.1518 0.1959 0.1738 Ib 0.1385 0.1589 0.1409 0.1461 OPM 0.2111 0.3308 0.1995 0.2471 Table 5 : Performance in BLEU of MEMT models with randomly chosen LMs . \n\t', '\n\t\t Model tCPC tEJP tPAT avg . \n\t', '\n\t\t rFLMO 0.1738 0.2717 0.1950 0.2135 rALMO 0.1735 0.2863 0.1954 0.2184 FLMO 0.1710 0.2301 0.1827 0.1946 ALMO 0.1745 0.2286 0.1871 0.1967 is that there is some discrepancy in the effectiveness of V-by-M between the fluency based and regression based models . \n\t', '\n\t\t We have no explanation for the cause of the discrepancy at this time , though we may suspect that in learning , as long as there is some pattern to exploit in m-precision and the probability estimates of test sentences , how accurate those estimates are may not matter much . \n\t', '\n\t\t Table 4 and Table 5 give results in BLEU.12 The results tend to replicate what we found with HFA . \n\t', '\n\t\t rFLMO and rALMO keep the edge over FLMO and ALMO whether or not V-by-M is brought into action . \n\t', '\n\t\t The differences in performance between rFLMO and rALMO with or without the V-by-M scheme are rather negligible . \n\t', '\n\t\t However , if we turn to FLMO and ALMO , the effects of the V-by-M are clearly visible . \n\t', '\n\t\t FLMO scores 0.2107 when coupled with the V-by-M . \n\t', '\n\t\t However , when disengaged , the score slips to 0.1946 . \n\t', '\n\t\t The same holds for ALMO . \n\t', '\n\t\t Table 6 : HF accuracy of OTS systems Model tCPC tEJP tPAT avg . \n\t', '\n\t\t Ai 0.2363 0.4319 0.0921 0.2534 Lo 0.1718 0.2124 0.0504 0.1449 At 0.4211 0.1681 0.8037 0.4643 Ib 0.1707 0.1876 0.0537 0.1373 OPM 1.0000 1.0000 1.0000 1.0000 12 The measurements in BLEU here take into account up to trigrams . \n\t', '\n\t\t Leaving the issue of MEMT models momentarily , let us see how the OTS systems Ai , Lo , At , and Ib are doing on tCPC , tEJP , and tPAT . \n\t', '\n\t\t Note that the whole business of MEMT would collapse if it slips behind any of the OTS systems that compose it . \n\t', '\n\t\t Table 6 and Table 7 show performance of the four OTS systems plus OPM , by HFA and by BLEU . \n\t', '\n\t\t OPM here denotes an oracle MEMT which operates by choosing in hindsight a translation that gives the best score in m-precision , among those produced by OTSs . \n\t', '\n\t\t It serves as a practical upper bound for MEMT while OTSs serve as baselines . \n\t', '\n\t\t First , let us look at Table 6 and compare it to Table 2 . \n\t', '\n\t\t A good news is that most of the OTS systems do not even come close to the MEMT models . \n\t', '\n\t\t At , a best performing OTS system , gets 0.4643 on the average , which is about 20 % less than that scored by rFLMO . \n\t', '\n\t\t Turning to BLEU , we find again in Table 7 that a best performing system among the OTSs , i.e. , Ai , is outperformed by FLMO , ALMO and all their varieties ( Table 4 ) . \n\t', '\n\t\t Also something of note here is that on tPAT , (r)FLMO and (r)ALMO in Table 4 , which operate by the V-by-M scheme , score somewhere from 0.1907 to 0.1954 in BLEU , coming close to OPM , which scores 0.1995 on tPAT ( Table 7 ) . \n\t', '\n\t\t It is interesting to note , incidentally , that there is some discrepancy between BLEU and HFA in performance of the OTSs : A top performing OTS in Table 6 , namely At , achieves the average HFA of 0.4643 , but scores only 0.1738 for BLEU ( Table 7 ) , which is worse than what Ai gets . \n\t', '\n\t\t Apparently , high HFA does not always mean a high BLEU score . \n\t', '\n\t\t Why ? \n\t', '\n\t\t The reason is that a best MT output need not mark a high BLEU score . \n\t', '\n\t\t Notice that \x91best\x92 here means the best among translations by the OTSs . \n\t', '\n\t\t It could happen that a poor translation still gets chosen as best , because other translations are far worse . \n\t', '\n\t\t To return to the discussion of (r)FLMO and (r)ALMO , an obvious fact about their behavior is that regressor based systems rFLMO and rALMO , whether V-by-M enabled or not , surpass in performance their less sophisticated counterparts ( see Table 8 : HF accuracy of MEMTs with perturbed S V regressor in the V-by-M scheme . \n\t', '\n\t\t Model tCPC tEJP tPAT avg . \n\t', ""\n\t\t rFLM'P 0.4230 0.4353 0.6712 0.5098 rALM'P 0.4195 0.4302 0.5582 0.4693 FLM'P 0.4277 0.4452 0.7342 0.5357 ALM'P 0.4453 0.4485 0.7702 0.5547 Table 9 : Performance in BLEU of MEMTs with perturbed SV regressor in the V-by-M scheme . \n\t"", '\n\t\t Model tCPC tEJP tPAT avg . \n\t', ""\n\t\t rFLM'P 0.1743 0.2823 0.1835 0.2134 rALM'P 0.1736 0.2843 0.1696 0.2092 FLM'P 0.1736 0.2677 0.1907 0.2107 ALM'P 0.1763 0.2622 0.1934 0.2106 Table 2,4 and also Table 3,5 ) . \n\t"", '\n\t\t Regression allows the MEMT models to correct themselves for some domain-specific bias of the OTS systems . \n\t', '\n\t\t But the downside of using regression to capitalize on their bias is that you may need to be careful about data you train a regressor on . \n\t', '\n\t\t Here is what we mean . \n\t', '\n\t\t We ran experiments using SVM regressors trained on a set of data randomly sampled from tCPC , tEJP , and tPAT . \n\t', ""\n\t\t ( In contrast , rFLM'P and rALM'P in earlier experiments had a regressor trained separately on each data set . \n\t"", '\n\t\t ) They all operated in the V-by-M mode . \n\t', '\n\t\t The results are shown in Table 8 and Table 9 . \n\t', ""\n\t\t What we find there is that with regressors trained on perturbed data , both rFLM'P and rALM'P are not performing as well as before ; in fact they even fall behind FLM'P and ALM'P in HFA and their performance in BLEU turns out to be just about as good as FLM'P and ALM'P . \n\t"", '\n\t\t So regression may backfire when trained on wrong data . \n\t', '\n\t\t 8 Conclusion Let us summarize what we have done and learned from the work . \n\t', '\n\t\t We started with a finding that the choice of language model could affect performance of MEMT models of which it is part . \n\t', '\n\t\t The V-by-M was introduced as a way of responding to the problem of how to choose among LMs so that we get the best MEMT . \n\t', '\n\t\t We have shown that the V-by-M scheme is indeed up to the task , predicting a right LM most of the time . \n\t', '\n\t\t Also worth mentioning is that the MEMT models here , when coupled with V-byM , are all found to surpass component OTS systems by a respectable margin ( cf. , Tables 4 , 7 for BLEU , 2 , 6 for HFA ) . \n\t', ""\n\t\t Regressive MEMTs such as rFLM'P and rALM'P , are found to be not affected as much by the choice of LM as their non-regressive counterparts . \n\t"", '\n\t\t We suspect this happens because they have access to extra information on the quality of translation derived from human judgments or translations , which may cloud effects of LMs on them . \n\t', '\n\t\t But we also pointed out that regressive models work well only when they are trained on right data ; if you train them across different sources of varying genres , they could fail . \n\t', '\n\t\t An interesting question that remains to be addressed is how we might deal with translations from a novel domain . \n\t', '\n\t\t One possible approach would be to use a dynamic language model which adapts itself for a new domain by re-training itself on data sampled from the Web \n\t\t']",Positive
"['\n\t\t References Yasuhiro Akiba , Taro Watanabe , and Eiichiro Sumita . \n\t', '\n\t\t 2002. Using language and translation models to select the best among outputs from multiple mt systems . \n\t', '\n\t\t In Proceedings of the 19th International Conference on Computational Linguistics ( COLING 2002 ) , Taipei . \n\t', '\n\t\t Adam Berger and Robert Miller . \n\t', '\n\t\t 1998. Just-intime language modelling . \n\t', '\n\t\t In Proceedings of ICASSP98 . \n\t', '\n\t\t Ralf Brown and Robert Frederking . \n\t', '\n\t\t 1995. Applying statistical English language modelling to symbolic machine translation . \n\t', '\n\t\t In Proceedings of the Sixth International Conference on Theoretical and Methodological Issues in Machine Translation ( TMI\x9295 ) , pages 221\x96239 , Leuven , Belgium , July . \n\t', '\n\t\t Peter F. Brown , Stephen A. Della Pietra , Vincent J.Della Pietra , and Robert L. Mercer . \n\t', '\n\t\t 1993. The mathematics of statistical machine translation : Parameter estimation . \n\t', '\n\t\t Computational Linguistics , 19(2):263\x96311 , June . \n\t', '\n\t\t Jamie Callan , Fabio Crestani , Henrik Nottelmann , Pietro Pala , and Xia Mang Shou . \n\t', '\n\t\t 2003. Resource selection and data fusion in multimedia distributed digital libaries . \n\t', '\n\t\t In Proceedings of the 26th Annual International ACM/SIGIR Conference on Research and Development in Information Retrieval . \n\t', '\n\t\t ACM . \n\t', '\n\t\t Jonathan G. Fiscus . \n\t', '\n\t\t 1997. A post-processing system to yield reduced word error rates : Recogniser output voting error reduction ( ROVER ) . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t IEEE ASRU Workshop , pages 347\x96352 , Santa Barbara . \n\t', '\n\t\t Rober Frederking and Sergei Nirenburg . \n\t', '\n\t\t 1994. Three heads are better than one . \n\t', '\n\t\t In Proceedings ofthe Fourth Conference on Applied Natural Language Processing , Stuttgart . \n\t', '\n\t\t Christopher Hogan and Robert E. Frederking . \n\t', '\n\t\t 1998. An evaluation of the multi-engine MT architecture . \n\t', '\n\t\t In Proceedings of the Third Conference of the Association for Machine Translation in the Americas ( AMTA \x9298 ) , pages 113\x96123 , Berlin , October . \n\t', '\n\t\t Springer-Verlag . \n\t', '\n\t\t Lecture Notes in Artificial Intelligence 1529 . \n\t', '\n\t\t Tadashi Nomoto . \n\t', '\n\t\t 2003 . \n\t', '\n\t\t Predictive models of performance in multi-engine machine translation . \n\t', '\n\t\t In Proceedings of Machine Translation Summit IX , New Orleans , September . \n\t', '\n\t\t IAMT . \n\t', '\n\t\t Kishore Papineni , Salim Roukos , Todd Ward , and Wei ing Zhu . \n\t', '\n\t\t 2002. BLEU : a method for automatic evaluation of machine translation . \n\t', '\n\t\t In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , pages 311\x96318 , July . \n\t', '\n\t\t Florence Reeder and John White . \n\t', '\n\t\t 2003. Granularity in MT evaluation . \n\t', '\n\t\t In MT Summit Workshop on Machine Translation Evaluation : Towards Systematizing MT Evaluation , pages 37\x9642 , New Orleans . \n\t', '\n\t\t AMTA . \n\t', '\n\t\t Bernhard Sch¨olkopf , Chirstpher J. C. Burges , and Alexander J. Smola , editors . \n\t', '\n\t\t 1998. Advances in Kernel Methods : Support Vector Learning . \n\t', '\n\t\t The MIT Press . \n\t', '\n\t\t Holger Schwenk and Jean-Luc Gauvain . \n\t', '\n\t\t 2000. Combining multiple speech recognizers using voting and language model information . \n\t', '\n\t\t In Proceedings of the IEEE International Conference on Speech and Language Proceesing ( ICSLP ) , volume 2 , pages 915\x96918 , Beijin , October . \n\t', '\n\t\t IEEE . \n\t', '\n\t\t Kohei Takubo and Mitsunori Hashimoto . \n\t', '\n\t\t 1999. A Dictionary of English Business Letter Expressions . \n\t', '\n\t\t Published in CDROM . \n\t', '\n\t\t Nihon Keizai Shinbun Sha . \n\t', '\n\t\t Calandra Tate , Sooyon Lee , and Clare R. Voss . \n\t', '\n\t\t 2003. Task-based MT evaluation : Tackling software , experimental design , & statistical models . \n\t', '\n\t\t In MT Summit Workshop on Machine Translation Evaluation : Towards Systematizing MT Evaluation , pages 43\x9650 . \n\t', '\n\t\t AMTA . \n\t', '\n\t\t Masao Utiyama and Hitoshi Isahara . \n\t', '\n\t\t 2002. Alignment of japanese-english news articles and sentences . \n\t', '\n\t\t In IPSJProceedings 2002-NL-1 51 , pages 15\x9622 . \n\t', '\n\t\t In Japanese . \n\t', '\n\t\t Takehito Utsuro , Yasuhiro Kodama , Tomohiro Watanabe , Hiromitsu Nishizaki , and Seiichi Nakagawa . \n\t', '\n\t\t 2003. Confidence of agreement among multiple LVCSR models and model combination by svm . \n\t', '\n\t\t In Proceedings ofthe 28th IEEE Interna- Table 10 : Language models in MEMT Models Train Size Voc . \n\t', '\n\t\t Genre paj98j 102t 1,020K 20K PAT paj96j5t 50K 20K PAT paj96j3t 30K 20K PAT paj98j5t 50K 20K PAT paj96j102t 1,020K 20K PAT paj98j3t 30K 20K PAT paj98j 1t 10K 14K PAT paj 1 t 10K 14K PAT paj98j5k 5K 10K PAT paj5k 5K 10K PAT lit8t 80K 20K LIT lit5t 50K 20K LIT lit3t 30K 20K LIT lit5k 5K 13K LIT lit1t 10K 13K LIT nikmai154t 1,540K 20K NWS nikmai5t 50K 20K NWS crl14t 40K 20K NWS crl5t 50K 20K NWS nikmai3t 30K 20K NWS nikmai1t 10K 17K NWS nikmai5k 5K 12K NWS crl3t 30K 20K NWS ejp8k 8K 8K BIZ tional Conference on Acoustics , Speech and Signal Processing , pages 16\x9619 . \n\t', '\n\t\t IEEE , April . \n\t', '\n\t\t John White . \n\t', '\n\t\t 2001. Predicting intelligibility from fidelity in MT evaluation . \n\t', '\n\t\t In Proceedings of the workshop \x94MT Evaluation : Who did What to Whom\x94 , pages 35\x9637 . \n\t', '\n\t\t Appendix A Language Models Table 10 lists language models used in the voting based MEMTs discussed in the paper . \n\t', '\n\t\t They are more or less arbitrarily built from parts of the copora CPC , EJP , NIKMAI , EJP , and LIT . \n\t', '\n\t\t \x91Train size\x92 indicates the number of sentences , given in kilo , in a corpus on which a particular model is trained . \n\t', '\n\t\t Under \x91Voc(abulary)\x92 is listed the number of type words for each LM ( also given in kilo ) . \n\t', '\n\t\t Notice the difference in the way the train set and vocabulary are measured . \n\t', '\n\t\t \x91Genre\x92 indicates the genre of a trainig data used for a given LM : PAT stands for patents ( from PAT ) , LIT literary texts ( from LIT ) , NWS news articles ( from CPC and NIKMAI ) , and BIZ business related texts ( from EJP ) . \n\t', '\n\t\t Aligning words using matrix factorisation Cyril Goutte , Kenji Yamada and Eric Gaussier Xerox Research Centre Europe 6 , chemin de Maupertuis F-38240 Meylan , France Cyril.Goutte,Kenji.Yamada,Eric.Gaussier@xrce.xerox.com Abstract Aligning words from sentences which are mutual translations is an important problem in different settings , such as bilingual terminology extraction , Machine Translation , or projection of linguistic features . \n\t', '\n\t\t Here , we view word alignment as matrix factorisation . \n\t', '\n\t\t In order to produce proper alignments , we show that factors must satisfy a number of constraints such as orthogonality . \n\t', '\n\t\t We then propose an algorithm for orthogonal non-negative matrix factorisation , based on a probabilistic model of the alignment data , and apply it to word alignment . \n\t', '\n\t\t This is illustrated on a French-English alignment task from the Hansard . \n\t', '\n\t\t 1 Introduction Aligning words from mutually translated sentences in two different languages is an important and difficult problem . \n\t', '\n\t\t It is important because a word- aligned corpus is typically used as a first step in order to identify phrases or templates in phrase-based Machine Translation \n\t\t']",Positive
"['\n\t\t 3 ) , or for projecting linguistic annotation across languages \n\t\t']",Positive
['\n\t\t Obtaining a word-aligned corpus usually involves training a word-based translation models \n\t\t'],Positive
"['\n\t\t Besides processing time , important issues are completeness and propriety of the resulting alignment , and the ability to reliably identify general Nto-M alignments . \n\t', '\n\t\t In the following section , we introduce the problem of aligning words from a corpus that is already aligned at the sentence level . \n\t', '\n\t\t We show how this problem may be phrased in terms of matrix factorisation . \n\t', '\n\t\t We then identify a number of constraints on word alignment , show that these constraints entail that word alignment is equivalent to orthogonal non-negative matrix factorisation , and we give a novel algorithm that solves this problem . \n\t', '\n\t\t This is illustrated using data from the shared tasks of the 2003 HLT-NAACL Workshop on Building the licence fee does not increase le droit de permis ne augmente pas Figure 1 : 1-1 , M-1 , 1-N and M-N alignments . \n\t', '\n\t\t and Using Parallel Texts \n\t\t']",Positive
"['\n\t\t 2 Word alignments We address the following problem : Given a source sentence f = fi ... fi ... fI and a target sentence e = el ... ej ... eJ , we wish to find words fi and ej on either side which are aligned , ie in mutual correspondence . \n\t', '\n\t\t Note that words may be aligned without being directly \x93dictionary translations\x94 . \n\t', '\n\t\t In order to have proper alignments , we want to enforce the following constraints : Coverage : Every word on either side must be aligned to at least one word on the other side ( Possibly taking \x93null\x94 words into account ) . \n\t', '\n\t\t Transitive closure : If fi is aligned to ej and et , any fk aligned to et must also de aligned to ej . \n\t', '\n\t\t Under these constraints , there are only 4 types of alignments : 1-1 , 1-N , M-1 and M-N ( fig . \n\t', '\n\t\t 1 ) . \n\t', '\n\t\t Although the first three are particular cases where N=1 and/or M=1 , the distinction is relevant , because most word-based translation models ( eg IBM models \n\t\t']",Positive
"['\n\t\t We formalise this using the notion of cepts : a cept is a central pivot through which a subset of e- words is aligned to a subset of f -words . \n\t', '\n\t\t General M-N alignments then correspond to M-1-N alignments from e-words , to a cept , to f -words ( fig . \n\t', '\n\t\t 2 ) . \n\t', '\n\t\t Cepts naturally guarantee transitive closure as long as each word is connected to a single cept . \n\t', '\n\t\t In addition , coverage is ensured by imposing that each the licence fee does not increase ( 1 ) ( 2 ) (3)(4) le droit de permis ne augmente pas Figure 2 : Same as figure 1 , using cepts (1)-(4). cepts where without loss of generality , we introduce a diagonal K x K scaling matrix 5 which may give different weights to the different cepts . \n\t', '\n\t\t As factors F and E contain only positive elements , this is an instance of non-negative matrix factorisation , aka NMF \n\t\t']",Positive
"['\n\t\t Because NMF decomposes a matrix into additive , positive components , it naturally yields a sparse representation . \n\t', '\n\t\t English words English words In addition , the propriety constraint imposes that words are aligned to exactly one cept , ie each row of E and F has exactly one non-zero component , a property we call extreme sparsity . \n\t', '\n\t\t With the notation F = [ FZk ] , this means that : bi , bk =~ l , FZk.FZl = 0 Figure 3 : Matrix factorisation of the example from fig . \n\t', '\n\t\t 1 , 2 . \n\t', '\n\t\t Black squares represent alignments . \n\t', '\n\t\t word is connected to a cept . \n\t', '\n\t\t A unique constraint therefore guarantees proper alignments : Propriety : Each word is associated to exactly one cept , and each cept is associated to at least one word on each side . \n\t', '\n\t\t Note that our use of cepts differs slightly from that of ( Brown et al. , 1993 , sec.3 ) , inasmuch cepts may not overlap , according to our definition . \n\t', '\n\t\t The motivation for our work is that better word alignments will lead to better translation models . \n\t', '\n\t\t For example , we may extract better chunks for phrase-based translation models . \n\t', '\n\t\t In addition , proper alignments ensure that cept-based phrases will cover the entire source and target sentences . \n\t', '\n\t\t 3 Matrix factorisation Alignments between source and target words may be represented by a I x J alignment matrix A= [ aZj ] , such that aZj > 0 if fZ is aligned with ej and aZj = 0 otherwise . \n\t', '\n\t\t Similarly , given K cepts , words to cepts alignments may be represented by a I x K matrix F and a J x K matrix E , with positive elements indicating alignments . \n\t', '\n\t\t It is easy to see that matrix A = F x ET then represents the resulting word-to-word alignment ( fig . \n\t', '\n\t\t 3 ) . \n\t', '\n\t\t Let us now assume that we start from a I x J matrix M = [ mZj ] , which we call the translation matrix , such that mZj > 0 measures the strength of the association between fZ and ej ( large values mean close association ) . \n\t', '\n\t\t This may be estimated using a translation table , a count ( eg from a N-best list ) , etc. . \n\t', '\n\t\t Finding a suitable alignment matrix A corresponds to finding factors F and E such that : MPz~Fx5xET ( 1 ) As line i contains a single non-zero element , either FZk or FZl must be 0 . \n\t', '\n\t\t An immediate consequence is that ~Z FZk.FZl = 0 : columns of F are orthogonal , that is F is an orthogonal matrix ( and similarly , E is orthogonal ) . \n\t', '\n\t\t Finding the best alignment starting from M therefore reduces to performing a decomposition into orthogonal non-negative factors . \n\t', '\n\t\t 4 An algorithm for Orthogonal Non-negative Matrix Factorisation Standard NMF algorithms \n\t\t']",Positive
"['\n\t\t We propose to perform the Orthogonal Non-negative Matrix Factorisation ( ONMF ) in two stages : We first factorise M using Probabilistic Latent Semantic Analysis , aka PLSA \n\t\t']",Positive
"['\n\t\t PLSA models a joint probability P(f , e ) as a mixture of conditionally independent multinomial distributions : P(f , e ) = E P(c)P(f lc)P(elc) ( 2 ) c With F = [ P(f l c ) ] , E = [ P(el c ) ] and 5 = diag(P(c)) , this is exactly eq . \n\t', '\n\t\t 1. Note also that despite the additional matrix 5 , if we set E = [ P(e , c ) ] , then P(f le ) would factor as F x ET , the original NMF formulation ) . \n\t', '\n\t\t All factors in eq . \n\t', '\n\t\t 2 are ( conditional ) probabilities , and therefore positive , so PLSA also implements NMF . \n\t', '\n\t\t The parameters are learned from a matrix M = [ mZj ] of ( fZ , ej ) counts , by maximising the likelihood using the iterative re-estimation formula of the Expectation-Maximisation algorithm \n\t\t']",Positive
"['\n\t\t 4. Convergence is guaranteed , leading to a non-negative factorisation of M . \n\t', '\n\t\t The second step of our algorithm is to orthogonalise E-step : P(cj fi , ej ) = P(c)P(fijc)P(ej jc ) ( 3 ) EcP(c)P(fijc)P(ejj c ) M-step : P(c) = 1N ij ~ M-step : P(fij c ) a j ~ M-step : P(ej jc ) a i Figure 4 : The EM algorithm iterates these E and M-steps until convergence . \n\t', '\n\t\t the resulting factors . \n\t', '\n\t\t Each source word fi is assigned the most probable cept , ie cept c for which P(cI fi ) a P(c)P(fiIc) is maximal . \n\t', '\n\t\t Factor F is therefore set to : ~1 if k = argmaxc P(cI fi ) Fik a 0 otherwise ( 7 ) where proportionality ensures that column of F sum to 1 , so that F stays a conditional probability matrix . \n\t', '\n\t\t We proceed similarly for target words ej to orthogonalise E . \n\t', '\n\t\t Thanks to the MAP assignment , each line of F and E contains exactly one non-zero element . \n\t', '\n\t\t We saw earlier that this is equivalent to having orthogonal factors . \n\t', '\n\t\t The result is therefore an orthogonal , non-negative factorisation of the original translation matrix M. 4.1 Number of cepts In general , the number of cepts is unknown and must be estimated . \n\t', '\n\t\t This corresponds to choosing the number of components in PLSA , a classical model selection problem . \n\t', '\n\t\t The likelihood may not be used as it always increases as components are added . \n\t', '\n\t\t A standard approach to optimise the complexity of a mixture model is to maximise the likelihood , penalised by a term that increases with model complexity , such as AIC \n\t\t']",Positive
"['\n\t\t BIC asymptotically chooses the correct model size ( for complete models ) , while AIC always overestimates the number of components , but usually yields good predictive performance . \n\t', '\n\t\t As the largest possible number of cepts is min(I , J ) , and the smallest is 1 ( all fi aligned to all ej ) , we estimate the optimal number of cepts by maximising AIC or BIC between these two extremes . \n\t', '\n\t\t 4.2 Dealing with null alignments Alignment to a \x93null\x94 word may be a feature of the underlying statistical model ( eg IBM models ) , or it may be introduced to accommodate words which have a low association measure with all other words . \n\t', '\n\t\t Using PLSA , we can deal with null alignments in a principled way by introducing a null word on each side ( fo and eo ) , and two null cepts ( \x93f-null\x94 and \x93e-null\x94 ) with a 1-1 alignment to the corresponding null word , to ensure that null alignments will only be 1-N or M-1 . \n\t', '\n\t\t This constraint is easily implemented using proper initial conditions in EM . \n\t', '\n\t\t Denoting the null cepts as c fO and ceO , 1-1 alignments between null cepts and the corresponding null words impose the conditions : 1 . \n\t', '\n\t\t P(f0 Ic fO ) = 1 and bi =~ 0 , P(fi Ic fO ) = 0 ; 2 . \n\t', '\n\t\t P(e0IceO) = 1 and bj =~ 0 , P(ej I ceO ) = 0 . \n\t', '\n\t\t Stepping through the E-step and M-step equations ( 3\x966 ) , we see that these conditions are preserved by each EM iteration . \n\t', '\n\t\t In order to deal with null alignments , the model is therefore augmented with two null cepts , for which the probabilities are initialised according to the above conditions . \n\t', '\n\t\t As these are preserved through EM , we maintain proper 1-N and M1 alignments to the null words . \n\t', '\n\t\t The main difference between null cepts and the other cepts is that we relax the propriety constraint and do not force null cepts to be aligned to at least one word on either side . \n\t', '\n\t\t This is because in many cases , all words from a sentence can be aligned to non-null words , and do not require any null alignments . \n\t', '\n\t\t 4.3 Modelling noise Most elements of M usually have a non-zero association measure . \n\t', '\n\t\t This means that for proper alignments , which give zero probability to alignments outside identified blocks , actual observations have exactly 0 probability , ie the log-likelihood of parameters corresponding to proper alignments is undefined . \n\t', '\n\t\t We therefore refine the model , adding a noise component indexed by c = 0 : P(f , e ) = E P(c)P(f Ic)P(eIc) c>0 +P(c = 0)P(f , eIc = 0 ) The simplest choice for the noise component is a uniform distribution , P(f , eIc = 0 ) a 1 . \n\t', '\n\t\t E-step and M-steps in eqs . \n\t', '\n\t\t ( 3\x966 ) are unchanged for c > 0 , and the E-step equation for c = 0 is easily adapted : P(c=0I f , e ) a P(c=0)P(f , eIc=0 ) . \n\t', '\n\t\t 5 Example We first illustrate the factorisation process on a simple example . \n\t', '\n\t\t We use the data provided for mijP(cj fi , ej ) ( 4 ) mijP(cj fi , ej ) ( 5 ) mijP(cj fi , ej ) ( 6 ) the French-English shared task of the 2003 HLTNAACL Workshop on Building and Using Parallel Texts \n\t\t']",Positive
"['\n\t\t The data is from the Canadian Hansard , and reference alignments were originally produced by Franz Och and Hermann Ney \n\t\t']",Positive
"['\n\t\t Using the entire corpus ( 20 million words ) , we trained English\x97*French and French\x97*English IBM4 models using GIZA++ . \n\t', '\n\t\t For all sentences from the trial and test set ( 37 and 447 sentences ) , we generated up to 100 best alignments for each sentence and in each direction . \n\t', '\n\t\t For each pair of source and target words ( fZ , ej ) , the association measure mZj is simply the number of times these words were aligned together in the two N-best lists , leading to a count between 0 ( never aligned ) and 200 ( always aligned ) . \n\t', '\n\t\t We focus on sentence 1023 , from the trial set . \n\t', '\n\t\t Figure 5 shows the reference alignments together with the generated counts . \n\t', '\n\t\t There is a background \x93noise\x94 count of 3 to 5 ( small dots ) and the largest counts are around 145-150 . \n\t', '\n\t\t The N-best counts seem to give a good idea of the alignments , although clearly there is no chance that our factorisation algorithm will recover the alignment of the two instances of \x92de\x92 to \x92need\x92 , as there is no evidence for it in the data . \n\t', '\n\t\t The ambiguity that the factorisation will have to address , and that is not easily resolved using , eg , thresholding , is whether \x92ont\x92 should be aligned to \x92They\x92 or to \x92need\x92 . \n\t', '\n\t\t The N-best count matrix serves as the translation matrix . \n\t', '\n\t\t We estimate PLSA parameters for K = 1 ... 6 , and find out that AIC and BIC reach their maximum for K = 6 . \n\t', '\n\t\t We therefore select 6 cepts for this sentence , and produce the alignment matrices shown on figure 6 . \n\t', '\n\t\t Note that the order of the cepts is arbitrary ( here the first cept correspond \x92et\x92 \x97 \x92and\x92 ) , except for the null cepts which are fixed . \n\t', '\n\t\t There is a fixed 1-1 correspondence between these null cepts and the corresponding null words on each side , and only the French words \x92de\x92 are mapped to a null cept . \n\t', '\n\t\t Finally , the estimated noise level is P(c = 0 ) = 0.00053 . \n\t', '\n\t\t The ambiguity associated with aligning \x92ont\x92 has been resolved through cepts 4 and 5 . \n\t', '\n\t\t In our resulting model , P(c=4I\x92ont\x92) Pz~ 0.40 while P(c=6I\x92ont\x92) Pz~ 0.54 : The MAP assignment forces \x92ont\x92 to be aligned to cept 5 only , and therefore to \x92need\x92 . \n\t', '\n\t\t Note that although the count for ( need,ont ) is slightly larger than the count for ( they,ont ) ( cf. fig . \n\t', '\n\t\t 5 ) , this is not a trivial result . \n\t', '\n\t\t The model was able to resolve the fact that they and need had to be aligned to 2 different cepts , rather than eg a larger cept corresponding to a 2-4 alignment , and to produce proper alignments through the use of these cepts . \n\t', '\n\t\t 6 Experiments In order to perform a more systematic evaluation of the use of matrix factorisation for aligning words , we tested this technique on the full trial and test data from the 2003 HLT-NAACL Workshop . \n\t', '\n\t\t Note that the reference data has both \x93Sure\x94 and \x93Probable\x94 alignments , with about 77 % of all alignments in the latter category . \n\t', '\n\t\t On the other hand , our system proposes only one type of alignment . \n\t', '\n\t\t The evaluation is done using the performance measures described in \n\t\t']",Positive
"['\n\t\t Given an alignment A and gold standards 9S and 9P ( for sure and probable alignments , respectively ) : PT = IA n 9TI ( 8 ) IAI RT = IA n 9TI ( 9 ) I9TI 2IAn9TI ( 10 ) I9TI + IAI where T is either S or P , and : AER = 1 ^ I 9S I RS + I AI PP ( 11 ) I9SI + IAI Using these measures , we first evaluate the performance on the trial set ( 37 sentences ) : as we produce only one type of alignment and evaluate against \x93Sure\x94 and \x93Probable\x94 , we observe , as expected , that the recall is very good on sure alignments , but precision relatively poor , with the reverse situation on the probable alignments ( table 1 ) . \n\t', '\n\t\t This is because we generate an intermediate number of alignments . \n\t', '\n\t\t There are 338 sure and 1446 probable alignments ( for 721 French and 661 English words ) in the reference trial data , and we produce 707 ( AIC ) or 766 ( BIC ) alignments with ONMF . \n\t', '\n\t\t Most of them are at least probably correct , as attested by PP , but only about half of them are in the \x93Sure\x94 subset , yielding a low value of PS . \n\t', '\n\t\t Similarly , because \x93Probable\x94 alignments were generated as the union of alignments produced by two annotators , they sometimes lead to very large MN alignments , which produce on average 2.5 to 2.7 alignments per word . \n\t', '\n\t\t By contrast ONMF produces less than 1.2 alignments per word , hence the low value of RP . \n\t', '\n\t\t As the AER is a weighted average of RS and PP , the resulting AER are relatively low for our method . \n\t', '\n\t\t FT = 2PTRT PT+RT Reference alignments N^best counts . \n\t', '\n\t\t loisirs de et jouets de besoin ont enfants les NULL . \n\t', '\n\t\t loisirs de et jouets de besoin ont enfants les NULL Figure 5 : Left : reference alignments , large squares are sure , medium squares are probable ; Right : accumulated counts from IBM4 N-best lists , bigger squares are larger counts . \n\t', '\n\t\t f^to^cept alignment Resulting alignment . \n\t', '\n\t\t e^to^cept alignment . \n\t', '\n\t\t loisirs cept1 cept2 cept3 cept4 cept5 cept6 f^null e^null loisirs de de et et jouets jouets de de besoin besoin ont ont enfants enfants les les NULL NULL Figure 6 : Resulting word-to-cept and word-to-word alignments for sentence 1023 . \n\t', '\n\t\t Method PS RS FS PP RP FP AER ONMF + AIC ONMF + BIC 45.26 % 94.67 % 61.24 % 86.56 % 34.30 % 49.14 % 10.81 % 42.69 % 96.75 % 59.24 % 83.42 % 35.82 % 50.12 % 12.50 % Table 1 : Performance on the 37 trial sentences for orthogonal non-negative matrix factorisation ( ONMF ) using the AIC and BIC criterion for choosing the number of cepts , discounting null alignments . \n\t', '\n\t\t We also compared the performance on the 447 test sentences to 1/ the intersection of the alignments produced by the top IBM4 alignments in either directions , and 2/ the best systems from \n\t\t']",Positive
"['\n\t\t On limited resources , Ralign.EF.1 \n\t\t']",Positive
"['\n\t\t Tables 2 and 3 show that ONMF improves on several of these results . \n\t', '\n\t\t In particular , we get better recall and F-score on the probable alignments ( and even a better precision than Ralign.EF.1 in table 2 ) . \n\t', '\n\t\t On the other hand , the performance , and in particular the precision , on sure alignments is dismal . \n\t', '\n\t\t We attribute this at least partly to a key difference between our model and the reference data : Method PS RS FS PP RP FP AER ONMF + AIC 49.86 % 95.12 % 65.42 % 84.63 % 37.39 % 51.87 % 11.76 % ONMF + BIC 46.50 % 96.01 % 62.65 % 80.92 % 38.69 % 52.35 % 14.16 % IBM4 intersection 71.46 % 90.04 % 79.68 % 97.66 % 28.44 % 44.12 % 5.71 % HLT-03 best F 72.54 % 80.61 % 76.36 % 77.56 % 38.19 % 51.18 % 18.50 % HLT-03 best AER 55.43 % 93.81 % 69.68 % 90.09 % 35.30 % 50.72 % 8.53 % Table 2 : Performance on the 447 English-French test sentences , discounting NULL alignments , for orthogonal non-negative matrix factorisation ( ONMF ) using the AIC and BIC criterion for choosing the number of cepts . \n\t', '\n\t\t HLT-03 best F is Ralign.EF . \n\t', '\n\t\t 1 and best AER is XRCE.Nolem.EF.3 \n\t\t']",Positive
"['\n\t\t our model enforces coverage and makes sure that all words are aligned , while the \x93Sure\x94 reference alignments have no such constraints and actually have a very bad coverage . \n\t', '\n\t\t Indeed , less than half the words in the test set have a \x93Sure\x94 alignment , which means that a method which ensures that all words are aligned will at best have a sub 50 % precision . \n\t', '\n\t\t In addition , many reference \x93Probable\x94 alignments are not proper alignments in the sense defined above . \n\t', '\n\t\t Note that the IBM4 intersection has a bias similar to the sure reference alignments , and performs very well in FS , PP and especially in AER , even though it produces very incomplete alignments . \n\t', '\n\t\t This points to a particular problem with the AER in the context of our study . \n\t', '\n\t\t In fact , a system that outputs exactly the set of sure alignments achieves a perfect AER of 0 , even though it aligns only about 23 % of words , clearly an unacceptable drawback in many applications . \n\t', '\n\t\t We think that this issue may be addressed in two different ways . \n\t', '\n\t\t One time-consuming possibility would be to post-edit the reference alignment to ensure coverage and proper alignments . \n\t', '\n\t\t Another possibility would be to use the probabilistic model to mimic the reference data and generate both \x93Sure\x94 and \x93Probable\x94 alignments using eg thresholds on the estimated alignment probabilities . \n\t', '\n\t\t This approach may lead to better performance according to our metrics , but it is not obvious that the produced alignments will be more reasonable or even useful in a practical application . \n\t', '\n\t\t We also tested our approach on the Romanian- English task of the same workshop , cf. table 4 . \n\t', '\n\t\t The \x92HLT-03 best\x92 is our earlier work \n\t\t']",Positive
['\n\t\t Slightly better results have been published since \n\t\t'],Positive
"['\n\t\t Note that the reference alignments for Romanian- English contain only \x93Sure\x94 alignments , and therefore we only report the performance on those . \n\t', '\n\t\t In addition , AER = 1^ FS in this setting . \n\t', '\n\t\t Table 4 shows that the matrix factorisation approach does not offer any quantitative improvements over these results . \n\t', '\n\t\t A gain of up to 10 points in recall does not offset a large decrease in precision . \n\t', '\n\t\t As a consequence , the AER for ONMF+AIC is about 10 % higher than in our earlier work . \n\t', '\n\t\t This seems mainly due to the fact that the \x92HLT-03 best\x92 produces alignments for only about 80 % of the words , while our technique ensure coverage and therefore aligns all words . \n\t', '\n\t\t These results suggest that remaining 20 % seem particularly problematic . \n\t', '\n\t\t These quantitative results are disappointing given the sofistication of the method . \n\t', '\n\t\t It should be noted , however , that ONMF provides the qualitative advantage of producing proper alignments , and in particular ensures coverage . \n\t', '\n\t\t This may be useful in some contexts , eg training a phrase- based translation system . \n\t', '\n\t\t 7 Discussion 7.1 Model selection and stability Like all mixture models , PLSA is subject to local minima . \n\t', '\n\t\t Although using a few random restarts seems to yield good performance , the results on difficult-to-align sentences may still be sensitive to initial conditions . \n\t', '\n\t\t A standard technique to stabilise the EM solution is to use deterministic annealing or tempered EM \n\t\t']",Positive
"['\n\t\t As a side effect , deterministic annealing actually makes model selection easier . \n\t', '\n\t\t At low temperature , all components are identical , and they differentiate as the temperature increases , until the final temperature , where we recover the standard EM algorithm . \n\t', '\n\t\t By keeping track of the component differentiations , we may consider multiple effective numbers of components in one pass , therefore alleviating the need for costly multiple EM runs with different cept numbers and multiple restarts . \n\t', '\n\t\t 7.2 Other association measures ONMF is only a tool to factor the original translation matrix M , containing measures of associations between fz and ej . \n\t', '\n\t\t The quality of the resulting alignment greatly depends on the way M is Method PS RS FS PP RP FP AER ONMF + AIC 42.88 % 95.12 % 59.11 % 75.17 % 37.20 % 49.77 % 18.63 % ONMF +BIC 40.17 % 96.01 % 56.65 % 72.20 % 38.49 % 50.21 % 20.78 % IBM4 intersection 56.39 % 90.04 % 69.35 % 81.14 % 28.90 % 42.62 % 15.43 % HLT-03 best 72.54 % 80.61 % 76.36 % 77.56 % 36.79 % 49.91 % 18.50 % Table 3 : Performance on the 447 English-French test sentences , taking NULL alignments into account , for orthogonal non-negative matrix factorisation ( ONMF ) using the AIC and BIC criterion for choosing the number of cepts . \n\t', '\n\t\t HLT-03 best is Ralign.EF.1 \n\t\t']",Positive
"['\n\t\t no NULL alignments with NULL alignments Method PS RS FS AER PS RS FS AER ONMF + AIC 70.34 % 65.54 % 67.85 % 32.15 % 62.65 % 62.10 % 62.38 % 37.62 % ONMF + BIC 55.88 % 67.70 % 61.23 % 38.77 % 51.78 % 64.07 % 57.27 % 42.73 % HLT-03 best 82.65 % 62.44 % 71.14 % 28.86 % 82.65 % 54.11 % 65.40 % 34.60 % Table 4 : Performance on the 248 Romanian-English test sentences ( only sure alignments ) , for orthogonal non-negative matrix factorisation ( ONMF ) using the AIC and BIC criterion for choosing the number of cepts . \n\t', '\n\t\t HLT-03 best is XRCE.Nolem \n\t\t']",Positive
"['\n\t\t filled . \n\t', '\n\t\t In our experiments we used counts from N- best alignments obtained from IBM model 4 . \n\t', '\n\t\t This is mainly used as a proof of concept : other strategies , such as weighting the alignments according to their probability or rank in the N-best list would be natural extensions . \n\t', '\n\t\t In addition , we are currently investigating the use of translation and distortion tables obtained from IBM model 2 to estimate M at a lower cost . \n\t', '\n\t\t Ultimately , it would be interesting to obtain association measures mij in a fully non- parametric way , using corpus statistics rather than translation models , which themselves perform some kind of alignment . \n\t', '\n\t\t We have investigated the use of co-occurrence counts or mutual information between words , but this has so far not proved successful , mostly because common words , such as function words , tend to dominate these measures . \n\t', '\n\t\t 7.3 M-1-0 alignments In our model , cepts ensure that resulting alignments are proper . \n\t', '\n\t\t There is however one situation in which improper alignments may be produced : If the MAP assigns f-words but no e-words to a cept ( because e-words have more probable cepts ) , we may produce \x93orphan\x94 cepts , which are aligned to words only on one side . \n\t', '\n\t\t One way to deal with this situation is simply to remove cepts which display this behaviour . \n\t', '\n\t\t Orphaned words may then be re-assigned to the remaining cepts , either directly or after retraining PLSA on the remaining cepts ( this is guaranteed to converge as there is an obvious solution for K = 1 ) . \n\t', '\n\t\t 7.4 Independence between sentences One natural comment on our factorisation scheme is that cepts should not be independent between sentences . \n\t', '\n\t\t However it is easy to show that the factorisation is optimally done on a sentence per sentence basis . \n\t', '\n\t\t Indeed , what we factorise is the association measures mij . \n\t', '\n\t\t For a sentence-aligned corpus , the association measure between source and target words from two different sentence pairs should be exactly 0 because words should not be aligned across sentences . \n\t', '\n\t\t Therefore , the larger translation matrix ( calculated on the entire corpus ) is block diagonal , with non-zero association measures only in blocks corresponding to aligned sentence . \n\t', '\n\t\t As blocks on the diagonal are mutually orthogonal , the optimal global orthogonal factorisation is identical to the block-based ( ie sentence-based ) factorisation . \n\t', '\n\t\t Any corpus-induced dependency between alignments from different sentences must therefore be built in the association measure mij , and cannot be handled by the factorisation method . \n\t', '\n\t\t Note that this is the case in our experiments , as model 4 alignments rely on parameters obtained on the entire corpus . \n\t', '\n\t\t 8 Conclusion In this paper , we view word alignment as 1/ estimating the association between source and target words , and 2/ factorising the resulting association measure into orthogonal , non-negative factors . \n\t', '\n\t\t For solving the latter problem , we propose an algorithm for ONMF , which guarantees both proper alignments and good coverage . \n\t', '\n\t\t Experiments carried out on the Hansard give encouraging results , in the sense that we improve in several ways over state-of-the-art results , despite a clear bias in the reference alignments . \n\t', '\n\t\t Further investigations are required to apply this technique on different association measures , and to measure the influence that ONMF may have , eg on a phrase-based Machine Translation system . \n\t', '\n\t\t Acknowledgements We acknowledge the Machine Learning group at XRCE for discussions related to the topic of word alignment . \n\t', '\n\t\t We would like to thank the three anonymous reviewers for their comments . \n\t', '\n\t\t References H. Akaike . \n\t', '\n\t\t 1974. A new look at the statistical model identification . \n\t', '\n\t\t IEEE Tr. . \n\t', '\n\t\t Automatic Control , 19(6):716\x96723 . \n\t', '\n\t\t A.-M. Barbu . \n\t', '\n\t\t 2004. Simple linguistic methods for improving a word alignment algorithm . \n\t', '\n\t\t In Le poids des mots \x97 Proc . \n\t', '\n\t\t JADT04 , pages 88\x9698 . \n\t', '\n\t\t P. F. Brown , S. A. Della Pietra , V. J. Della Pietra , and R. L. Mercer . \n\t', '\n\t\t 1993. The mathematics of statistical machine translation : Parameter estimation . \n\t', '\n\t\t Computational linguistics , 19:263\x96312 . \n\t', '\n\t\t H. Dejean , E. Gaussier , C. Goutte , and K. Yamada . \n\t', '\n\t\t 2003. Reducing parameter space for word alignment . \n\t', '\n\t\t In HLT-NAACL 2003 Workshop : Building and Using Parallel Texts , pages 23\x9626 . \n\t', '\n\t\t A. P. Dempster , N. M. Laird , and D. B. Rubin . \n\t', '\n\t\t 1977. Maximum likelihood from incomplete data via the EM algorithm . \n\t', '\n\t\t J. Royal Statistical Society , Series B , 39(1):1\x9638 . \n\t', '\n\t\t T. Hofmann. 1999 . \n\t', '\n\t\t Probabilistic latent semantic analysis . \n\t', '\n\t\t In Uncertainty in Artificial Intelligence , pages 289\x96296 . \n\t', '\n\t\t P. Koehn , F. Och , and D. Marcu . \n\t', '\n\t\t 2003. Statistical phrase-based translation . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t HLT-NAACL 2003 . \n\t', '\n\t\t D. D. Lee and H. S. Seung . \n\t', '\n\t\t 1999. Learning the parts of objects by non-negative matrix factorization . \n\t', '\n\t\t Nature , 401:788\x96791 . \n\t', '\n\t\t D. D. Lee and H. S. Seung . \n\t', '\n\t\t 2001. Algorithms for non-negative matrix factorization . \n\t', '\n\t\t In NIPS*13 , pages 556\x96562 . \n\t', '\n\t\t R. Mihalcea and T. Pedersen . \n\t', '\n\t\t 2003. An evaluation exercise for word alignment . \n\t', '\n\t\t In HLT-NAACL 2003 Workshop : Building and Using Parallel Texts , pages 1\x9610 . \n\t', '\n\t\t F. Och and H. Ney . \n\t', '\n\t\t 2000. A comparison of alignment models for statistical machine translation . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t COLING\x9200 , pages 1086\x961090 . \n\t', '\n\t\t F. Och , C. Tillmann , and H. Ney . \n\t', '\n\t\t 1999. Improved alignment models for statistical machine translation . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t EMNLP , pages 20\x9628 . \n\t', '\n\t\t K. Rose , E. Gurewitz , and G. Fox. 1990 . \n\t', '\n\t\t A deterministic annealing approach to clustering . \n\t', '\n\t\t Pattern Recognition Letters , 11(11):589\x96594 . \n\t', '\n\t\t G. Schwartz . \n\t', '\n\t\t 1978 . \n\t', '\n\t\t Estimating the dimension of a model . \n\t', '\n\t\t The Annals of Statistics , 6(2):461\x96464 . \n\t', '\n\t\t M. Simard and P. Langlais . \n\t', '\n\t\t 2003. Statistical translation alignment with compositionality constraints . \n\t', '\n\t\t In HLT-NAACL 2003 Workshop : Building and Using Parallel Texts , pages 19\x9622 . \n\t', '\n\t\t C. Tillmann and F. Xia . \n\t', '\n\t\t 2003. A phrase-based unigram model for statistical machine translation . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t HLT-NAACL 2003 . \n\t', '\n\t\t D. Yarowsky , G. Ngai , and R. Wicentowski . \n\t', '\n\t\t 2001. Inducing multilingual text analysis tools via robust projection across aligned corpora . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t HLT 2001. \n\t', '\n\t\t FSA : An Efficient and Flexible C++ Toolkit for Finite State Automata Using On-Demand Computation Stephan Kanthak and Hermann Ney Lehrstuhl f¨ur Informatik VI , Computer Science Department RWTH Aachen \x96 University of Technology 52056 Aachen , Germany {kanthak,ney}@informatik.rwth-aachen.de Abstract In this paper we present the RWTH FSA toolkit \x96 an efficient implementation of algorithms for creating and manipulating weighted finite-state automata . \n\t', '\n\t\t The toolkit has been designed using the principle of on-demand computation and offers a large range of widely used algorithms . \n\t', '\n\t\t To prove the superior efficiency of the toolkit , we compare the implementation to that of other publically available toolkits . \n\t', '\n\t\t We also show that on-demand computations help to reduce memory requirements significantly without any loss in speed . \n\t', '\n\t\t To increase its flexibility , the RWTH FSA toolkit supports high-level interfaces to the programming language Python as well as a command-line tool for interactive manipulation of FSAs . \n\t', '\n\t\t Furthermore , we show how to utilize the toolkit to rapidly build a fast and accurate statistical machine translation system . \n\t', '\n\t\t Future extensibility of the toolkit is ensured as it will be publically available as open source software . \n\t', '\n\t\t 1 Introduction Finite-state automata ( FSA ) methods proved to elegantly solve many difficult problems in the field of natural language processing . \n\t', '\n\t\t Among the most recent ones are full and lazy compilation of the search network for speech recognition \n\t\t']",Positive
"['\n\t\t From this list of different applications it is clear that there is a high demand for generic tools to create and manipulate FSAs . \n\t', '\n\t\t In the past , a number of toolkits have been published , all with different design principles . \n\t', '\n\t\t Here , we give a short overview of toolkits that offer an almost complete set of algorithms : \x95 The FSM LibraryTM from AT&T \n\t\t']",Negative
"['\n\t\t \x95 FSA6.1 from ( van Noord , 2000 ) is implemented in Prolog . \n\t', '\n\t\t It is licensed under the terms of the ( GPL , 1991 ) . \n\t', '\n\t\t \x95 The WFST toolkit from \n\t\t']",Negative
"['\n\t\t Also licensed under the terms of the ( GPL , 1991 ) . \n\t', '\n\t\t This paper describes a highly efficient new implementation of a finite-state automata toolkit that uses on-demand computation . \n\t', '\n\t\t Currently , it is being used at the Lehrstuhl f¨ur Informatik VI , RWTH Aachen in different speech recognition and translation research applications . \n\t', '\n\t\t The toolkit will be available under an open source license ( GPL , 1991 ) and can be obtained from our website http://www-i6.informatik.rwth-aachen.de . \n\t', '\n\t\t The remaining part of the paper is organized as follows : Section 2 will give a short introduction to the theory of finite-state automata to recall part of the terminology and notation . \n\t', '\n\t\t We will also give a short explanation of composition which we use as an exemplary object of study in the following sections . \n\t', '\n\t\t In Section 2.3 we will discuss the locality of algorithms defined on finite-state automata . \n\t', '\n\t\t This forms the basis for implementations using on-demand computations . \n\t', '\n\t\t Then the RWTH FSA toolkit implementation is detailed in Section 3 . \n\t', '\n\t\t In Section 4.1 we will compare the efficiency of different toolkits . \n\t', '\n\t\t As a showcase for the flexibility we show how to use the toolkit to build a statistical machine translation system in Section 4.2 . \n\t', '\n\t\t We conclude the paper with a short summary in Section 5 and discuss some possible future extensions in Section 6 . \n\t', '\n\t\t 2 Finite-State Automata 2.1 Weighted Finite-State Transducer The basic theory of weighted finite-state automata has been reviewed in numerous papers \n\t\t']",Positive
"['\n\t\t We will introduce the notation briefly . \n\t', '\n\t\t A semiring ( K , ® , ® , 0,1 ) is a structure with a set K and two binary operations ® and ® such that ( K , ® , 0 ) is a commutative monoid , ( K , ® , 1 ) is a monoid and ® distributes over ® and 0 ~ x = x ® 0 = 0 for any x E K. We will also associate the term weights with the elements of a semiring . \n\t', '\n\t\t Semirings that are frequently used in speech recognition are the positive real semiring ( IR U { \x9700 , +001,~log , + , +00 , 0 ) with a ® log b = \x97log(e\x97a + e\x97b ) and the tropical semiring ( IRU { \x9700 , +001 , min , + , +00 , 0 ) representing the well-known sum and maximum weighted path criteria . \n\t', '\n\t\t A weighted finite-state transducer ( Q , E U { E1 , Q U { E1 , K , E , i , F , ^ , p ) is a structure with a set Q of states1 , an alphabet E of input symbols , an alphabet Q of output symbols , a weight semiring K ( we assume it k-closed here for some algorithms as described in \n\t\t']",Positive
"['\n\t\t To simplify the notation we will also denote with QT and ET the set of states and arcs of a transducer T . \n\t', '\n\t\t A weighted finite-state acceptor is simply a weighted finite-state transducer without the output alphabet . \n\t', '\n\t\t 2.2 Composition As we will refer to this example throughout the paper we shortly review the composition algorithm here . \n\t', ""\n\t\t Let T1 : E* x Q* \x97* K and T2 : Q* x I'* \x97* K be two transducers defined over the same semiring K . \n\t"", ""\n\t\t Their composition T1 o T2 realizes the function T : E* x I'* \x97* K and the theory has been described in detail in \n\t\t""]",Positive
"['\n\t\t For simplification purposes , let us assume that the input automata are c-free and S = ( Q1 x Q2 , +\x97 , \x97* , empty ) is a stack of state tuples of T1 and T2 with push , pop and empty test operations . \n\t', '\n\t\t A non lazy version of composition is shown in Figure 1 . \n\t', '\n\t\t Composition of automata containing c labels is more complex and can be solved by using an intermediate filter transducer that also has been described in \n\t\t']",Positive
"['\n\t\t 1we do not restrict this to be a finite set as most algorithms of the lazy implementation presented in this paper also support a virtually infinite set T=T1oT2 : i = ( i1 , i2 ) S +\x97 ( i1 , i2 ) while not S empty (81,82)+\x97S QT = QT U ( 81 , 82 ) foreach ( 81 , i1 , o1 , w1 , t1 ) E ET , foreach ( 82 , i2 , o2 , w2 , t2 ) E ET , with o1 = i2 ET = ET U ( ( 81 , 82 ) , i1 , o2 , w1 ( 9 w2 , ( t1 , t2 ) ) if ( t1 , t2 ) E~ QT then S +\x97 ( t1 , t2 ) Figure 1 : Simplified version of composition ( assumes c-free input transducers ) . \n\t', '\n\t\t What we can see from the pseudo-code above is that composition uses tuples of states of the two input transducers to describe states of the target transducer . \n\t', '\n\t\t Other operations defined on weighted finite- state automata use different abstract states . \n\t', '\n\t\t For example transducer determinization \n\t\t']",Negative
"['\n\t\t However , it is more convenient to use integers as state indices for an implementation . \n\t', '\n\t\t Therefore algorithms usually maintain a mapping from abstract states to integer state indices . \n\t', '\n\t\t This mapping has linear memory requirements of O ( I QT I ) which is quite attractive , but that depends on the structure of the abstract states . \n\t', '\n\t\t Especially in case of determinization where the size of an abstract state may vary , the complexity is no longer linear in general . \n\t', '\n\t\t 2.3 Local Algorithms Mohri and colleagues pointed out \n\t\t']",Positive
"['\n\t\t We will give a more detailed analysis here . \n\t', '\n\t\t We focus on algorithms that produce a single transducer and refer to them as algorithmic transducers . \n\t', '\n\t\t Definition : Let 0 be the input configuration of an algorithm A(0) that outputs a single finite-state transducer T . \n\t', '\n\t\t Additionally , let M : S \x97* QT be a one-to-one mapping from the set of abstract state descriptions S that A generates onto the set of states of T . \n\t', ""\n\t\t We call A local iff for all states s E QT A can generate a state s of T and all outgoing arcs ( s , i , o , w , s ' ) E ET , depending only on its abstract state M\x971 ( s ) and the input configuration 0 . \n\t"", '\n\t\t With the preceding definition it is quite easy to prove the following lemma : Lemma : An algorithm A that has the local property can be built on demand starting with the initial state iTA of its associated algorithmic transducer TA . \n\t', '\n\t\t Proof : For the proof it is sufficient to show that we can generate and therefore reach all states of TA . \n\t', '\n\t\t Let S be a stack of states of TA that we still have to process . \n\t', '\n\t\t Due to the one-to-one mapping M we can map each state of TA back to an abstract state of A . \n\t', '\n\t\t By definition the abstract state is sufficient to generate the complete state and its outgoing arcs . \n\t', '\n\t\t We then push those target states of all outgoing arcs onto the stack S that have not yet been processed . \n\t', '\n\t\t As TA is finite the traversal ends after all states of TA as been processed exactly once . \n\t', '\n\t\t ^ Algorithmic transducers that can be computed on-demand are also called lazy or virtual transducers . \n\t', '\n\t\t Note , that due to the local property the set of states does not necessarily be finite anymore . \n\t', '\n\t\t 3 The Toolkit The current implementation is the second version of this toolkit . \n\t', '\n\t\t For the first version \x96 which was called FSM\x96 we opted for using C++ templates to gain efficiency , but algorithms were not lazy . \n\t', '\n\t\t It turned out that the implementation was fast , but many operations wasted a lot of memory as their resulting transducer had been fully expanded in memory . \n\t', '\n\t\t However , we plan to also make this initial version publically available . \n\t', '\n\t\t The design principles of the second version of the toolkit , which we will call FSA , are : \x95 decoupling of data structures and algorithms , \x95 on-demand computation for increased memory efficiency , \x95 low computational costs , \x95 an abstract interface to alphabets to support lazy mappings from strings to indices for arc labels , \x95 an abstract interface to semirings ( should be k- closed for at least some algorithms ) , \x95 implementation in C++ , as it is fast , ubiquitous and well-known by many other researchers , \x95 easy to use interfaces . \n\t', '\n\t\t 3.1 The C++ Library Implementation We use the lemma from Section 2.3 to specify an interface for lazy algorithmic transducers directly . \n\t', '\n\t\t The code written in pseudo-C++ is given in Figure 2 . \n\t', '\n\t\t Note that all lazy algorithmic transducers are derived from the class Automaton . \n\t', '\n\t\t The lazy interface also has disadvantages . \n\t', '\n\t\t The virtual access to the data structure might slow computations down , and obtaining global information about the automaton becomes more complicated . \n\t', '\n\t\t For example the size of an automaton can only be class Automaton { public : struct Arc { StateId target() ; Weight weight() ; LabelId input() ; LabelId output() ; } ; struct State { StateId id() ; Weight weight() ; ConstArcIterator arcsBegin() ; ConstArcIterator arcsEnd() ; } ; virtual R<Alphabet> inputAlphabet() ; virtual R<Alphabet> outputAlphabet() ; virtual StateId initialState() ; virtual R<State> getState(StateId) ; } ; Figure 2 : Pseudo-C++ code fragment for the abstract datatype of transducers . \n\t', '\n\t\t Note that R<T> refers to a smart pointer of T. computed by traversing it . \n\t', '\n\t\t Therefore central algorithms of the RWTH FSA toolkit are the depth- first search ( DFS ) and the computation of strongly connected components ( SCC ) . \n\t', '\n\t\t Efficient versions of these algorithms are described in \n\t\t']",Positive
"['\n\t\t It is very costly to store arbitrary types as arc labels within the arcs itself . \n\t', '\n\t\t Therefore the RWTH FSA toolkit offers alphabets that define mappings between strings and label indices . \n\t', '\n\t\t Alphabets are implemented using the abstract interface shown in Figure 4 . \n\t', '\n\t\t With alphabets arcs only need to store the abstract label indices . \n\t', '\n\t\t The interface for alphabets is defined using a single constant : for each label index an alphabet reports it must ensure to always deliver the same symbol on request through getSymbol ( ) . \n\t', '\n\t\t class Alphabet { public : virtual LabelId begin() ; virtual LabelId end() ; virtual LabelId next(LabelId) ; virtual string getSymbol(LabelId) ; } ; Figure 4 : Pseudo-C++ code fragment for the abstract datatype of alphabets . \n\t', '\n\t\t 3.2 Algorithms The current implementation of the toolkit offers a wide range of well-known algorithms defined on weighted finite-state transducers : \x95 basic operations sort ( by input labels , output labels or by to- compose ( T1,T2 ) = simple-compose( cache ( sort-output ( map-output ( T1 , AT2,I ) ) ) , cache ( sort-input ( T2 ) ) ) Figure 3 : Optimized composition where AT2,I denotes the input alphabet of T2 . \n\t', '\n\t\t Six algorithmic transducers are used to gain maximum efficiency . \n\t', '\n\t\t Mapping of arc labels is necessary as symbol indices may differ between alphabets . \n\t', '\n\t\t tal arc ) , map-input and -output labels symbolically ( as the user expects that two alphabets match symbolically , but their mapping to label indices may differ ) , cache ( helps to reduce computations with lazy implementations ) , topologically-sort states \x95 rational operations project-input , project-output , transpose ( also known as reversal : calculates an equivalent automaton with the adjacency matrix being transposed ) , union , concat , invert \x95 classical graph operations depth-first search ( DFS ) , single-source shortest path ( SSSP ) , connect ( only keep accessible and coaccessible state ) , strongly connected components ( SCCs ) \x95 operations on relations of sets compose ( filtered ) , intersect , complement \x95 equivalence transformations determinize , minimize , remove-epsilons \x95 search algorithms best , n-best \x95 weight/probability-based algorithms prune ( based on forward/backward state potentials ) , posterior , push ( push weights toward initial/final states ) , failure ( given an acceptor/transducer defined over the tropical semiring converts E-transitions to failure transitions ) \x95 diagnostic operations count ( counts states , final states , different arc types , SCCs , alphabet sizes , ... ) \x95 input/output operations supported input and/or output formats are : AT&T ( currently , ASCII only ) , binary ( fast , uses fixed byte-order ) , XML ( slower , any encoding , fully portable ) , memory-mapped ( also on-demand ) , dot ( AT&T graphviz ) We will discuss some details and refer to the publication of the algorithms briefly . \n\t', '\n\t\t Most of the basic operations have a straigthforward implementation . \n\t', '\n\t\t As arc labels are integers in the implementation and their meaning is bound to an appropriate symbolic alphabet , there is the need for symbolic mapping between different alphabets . \n\t', '\n\t\t Therefore the toolkit provides the lazy map-input and map-output transducers , which map the input and output arc indices of an automaton to be compatible with the indices of another given alphabet . \n\t', '\n\t\t The implementations of all classical graph algorithms are based on the descriptions of \n\t\t']",Positive
"['\n\t\t The general graph algorithms DFS and SCC are helpful in the realisation of many other operations , examples are : transpose , connect and count . \n\t', '\n\t\t However , counting the number of states of an automaton or the number of symbols of an alphabet is not well-defined in case of an infinite set of states or symbols . \n\t', '\n\t\t SSSP and transpose are the only two algorithms without a lazy implementation . \n\t', '\n\t\t The result of SSSP is a list of state potentials ( see also \n\t\t']",Positive
"['\n\t\t And a lazy implementation for transpose would be possible if the data structures provide lists of both successor and predecessor arcs at each state . \n\t', '\n\t\t This needs either more memory or more computations and increases the size of the abstract interface for the lazy algorithms , so as a compromise we omitted this . \n\t', '\n\t\t The implementations of compose \n\t\t']",Positive
"['\n\t\t All use at least the lazy cache transducer as they refer to states of the input transducer(s) more than once . \n\t', '\n\t\t With respect to the number of lazy transducers involved in computing the result , compose has the most complicated implementation . \n\t', '\n\t\t Given the implementations for the algorithmic transducers cache , map-output , sort-input , sort-output and simple-compose that assumes arc labels to be compatible and sorted in order to perform matching as fast as possible , the final implementation of compose in the RWTH FSA toolkit is given in figure 3 . \n\t', '\n\t\t So , the current implementation of compose uses 6 algorithmic transducers in addition to the two input automata . \n\t', '\n\t\t Determinize additionally uses lazy cache and sort-input transducers . \n\t', '\n\t\t The search algorithms best and n-best are based on \n\t\t']",Positive
"['\n\t\t The algorithms posterior and prune compute arc posterior probabilities and prune arcs with respect to them . \n\t', '\n\t\t We believe they are standard algorithms defined on probabilistic networks and they were simply ported to the framework of weighted finite-state automata . \n\t', '\n\t\t Finally , the RWTH FSA toolkit can be loosely interfaced to the AT&T FSM LibraryTM through its ASCII-based input/output format . \n\t', '\n\t\t In addition , a new XML-based file format primarly designed as being human readable and a fast binary file format are also supported . \n\t', '\n\t\t All file formats support optional on-the-fly compression using gzip . \n\t', '\n\t\t 3.3 High-Level Interfaces In addition to the C++ library level interface the toolkit also offers two high-level interfaces : a Python interface , and an interactive command-line interface . \n\t', '\n\t\t The Python interface has been built using the SWIG interface generator \n\t\t']",Positive
"['\n\t\t The command-line interface comes handy for quickly applying various combinations of algorithms to transducers without writing any line of code at all . \n\t', '\n\t\t As the Python interface is mainly identical to the C++ interface we will only give a short impression of how to use the command-line interface . \n\t', '\n\t\t The command-line interface is a single executable and uses a stack-based execution model ( postfix notation ) for the application of operations . \n\t', '\n\t\t This is different from the pipe model that AT&T command-line tools use . \n\t', '\n\t\t The disadvantage of using pipes is that automata must be serialized and get fully expanded by the next executable in chain . \n\t', '\n\t\t However , an advantage of multiple executables is that memory does not get fragmented through the interaction of different algorithms . \n\t', '\n\t\t With the command-line interface , operations are applied to the topmost transducers of the stack and the results are pushed back onto the stack again . \n\t', '\n\t\t For example , > fsa A B compose determinize draw - reads A and B from files , calculates the determinized composition and writes the resulting automaton to the terminal in dot format ( which may be piped to dot directly ) . \n\t', '\n\t\t As you can see from the examples some operations like write or draw take additional arguments that must follow the name of the operation . \n\t', '\n\t\t Although this does not follow the strict postfix design , we found it more convenient as these parameters are not automata . \n\t', '\n\t\t 4 Experimental Results 4.1 Comparison of Toolkits A crucial aspect of an FSA toolkit is its computational and memory efficiency . \n\t', '\n\t\t In this section we will compare the efficiency of four different implementations of weighted-finite state toolkits , namely : \x95 RWTH FSA , \x95 RWTH FSM ( predecessor of RWTH FSA ) , \x95 AT&T FSM LibraryTM 4.0 \n\t\t']",Positive
"['\n\t\t We opted to not evaluate the FSA6.1 from ( van Noord , 2000 ) as we found that it is not easy to install and it seemed to be significantly slower than any of the other implementations . \n\t', '\n\t\t RWTH FSA and the AT&T FSM LibraryTM use on-demand computations whereas FSM and WFST do not . \n\t', '\n\t\t As the algorithmic code between RWTH FSA and its predecessor RWTH FSM has not changed much except for the interface of lazy transducers , we can also compare lazy versus non lazy implementation . \n\t', '\n\t\t Nevertheless , this direct comparison is also possible with RWTH FSA as it provides a static storage class transducer and a traversing deep copy operation . \n\t', '\n\t\t Table 1 summarizes the tasks used for the evaluation of efficiency together with the sizes of the resulting transducers . \n\t', '\n\t\t The exact meaning of the different transducers is out of scope of this comparison . \n\t', '\n\t\t We simply focus on measuring the efficiency of the algorithms . \n\t', '\n\t\t Experiment 1 is the full expansion of the static part of a speech recognition search network . \n\t', '\n\t\t Experiment 2 deals with a translation problem and splits words of a \x93bilanguage\x94 into single words . \n\t', '\n\t\t The meaning of the transducers used for Experiment 2 will be described in detail in Section 4.2 . \n\t', '\n\t\t Experiment 3 is similar to Experiment 1 except for that the grammar transducer is exchanged with a translation transducer and the result represents the static network for a speech-to-text translation system . \n\t', '\n\t\t Table 1 : Tasks used for measuring the efficiency of the toolkits . \n\t', '\n\t\t Sizes are given for the resulting transducers ( VM = Verbmobil ) . \n\t', '\n\t\t Experiment states arcs 1 VM , HCL o G 12,203,420 37,174,684 2 VM , C1 o A o C2 341,614 832,225 3 Eutrans , HCL o T 1,201,718 3,572,601 All experiments were performed on a PC with a 1.2GHz AMD Athlon processor and 2 GB of memory using Linux as operating system . \n\t', '\n\t\t Table 2 sum- marizes the peak memory usage of the different toolkit implementations for the given tasks and Table 3 shows the CPU usage accordingly . \n\t', '\n\t\t As can be seen from Tables 2 and 3 for all given tasks the RWTH FSA toolkit uses less memory and computational power than any of the other toolkits . \n\t', '\n\t\t However , it is unclear to the authors why the AT&T LibraryTM is a factor of 1800 slower for experiment 2 . \n\t', '\n\t\t The numbers also do not change much after additionally connecting the composition result ( as in RWTH FSA compose does not connect the result by default ) : memory usage rises to 62 MB and execution time increases to 9.7 seconds . \n\t', '\n\t\t However , a detailed analysis for the RWTH FSA toolkit has shown that the composition task of experiment 2 makes intense use of the lazy cache transducer due to the loop character of the two transducers C1 and C2 . \n\t', '\n\t\t It can also be seen from the two tables that the lazy implementation RWTH FSA uses significantly less memory than the non lazy implementation RWTH FSM and less than half of the CPU time . \n\t', '\n\t\t One explanation for this is the poor memory management of RWTH FSM as all intermediate results need to be fully expanded in memory . \n\t', '\n\t\t In contrast , due to its lazy transducer interface , RWTH FSA may allocate memory for a state only once and reuse it for all subsequent calls to the getState ( ) method . \n\t', '\n\t\t Table 2 : Comparison of peak memory usage in MB ( * aborted due to exceeded memory limits ) . \n\t', '\n\t\t Exp . \n\t', '\n\t\t FSA FSM AT&T WFST 1 360 1700 1500 > 1850* 2 59 310 69 > 1850* 3 48 230 176 550 Table 3 : Comparison of CPU time in seconds including I/O using a 1.2GHz AMD Athlon processor ( * exceeded memory limits : given time indicates point of abortion ) . \n\t', '\n\t\t Exp . \n\t', '\n\t\t FSA FSM AT&T WFST 1 105 203 515 > 40* 2 6.5 182 11760 > 64* 3 6.6 21 28 3840 4.2 Statistical Machine Translation Statistical machine translation may be viewed as a weighted language transduction problem \n\t\t']",Positive
"['\n\t\t Therefore it is fairly easy to build a machine translation system with the use of weighted finite- state transducers . \n\t', '\n\t\t Let fJ1 and eIi be two sentences from a source and target language respectively . \n\t', '\n\t\t Also assume that we have word level alignments A of all sentences from a bilingual training corpus . \n\t', '\n\t\t We denote with ePl the segmentation of a target sentence eI1 into phrases such that f J1 and ePl can be aligned monotoneously . \n\t', '\n\t\t This segmentation can be directly calculated from the alignments A . \n\t', '\n\t\t Then we can formulate the problem of finding the best translation \x88eI1 of a source sentence as follows : \x88eI1 = argmax eI 1 Pr(fJ1 , eP1 ) = argmax 11 Pr(fj , epj I fi-1 , ePl-1 ) AePi fj:j=1..J argmax 11 Pr ( fj , epj Ifjj-n,epj^1 pj^~ ) A,epi fj:j=1..J The last line suggests to solve the translation problem by estimating a language model on a bilanguage ( see also \n\t\t']",Positive
"['\n\t\t An example of sentences from this bilanguage is given in Figure 5 for the translation task Vermobil ( German ^ English ) . \n\t', '\n\t\t For technical reasons , E-labels are represented by a $ symbol . \n\t', '\n\t\t Note , that due to the fixed segmentation given by the alignments , phrases in the target language are moved to the last source word of an alignment block . \n\t', '\n\t\t So , given an appropriate alignment which can be obtained by means of the pubically available GIZA++ toolkit \n\t\t']",Positive
"['\n\t\t Transform the training corpus with a given alignment into the corresponding bilingual corpus 2 . \n\t', '\n\t\t Train a language model on the bilingual corpus 3 . \n\t', '\n\t\t Build an acceptor A from the language model The symbols of the resulting acceptor are still a mixture of words from the source language and phrases from the target language . \n\t', '\n\t\t So , we additionally use two simple transducers to split these bilingual words ( C1 maps source words fj to bilingual words that start with fj and C2 maps bilingual words with the target sequence epj to the sequences of target words the phrase was made of ) : 4 . \n\t', '\n\t\t Split the bilingual phrases of A into single words : T=C1oAoC2 Then the translation problem from above can be rewritten using finite-state terminology : Pr(fJ1 , eI1 ) ^argmax p A,epi dann| $ melde| $ ich|I_am_calling mich| $ noch| $ einmal|once_more .| . \n\t', '\n\t\t 11U|eleven Uhr|o\x92clock ist|is hervorragend|excellent .| . \n\t', '\n\t\t ich|I bin|have da| $ relativ|quite_a_lot_of frei|free_days_then .| . \n\t', '\n\t\t Figure 5 : Example corpus for the bilanguage ( Verbmobil , German English ) . \n\t', '\n\t\t Table 4 : Translation results for different tasks compared to similar systems using the alignment template ( AT ) approach ( Tests were performed on a 1.2GHz AMD Athlon ) . \n\t', ""\n\t\t Task System Translation WER PER 100-BLEU Memory Time/Sentence [ % ] [ % ] [ MB ] [ ms ] Eutrans FSA Spanish English 8.12 7.64 10.7 6-8 20 AT 8.25 - - - - FUB FSA Italian English 27.0 21.5 37.7 3-5 22 AT 23.7 18.1 36.0 - - Verbmobil FSA German English 48.3 41.6 69.8 65-90 460 AT 40.5 30.1 62.2 - - PF-Star FSA Italian English 39.8 34.1 58.4 12-15 35 AT 36.8 29.1 54.3 - - e'= project-output(best(f oT ) ) Translation results using this approach are summarized in Table 4 and are being compared with results obtained using the alignment template approach \n\t\t""]",Positive
"['\n\t\t Results for both approaches were obtaining using the same training corpus alignments . \n\t', '\n\t\t Detailed task descriptions for Eutrans/FUB and Verbmobil can be found in \n\t\t']",Positive
"['\n\t\t We use the usual definitions for word error rate ( WER ) , position independent word error rate ( PER ) and BLEU statistics here . \n\t', '\n\t\t For the simpler tasks Eutrans , FUB and PF-Star , the WER , PER and the inverted BLEU statistics are close for both approaches . \n\t', '\n\t\t On the German-toEnglish Verbmobil task the FSA approach suffers from long distance reorderings ( captured through the fixed training corpus segmentation ) , which is not very surprising . \n\t', '\n\t\t Although we do not have comparable numbers of the memory usage and the translation times for the alignment template approach , resource usage of the finite-state approach is quite remarkable as we only use generic methods from the RWTH FSA toolkit and full search ( i.e. we do not prune the search space ) . \n\t', '\n\t\t However , informal tests have shown that the finite-state approach uses much less memory and computations than the current implementation of the alignment template approach . \n\t', '\n\t\t Two additional advantages of finite-state methods for translation in general are : the input to the search algorithm may also be a word lattice and it is easy to combine speech recognition with translation in order to do speech-to-speech translation . \n\t', '\n\t\t 5 Summary In this paper we have given a characterization of algorithms that produce a single finite-state automaton and bear an on-demand implementation . \n\t', '\n\t\t For this purpose we formally introduced the local property of such an algorithm . \n\t', '\n\t\t We have described the efficient implementation of a finite-state toolkit that uses the principle of lazy algorithmic transducers for almost all algorithms . \n\t', '\n\t\t Among several publically available toolkits , the RWTH FSA toolkit presented here turned out to be the most efficient one , as several tests showed . \n\t', '\n\t\t Additionally , with lazy algorithmic transducers we have reduced the memory requirements and even increased the speed significantly compared to a non lazy implementation . \n\t', '\n\t\t We have also shown that a finite-state automata toolkit supports rapid solutions to problems from the field of natural language processing such as statistical machine translation . \n\t', '\n\t\t Despite the genericity of the methods , statistical machine translation can be done very efficiently . \n\t', '\n\t\t 6 Shortcomings and Future Extensions There is still room to improve the RWTH FSA toolkit . \n\t', '\n\t\t For example , the current implementation of determinization is not as general as described in \n\t\t']",Positive
"['\n\t\t In case of ambiguous input the algorithm still produces an infinite transducer . \n\t', '\n\t\t At the moment this can be solved in many cases by adding disambiguation symbols to the input transducer manually . \n\t', '\n\t\t As the implementation model is based on virtual C++ methods for all types of objects in use ( semir- ings , alphabets , transducers and algorithmic transducers ) it should also be fairly easy to add support for dynamically loadable objects to the toolkit . \n\t', '\n\t\t Other semirings like the expectation semiring described in \n\t\t']",Positive
"['\n\t\t 7 Acknowledgment The authors would like to thank Andre Altmann for his help with the translation experiments . \n\t', '\n\t\t References Alfred V. Aho and Jeffrey D. Ullman , 1972 , The Theory ofParsing , Translation and Compiling , volume 1 , Prentice-Hall , Englewood Cliffs , NJ , 1972 . \n\t', '\n\t\t Arnaud Adant , 2000 , WFST : A Finite-State Template Li- brary in C++ , http://membres.lycos.fr/adant/tfe/ . \n\t', '\n\t\t Cyril Allauzen , Mehryar Mohri , and Brian Roark , 2003 , Generalized Algorithms for Constructing Statistical Language Models , In Proc . \n\t', '\n\t\t of the 41 st Meeting of the Association for Computational Linguistics , Sapporo , Japan , July 2003 . \n\t', '\n\t\t Cyril Allauzen and Mehryar Mohri , 2003 , Generalized Optimization Algorithm for Speech Recognition Transducers , In Proc . \n\t', '\n\t\t of the IEEE Int. Conf . \n\t', '\n\t\t on Acoustics , Speech , and Signal Processing , pp. , Hong Kong , China , April 2003 . \n\t', '\n\t\t Srinivas Bangalore and Giuseppe Riccardi , 2000 , Stochastic Finite-State models for Spoken Language Machine Translation , In Proc . \n\t', '\n\t\t of the Workshop on Embedded Machine Translation Systems , pp. 52\x9659 , 2000 . \n\t', '\n\t\t David Beazley , William Fulton , Matthias K¨oppe , Lyle Johnson , Richard Palmer , 1996 , SWIG - Simplified Wrapper and Interface Generator , Electronic Document , http://www.swig.org , February 1996 . \n\t', '\n\t\t F. Casacuberta , D. Llorens , C. Martinez , S. Molau , F. Nevado , H. Ney , M. Pasto , D. Pico , A. Sanchis , E. Vidal and J.M. Vilar , 2001 , Speech-to-Speech Translation based on Finite-State Transducer , In Proc . \n\t', '\n\t\t IEEE Int. Conf . \n\t', '\n\t\t on Acoustics , Speech and Signal Processing , pp. 613-616 , Salt Lake City , Utah , May 2001 . \n\t', '\n\t\t Thomas H. Cormen , Charles E. Leiserson and Ronald L. Rivest , 1990 , Introductions to Algorithms , The MIT Press , Cambridge , MA , 1990 . \n\t', '\n\t\t Jason Eisner , 2001 , Expectation Semirings : Flexible EM for Finite-State Transducers , In Proc . \n\t', '\n\t\t of the ESSLLI Workshop on Finite-State Methods in NLP ( FSMNLP ) , Helsinki , August 2001 . \n\t', '\n\t\t Free Software Foundation , 1991 , GNU General Public License , Version 2 , Electronic Document , http://www.gnu.org/copyleft/gpl.html , June 1991 . \n\t', '\n\t\t Takaaki Hori , Chiori Hori and Yasuhiro Minami , 2003 , Speech Summarization using Weighted Finite-State Transducers , In Proc . \n\t', '\n\t\t of the European Conf . \n\t', '\n\t\t on Speech Communication and Technology , Geneva , Switzerland , September 2003 . \n\t', '\n\t\t Vincent Le Maout , 1998 , ASTL : Automaton StandardTemplate Library , http://www-igm.univmlv.fr/\x98lemaout/ . \n\t', '\n\t\t Kurt Mehlhorn , 1984 , Data Structures and Efficient Algorithms , Chapter 4 , Springer Verlag , EATCS Monographs , 1984 , also available from http://www.mpisb.mpg.de/\x98mehlhorn/DatAlgbooks.html . \n\t', '\n\t\t Mehryar Mohri , 1997 , Finite-State Transducers in Language and Speech Processing , Computational Linguistics , 23:2 , 1997 . \n\t', '\n\t\t Mehryar Mohri , Fernando C.N. Pereira , and Michael Riley , 2000 , Weighted Finite-State Transducers in Speech Recognition , In Proc . \n\t', '\n\t\t of the ISCA Tutorial and Research Workshop , Automatic Speech Recognition : Challenges for the new Millenium ( ASR2000 ) , Paris , France , September 2000 . \n\t', '\n\t\t Mehryar Mohri , Fernando C.N. Pereira , and Michael Riley , 2000 , The Design Principles ofa Weighted Finite- State Transducer Library , Theoretical Computer Science , 231:17-32 , January 2000 . \n\t', '\n\t\t Mehryar Mohri and Michael Riley , 2000 , A Weight Pushing Algorithm for Large Vocabulary Speech Recognition , In Proc . \n\t', '\n\t\t of the European Conf . \n\t', '\n\t\t on Speech Communication and Technology , pp. 1603\x96 1606 , ^Aalborg , Denmark , September 2001 . \n\t', '\n\t\t Mehryar Mohri , 2001 , Generic Epsilon-Removal Algorithm for Weighted Automata , In Sheng Yu and Andrei Paun , editor , 5th Int. Conf. , CIAA 2000 , London Ontario , Canada . \n\t', '\n\t\t volume 2088 of Lecture Notes in Computer Science , pages 230-242 . \n\t', '\n\t\t Springer-Verlag , Berlin-NY , 2001 . \n\t', '\n\t\t Mehryar Mohri and Michael Riley , 2002 , An Efficient Algorithm for the N-Best-Strings Problem , In Proc . \n\t', '\n\t\t of the Int. Conf . \n\t', '\n\t\t on Spoken Language Processing , pp. 1313\x961316 , Denver , Colorado , September 2002 . \n\t', '\n\t\t Franz J. Och and Hermann Ney , 2000 , Improved Statistical Alignment Models , In Proc . \n\t', '\n\t\t of the 38th Annual Meeting of the Association for Computational Linguistics , pp. 440-447 , Hongkong , China , October 2000 . \n\t', '\n\t\t Fernando C.N. Pereira and Michael Riley , 1996 , Speech Recognition by Composition of Weighted Finite Automata , Available from http://xxx.lanl.gov/cmplg/9603001 , Computation and Language , 1996 . \n\t', '\n\t\t Gertjan van Noord , 2000 , FSA6 Reference Manual , http://odur.let.rug.nl/\x98vannoord/Fsa/ . \n\t', '\n\t\t Enrique Vidal , 1997 , Finite-State Speech-to-Speech Translation , In Proc . \n\t', '\n\t\t of the IEEE Int. Conf . \n\t', '\n\t\t on Acoustics , Speech and Signal Processing , pp. 111\x96114 , Munich , Germany , 1997 . \n\t', '\n\t\t Richard Zens , Franz J. Och and H. Ney , 2002 , Phrase- Based Statistical Machine Translation , In : M. Jarke , J. Koehler , G. Lakemeyer ( Eds . \n\t', '\n\t\t ) : KI - 2002 : Advances in artificial intelligence . \n\t', '\n\t\t 25. Annual German Conference on AI , KI 2002 , Vol. LNAI 2479 , pp. 18- 32 , Springer Verlag , September 2002 . \n\t', '\n\t\t Improving IBM Word-Alignment Model 1 Robert C. MOORE Microsoft Research One Microsoft Way Redmond , WA 90052 USA bobmoore@microsoft.com Abstract We investigate a number of simple methods for improving the word-alignment accuracy of IBM Model 1 . \n\t', '\n\t\t We demonstrate reduction in alignment error rate of approximately 30 % resulting from ( 1 ) giving extra weight to the probability of alignment to the null word , ( 2 ) smoothing probability estimates for rare words , and ( 3 ) using a simple heuristic estimation method to initialize , or replace , EM training of model parameters . \n\t', '\n\t\t 1 Introduction IBM Model 1 \n\t\t']",Positive
"['\n\t\t It was originally developed to provide reasonable initial parameter estimates for more complex word-alignment models , but it has subsequently found a host of additional uses . \n\t', '\n\t\t Among the applications of Model 1 are segmenting long sentences into subsentental units for improved word alignment \n\t\t']",Positive
"['\n\t\t Furthermore , at the 2003 Johns Hopkins summer workshop on statistical machine translation , a large number of features were tested to discover which ones could improve a state-of-the-art translation system , and the only feature that produced a \x93truly significant improvement\x94 was the Model 1 score \n\t\t']",Positive
"['\n\t\t Despite the fact that IBM Model 1 is so widely used , essentially no attention seems to have been paid to whether it is possible to improve on the standard Expectation-Maximization ( EM ) procedure for estimating its parameters . \n\t', '\n\t\t This may be due in part to the fact that \n\t\t']",Positive
"['\n\t\t This , in turn , means that EM training will converge to that maximum from any starting point in which none of the initial parameter values is zero . \n\t', '\n\t\t If one equates optimum parameter estimation with finding the global maximum for the likelihood of the training data , then this result would seem to show no improvement is possible . \n\t', '\n\t\t However , in virtually every application of statistical techniques in natural-language processing , maximizing the likelihood of the training data causes overfitting , resulting in lower task performance than some other estimates for the model parameters . \n\t', '\n\t\t This is implicitly recognized in the widespread adoption of early stopping in estimating the parameters of Model 1 . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t Both of these are far short of convergence to the maximum likelihood estimates for the model parameters . \n\t', '\n\t\t We have identified at least two ways in which the standard EM training method for Model 1 leads to suboptimal performance in terms of word- alignment accuracy . \n\t', '\n\t\t In this paper we show that by addressing these issues , substantial improvements in word-alignment accuracy can be achieved . \n\t', '\n\t\t 2 Definition of Model 1 Model 1 is a probabilistic generative model within a framework that assumes a source sentence S of length l translates as a target sentence T , according to the following stochastic process : \x95 A length m for sentence T is generated . \n\t', '\n\t\t \x95 For each target sentence position j ^ { 1 , . \n\t', '\n\t\t .. , m } : \x96 A generating word si in S ( including a null word so ) is selected , and \x96 The target word tj at position j is gener- ated depending on si . \n\t', '\n\t\t Model 1 is defined as a particularly simple instance of this framework , by assuming all possible lengths for T ( less than some arbitrary upper bound ) have a uniform probability E , all possible choices of source sentence generating words are equally likely , and the translation probability tr(tj | sz ) of the generated target language word depends only on the generating source language word\x97which \n\t\t']",Positive
"['\n\t\t We may also be interested in the question of what is the most likely alignment of a source sentence and a target sentence , given an instance of Model 1 ; where , by an alignment , we mean a specification of which source words generated which target words according to the generative model . \n\t', '\n\t\t Since Model 1 , like many other word-alignment models , requires each target word to be generated by exactly one source word ( including the null word ) , an alignment a can be represented by a vector a1 , ... , aM , where each aj is the sentence position of the source word generating tj according to the alignment . \n\t', '\n\t\t It is easy to show that for Model 1 , the most likely alignment a\x88 of S and T is given by this equation : M a\x88 = argmaxa H tr(tj|sa ; ) ( 2 ) j=1 Since in applying Model 1 , there are no dependencies between any of the aj s , we can find the most likely aligment simply by choosing , for each j , the value for aj that leads to the highest value for tr(tj|sa ; ) . \n\t', '\n\t\t The parameters of Model 1 for a given pair of languages are normally estimated using EM , taking as training data a corpus of paired sentences of the two languages , such that each pair consists of sentence in one language and a possible translation in the other language . \n\t', '\n\t\t The training is normally initialized by setting all translation probability distributions to the uniform distribution over the target language vocabulary . \n\t', '\n\t\t 3 Problems with Model 1 Model 1 clearly has many shortcomings as a model of translation . \n\t', '\n\t\t Some of these are structural limitations , and cannot be remedied without making the model significantly more complicated . \n\t', '\n\t\t Some of the major structural limitations include : \x95 ( Many-to-one ) Each word in the target sentence can be generated by at most one word in the source sentence . \n\t', '\n\t\t Situations in which a phrase in the source sentence translates as a single word in the target sentence are not well- modeled . \n\t', '\n\t\t \x95 ( Distortion ) The position of any word in the target sentence is independent of the position of the corresponding word in the source sentence , or the positions of any other source language words or their translations . \n\t', '\n\t\t The tendency for a contiguous phrase in one language to be translated as a contiguous phrase in another language is not modeled at all . \n\t', '\n\t\t \x95 ( Fertility ) Whether a particular source word is selected to generate the target word for a given position is independent of which or how many other target words the same source word is selected to generate . \n\t', '\n\t\t These limitations of Model 1 are all well known , they have been addressed in other word-alignment models , and we will not discuss them further here . \n\t', '\n\t\t Our concern in this paper is with two other problems with Model 1 that are not deeply structural , and can be addressed merely by changing how the parameters of Model 1 are estimated . \n\t', '\n\t\t The first of these nonstructural problems with Model 1 , as standardly trained , is that rare words in the source language tend to act as \x93garbage collectors\x94 \n\t\t']",Positive
"['\n\t\t This problem is not unique to Model 1 , but anecdotal examination of Model 1 alignments suggests that it may be worse for Model 1 , perhaps because Model 1 lacks the fertility and distortion parameters that may tend to mitigate the problem in more complex models . \n\t', '\n\t\t The cause of the problem can be easily understood if we consider a situation in which the source sentence contains a rare word that only occurs once in our training data , plus a frequent word that has an infrequent translation in the target sentence . \n\t', '\n\t\t Suppose the frequent source word has the translation present in the target sentence only 10 % of the time in our training data , and thus has an estimated translation probability of around 0.1 for this target word . \n\t', '\n\t\t Since the rare source word has no other occurrences in the data , EM training is free to assign whatever probability distribution is required to maximize the joint probability of this sentence pair . \n\t', '\n\t\t Even if the rare word also needs to be used to generate its actual translation in the sentence pair , a relatively high joint probability will be obtained by giving the rare M l H E tr(tj | sz ) ( 1 ) j=1 z=0 word a probability of 0.5 of generating its true translation and 0.5 of spuriously generating the translation of the frequent source word . \n\t', '\n\t\t The probability of this incorrect alignment will be higher than that obtained by assigning a probability of 1.0 to the rare word generating its true translation , and generating the true translation of the frequent source word with a probability of 0.1 . \n\t', '\n\t\t The usual fix for over-fitting problems of this type in statistical NLP is to smooth the probability estimates involved in some way . \n\t', '\n\t\t The second nonstructural problem with Model 1 is that it seems to align too few target words to the null source word . \n\t', '\n\t\t Anecdotal examination of Model 1 alignments of English source sentences with French target sentences reveals that null word alignments rarely occur in the highest probability alignment , despite the fact that French sentences often contain function words that do not correspond directly to anything in their English translation . \n\t', '\n\t\t For example , English phrases of the form ( nouns ) ( noun~ ) are often expressed in French by a phrase of the form ( noun~ ) de ( nouns ) , which may also be expressed in English ( but less often ) by a phrase of the form ( noun~ ) of ( nouns ) . \n\t', '\n\t\t The structure of Model 1 again suggests why we should not be surprised by this problem . \n\t', '\n\t\t As normally defined , Model 1 hypothesizes only one null word per sentence . \n\t', '\n\t\t A target sentence may contain many words that ideally should be aligned to null , plus some other instances of the same word that should be aligned to an actual source language word . \n\t', '\n\t\t For example , we may have an English/French sentence pair that contains two instances of of in the English sentence , and five instances of de in the French sentence . \n\t', '\n\t\t Even if the null word and of have the same initial probabilty of generating de , in iterating EM , this sentence is going to push the model towards estimating a higher probabilty that of generates de and a lower estimate that the null word generates de . \n\t', '\n\t\t This happens because there are are two instances of of in the source sentence and only one hypothetical null word , and Model 1 gives equal weight to each occurrence of each source word . \n\t', '\n\t\t In effect , of gets two votes , but the null word gets only one . \n\t', '\n\t\t We seem to need more instances of the null word for Model 1 to assign reasonable probabilities to target words aligning to the null word . \n\t', '\n\t\t 4 Smoothing Translation Counts We address the nonstructural problems of Model 1 discussed above by three methods . \n\t', '\n\t\t First , to address the problem of rare words aligning to too many words , at each interation of EM we smooth all the translation probability estimates by adding virtual counts according to a uniform probability distribution over all target words . \n\t', '\n\t\t This prevents the model from becoming too confident about the translation probabilities for rare source words on the basis of very little evidence . \n\t', '\n\t\t To estimate the smoothed probabilties we use the following formula : tr(tIs) = C(t , s ) + n ( 3 ) C(s) + n \x95 IV where C(t , s ) is the expected count of s generating t , C(s) is the corresponding marginal count for s , IV I is the hypothesized size of the target vocabulary V , and n is the added count for each target word in V. IV I and n are both free parameters in this equation . \n\t', '\n\t\t We could take IV I simply to be the total number of distinct words observed in the target language training , but we know that the target language will have many words that we have never observed . \n\t', '\n\t\t We arbitrarily chose IV I to be 100,000 , which is somewhat more than the total number of distinct words in our target language training data . \n\t', '\n\t\t The value of n is empirically optimized on annotated development test data . \n\t', '\n\t\t This sort of \x93add-n\x94 smoothing has a poor reputation in statistical NLP , because it has repeatedly been shown to perform badly compared to other methods of smoothing higher-order n-gram models for statistical language modeling ( e.g. , Chen and Goodman , 1996 ) . \n\t', '\n\t\t In those studies , however , add-n smoothing was used to smooth bigram or trigram models . \n\t', '\n\t\t Add-n smoothing is a way of smoothing with a uniform distribution , so it is not surprising that it performs poorly in language modeling when it is compared to smoothing with higher order models ; e.g , smoothing trigrams with bigrams or smoothing bigrams with unigrams . \n\t', '\n\t\t In situations where smoothing with a uniform distribution is appropriate , it is not clear that add-n is a bad way to do it . \n\t', '\n\t\t Furthermore , we would argue that the word translation probabilities of Model 1 are a case where there is no clearly better alternative to a uniform distribution as the smoothing distribution . \n\t', '\n\t\t It should certainly be better than smoothing with a unigram distribution , since we especially want to benefit from smoothing the translation probabilities for the rarest words , and smoothing with a unigram distribution would assume that rare words are more likely to translate to frequent words than to other rare words , which seems counterintuitive . \n\t', '\n\t\t 5 Adding Null Words to the Source Sentence We address the lack of sufficient alignments of target words to the null source word by adding extra null words to each source sentence . \n\t', '\n\t\t Mathematically , there is no reason we have to add an integral number of null words , so in fact we let the number of null words in a sentence be any positive number . \n\t', '\n\t\t One can make arguments in favor of adding the same number of null words to every sentence , or in favor of letting the number of null words be proportional to the length of the sentence . \n\t', '\n\t\t We have chosen to add a fixed number of null words to each source sentence regardless of length , and will leave for another time the question of whether this works better or worse than adding a number of null words proportional to the sentence length . \n\t', '\n\t\t Conceptually , adding extra null words to source sentences is a slight modification to the structure of Model 1 , but in fact , we can implement it without any additional model parameters by the simple expedient of multiplying all the translation probabilities for the null word by the number of null words per sentence . \n\t', '\n\t\t This multiplication is performed during every iteration of EM , as the translation probabilities for the null word are re-estimated from the corresponding expected counts . \n\t', '\n\t\t This makes these probabilities look like they are not normalized , but Model 1 can be applied in such a way that the translation probabilities for the null word are only ever used when multiplied by the number of null words in the sentence , so we are simply using the null word translation parameters to keep track of this product pre-computed . \n\t', '\n\t\t In training a version of Model 1 with only one null word per sentence , the parameters have their normal interpretation , since we are multiplying the standard probability estimates by 1 . \n\t', '\n\t\t 6 Initializing Model 1 with Heuristic Parameter Estimates Normally , the translation probabilities of Model 1 are initialized to a uniform distribution over the target language vocabulary to start iterating EM . \n\t', '\n\t\t The unspoken justification for this is that EM training of Model 1 will always converge to the same set of parameter values from any set of initial values , so the intial values should not matter . \n\t', '\n\t\t But this is only the case if we want to obtain the parameter values at convergence , and we have strong reasons to believe that these values do not produce the most accurate sentence alignments . \n\t', '\n\t\t Even though EM will head towards those values from any initial position in the parameter space , there may be some starting points we can systematically find that will take us closer to the optimal parameter values for alignment accuracy along the way . \n\t', '\n\t\t To test whether a better set of initial parameter estimates can improve Model 1 alignment ac curacy , we use a heuristic model based on the loglikelihood-ratio ( LLR ) statistic recommended by \n\t\t']",Positive
"['\n\t\t We chose this statistic because it has previously been found to be effective for automatically constructing translation lexicons ( e.g. , Melamed , 2000 ; Moore , 2001 ) . \n\t', '\n\t\t In our application , the statistic can be defined by the following formula : C(t ? \n\t', '\n\t\t , s ? \n\t', '\n\t\t ) log p(t?| s ? \n\t', '\n\t\t ) ( 4 ) p(t?) In this formula t and s mean that the corresponding words occur in the respective target and source sentences of an aligned sentence pair , -,t and -,s mean that the corresponding words do not occur in the respective sentences , t ? \n\t', '\n\t\t and s ? \n\t', '\n\t\t are variables ranging over these values , and C(t ? \n\t', '\n\t\t , s ? \n\t', '\n\t\t ) is the observed joint count for the values of t ? \n\t', '\n\t\t and s ? \n\t', '\n\t\t . \n\t', '\n\t\t All the probabilities in the formula refer to maximum likelihood estimates.1 These LLR scores can range in value from 0 to N · log ( 2 ) , where N is the number of sentence pairs in the training data . \n\t', '\n\t\t The LLR score for a pair of words is high if the words have either a strong positive association or a strong negative association . \n\t', '\n\t\t Since we expect translation pairs to be positively associated , we discard any negatively associated word pairs by requiring that p(t , s ) > p(t) \x95 p(s) . \n\t', '\n\t\t To use LLR scores to obtain initial estimates for the translation probabilities of Model 1 , we have to somehow transform them into numbers that range from 0 to 1 , and sum to no more than 1 for all the target words associated with each source word . \n\t', '\n\t\t We know that words with high LLR scores tend to be translations , so we want high LLR scores to correspond to high probabilities , and low LLR scores to correspond to low probabilities . \n\t', '\n\t\t The simplest approach would be to divide each LLR score by the sum of the scores for the source word of the pair , which would produce a normalized conditional probability distribution for each source word . \n\t', '\n\t\t Doing this , however , would discard one of the major advantages of using LLR scores as a measure of word association . \n\t', '\n\t\t All the LLR scores for rare words tend to be small ; thus we do not put too much confidence in any of the hypothesized word associations for such words . \n\t', '\n\t\t This is exactly the property needed to prevent rare source words from becoming garbage collectors . \n\t', '\n\t\t To maintain this property , for each source word we compute the sum of the 1 This is not the form in which the LLR statistic is usually presented , but it can easily be shown by basic algebra to be equivalent to \x97A in Dunning\x92s paper . \n\t', '\n\t\t See \n\t\t']",Positive
"['\n\t\t t?^~ t } s?^~ s } LLR scores over all target words , but we then divide every LLR score by the single largest of these sums . \n\t', '\n\t\t Thus the source word with the highest LLR score sum receives a conditional probability distribution over target words summing to 1 , but the corresponding distribution for every other source word sums to less than 1 , reserving some probability mass for target words not seen with that word , with more probability mass being reserved the rarer the word . \n\t', '\n\t\t There is no guarantee , of course , that this is the optimal way of discounting the probabilities assigned to less frequent words . \n\t', '\n\t\t To allow a wider range of possibilities , we add one more parameter to the model by raising each LLR score to an empirically optimized exponent before summing the resulting scores and scaling them from 0 to 1 as described above . \n\t', '\n\t\t Choosing an exponent less than 1.0 decreases the degree to which low scores are discounted , and choosing an exponent greater than 1.0 increases degree of discounting . \n\t', '\n\t\t We still have to define an initialization of the translation probabilities for the null word . \n\t', '\n\t\t We cannot make use of LLR scores because the null word occurs in every source sentence , and any word occuring in every source sentence will have an LLR score of 0 with every target word , since p(t|s) = p(t) in that case . \n\t', '\n\t\t We could leave the distribution for the null word as the uniform distribution , but we know that a high proportion of the words that should align to the null word are frequently occuring function words . \n\t', '\n\t\t Hence we initialize the distribution for the null word to be the unigram distribution of target words , so that frequent function words will receive a higher probability of aligning to the null word than rare words , which tend to be content words that do have a translation . \n\t', '\n\t\t Finally , we also effectively add extra null words to every sentence in this heuristic model , by multiplying the null word probabilities by a constant , as described in Section 5 . \n\t', '\n\t\t 7 Training and Evaluation We trained and evaluated our various modifications to Model 1 on data from the bilingual word alignment workshop held at HLT-NAACL 2003 \n\t\t']",Positive
"['\n\t\t We used a subset of the Canadian Hansards bilingual corpus supplied for the workshop , comprising 500,000 English-French sentences pairs , including 37 sentence pairs designated as \x93trial\x94 data , and 447 sentence pairs designated as test data . \n\t', '\n\t\t The trial and test data had been manually aligned at the word level , noting particular pairs of words either as \x93sure\x94 or \x93possible\x94 alignments , as described by \n\t\t']",Positive
"['\n\t\t To limit the number of translation probabilities that we had to store , we first computed LLR association scores for all bilingual word pairs with a positive association ( p(t , s ) > p(t) ·p(s)) , and discarded from further consideration those with an LLR score of less that 0.9 , which was chosen to be just low enough to retain all the \x93sure\x94 word alignments in the trial data . \n\t', '\n\t\t This resulted in 13,285,942 possible word-to-word translation pairs ( plus 66,406 possible null-word-to-word pairs ) . \n\t', '\n\t\t For most models , the word translation parameters are set automatically by EM . \n\t', '\n\t\t We trained each variation of each model for 20 iterations , which was enough in almost all cases to discern a clear minimum error on the 37 sentence pairs of trial data , and we chose as the preferred iteration the one with the lowest alignment error rate on the trial data . \n\t', '\n\t\t The other parameters of the various versions of Model 1 described in Sections 4\x966 were optimized with respect to alignment error rate on the trial data using simple hill climbing . \n\t', '\n\t\t All the results we report for the 447 sentence pairs of test data use the parameter values set to their optimal values for the trial data . \n\t', '\n\t\t We report results for four principal versions of Model 1 , trained using English as the source language and French as the target language : \x95 The standard model is initialized using uniform distributions , and trained without smoothing using EM , for a number of iterations optimized on the trial data . \n\t', '\n\t\t \x95 The smoothed model is like the standard model , but with optimized values of the null- word weight and add-n parameter . \n\t', '\n\t\t \x95 The heuristic model simply uses the initial heuristic estimates of the translation parameter values , with an optimized LLR exponent and null-word weight , but no EM re-estimation . \n\t', '\n\t\t \x95 The combined model initializes the translation parameter values with the heuristic estimates , using the LLR exponent and null-word weight from the optimal heuristic model , and applies EM using optimized values of the null-word weight and add-n parameters . \n\t', '\n\t\t The null-word weight used during EM is optimized separately from the null-word weight used in the initial heuristic parameter estimates . \n\t', '\n\t\t We also performed ablation experiments in which we ommitted each applicable modification in turn from each principal version of Model 1 , to observe the effect on alignment error . \n\t', '\n\t\t All non-EM-trained parameters were re-optimized on the trial data for each version of Model 1 tested , with the exception Model Trial AER Test AER Test Test LLR Exp Init NW EM NW Add EM Iter ( Ablation ) Recall Precision n Standard 0.311 0.298 0.810 0.646 NA NA 1.0 0.0000 17 Smoothed 0.261 0.271 0.646 0.798 NA NA 10.0 0.0100 15 ( EM NW ) 0.285 0.273 0.833 0.671 NA NA 1.0 0.0100 20 ( Add n ) 0.302 0.300 0.638 0.751 NA NA 13.0 0.0000 14 Heuristic 0.234 0.255 0.655 0.844 1.3 2.4 NA NA NA ( LLR Exp ) 0.257 0.259 0.655 0.844 1.0 2.4 NA NA NA ( Init NW ) 0.300 0.308 0.740 0.657 1.5 1.0 NA NA NA Combined 0.203 0.215 0.724 0.839 1.3 2.4 7.0 0.005 1 ( LLR Exp ) 0.258 0.272 0.636 0.809 1.0 2.4 10.0 0.0035 3 ( Init NW ) 0.197 0.209 0.722 0.854 1.5 1.0 10.0 0.0005 1 ( EM NW ) 0.281 0.267 0.833 0.680 1.3 2.4 1.0 0.0080 8 ( Add n ) 0.208 0.221 0.724 0.826 1.3 2.4 8.0 0.0000 1 Table 1 : Evaluation Results . \n\t', '\n\t\t that the value of the LLR exponent and initial null- word weight in the combined model were carried over from the heuristic model . \n\t', '\n\t\t 8 Results We report the performance of our different versions of Model 1 in terms of precision , recall , and alignment error rate ( AER ) as defined by \n\t\t']",Positive
"['\n\t\t These three performance statistics are defined as recall = |A|S|SJ ( 5 ) precision = JAI^|PJ ( 6 ) AER =1^ |A^S|+|A^P| ( 7 ) |A| + |S| where S denotes the annotated set of sure alignments , P denotes the annotated set of possible alignments , and A denotes the set of alignments produced by the model under test.2 We take AER , which is derived from F-measure , as our primary evaluation metric . \n\t', '\n\t\t The results of our evaluation are presented in Table 1 . \n\t', '\n\t\t The columns of the table present ( in order ) a description of the model being tested , the AER on the trial data , the AER on the test data , test data recall , and test data precision , followed by the optimal values on the trial data for the LLR exponent , the initial ( heuristic model ) null-word weight , the null- word weight used in EM re-estimation , the add-n parameter value used in EM re-estimation , and the number of iterations of EM . \n\t', '\n\t\t \x93NA\x94 means a parameter is not applicable in a particular model . \n\t', '\n\t\t 2As is customary , alignments to the null word are not explicitly counted . \n\t', '\n\t\t Results for the four principal versions of Model 1 are presented in bold . \n\t', '\n\t\t For each principal version , results of the corresponding ablation experiments are presented in standard type , giving the name of each omitted modification in parentheses.3 Probably the most striking result is that the heuristic model substantially reduces the AER compared to the standard or smoothed model , even without EM re-estimation . \n\t', '\n\t\t The combined model produces an additional substantial reduction in alignment error , using a single iteration of EM . \n\t', '\n\t\t The ablation experiments show how important the different modifications are to the various models . \n\t', '\n\t\t It is interesting to note that the importance of a given modification varies from model to model . \n\t', '\n\t\t For example , the re-estimation null-word weight makes essentially no contribution to the smoothed model . \n\t', '\n\t\t It can be tuned to reduce the error on the trial data , but the improvement does not carry over to the test data . \n\t', '\n\t\t The smoothed model with only the null- word weight and no add-n smoothing has essentially the same error as the standard model ; and the smoothed model with add-n smoothing alone has essentially the same error as the smoothed model with both the null-word weight and add-n smoothing . \n\t', '\n\t\t On the other hand , the re-estimation null-word weight is crucial to the combined model . \n\t', '\n\t\t With it , the combined model has substantially lower error than the heuristic model without re-estimation ; without it , for any number of EM iterations , the combined model has higher error than the heuristic model . \n\t', '\n\t\t A similar analysis shows that add-n smoothing is much less important in the combined model than 3Modificiations are \x93omitted\x94 by setting the corresponding parameter to a value that is equivalent to removing the modification from the model . \n\t', '\n\t\t the smoothed model . \n\t', '\n\t\t The probable explanation for this is that add-n smoothing is designed to address over-fitting from many iterations of EM . \n\t', '\n\t\t While the smoothed model does require many EM iterations to reach its minimum AER , the combined model , with or without add-n smoothing , is at its minimum AER with only one EM iteration . \n\t', '\n\t\t Finally , we note that , while the initial null-word weight is crucial to the heuristic model without re- estimation , the combined model actually performs better without it . \n\t', '\n\t\t Presumably , the re-estimation null-word weight makes the inital null-word weight redundant . \n\t', '\n\t\t In fact , the combined model without the initial null word-weight has the lowest AER on both the trial and test data of any variation tested ( note AERs in italics in Figure 1 ) . \n\t', '\n\t\t The relative reduction in AER for this model is 29.9 % compared to the standard model . \n\t', '\n\t\t We tested the significance of the differences in alignment error between each pair of our principal versions of Model 1 by looking at the AER for each sentence pair in the test set using a 2-tailed paired t test . \n\t', '\n\t\t The differences between all these models were significant at a level of 10-7 or better , except for the difference between the standard model and the smoothed model , which was \x93significant\x94 at the 0.61 level\x97that is , not at all significant . \n\t', '\n\t\t The reason for this is probably the very different balance between precision and recall with the standard and smoothed models , which indicates that the models make quite different sorts of errors , making statistical significance hard to establish . \n\t', '\n\t\t This conjecture is supported by considering the smoothed model omitting the re-estimation null-word weight , which has substantially the same AER as the full smoothed model , but with a precision/recall balance much closer to the standard model . \n\t', '\n\t\t The 2-tailed paired t test comparing this model to the standard model showed significance at a level of better than 10^10 . \n\t', '\n\t\t We also compared the combined model with and without the initial null-word weight , and found that the improvement without the weight was significant at the 0.008 level . \n\t', '\n\t\t 9 Conclusions We have demonstrated that it is possible to improve the performance of Model 1 in terms of alignment error by about 30 % , simply by changing the way its parameters are estimated . \n\t', '\n\t\t Almost half this improvement is obtained with a simple heuristic model that does not require EM re-estimation . \n\t', '\n\t\t It is interesting to contrast our heuristic model with the heuristic models used by \n\t\t']",Positive
"['\n\t\t The major difference between our model and theirs is that they base theirs on the Dice coefficient , which is computed by the formula4 C(t) + C(s) ( 8 ) while we use the log-likelihood-ratio statistic defined in Section 6 . \n\t', '\n\t\t Och and Ney find that the standard version of Model 1 produces more accurate alignments after only one iteration of EM than either of the heuristic models they consider , while we find that our heuristic model outperforms the standard version of Model 1 , even with an optimal number of iterations of EM . \n\t', '\n\t\t While the Dice coefficient is simple and intuitive\x97the value is 0 for words never found together , and 1 for words always found together\x97it lacks the important property of the LLR statistic that scores for rare words are discounted ; thus it does not address the over-fitting problem for rare words . \n\t', '\n\t\t The list of applications of IBM word-alignment Model 1 given in Section 1 should be sufficient to convince anyone of the relevance of improving the model . \n\t', '\n\t\t However , it is not clear that AER as defined by \n\t\t']",Negative
"['\n\t\t If AER does not reflect the optimal balance between precision and recall for a particular application , then optimizing AER may not produce the best task-based performance for that application . \n\t', '\n\t\t Thus the next step in this research must be to test whether the improvements in AER we have demonstrated for Model 1 lead to improvements on task-based performance measures . \n\t', '\n\t\t References Peter F. Brown , Stephen A. Della Pietra , Vincent J. Della Pietra , and Robert L. Mercer . \n\t', '\n\t\t 1993a . \n\t', '\n\t\t 4Och and Ney give a different formula in their paper , in which the addition in the denominator is replaced by a multiplication . \n\t', '\n\t\t According to Och ( personal communication ) , however , this is merely a typographical error in the publication , and the results reported are for the standard definition of the Dice coefficient . \n\t', '\n\t\t 5A possible exception is suggested by the results of \n\t\t']",Positive
"['\n\t\t 2 \x95 C(t , s ) The mathematics of statistical machine translation : parameter estimation . \n\t', '\n\t\t Computational Linguistics , 19(2):263\x96311 . \n\t', '\n\t\t Peter F. Brown , Stephen A. Della Pietra , Vincent J. Della Pietra , Meredith J. Goldsmith , Jan Hajic , Robert L. Mercer , and Surya Mohanty . \n\t', '\n\t\t 1993b . \n\t', '\n\t\t But dictionaries are data too . \n\t', '\n\t\t In Proceedings of the ARPA Workshop on Human Language Technology , pp. 202\x96205 , Plainsboro , New Jersey , USA . \n\t', '\n\t\t Stanley F. Chen and Joshua Goodman . \n\t', '\n\t\t 1996. An empirical study of smoothing techniques for language modeling . \n\t', '\n\t\t In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics , pp. 310\x96318 , Santa Cruz , California , USA . \n\t', '\n\t\t Yuan Ding , Daniel Gildea , and Martha Palmer . \n\t', '\n\t\t 2003. An algorithm for word-level alignment of parallel dependency trees . \n\t', '\n\t\t In Proceedings of the Ninth Machine Translation Summit , pp. 95\x96101 , New Orleans , Louisiana , USA . \n\t', '\n\t\t Ted Dunning . \n\t', '\n\t\t 1993 . \n\t', '\n\t\t Accurate methods for the statistics of surprise and coincidence . \n\t', '\n\t\t Computational Linguistics , 19(1):61\x9674 . \n\t', '\n\t\t Philipp Koehn , Franz Joseph Och , and Daniel Marcu . \n\t', '\n\t\t 2003. Statistical phrase-based translation . \n\t', '\n\t\t In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics ( HLT-NAACL 2003 ) , pp. 127\x96133 , Edmonton , Alberta , Canada . \n\t', '\n\t\t I. Dan Melamed . \n\t', '\n\t\t 2000. Models of Translational Equivalence . \n\t', '\n\t\t Computational Linguistics , 26(2):221\x96249 . \n\t', '\n\t\t Rada Mihalcea and Ted Pedersen . \n\t', '\n\t\t 2003. An evaluation exercise for word alignment . \n\t', '\n\t\t In Proceedings ofthe HLT-NAACL 2003 Workshop , Building and Using Parallel Texts : Data Driven Machine Translation and Beyond , pp. 1\x966 , Edmonton , Alberta , Canada . \n\t', '\n\t\t Robert C. Moore . \n\t', '\n\t\t 2001. Towards a simple and accurate statistical approach to learning translation relationships among words . \n\t', '\n\t\t In Proceedings of the Workshop Data-driven Machine Translation at the 39th Annual Meeting of the Association for Computational Linguistics , pp. 79\x9686 , Toulouse , France . \n\t', '\n\t\t Robert C. Moore . \n\t', '\n\t\t 2002. Fast and accurate sentence alignment of bilingual corpora . \n\t', '\n\t\t In S. Richardson ( ed . \n\t', '\n\t\t ) , Machine Translation : From Research to Real Users ( Proceedings , 5th Conference of the Association for Machine Translation in the Americas , Tiburon , California ) , pp. 135\x96244 , Springer-Verlag , Heidelberg , Germany . \n\t', '\n\t\t Robert C. Moore . \n\t', '\n\t\t 2004. On log-likelihood-ratios and the significance of rare events . \n\t', '\n\t\t In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing , Barcelona , Spain . \n\t', '\n\t\t Dragos S. Munteanu , Alexander Fraser , and Daniel Marcu . \n\t', '\n\t\t 2004. Improved machine translation performance via parallel sentence extraction from comparable corpora . \n\t', '\n\t\t In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics ( HLT-NAACL 2004 ) , pp. 265\x96272 , Boston , Massachusetts , USA . \n\t', '\n\t\t Francisco Nevado , Francisco Casacuberta , and Enrique Vidal . \n\t', '\n\t\t 2003. Parallel corpora segmentation using anchor words . \n\t', '\n\t\t In Proceedings of the 7th International EAMT workshop on MT and other language technology tools , Improving MT through other language technology tools , Resources and tools for building MT , pp. 33\x9640 , Budapest , Hungary . \n\t', '\n\t\t Franz Joseph Och and Hermann Ney . \n\t', '\n\t\t 2003. A systematic comparison of various statistical alignment models . \n\t', '\n\t\t Computational Linguistics , 29(1):19\x9651 . \n\t', '\n\t\t Franz Josef Och et al . 2004. A smorgasbord of features for statistical machine translation . \n\t', '\n\t\t In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics ( HLT-NAACL 2004 ) , pp. 161\x96168 , Boston , Massachusetts , USA . \n\t', '\n\t\t Ashish Venugopal , Stephan Vogel , and Alex Waibel . \n\t', '\n\t\t 2003. Effective phrase translation extraction from alignment models . \n\t', '\n\t\t In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics , pp. 319\x96326 , Sapporo , Japan . \n\t', '\n\t\t A Geometric View on Bilingual Lexicon Extraction from Comparable Corpora E. Gaussiert , J.-M. Renderst , I. Matveeva* , C. Gouttet , H. D´ejeant tXerox Research Centre Europe 6 , Chemin de Maupertuis \x97 38320 Meylan , France Eric.Gaussier@xrce.xerox.com *Dept of Computer Science , University of Chicago 1100 E. 58th St. Chicago , IL 60637 USA matveeva@cs.uchicago.edu Abstract We present a geometric view on bilingual lexicon extraction from comparable corpora , which allows to re-interpret the methods proposed so far and identify unresolved problems . \n\t', '\n\t\t This motivates three new methods that aim at solving these problems . \n\t', '\n\t\t Empirical evaluation shows the strengths and weaknesses of these methods , as well as a significant gain in the accuracy of extracted lexicons . \n\t', '\n\t\t 1 Introduction Comparable corpora contain texts written in different languages that , roughly speaking , \x94talk about the same thing\x94 . \n\t', '\n\t\t In comparison to parallel corpora , ie corpora which are mutual translations , comparable corpora have not received much attention from the research community , and very few methods have been proposed to extract bilingual lexicons from such corpora . \n\t', '\n\t\t However , except for those found in translation services or in a few international organisations , which , by essence , produce parallel documentations , most existing multilingual corpora are not parallel , but comparable . \n\t', ""\n\t\t This concern is reflected in major evaluation conferences on cross- language information retrieval ( CLIR ) , e.g. CLEF ' , which only use comparable corpora for their multilingual tracks . \n\t"", '\n\t\t We adopt here a geometric view on bilingual lexicon extraction from comparable corpora which allows one to re-interpret the methods proposed thus far and formulate new ones inspired by latent semantic analysis ( LSA ) , which was developed within the information retrieval ( IR ) community to treat synonymous and polysemous terms \n\t\t']",Positive
"['\n\t\t We will explain in this paper the motivations behind the use of such methods for bilingual lexicon extraction from comparable corpora , and show how to apply them . \n\t', '\n\t\t Section 2 is devoted to the presentation of the standard approach , ie the approach adopted by most researchers so far , its geometric interpretation , and the unresolved synonymy 1 http : //clef . \n\t', '\n\t\t iei.pi . \n\t', '\n\t\t cnr. it:2002/ and polysemy problems . \n\t', '\n\t\t Sections 3 to 4 then describe three new methods aiming at addressing the issues raised by synonymy and polysemy : in section 3 we introduce an extension of the standard approach , and show in appendix A how this approach relates to the probabilistic method proposed in \n\t\t']",Positive
"['\n\t\t Section 6 is then devoted to a large-scale evaluation of the different methods proposed . \n\t', '\n\t\t Open issues are then discussed in section 7 . \n\t', '\n\t\t 2 Standard approach Bilingual lexicon extraction from comparable corpora has been studied by a number of researchers , ( Rapp , 1995 ; Peters and Picchi , 1995 ; Tanaka and Iwasaki , 1996 ; Shahzad et al. , 1999 ; Fung , 2000 , among others ) . \n\t', '\n\t\t Their work relies on the assumption that if two words are mutual translations , then their more frequent collocates ( taken here in a very broad sense ) are likely to be mutual translations as well . \n\t', '\n\t\t Based on this assumption , the standard approach builds context vectors for each source and target word , translates the target context vectors using a general bilingual dictionary , and compares the translation with the source context vector : 1 . \n\t', '\n\t\t For each source word v ( resp . \n\t', '\n\t\t target word w ) , build a context vector ~^v ( resp . \n\t', '\n\t\t ~^w ) consisting in the measure of association of each word e ( resp . \n\t', '\n\t\t f ) in the context of v ( resp . \n\t', '\n\t\t w ) , a(v , e ) . \n\t', '\n\t\t 2. Translate the context vectors with a general bilingual dictionary D , accumulating the contributions from words that yield identical translations . \n\t', '\n\t\t 3. Compute the similarity between source word v and target word w using a similarity measures , such as the Dice or Jaccard coefficients , or the cosine measure . \n\t', '\n\t\t As the dot-product plays a central role in all these measures , we consider , without loss of generality , the similarity given by the dot-product between ~^v and the translation of ~^w : ^^^~(v , tr(w) = E a(v , e ) E a(w , f ) e f,(e,f)inD 11 a(v , e ) a(w , f ) ( 1 ) (e,f)ED Because of the translation step , only the pairs ( e , f ) that are present in the dictionary contribute to the dot-product . \n\t', '\n\t\t Note that this approach requires some general bilingual dictionary as initial seed . \n\t', '\n\t\t One way to circumvent this requirement consists in automatically building a seed lexicon based on spelling and cognates clues \n\t\t']",Positive
['\n\t\t Another approach directly tackles the problem from scratch by searching for a translation mapping which optimally preserves the intralingual association measure between words \n\t\t'],Positive
"['\n\t\t In this latter case , the association measure is defined as the Spearman rank order correlation between their context vectors restricted to \x93peripheral tokens\x94 ( highly frequent words ) . \n\t', '\n\t\t The search method is based on a gradient descent algorithm , by iteratively changing the mapping of a single word until ( locally ) minimizing the sum of squared differences between the association measure of all pairs of words in one language and the association measure of the pairs of translated words obtained by the current mapping . \n\t', '\n\t\t 2.1 Geometric presentation We denote by si,1 < i < p and td,1 < j < q the source and target words in the bilingual dictionary D. D is a set of n translation pairs ( si , td ) , and may be represented as a p x q matrix M , such that Mid = 1 iff ( si , td ) E D ( and 0 otherwise).2 Assuming there are m distinct source words e1 , \x95 \x95 \x95 , em and r distinct target words f1 , \x95 \x95 \x95 , fr in the corpus , figure 1 illustrates the geometric view of the standard method . \n\t', '\n\t\t The association measure a(v , e ) may be viewed as the coordinates of the m-dimensional context vector ~^v in the vector space formed by the orthogonal basis ( e1 , \x95 \x95 \x95 , em ) . \n\t', '\n\t\t The dot-product in ( 1 ) only involves source dictionary entries . \n\t', '\n\t\t The corresponding dimensions are selected by an orthogonal 2The extension to weighted dictionary entries Mij E [ 0 , 1 ] is straightforward but not considered here for clarity . \n\t', '\n\t\t projection on the sub-space formed by ( s1 , \x95 \x95 \x95 , sp ) , using a p x m projection matrix P3 . \n\t', '\n\t\t Note that ( s1 , \x95 \x95 \x95 , sp ) , being a sub-family of ( e1 , \x95 \x95 \x95 , em ) , is an orthogonal basis of the new sub-space . \n\t', '\n\t\t Similarly , ~^w is projected on the dictionary entries ( t1 , \x95 \x95 \x95 , tQ ) using a q x r orthogonal projection matrix Pt. . \n\t', '\n\t\t As M encodes the relationship between the source and target entries of the dictionary , equation 1 may be rewritten as : S(v , w ) = ( ^~v , tr(w)) = ( P3 ^~ ^^^~v )T M ( Pt ^~w ) ( 2 ) where T denotes transpose . \n\t', '\n\t\t In addition , notice that M can be rewritten as ST T , with S an n x p and T an n x q matrix encoding the relations between words and pairs in the bilingual dictionary ( e.g. . \n\t', '\n\t\t Ski is 1 iff si is in the kth translation pair ) . \n\t', '\n\t\t Hence : S(v,w)= ^~vTPT3 STTPt^~w =(SP3^~v,TPt^~w) ( 3 ) which shows that the standard approach amounts to performing a dot-product in the vector space formed by the n pairs ( ( s1 , tl ) , \x95 \x95 \x95 , ( sp , tk ) ) , which are assumed to be orthogonal , and correspond to translation pairs . \n\t', '\n\t\t 2.2 Problems with the standard approach There are two main potential problems associated with the use of a bilingual dictionary . \n\t', '\n\t\t Coverage . \n\t', '\n\t\t This is a problem if too few corpus words are covered by the dictionary . \n\t', '\n\t\t However , if the context is large enough , some context words are bound to belong to the general language , so a general bilingual dictionary should be suitable . \n\t', '\n\t\t We thus expect the standard approach to cope well with the coverage problem , at least for frequent words . \n\t', '\n\t\t For rarer words , we can bootstrap the bilingual dictionary by iteratively augmenting it with the most probable translations found in the corpus . \n\t', '\n\t\t Polysemy/synonymy . \n\t', '\n\t\t Because all entries on either side of the bilingual dictionary are treated as orthogonal dimensions in the standard methods , problems may arise when several entries have the same meaning ( synonymy ) , or when an entry has several meanings ( polysemy ) , especially when only one meaning is represented in the corpus . \n\t', '\n\t\t Ideally , the similarities wrt synonyms should not be independent , but the standard method fails to account for that . \n\t', '\n\t\t The axes corresponding to synonyms si and sd are orthogonal , so that projections of a context vector on si and sd will in general be uncorrelated . \n\t', '\n\t\t Therefore , a context vector that is similar to si may not necessarily be similar to sd . \n\t', '\n\t\t A similar situation arises for polysemous entries . \n\t', '\n\t\t Suppose the word bank appears as bothfinancial institution ( French : banque ) and ground near a river e2 em P e1 S1 ( S It i ) v "" ( S It 1 ) ( Sp , t0 tp f2 S sp v\x92 w "" fr T w\x92 Pt t1 f1 v w Figure 1 : Geometric view of the standard approach ( French : berge ) , but only the pair ( banque , bank ) is in the bilingual dictionary . \n\t', '\n\t\t The standard method will deem similar river , which co-occurs with bank , and argent ( money ) , which co-occurs with banque . \n\t', '\n\t\t In both situations , however , the context vectors of the dictionary entries provide some additional information : for synonyms si and sj , it is likely that ~^si and ~^sj are similar ; for polysemy , if the context vec- tors banque and bank have few translations pairs in common , it is likely that banque and bank are used with somewhat different meanings . \n\t', '\n\t\t The following methods try to leverage this additional information . \n\t', '\n\t\t 3 Extension of the standard approach The fact that synonyms may be captured through similarity of context vectors leads us to question the projection that is made in the standard method , and to replace it with a mapping into the sub-space formed by the context vectors of the dictionary entries , that is , instead of projecting ~^v on the subspace formed by ( s1 , \x95 \x95 \x95 , sp ) , we now map it onto the sub-space generated by ( ^~s1 , \x95 \x95 \x95 , ~^sp ) . \n\t', '\n\t\t With this mapping , we try to find a vector space in which synonymous dictionary entries are close to each other , while polysemous ones still select different neighbors . \n\t', '\n\t\t This time , if ~^v is close to ~^si and ~^sj , si and sj being synonyms , the translations of both si and sj will be used to find those words w close to v. . \n\t', '\n\t\t Figure 2 illustrates this process . \n\t', '\n\t\t By denoting Q3 , respectively Qt , such a mapping in the source ( resp . \n\t', '\n\t\t target ) side , and using the same translation mapping ( S , T ) as above , the similarity between source and target words becomes : S(v , w)= ~SQ3 ^~v , TQt^~w ~= ^~vT QT3 STTQt^~w ( 4 ) A natural choice for Q3 ( and similarly for Qt ) is the following m x p matrix : ~ ~ Q3=RT 3 = ~ 3This assumption has been experimentally validated in several studies , e.g. \n\t\t']",Positive
"['\n\t\t but other choices , such as a pseudo-inverse of R3 , are possible . \n\t', '\n\t\t Note however that computing the pseudo-inverse of R3 is a complex operation , while the above projection is straightforward ( the columns of Q correspond to the context vectors of the dictionary words ) . \n\t', '\n\t\t In appendix A we show how this method generalizes over the probabilistic approach presented in \n\t\t']",Positive
"['\n\t\t The above method bears similarities with the one described in ( Besanc¸on et al. , 1999 ) , where a matrix similar to Q3 is used to build a new term-document matrix . \n\t', '\n\t\t However , the motivations behind their work and ours differ , as do the derivations and the general framework , which justifies e.g. the choice of the pseudo-inverse of R3 in our case . \n\t', '\n\t\t 4 Canonical correlation analysis The data we have at our disposal can naturally be represented as an n x ( m + r ) matrix in which the rows correspond to translation pairs , and the columns to source and target vocabularies : e1 \x95\x95\x95 em f1 \x95\x95\x95 fr \x95 \x95 \x95 \x95 \x95 \x95 \x95 \x95 \x95 ... ... ... \x95 \x95 \x95 \x95 \x95 \x95 \x95 \x95 \x95 where ( s(k) , t(k)) is just a renumbering of the translation pairs ( si , tj ) . \n\t', '\n\t\t Matrix C shows that each translation pair supports two views , provided by the context vectors in the source and target languages . \n\t', '\n\t\t Each view is connected to the other by the translation pair it represents . \n\t', '\n\t\t The statistical technique of canonical correlation analysis ( CCA ) can be used to identify directions in the source view ( first m columns of C ) and target view ( last r columns of C ) that are maximally correlated , ie \x93behave in the same way\x94 wrt the translation pairs . \n\t', '\n\t\t We are thus looking for directions in the source and target vector spaces ( defined by the orthogonal bases ( e1 , \x95 \x95 \x95 , em ) and ( f1 , \x95 \x95 \x95 , fr ) ) such that the projections of the translation pairs on these directions are maximally correlated . \n\t', '\n\t\t Intuitively , those directions define latent semantic axes a(s1,e1) \x95\x95\x95 a(sp,e1) ... ... ... a(s1 , em ) \x95\x95\x95 a(sp , em ) ~ ~ ~ C= \x95 \x95 \x95 \x95 \x95 \x95 \x95 \x95 \x95 ... ... ... \x95 \x95 \x95 \x95 \x95 \x95 \x95 \x95 \x95 ( s(1) , t(1)) ... ( s(n) , t(n)) Figure 2 : Geometric view of the extended approach i s em em f2 ( s It ) ( s Vt k ) w "" v "" ( s It 1 ) T fr f2 t tk 1 w Qt t2 tq f1 f1 fr e2 s2 Qs v s e1 p e1 s1 v e2 w that capture the implicit relations between translation pairs , and induce a natural mapping across languages . \n\t', ""\n\t\t Denoting by fi3 and fit the directions in the source and target spaces , respectively , this may be formulated as : i(3,-§'(i)) ( fit , t ( i ) ) ~Ei(fi3 , - § , ( i ) ) Ej(fit , t ( j ) ) As in principal component analysis , once the first two directions ( fi13 , fi1t ) have been identified , the process can be repeated in the sub-space orthogonal to the one formed by the already identified directions . \n\t"", '\n\t\t However , a general solution based on a set of eigenvalues can be proposed . \n\t', '\n\t\t Following e.g. \n\t\t']",Positive
['\n\t\t 5 is to perform an incomplete Cholesky decomposition of a regularized form of D \n\t\t'],Positive
"['\n\t\t This yields pairs of source and target directions ( fi13 , fi1t ) , \x95 \x95 \x95 , ( fil3 , filt ) that define a new sub-space in which to project words from each language . \n\t', '\n\t\t This sub-space plays the same role as the sub-space defined by translation pairs in the standard method , although with CCA , it is derived from the corpus via the context vectors of the translation pairs . \n\t', '\n\t\t Once projected , words from different languages can be compared through their dot-product or cosine . \n\t', '\n\t\t De- T \x97 r 1 l]T noting °3=r fi31 , \x95\x95\x95 fi3l ] , and °t fit , \x95 \x95 \x95 fit , the similarity becomes ( figure 3 ) : S(v , w ) = ( °3 ^~v , °t ^~w ) = ~^vT °T3 °t ^~w(6) The number l of vectors retained in each language directly defines the dimensions of the final subspace used for comparing words across languages . \n\t', '\n\t\t CCA and its kernelised version were used in \n\t\t']",Positive
"['\n\t\t We show here that it can be used to infer language-independent semantic representations from comparable corpora , which induce a similarity between words in the source and target languages . \n\t', '\n\t\t 5 Multilingual probabilistic latent semantic analysis The matrix C described above encodes in each row k the context vectors of the source ( first m columns ) and target ( last r columns ) of each translation pair . \n\t', '\n\t\t Ideally , we would like to cluster this matrix such that translation pairs with synonymous words appear in the same cluster , while translation pairs with polysemous words appear in different clusters ( soft clustering ) . \n\t', '\n\t\t Furthermore , because of the symmetry between the roles played by translation pairs and vocabulary words ( synonymous and polysemous vocabulary words should also behave as described above ) , we want the clustering to behave symmetrically with respect to translation pairs and vocabulary words . \n\t', '\n\t\t One well-motivated method that fulfills all the above criteria is Probabilistic Latent Semantic Analysis ( PLSA ) \n\t\t']",Positive
"['\n\t\t Assuming that C encodes the co-occurrences between vocabulary words w and translation pairs d , PLSA models the probability of co-occurrence w and d via latent classes a : S G-~ Tt)2 / , fi = \\ fit / P(w , d ) = E P(a) P(wI a ) P(dI a ) ( 7 ) a where , for a given class , words and translation pairs are assumed to be independently generated from class-conditional probabilities P(wI a ) and P(dI a ) . \n\t', '\n\t\t Note here that the latter distribution is language- independent , and that the same latent classes are used for the two languages . \n\t', '\n\t\t The parameters of the model are obtained by maximizing the likelihood of the observed data ( matrix C ) through Expectation- Maximisation algorithm \n\t\t']",Positive
"['\n\t\t In p = max ~s,~t , B = \\ 0 RtRTR3RT3 1 R3RT3 RtRT 0 J \\D ( R3RT 2 03)(RtR f2 e2 ( CCA ) ~ls e1 em em e2 v ~2 s v ~1s ~it ( ~1s , ~1t ) f1 ( ~2s,~2t ) w "" f2 V ( CCA ) w v "" ~2 t ( ~ls , ~l t ) ~l t fr e1 ~is f1 fr w Figure 3 : Geometric view of the Canonical Correlation Analysis approach addition , in order to reduce the sensitivity to initial conditions , we use a deterministic annealing scheme \n\t\t']",Positive
"['\n\t\t The update formulas for the EM algorithm are given in appendix B . \n\t', '\n\t\t This model can identify relevant bilingual latent classes , but does not directly define a similarity between words across languages . \n\t', '\n\t\t That may be done by using Fisher kernels as described below . \n\t', '\n\t\t Associated similarities : Fisher kernels Fisher kernels \n\t\t']",Positive
"['\n\t\t They are useful whenever a direct similarity between observed feature is hard to define or insufficient . \n\t', '\n\t\t Denoting e(w) = lnP(wlB) the log- likelihood for example w , the Fisher kernel is : K(w1 , w2 ) = V~(w1)TIF-1V~(w2) ( 8 ) The Fisher information matrix IF E (VE(x)VE(x)T) keeps the kernel indepen- dent of reparameterisation . \n\t', '\n\t\t With a suitable parameterisation , we assume IF pz~ 1 . \n\t', '\n\t\t For PLSA \n\t\t']",Positive
"['\n\t\t The Fisher kernel performs a dot-product in a vector space defined by the parameters of the model . \n\t', '\n\t\t With only one class , the expression of the Fisher kernel ( 9 ) reduces to : EK(w1 , w2 ) = 1 + d Apart from the additional intercept ( \x921\x92 ) , this is exactly the similarity provided by the standard method , with associations given by scaled empirical frequencies a(w , d ) = P~(dl w)/ ~P(d) . \n\t', '\n\t\t Accordingly , we expect that the standard method and the Fisher kernel with one class should have similar behaviors . \n\t', '\n\t\t In addition to the above kernel , we consider two additional versions , obtained:through normalisation ( NFK ) and exponentiation ( EFK ) : NFK(w1 , w2 ) = K(w1 , w2 ) ( 10 ) ~IK(w1)K(w2) 2(K(wl)+K(w2)-2K(wl,w2)) EFK(w1,w2) = e 1 where K(w) stands for K(w , w ) . \n\t', '\n\t\t 6 Experiments and results We conducted experiments on an English-French corpus derived from the data used in the multilingual track of CLEF2003 , corresponding to the newswire of months May 1994 and December 1994 of the Los Angeles Times ( 1994 , English ) and Le Monde ( 1994 , French ) . \n\t', '\n\t\t As our bilingual dictionary , we used the ELRA multilingual dictionary,4 which contains ca . \n\t', '\n\t\t 13,500 entries with at least one match in our corpus . \n\t', '\n\t\t In addition , the following linguistic preprocessing steps were performed on both the corpus and the dictionary : tokenisation , lemmatisation and POS-tagging . \n\t', '\n\t\t Only lexical words ( nouns , verbs , adverbs , adjectives ) were indexed and only single word entries in the dicitonary were retained . \n\t', '\n\t\t Infrequent words ( occurring less than 5 times ) were discarded when building the indexing terms and the dictionary entries . \n\t', '\n\t\t After these steps our corpus contains 34,966 distinct English words , and 21,140 distinct French words , leading to ca . \n\t', '\n\t\t 25,000 English and 13,000 French words not present in the dictionary . \n\t', '\n\t\t To evaluate the performance of our extraction methods , we randomly split the dictionaries into a training set with 12,255 entries , and a test set with 1,245 entries . \n\t', '\n\t\t The split is designed in such a way that all pairs corresponding to the same source word are in the same set ( training or test ) . \n\t', '\n\t\t All methods use the training set as the sole available resource and predict the most likely translations of the terms in the source language ( English ) belonging to the 4Available through www.elra.info = +E P~(dl w1 ) P(dl w2)E d a P~(dl w1 ) P~(dlw2) P(d) test set . \n\t', '\n\t\t The context vectors were defined by computing the mutual information association measure between terms occurring in the same context window of size 5 ( ie. by considering a neighborhood of +/- 2 words around the current word ) , and summing it over all contexts of the corpora . \n\t', '\n\t\t Different association measures and context sizes were assessed and the above settings turned out to give the best performance even if the optimum is relatively flat . \n\t', '\n\t\t For memory space and computational efficiency reasons , context vectors were pruned so that , for each term , the remaining components represented at least 90 percent of the total mutual information . \n\t', '\n\t\t After pruning , the context vectors were normalised so that their Euclidean norm is equal to 1 . \n\t', '\n\t\t The PLSA-based methods used the raw co-occurrence counts as association measure , to be consistent with the underlying generative model . \n\t', '\n\t\t In addition , for the extended method , we retained only the N ( N = 200 is the value which yielded the best results in our experiments ) dictionary entries closest to source and target words when doing the projection with Q . \n\t', '\n\t\t As discussed below , this allows us to get rid of spurious relationships . \n\t', '\n\t\t The upper part of table 1 summarizes the results we obtained , measured in terms of F-1 score for different lengths of the candidate list , from 20 to 500 . \n\t', '\n\t\t For each length , precision is based on the number of lists that contain an actual translation of the source word , whereas recall is based on the number of translations provided in the reference set and found in the list . \n\t', '\n\t\t Note that our results differ from the ones previously published , which can be explained by the fact that first our corpus is relatively small compared to others , second that our evaluation relies on a large number of candidates , which can occur as few as 5 times in the corpus , whereas previous evaluations were based on few , high frequent terms , and third that we do not use the same bilingual dictionary , the coverage of which being an important factor in the quality of the results obtained . \n\t', '\n\t\t Long candidate lists are justified by CLIR considerations , where longer lists might be preferred over shorter ones for query expansion purposes . \n\t', '\n\t\t For PLSA , the normalised Fisher kernels provided the best results , and increasing the number of latent classes did not lead in our case to improved results . \n\t', '\n\t\t We thus display here the results obtained with the normalised version of the Fisher kernel , using only one component . \n\t', '\n\t\t For CCA , we empirically optimised the number of dimensions to be used , and display the results obtained with the optimal value ( l = 300 ) . \n\t', '\n\t\t As one can note , the extended approach yields the best results in terms of F1-score . \n\t', '\n\t\t However , its performance for the first 20 candidates are below the standard approach and comparable to the PLSAbased method . \n\t', '\n\t\t Indeed , the standard approach leads to higher precision at the top of the list , but lower recall overall . \n\t', '\n\t\t This suggests that we could gain in performance by re-ranking the candidates of the extended approach with the standard and PLSA methods . \n\t', '\n\t\t The lower part of table 1 shows that this is indeed the case . \n\t', '\n\t\t The average precision goes up from 0.4 to 0.44 through this combination , and the F1-score is significantly improved for all the length ranges we considered ( bold line in table 1 ) . \n\t', '\n\t\t 7 Discussion Extended method As one could expect , the extended approach improves the recall of our bilingual lexicon extraction system . \n\t', '\n\t\t Contrary to the standard approach , in the extended approach , all the dictionary words , present or not in the context vector of a given word , can be used to translate it . \n\t', '\n\t\t This leads to a noise problem since spurious relations are bound to be detected . \n\t', '\n\t\t The restriction we impose on the translation pairs to be used ( N nearest neighbors ) directly aims at selecting only the translation pairs which are in true relation with the word to be translated . \n\t', '\n\t\t Multilingual PLSA Even though theoretically well-founded , PLSA does not lead to improved performance . \n\t', '\n\t\t When used alone , it performs slightly below the standard method , for different numbers of components , and performs similarly to the standard method when used in combination with the extended method . \n\t', '\n\t\t We believe the use of mere co- occurrence counts gives a disadvantage to PLSA over other methods , which can rely on more sophisticated measures . \n\t', '\n\t\t Furthermore , the complexity of the final vector space ( several millions of dimensions ) in which the comparison is done entails a longer processing time , which renders this method less attractive than the standard or extended ones . \n\t', '\n\t\t Canonical correlation analysis The results we obtain with CCA and its kernel version are disappointing . \n\t', '\n\t\t As already noted , CCA does not directly solve the problems we mentioned , and our results show that CCA does not provide a good alternative to the standard method . \n\t', '\n\t\t Here again , we may suffer from a noise problem , since each canonical direction is defined by a linear combination that can involve many different vocabulary words . \n\t', '\n\t\t Overall , starting with an average precision of 0.35 as provided by the standard approach , we were able to increase it to 0.44 with the methods we consider . \n\t', '\n\t\t Furthermore , we have shown here that such an improvement could be achieved with relatively simple 20 60 100 160 200 260 300 400 500 Avg.Prec . \n\t', '\n\t\t standard 0.14 0.20 0.24 0.29 0.30 0.33 0.35 0.38 0.40 0.35 Ext ( N=500 ) 0.11 0.21 0.27 0.32 0.34 0.38 0.41 0.45 0.50 0.40 CCA ( l=300 ) 0.04 0.10 0.14 0.20 0.22 0.26 0.29 0.35 0.41 0.25 NFK(k=1) 0.10 0.15 0.20 0.23 0.26 0.27 0.28 0.32 0.34 0.30 Ext + standard 0.16 0.26 0.32 0.37 0.40 0.44 0.45 0.47 0.50 0.44 Ext + NFK(k=1) 0.13 0.23 0.28 0.33 0.38 0.42 0.44 0.48 0.50 0.42 Ext + NFK(k=4) 0.13 0.22 0.26 0.33 0.37 0.40 0.42 0.47 0.50 0.41 Ext + NFK ( k=16 ) 0.12 0.20 0.25 0.32 0.36 0.40 0.42 0.47 0.50 0.40 Table 1 : Results of the different methods ; F-1 score at different number of candidate translations . \n\t', '\n\t\t Ext refers to the extended approach , whereas NFK stands for normalised Fisher kernel . \n\t', '\n\t\t methods . \n\t', '\n\t\t Nevertheless , there are still a number of issues that need be addressed . \n\t', '\n\t\t The most important one concerns the combination of the different methods , which could be optimised on a validation set . \n\t', '\n\t\t Such a combination could involve Fisher kernels with different latent classes in a first step , and a final combination of the different methods . \n\t', '\n\t\t However , the results we obtained so far suggest that the rank of the candidates is an important feature . \n\t', '\n\t\t It is thus not guaranteed that we can gain over the combination we used here . \n\t', '\n\t\t 8 Conclusion We have shown in this paper how the problem of bilingual lexicon extraction from comparable corpora could be interpreted in geometric terms , and how this view led to the formulation of new solutions . \n\t', '\n\t\t We have evaluated the methods we propose on a comparable corpus extracted from the CLEF colection , and shown the strengths and weaknesses of each method . \n\t', '\n\t\t Our final results show that the combination of relatively simple methods helps improve the average precision of bilingual lexicon extraction methods from comparale corpora by 10 points . \n\t', '\n\t\t We hope this work will help pave the way towards a new generation of cross-lingual information retrieval systems . \n\t', '\n\t\t Acknowledgements We thank J.-C. Chappelier and M. Rajman who pointed to us the similarity between our extended method and the model DSIR ( distributional semantics information retrieval ) , and provided us with useful comments on a first draft of this paper . \n\t', '\n\t\t We also want to thank three anonymous reviewers for useful comments on a first version of this paper . \n\t', '\n\t\t References F. R. Bach and M. I. Jordan . \n\t', '\n\t\t 2001. Kernel independent component analysis . \n\t', '\n\t\t Journal ofMachine Learning Research . \n\t', '\n\t\t R. Besanc¸on , M. Rajman , and J.-C. Chappelier . \n\t', '\n\t\t 1999. Textual similarities based on a distributional approach . \n\t', '\n\t\t In Proceedings of the Tenth International Workshop on Database and Expert Systems Applications ( DEX\x9299 ) , Florence , Italy . \n\t', '\n\t\t S. Deerwester , S. T. Dumais , G. W. Furnas , T. K. Landauer , and R. Harshman . \n\t', '\n\t\t 1990 . \n\t', '\n\t\t Indexing by latent semantic analysis . \n\t', '\n\t\t Journal ofthe American Society for Information Science , 41(6):391\x96407 . \n\t', '\n\t\t H. Dejean , E. Gaussier , and F. Sadat . \n\t', '\n\t\t 2002. An approach based on multilingual thesauri and model combination for bilingual lexicon extraction . \n\t', '\n\t\t In International Conference on Computational Linguistics , COLING\x9202 . \n\t', '\n\t\t A. P. Dempster , N. M. Laird , and D. B. Rubin . \n\t', '\n\t\t 1977. Maximum likelihood from incomplete data via the EM algorithm . \n\t', '\n\t\t Journal of the Royal Statistical Society , Series B , 39(1):1\x9638 . \n\t', '\n\t\t Mona Diab and Steve Finch . \n\t', '\n\t\t 2000. A statistical word-level translation model for comparable corpora . \n\t', '\n\t\t In Proceeding of the Conference on Content-Based Multimedia Information Access ( RIAO ) . \n\t', '\n\t\t Pascale Fung . \n\t', '\n\t\t 2000. A statistical view on bilingual lexicon extraction - from parallel corpora to nonparallel corpora . \n\t', '\n\t\t In J. V´eronis , editor , Parallel Text Processing . \n\t', '\n\t\t Kluwer Academic Publishers . \n\t', '\n\t\t G. Grefenstette . \n\t', '\n\t\t 1994. Explorations in Automatic Thesaurus Construction . \n\t', '\n\t\t Kluwer Academic Publishers . \n\t', '\n\t\t Thomas Hofmann. 1999 . \n\t', '\n\t\t Probabilistic latent semantic analysis . \n\t', '\n\t\t In Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence , pages 289\x96296 . \n\t', '\n\t\t Morgan Kaufmann. Thomas Hofmann. 2000 . \n\t', '\n\t\t Learning the similarity of documents : An information-geometric approach to document retrieval and categorization . \n\t', '\n\t\t In Advances in Neural Information Processing Systems 12 , page 914 . \n\t', '\n\t\t MIT Press . \n\t', '\n\t\t Tommi S. Jaakkola and David Haussler . \n\t', '\n\t\t 1999. Ex- ploiting generative models in discriminative classifiers . \n\t', '\n\t\t In Advances in Neural Information Processing Systems 11 , pages 487\x96493 . \n\t', '\n\t\t Philipp Koehn and Kevin Knight . \n\t', '\n\t\t 2002. Learning a translation lexicon from monolingual corpora . \n\t', '\n\t\t In ACL 2002 Workshop on Unsupervised Lexical Acquisition . \n\t', '\n\t\t P.A.W. Lewis , P.B. Baxendale , and J.L. Bennet . \n\t', '\n\t\t 1967. Statistical discrimination of the synonym/antonym relationship between words . \n\t', '\n\t\t Journal of the ACM . \n\t', '\n\t\t C. Peters and E. Picchi . \n\t', '\n\t\t 1995 . \n\t', '\n\t\t Capturing the comparable : A system for querying comparable text corpora . \n\t', '\n\t\t In JADT\x9295 - 3rd International Conference on Statistical Analysis of Textual Data , pages 255\x96262 . \n\t', '\n\t\t R. Rapp . \n\t', '\n\t\t 1995. Identifying word translations in nonparallel texts . \n\t', '\n\t\t In Proceedings of the Annual Meeting of the Association for Computational Linguistics . \n\t', '\n\t\t I. Shahzad , K. Ohtake , S. Masuyama , and K. Yamamoto . \n\t', '\n\t\t 1999. Identifying translations of compound nouns using non-aligned corpora . \n\t', '\n\t\t In Proceedings of the Workshop MAL\x9299 , pages 108\x96 113 . \n\t', '\n\t\t K. Tanaka and Hideya Iwasaki . \n\t', '\n\t\t 1996. Extraction of lexical translations from non-aligned corpora . \n\t', '\n\t\t In International Conference on Computational Linguistics , COLING\x9296 . \n\t', '\n\t\t Naonori Ueda and Ryohei Nakano . \n\t', '\n\t\t 1995. Deterministic annealing variant of the EM algorithm . \n\t', '\n\t\t In Advances in Neural Information Processing Systems 7 , pages 545\x96552 . \n\t', '\n\t\t A. Vinokourov , J. Shawe-Taylor , and N. Cristianini . \n\t', '\n\t\t 2002. Finding language-independent semantic representation of text using kernel canonical correlation analysis . \n\t', '\n\t\t In Advances in Neural Information Processing Systems 12 . \n\t', '\n\t\t Appendix A : probabilistic interpretation of the extension of standard approach As in section 3 , SQ3 ~~v is an n-dimensional vector , defined over ( ( s1 , tl ) , \x95 \x95 \x95 , ( sp , tk ) ) . \n\t', '\n\t\t The coordinate of SQ3 ~~v on the axis corresponding to the translation pair ( si , tj ) is ( ~~si , ~~v ) ( the one for TQt~~w on the same axis being ( ~~tj , ~~w ) ) . \n\t', '\n\t\t Thus , equation 4 can be rewritten as : which we can normalised in order to get a probability distribution , leading to : S(v , w ) _ E P(v)P(si l v)P(w l tj)P(tj ) ( 3i,tj ) By imposing P(tj) to be uniform , and by denoting C a translation pair , one arrives at : S(v , w ) a E P(v)P(Cl v)P(wl C ) C with the interpretation that only the source , resp . \n\t', '\n\t\t target , word in C is relevant for P(Clv) , resp . \n\t', '\n\t\t P(wl C ) . \n\t', '\n\t\t Now , if we are looking for those ws clos- est to a given v , we rely on : S(wlv) a E C P(Cl v)P(wl C ) which is the probabilistic model adopted in \n\t\t']",Positive
"['\n\t\t This latter model is thus a special case of the extension we propose . \n\t', '\n\t\t Appendix B : update formulas for PLSA The deterministic annealing EM algorithm for PLSA \n\t\t']",Positive
"['\n\t\t P(t+1)(dla) _ S(v , w ) _ E ( ~~si , ~~v)(~~tj,~~w ) ( 3i , tj ) \n\t', '\n\t\t Creating Multilingual Translation Lexicons with Regional Variations Using Web Corpora Pu-Jen Cheng "" , Yi-Cheng Pan "" , Wen-Hsiang Lu+ , and Lee-Feng Chien""^ "" Institute of Information Science , Academia Sinica , Taiwan + Dept. of Computer Science and Information Engineering , National Cheng Kung Univ. , Taiwan ^ Dept. of Information Management , National Taiwan University , Taiwan { pjcheng , thomas02 , whlu , lfchien}@iis.sinica.edu.tw Abstract The purpose of this paper is to automatically create multilingual translation lexicons with regional variations . \n\t', '\n\t\t We propose a transitive translation approach to determine translation variations across languages that have insufficient corpora for translation via the mining of bilingual search-result pages and clues of geographic information obtained from Web search engines . \n\t', '\n\t\t The experimental results have shown the feasibility of the proposed approach in efficiently generating translation equivalents of various terms not covered by general translation dictionaries . \n\t', '\n\t\t It also revealed that the created translation lexicons can reflect different cultural aspects across regions such as Taiwan , Hong Kong and mainland China . \n\t', '\n\t\t 1 Introduction Compilation of translation lexicons is a crucial process for machine translation ( MT ) \n\t\t']",Positive
['\n\t\t A lot of effort has been spent on constructing translation lexicons from domain-specific corpora in an automatic way \n\t\t'],Negative
"['\n\t\t However , such methods encounter two fundamental problems : translation of regional variations and the lack of up-to-date and high-lexical-coverage corpus source , which are worthy of further investigation . \n\t', '\n\t\t The first problem is resulted from the fact that the translations of a term may have variations in different dialectal regions . \n\t', '\n\t\t Translation lexicons constructed with conventional methods may not adapt to regional usages . \n\t', '\n\t\t For example , a Chinese-English lexicon constructed using a Hong Kong corpus cannot be directly adapted to the use in mainland China and Taiwan . \n\t', '\n\t\t An obvious example is that the word \x93taxi\x94 is normally translated into \x93 ^^ \x94 ( Chinese transliteration of taxi ) in Hong Kong , which is completely different from the translated Chinese words of \x93^^^\x94 ( rental cars ) in mainland China and \x93^^^\x94 ( cars with meters ) in Taiwan . \n\t', '\n\t\t Besides , trans- literations of a term are often pronounced differently across regions . \n\t', '\n\t\t For example , the company name \x93Sony\x94 is transliterated into \x93^^\x94 ( xinli ) in Tai- wan and \x93^^\x94 ( suoni ) in mainland China . \n\t', '\n\t\t Such terms , in today\x92s increasingly internationalized world , are appearing more and more often . \n\t', '\n\t\t It is believed that their translations should reflect the cultural aspects across different dialectal regions . \n\t', '\n\t\t Translations without consideration of the regional usages will lead to many serious misunderstandings , especially if the context to the original terms is not available . \n\t', '\n\t\t \n\t\t']",Negative
"['\n\t\t However , previous work on constructing translation lexicons for use in different regions was limited . \n\t', '\n\t\t That might be resulted from the other problem that most of the conventional approaches are based heavily on domain-specific corpora . \n\t', '\n\t\t Such corpora may be insufficient , or unavailable , for certain domains . \n\t', '\n\t\t The Web is becoming the largest data repository in the world . \n\t', '\n\t\t A number of studies have been reported on experiments in the use of the Web to complement insufficient corpora . \n\t', '\n\t\t Most of them \n\t\t']",Negative
"['\n\t\t These methods are feasible but only certain pairs of languages and subject domains can extract sufficient parallel texts as corpora . \n\t', '\n\t\t Different from the previous work , \n\t\t']",Negative
"['\n\t\t This approach is applicable to the compilation of translation lexicons in diverse domains but requires powerful crawlers and high network bandwidth to gather Web data . \n\t', '\n\t\t It is fortunate that the Web contains rich pages in a mixture of two or more languages for some lan- guage pairs such as Asian languages and English . \n\t', ""\n\t\t Many of them contain bilingual translations of terms , including OOV terms , e.g. companies ' , personal and technical names . \n\t"", '\n\t\t In addition , geographic information about Web pages also provides useful clues to the regions where translations appear . \n\t', '\n\t\t We are , therefore , interested in realizing whether these nice characteristics make it possible to automatically construct multilingual translation lexicons with regional variations . \n\t', '\n\t\t Real search engines , such as Google ( http://www.google.com ) and AltaVista ( http://www. altavista.com ) , allow us to search English terms only for pages in a certain language , e.g. Chinese or Japanese . \n\t', '\n\t\t This motivates us to investigate how to construct translation lexicons from bilingual search- result pages ( as the corpus ) , which are normally returned in a long ordered list of snippets of summaries ( including titles and page descriptions ) to help users locate interesting pages . \n\t', '\n\t\t The purpose of this paper is trying to propose a systematic approach to create multilingual translation lexicons with regional variations through mining of bilingual search-result pages . \n\t', '\n\t\t The bilingual pages retrieved by a term in one language are adopted as the corpus for extracting its translations in another language . \n\t', '\n\t\t Three major problems are found and have to be dealt with , including : ( 1 ) ex- tracting translations for unknown terms \x97 how to extract translations with correct lexical boundaries from noisy bilingual search-result pages , and how to estimate term similarity for determining correct translations from the extracted candidates ; ( 2 ) find- ing translations with regional variations \x97 how to find regional translation variations that seldom co- occur in the same Web pages , and how to identify the corresponding languages of the retrieved search- result pages once if the location clues ( e.g. URLs ) in them might not imply the language they are written in ; and ( 3 ) translation with limited corpora \x97 how to translate terms with insufficient search-result pages for particular pairs of languages such as Chinese and Japanese , and simplified Chinese and traditional Chinese . \n\t', '\n\t\t The goal of this paper is to deal with the three problems . \n\t', '\n\t\t Given a term in one language , all possible translations will be extracted from the obtained bilingual search-result pages based on their similarity to the term . \n\t', '\n\t\t For those language pairs with unavailable corpora , a transitive translation model is proposed , by which the source term is translated into the target language through an intermediate language . \n\t', '\n\t\t The transitive translation model is further enhanced by a competitive linking algorithm . \n\t', '\n\t\t The algorithm can effectively alleviate the problem of error propagation in the process of translation , where translation errors may occur due to incorrect identification of the ambiguous terms in the intermediate language . \n\t', '\n\t\t In addi tion , because the search-result pages might contain snippets that do not be really written in the target language , a filtering process is further performed to eliminate the translation variations not of interest . \n\t', '\n\t\t Several experiments have been conducted to examine the performance of the proposed approach . \n\t', '\n\t\t The experimental results have shown that the approach can generate effective translation equivalents of various terms \x97 especially for OOV terms such as proper nouns and technical names , which can be used to enrich general translation dictionaries . \n\t', '\n\t\t The results also revealed that the created translation lexicons can reflect different cultural aspects across regions such as Taiwan , Hong Kong and mainland China . \n\t', '\n\t\t In the rest of this paper , we review related work in translation extraction in Section 2 . \n\t', '\n\t\t We present the transitive model and describe the direct translation process in Sections 3 and 4 , respectively . \n\t', '\n\t\t The conducted experiments and their results are described in Section 5 . \n\t', '\n\t\t Finally , in Section 6 , some concluding remarks are given . \n\t', '\n\t\t 2 Related Work In this section , we review some research in generating translation equivalents for automatic construction of translational lexicons . \n\t', '\n\t\t Transitive translation : Several transitive translation techniques have been developed to deal with the unreliable direct translation problem . \n\t', '\n\t\t \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t In addition , \n\t\t']",Positive
"['\n\t\t Corpus-based translation : To automatically construct translation lexicons , conventional research in MT has generally used statistical techniques to extract translations from domain-specific sentence- aligned parallel bilingual corpora . \n\t', '\n\t\t \n\t\t']",Negative
['\n\t\t \n\t\t'],Negative
['\n\t\t \n\t\t'],Negative
"['\n\t\t Although high accuracy of translation extraction can be easily achieved by these techniques , sufficiently large parallel corpora for ( a ) Taiwan ( Traditional Chinese ) ( b ) Mainland China ( Simplified Chinese ) ( c ) Hong Kong ( Traditional Chinese ) Figure 1 : Examples of the search-result pages in different Chinese regions that were obtained via the English query term \x93George Bush\x94 from Google . \n\t', '\n\t\t various subject domains and language pairs are not always available . \n\t', '\n\t\t Some attention has been devoted to automatic extraction of term translations from comparable or even unrelated texts . \n\t', '\n\t\t Such methods encounter more difficulties due to the lack of parallel correlations aligned between documents or sentence pairs . \n\t', '\n\t\t \n\t\t']",Negative
['\n\t\t \n\t\t'],Negative
['\n\t\t Web-based translation : Collecting parallel texts of different language versions from the Web has recently received much attention \n\t\t'],Positive
['\n\t\t \n\t\t'],Negative
"['\n\t\t They assumed a Web page\x92s parents might contain the links to different versions of it and Web pages with the same content might have similar structures and lengths . \n\t', '\n\t\t \n\t\t']",Negative
['\n\t\t \n\t\t'],Negative
"['\n\t\t These methods often require powerful crawlers to gather sufficient Web data , as well as more network bandwidth and storage . \n\t', '\n\t\t On the other hand , \n\t\t']",Positive
"['\n\t\t 3 Construction of Translation Lexicons To construct translation lexicons with regional variations , we propose a transitive translation model Strans(s,t) to estimate the degree of possibility of the translation of a term s in one ( source ) language ls into a term t in another ( target ) language lt . \n\t', '\n\t\t Given the term s in ls , we first extract a set of terms C={t;} , where t ; in lt acts as a translation candidate of s , from a corpus . \n\t', '\n\t\t In this case , the corpus consists of a set of search-result pages retrieved from search engines using term s as a query . \n\t', '\n\t\t Based on our previous work \n\t\t']",Positive
"['\n\t\t The association measurement is determined by the degree of cohesion holding the words together within a word n- gram , and enhanced by examining if a word n-gram has complete lexical boundaries . \n\t', '\n\t\t Next , we rank the extracted candidates C as a list T in a decreasing order by the model Strans(s,t) as the result . \n\t', '\n\t\t 3.1 Bilingual Search-Result Pages The Web contains rich texts in a mixture of multiple languages and in different regions . \n\t', '\n\t\t For example , Chinese pages on the Web may be written in traditional or simplified Chinese as a principle language and in English as an auxiliary language . \n\t', '\n\t\t According to our observations , translated terms frequently occur together with a term in mixed-language texts . \n\t', '\n\t\t For example , Figure 1 illustrates the search-result pages of the English term \x93George Bush,\x94 which was submitted to Google for searching Chinese pages in different regions . \n\t', '\n\t\t In Figure 1 ( a ) it contains the translations \x93I 1 jI JIr \x94 ( George Bush ) and \x93I J ( Bush ) obtainedfrom the pages in Taiwan . \n\t', '\n\t\t In Figures 1 ( b ) and ( c ) the term \x93George Bush\x94 is translated into \x93I J j- f \x94(busir) or \x93I J &I \x94(buson) in mainland China and \x93 I J~\x94(busu) in Hong Kong . \n\t', '\n\t\t This characteristic of bilingual search-result pages is also useful for other language pairs such as other Asian languages mixed with English . \n\t', '\n\t\t For each term to be translated in one ( source ) language , we first submit it to a search engine for locating the bilingual Web documents containing the term and written in another ( target ) language from a specified region . \n\t', '\n\t\t The returned search-result pages containing snippets ( illustrated in Figure 1 ) , instead of the documents themselves , are collected as a corpus from which translation candidates are extracted and correct translations are then selected . \n\t', '\n\t\t Compared with parallel corpora and anchor texts , bilingual search-result pages are easier to collect and can promptly reflect the dynamic content of the Web. . \n\t', '\n\t\t In addition , geographic information about Web pages such as URLs also provides useful clues to the regions where translations appear . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t ei ; 3.2 The Transitive Translation Model s1 ~~~~ ( Internet ) ^ ^^ ^^ Transitive translation is particularly necessary for the translation of terms with regional variations because the variations seldom co-occur in the same bilingual pages . \n\t', '\n\t\t To estimate the possibility of being the translation t ^ T of term s , the transitive translation model first performs so-called direct translation , which attempts to learn translational equivalents directly from the corpus . \n\t', '\n\t\t The direct translation method is simple , but strongly affected by the quality of the adopted corpus . \n\t', '\n\t\t ( Detailed description of the direct translation method will be given in Section 4 . \n\t', '\n\t\t ) If the term s and its translation t appear infrequently , the statistical information obtained from the corpus might not be reliable . \n\t', '\n\t\t For example , a term in simplified Chinese , e.g. ^^^ ( Internet ) does not usually co-occur together with its variation in traditional Chinese , e.g. ~~~~ ( Internet ) . \n\t', '\n\t\t To deal with this problem , our idea is that the term s can be first translated into an intermediate translation m , which might co-occur with s , via a third ( or intermediate ) language lm . \n\t', '\n\t\t The correct translation t can then be extracted if it can be found as a translation of m . \n\t', '\n\t\t The transitive translation model , therefore , combines the processes of both direct translation and indirect translation , and is defined as : Strans ( s , t ) Sdirec~ , t ) , if Sdirect(s , t ) Sindirect(s , t )=^S~~ect(s , m ) × S~~rect(m , t ) ×v(m) , otherwise ^m where m is one of the top k most probable intermediate translations of s in language lm , and tv is the confidence value of m\x92s accuracy , which can be estimated based on m\x92s probability of occurring in the corpus , and B is a predefined threshold value . \n\t', '\n\t\t 3.3 The Competitive Linking Algorithm One major challenge of the transitive translation model is the propagation of translation errors . \n\t', '\n\t\t That is , incorrect m will significantly reduce the accuracy of the translation of s into t . \n\t', '\n\t\t A typical case is the indirect association problem \n\t\t']",Positive
"['\n\t\t Assume that t1 is s1\x92s corresponding translation , but appears infrequently with s1 . \n\t', '\n\t\t An indirect association error might arise when t2 , the translation of s1\x92s highly relevant term s2 , co-occurs often with s1 . \n\t', '\n\t\t This problem is very important for the situation in which translation is a many-to-many mapping . \n\t', '\n\t\t To reduce such errors and enhance the reliability of the estimation , a competitive linking algorithm , which is extended from Melamed\x92s work s2 ~~ ( Technology ) s3 NIFf-M ( Browser ) s4 WWI ( Computer ) s5 ~p ( Information ) Figure 2 : An illustration of a bipartite graph . \n\t', '\n\t\t The idea of the algorithm is described below . \n\t', '\n\t\t For each translated term t;^ T in lt , we translate it back into original language ls and then model the translation mappings as a bipartite graph , as shown in Figure 2 , where the vertices on one side correspond to the terms { si } or { t ; } in one language . \n\t', '\n\t\t An edge ei ; indicates the corresponding two terms si and t ; might be the translations of each other , and is weighted by the sum of Sdirect(si,t;) and Sdirect(t;,si,) . \n\t', '\n\t\t Based on the weighted values , we can examine if each translated term t;^ T in lt can be correctly translated into the original term s1 . \n\t', '\n\t\t If term t ; has any translations better than term s1 in ls , term t ; might be a so-called indirect association error and should be eliminated from T . \n\t', '\n\t\t In the above example , if the weight of e22 is larger than that of e12 , the term \x93Technology\x94 will be not con- sidered as the translation of \x93~~~~\x94 ( Internet ) . \n\t', '\n\t\t Finally , for all translated terms { t ; } ^ T that are not eliminated , we re-rank them by the weights of the edges { ei ; } and the top k ones are then taken as the translations . \n\t', '\n\t\t More detailed description of the algorithm could be referred to \n\t\t']",Positive
"['\n\t\t 4 Direct Translation In this section , we will describe the details of the direct translation process , i.e. the way to compute Sdirect(s,t) . \n\t', '\n\t\t Three methods will be presented to estimate the similarity between a source term and each of its translation candidates . \n\t', '\n\t\t Moreover , because the search- result pages of the term might contain snippets that do not actually be written in the target language , we will introduce a filtering method to eliminate the translation variations not of interest . \n\t', '\n\t\t 4.1 Translation Extraction The Chi-square Method : A number of statistical measures have been proposed for estimating term association based on co-occurrence analysis , including mutual information , DICE coefficient , chi-square test , and log-likelihood ratio \n\t\t']",Positive
"['\n\t\t Chi- square test ( X2 ) is adopted in our study because the required parameters for it can be obtained by submit- > Technology Internet t1 t2 ting Boolean queries to search engines and utilizing the returned page counts ( number of pages ) . \n\t', '\n\t\t Given a term s and a translation candidate t , suppose the total number of Web pages is N ; the number of pages containing both s and t , n(s,t) , is a ; the number of pages containing s but not t , n(s,\x97t) , is b ; the number of pages containing t but not s , n( \x97s,t ) , is c ; and the number of pages containing neither s nor t , n(\x97s , \x97t ) , is d . \n\t', '\n\t\t ( Although d is not provided by search engines , it can be computed by d=N-a-b-c . \n\t', '\n\t\t ) Assume s and t are independent . \n\t', '\n\t\t Then , the expected frequency of ( s,t ) , E(s,t) , is (a+c)(a+b)/N ; the expected frequency of ( s,\x97t ) , E(s,\x97t) , is (b+d)(a+b)/N ; the expected frequency of ( \x97s,t ) , E(\x97s,t) , is (a+c)(c+d)/N ; and the ex- pected frequency of ( \x97s,\x97t ) , E(\x97s,\x97t) , is (b+d)(c+d)/N . \n\t', '\n\t\t Hence , the conventional chi-square test can be computed as : c2 Sdi,.ect ( s , t ) [ n(X , Y ) ^ E(X , Y ) ] E(X,Y) = ^ ^ X^ { s,¬ s},^Y^ { t,¬t } N×(a×d^b×c) = (a+b)×(a+c)×(b+d)×(c+d) wti=(ti,p)×log(N) ; f(t;,) p max 2 2 Althoughthechi-squaremethodissimpletocom- pute,itismoreapplicabletohigh-frequencytermsthanlow-frequencytermssincetheformeraremorelikelytoappearwiththeircandidates.Moreover,cer- taincandidatesthatfrequentlyco-occurwithtermsmaynotimplythattheyareappropriatetranslations.Thus,anothermethodispresented.TheContext-VectorMethod:Thebasicideaofthismethodisthattheterms\x92stranslationequivalentsmaysharecommoncontextualtermswithsinthesearch-resultpages,similartoRapp(1999).ForbothsanditscandidatesC,wetaketheircontextualtermsconstitutingthesearch-resultpagesastheirfeatures.ThesimilaritybetweensandeachcandidateinCwillbecomputedbasedontheirfeaturevectorsinthevec- tor-spacemodel.Herein,weadopttheconventional tf- idfweightingschemetoestimatethesigni fi cance of features and define it as : n where f(ti,p) is the frequency of term ti insearch-result page p , Nis the total numberofWeb pages , and n is the numberofthe pages containing ti . \n\t', '\n\t\t Finally , the similaritybetween terms and its translation candidate tcanbe estimatedwiththecosine measure , i.e. S c e ct(s,t)=c o s(cvs,cvt),wherecvsandcvtarethecon- textvectors of s andt , respectively . \n\t', '\n\t\t Inthe context-vectormethod , a low-frequency termstillhas achance ofextractingcorrecttransla- tions , ifitshares commoncontexts with its tran sla- tions in the search-result pages . \n\t', '\n\t\t Although the method provides aneffective wayto overcome the chi-square method\x92s problem , its performance dependsheavily on thequality ofthe retrievedsearch-resultpages , such as thesizes andamounts ofsnippets . \n\t', '\n\t\t Also , feature selectionneeds to be carefullyhandledinsome cases . \n\t', '\n\t\t The CombinedMethod : The context-vectorandchi- square methods are basicallycomplementary . \n\t', '\n\t\t Intuitively , a more completesolutionis to integrate the two methods . \n\t', '\n\t\t Consideringthe various ranges ofsimilarity values betweenthe two methods , we compute the similaritybetweenterms andits translationcandidate tbythe weightedsumof 1/Rx2(s,t) and 1/RCV(s,t).Rx2(s,t) ( or RCV(s,t)) represents the similar- ityranking ofeachtranslation candidate t with respect to s andis assignedto be from1 to k(numberofoutput) in decreasing orderofsimilaritymeasure SX2di,.ect(s,t) ( or SCVdi,.ect(s,t)) . \n\t', '\n\t\t Thatis , ifthe similari ty rankings of t are high in both of the context-vector and chi-square methods , it will be also ranked high in the combined method . \n\t', '\n\t\t 4.2 Translation Filtering Thedirecttranslation process assumes thattheretrieved search-resultpages ofatermexactly contain snippets fromacertain region(e.g . \n\t', '\n\t\t HongKong ) and written inthe targetlanguage(e.g . \n\t', '\n\t\t traditional Chinese ) . \n\t', '\n\t\t However , the assumption mightnotbe reliable because thelocation(e.g . \n\t', '\n\t\t URL ) ofaWeb pagemay notimplythatitis written bythe principle language used inthatregion . \n\t', '\n\t\t Also , we cannotidentify the language ofasnippetsimply usingits characterencoding scheme , becausedifferentregions may use the same characterencodingschemes ( e.g. TaiwanandHong Kong mainlyusethe same traditional Chineseencoding scheme ) . \n\t', '\n\t\t From previous work(Tsouetal. , 2004 ) we know thatword entropies significantly reflectlanguage differences inHong Kong , Taiwan andChina . \n\t', '\n\t\t Herein , wepropose anothermethod fordealingwith the above problem . \n\t', '\n\t\t Since ourgoal is trying to eliminate the translationcandidates { t ; } thatare notfrom the snippets inlanguage lt , foreachcandidate t;we merge all ofthesnippets thatcontaint;into adocumentand thenidentifythe correspondinglanguage of t;basedonthe document . \n\t', '\n\t\t We trainauni-gramlanguage model foreachlanguageofconcernandperformlanguage identification basedon a discri mination function , which locates maximum character or word entropy and is defined as : lang(t ;)=argmax^^p(w|l)lnp(w|l),l^L t\x97 N(t;) ^ where N(t;) is the collection ofthe snippets containing t;andL is asetoflanguages to be identified . \n\t', '\n\t\t The candidate t;will be eliminated if lang(t;)^lt . \n\t', '\n\t\t To examine the feasibility of the proposed method in identifying Chinese in Taiwan , mainland China and Hong Kong , we conducted a preliminary experiment . \n\t', '\n\t\t To avoid the data sparseness of using a tri-gram language model , we simply use the above unigram model to perform language identification . \n\t', '\n\t\t Even so , the experimental result has shown that very high identification accuracy can be achieved . \n\t', '\n\t\t Some Web portals contain different versions for specific regions such as Yahoo ! \n\t', '\n\t\t Taiwan ( http://tw.yahoo . \n\t', '\n\t\t com ) and Yahoo ! \n\t', '\n\t\t Hong Kong ( http://hk.yahoo.com ) . \n\t', '\n\t\t This allows us to collect regional training data for constructing language models . \n\t', '\n\t\t In the task of translating English terms into traditional Chinese in Taiwan , the extracted candidates for \x93laser\x94 contained \x93^ ^\x94 ( translation of laser mainly used in Taiwan ) and \x93^^\x94 ( translation of laser mainly used in mainland China ) . \n\t', '\n\t\t Based on the merged snippets , we found that \x93^^\x94 had higher entropy value for the language model of mainland China while \x93^^ \x94 had higher entropy value for the language models of Taiwan and Hong Kong . \n\t', '\n\t\t 5 Performance Evaluation We conducted extensive experiments to examine the performance of the proposed approach . \n\t', '\n\t\t We obtained the search-result pages of a term by submitting it to the real-world search engines , including Google and Openfind ( http://www.openfind.com.tw ) . \n\t', '\n\t\t Only the first 100 snippets received were used as the corpus . \n\t', '\n\t\t Performance Metric : The average top-n inclusion rate was adopted as a metric on the extraction of translation equivalents . \n\t', '\n\t\t For a set of terms to be translated , its top-n inclusion rate was defined as the percentage of the terms whose translations could be found in the first n extracted translations . \n\t', '\n\t\t The experiments were categorized into direct translation and transitive translation . \n\t', '\n\t\t 5.1 Direct Translation Data set : We collected English terms from two real- world Chinese search engine logs in Taiwan , i.e. Dreamer ( http://www.dreamer.com.tw ) and GAIS ( http://gais.cs.ccu.edu.tw ) . \n\t', '\n\t\t These English terms were potential ones in the Chinese logs that needed correct translations . \n\t', '\n\t\t The Dreamer log contained 228,566 unique query terms from a period of over 3 months in 1998 , while the GAIS log contained 114,182 unique query terms from a period of two weeks in 1999 . \n\t', '\n\t\t The collection contained a set of 430 frequent English terms , which were obtained from the 1,230 English terms out of the most popular 9,709 ones ( with frequencies above 10 in both logs ) . \n\t', '\n\t\t About 36 % ( 156/430 ) of the collection could be found in the LDC ( Linguistic Data Consortium , http://www.ldc.upenn. edu/Projects/Chinese ) English-to-Chinese lexicon with 120K entries , while about 64 % ( 274/430 ) were not covered by the lexicon . \n\t', '\n\t\t English-to-Chinese Translation : In this experiment , we tried to directly translate the collected 430 English terms into traditional Chinese . \n\t', '\n\t\t Table 1 shows the results in terms of the top 1-5 inclusion rates for the translation of the collected English terms . \n\t', '\n\t\t \x93^2 \x94 , \x93CV\x94 , and \x93^2+CV\x94 represent the methods based on the chi- square , context-vector , and chi-square plus context- vector methods , respectively . \n\t', '\n\t\t Although either the chi-square or context-vector method was effective , the method based on both of them ( x2+CV ) achieved the best performance in maximizing the inclusion rates in every case because they looked complementary . \n\t', '\n\t\t The proposed approach was found to be effective in finding translations of proper names , e.g. personal names \x93Jordan\x94 ( ^ ^ , ^^ ) , \x93Keanu Reeves\x94 ( ^^^^ , ^^^^ ) , companies\x92 names \x93TOYOTA\x94 ( ^^ ) , \x93EPSON\x94 ( ^^^ ) , and tech- nical terms \x93EDI\x94 ( ^^^^^^ ) , \x93Ethernet\x94 ( ^ ^^^ ) , etc. English-to-Chinese Translation for Mainland China , Taiwan and Hong Kong : Chinese can be classified into simplified Chinese ( SC ) and traditional Chinese ( TC ) based on its writing form or character encoding scheme . \n\t', '\n\t\t SC is mainly used in mainland China while TC is mainly used in Taiwan and Hong Kong ( HK ) . \n\t', '\n\t\t In this experiment , we further investigated the effectiveness of the proposed approach in English-to-Chinese translation for the three different regions . \n\t', '\n\t\t The collected 430 English terms were classified into five types : people , organization , place , computer and network , and others . \n\t', '\n\t\t Tables 2 and 3 show the statistical results and some examples , respectively . \n\t', '\n\t\t In Table 3 , the number stands for a translated term\x92s ranking . \n\t', '\n\t\t The underlined terms were correct translations and the others were relevant translations . \n\t', '\n\t\t These translations might benefit the CLIR tasks , whose performance could be referred to our earlier work which emphasized on translating unknown queries \n\t\t']",Positive
"['\n\t\t The results in Table 2 show that the translations for mainland China and HK were not reliable enough in the top-1 , compared with the translations for Taiwan . \n\t', '\n\t\t One possible reason was that the test terms were collected from Taiwan\x92s search engine logs . \n\t', '\n\t\t Most of them were popular in Taiwan but not in the others . \n\t', '\n\t\t Only 100 snippets retrieved might not balance or be sufficient for translation extraction . \n\t', '\n\t\t However , the inclusion rates for the three regions were close in the top-5 . \n\t', ""\n\t\t Observing the five types , we could find that type place containing the names of well-known countries and cities achieved the best performance in maximizing the inclusion rates in every case and almost had no regional variations ( 9 % , 1/11 ) except Table 1 : Inclusion rates for Web query terms using various similarity measurements Method Dic OOV All Top-1 Top-3 Top-5 Top-1 Top-3 Top-5 Top-1 Top-3 Top-5 X2 42.1 % 57.9 % 62.1 % 40.2 % 53.8 % 56.2 % 41.4 % 56.3 % 59.8 % CV 51.7 % 59.8 % 62.5 % 45.0 % 55.6 % 57.4 % 49.1 % 58.1 % 60.5 % X2+ CV 52.5 % 60.4 % 63.1 % 46.1 % 56.2 % 58.0 % 50.7 % 58.8 % 61.4 % Table 2 : Inclusion rates for different types of Web query terms Type Extracted Translations Taiwan ( Big5 ) Mainland China ( GB ) Hong Kong ( Big5 ) Top-1 Top-3 Top-5 Top-1 Top-3 Top-5 Top-1 Top-3 Top-5 People ( 14 ) 57.1 % 64.3 % 64.3 % 35.7 % 57.1 % 64.3 % 21.4 % 57.1 % 57.1 % Organization ( 147 ) 44.9 % 55.1 % 56.5 % 47.6 % 58.5 % 62.6 % 37.4 % 46.3 % 53.1 % Place ( 11 ) 90.9 % 90.9 % 90.9 % 63.6 % 100.0 % 100.0 % 81.8 % 81.8 % 81.8 % Computer & Network ( 115 ) 55.8 % 59.3 % 63.7 % 32.7 % 59.3 % 64.6 % 42.5 % 65.5 % 68.1 % Others ( 143 ) 49.0 % 58.7 % 62.2 % 30.8 % 49.7 % 58.7 % 28.7 % 50.3 % 60.8 % Total ( 430 ) 50.7 % 58.8 % 61.4 % 38.1 % 56.7 % 62.8 % 36.5 % 54.0 % 60.5 % Table 3 : Examples of extracted correct/relevant translations of English terms in three Chinese regions English Terms Extracted Correct or Relevant Target Translations Taiwan ( Traditional Chinese ) Mainland China ( Simplified Chinese ) Hong Kong ( Traditional Chinese ) Police ~-1 ( 1 ) ~-1V* ( 2 ) ~-1~ ( 4 ) ~-1 ( 1 ) IVY , ( 2 ) ~4 ' ( 4 ) i a ( 1 ) ~-1 ( 3 ) V~r 1J ( 5 ) Taxi ! \n\t"", '\n\t\t . \n\t', '\n\t\t i ( 1 ) *4 ( 3 ) << <)""~ ( 1 ) P°J~ ( 4 ) P°J~ ( 1 ) P°J~ ~~ ( 2 ) 44""~ ( 15 ) Laser ~s ( 1 ) ~st JWJ ( 3 ) jf~X1 ( 4 ) ~t ( 1 ) F11D \' ( 2 ) ~t % ( 3 ) ~s ( 4 ) ~t ( 1 ) ~s ( 2 ) ~tP°J ( 3 ) ( 4 ) , . \n\t', '\n\t\t Hacker i4 (1)""r ( 2 ) r\'JP . \n\t', '\n\t\t ( 7 ) 4 ( 1 ) ( 5 ) [ ffj\'F ( 6 ) i4 (1)4 ( 2 ) Y \' ( 9 ) Database ~~i ( 1 ) F11_*~~i ( 3 ) ~MF* ( 1 ) ~MF*~Y1 ( 9 ) ~~i ( 1 ) { ~i ( 3 ) ~~ ( 5 ) Information ~f.A ( 1 ) ~~ ( 3 ) ~f.A-,M ( 4 ) j 4( ( 1 ) j 4(VXJ ( 3 ) ~4 ; t ( 7 ) ~~ ( 1 ) Vf.A ( 6 ) Internet caf6 -,M""rp~ ( 3 ) -,M""r ( 4 ) -,Mp ( 5 ) VXJ~p~ ( 1 ) VXJ~p~~ ( 2 ) VXJP ° \' ( 6 ) -,MP°\' ( 1 ) ~14 ( 3 ) -,M : r ( 4 ) Search Engine ~~ % ( 2 ) ~~`J [ IV ( 5 ) ~~`J [ IV r ( 1 ) ~~`J [ IV ( 3 ) ~~ % ( 1 ) ~~ % ( 8 ) Digital Camera ffI~ ( 1 ) { f ffI~ ( 2 ) ~V ffIfr \' ( 1 ) ~V Wj~ ( 6 ) f~* ( 1 ) { ~ffI~ ( 2 ) ffIM ( 3 ) Table 4 : Inclusion rates of transitive translations of proper names and technical terms Type Source Target Intermediate Language Top-1 Top-3 Top5 Language Language Scientist Name Chinese English None 70.0 % 84.0 % 86.0 % English Japanese None 32.0 % 56.0 % 64.0 % English Korean None 34.0 % 58.0 % 68.0 % Chinese Japanese English 26.0 % 40.0 % 48.0 % Chinese Korean English 30.0 % 42.0 % 50.0 % Disease Name Chinese English None 50.0 % 74.0 % 74.0 % English Japanese None 38.0 % 48.0 % 62.0 % English Korean None 30.0 % 50.0 % 58.0 % Chinese Japanese English 32.0 % 44.0 % 50.0 % Chinese Korean English 24.0 % 38.0 % 44.0 % that the city \x93Sydney\x94 was translated into ; , FU ( Syd ney ) in SC for mainland China and HK and ~~ ( Sydney ) in TC for Taiwan . \n\t', '\n\t\t Type computer and network containing technical terms had the most regional variations ( 41 % , 47/115 ) and type people had 36 % ( 5/14 ) . \n\t', '\n\t\t In general , the translations in the two types were adapted to the use in different regions . \n\t', '\n\t\t On the other hand , 10 % ( 15/147 ) and 8 % ( 12/143 ) of the translations in types organization and others , respec- tively , had regional variations , because most of the terms in type others were general terms such as \x93bank\x94 and \x93movies\x94 and in type organization many local companies in Taiwan had no translation variations in mainland China and HK . \n\t', ""\n\t\t Moreover , many translations in the types of people , organization , and computer and network were quite different in Taiwan and mainland China such as the personal name \x93Bred Pitt\x94 was translated into \x93^~I\x94 in SC and \x93If J'-k'- hV~I\x94 in TC , the com- pany name \x93Ericsson\x94 into \x93u~j \x94 in SC and \x93V1 ~ j \x94 in TC , and the computer-related term \x93EDI\x94 into \x93~~{~q~\x94 in SC and \x93~~~~~~\x94 in TC . \n\t"", '\n\t\t In general , the translations in HK had a higher chance to cover both of the translations in mainland China and Taiwan . \n\t', '\n\t\t 5.2 Multilingual & Transitive Translation Data set : Since technical terms had the most region variations among the five types as mentioned in the previous subsection , we collected two other data sets for examining the performance of the proposed approach in multilingual and transitive translation . \n\t', '\n\t\t The data sets contained 50 scientists\x92 names and 50 dis- ease names in English , which were randomly selected from 256 scientists ( Science/People ) and 664 diseases ( Health/Diseases ) in the Yahoo ! \n\t', '\n\t\t Directory ( http://www.yahoo.com ) , respectively . \n\t', '\n\t\t English-to-Japanese/Korean Translation : In this experiment , the collected scientists\x92 and disease names in English were translated into Japanese and Korean to examine if the proposed approach could be applicable to other Asian languages . \n\t', '\n\t\t As the result in Table 4 shows , for the English-to-Japanese translation , the top-1 , top-3 , and top-5 inclusion rates were 35 % , 52 % , and 63 % , respectively ; for the English-to-Korean translation , the top-1 , top-3 , and top- 5 inclusion rates were 32 % , 54 % , and 63 % , respectively , on average . \n\t', '\n\t\t Chinese-to-Japanese/Korean Translation via English : To further investigate if the proposed transitive approach can be applicable to other language pairs that are not frequently mixed in documents such as Chinese and Japanese ( or Korean ) , we did transitive translation via English . \n\t', '\n\t\t In this experiment , we first manually translated the collected data sets in English into traditional Chinese and then did the Chinese-to-Japanese/Korean translation via the third language English . \n\t', '\n\t\t The results in Table 4 show that the propagation of translation errors reduced the translation accuracy . \n\t', '\n\t\t For example , the inclusion rates of the Chinese-toJapanese translation were lower than those of the English-to-Japanese translation since only 70%-86 % inclusion rates were reached in the Chinese-toEnglish translation in the top 1-5 . \n\t', '\n\t\t Although transitive translation might produce more noisy translations , it still produced acceptable translation candidates for human verification . \n\t', '\n\t\t In Table 4 , 45%- 50 % of the extracted top 5 Japanese or Korean terms might have correct translations . \n\t', '\n\t\t 6 Conclusion It is important that the translation of a term can be automatically adapted to its usage in different dialectal regions . \n\t', '\n\t\t We have proposed a Web-based translation approach that takes into account limited bilingual search-result pages from real search engines as comparable corpora . \n\t', '\n\t\t The experimental results have shown the feasibility of the automatic approach in generation of effective translation equivalents of various terms and construction of multilingual translation lexicons that reflect regional translation variations . \n\t', '\n\t\t References L. Borin . \n\t', '\n\t\t 2000. You\x92ll take the high road and I\x92ll take the low road : using a third language to improve bilingual word alignment . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of COLING-2000 , pp. 97-103 . \n\t', '\n\t\t P. F. Brown , J. Cocke , S. A. D. Pietra , V. J. D. Pietra , F. Jelinek , J. D. Lafferty , R. L. Mercer , and P. S. Roossin . \n\t', '\n\t\t 1990. A statistical approach to machine translation . \n\t', '\n\t\t Computational Linguistics , 16(2):79-85 . \n\t', '\n\t\t Y.-B. Cao and H. Li . \n\t', '\n\t\t 2002. Base noun phrase translation using Web data the EM algorithm . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of COLING-2002 , pp. 127-133 . \n\t', '\n\t\t P.-J. Cheng , J.-W. Teng , R.-C. Chen , J.-H. Wang , W.-H. Lu , and L.-F. Chien . \n\t', '\n\t\t 2004 . \n\t', '\n\t\t Translating unknown queries with Web corpora for cross-language information retrieval . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of ACM SIGIR-2004 . \n\t', '\n\t\t P. Fung and L. Y. Yee . \n\t', '\n\t\t 1998. An IR approach for translating new words from nonparallel , comparable texts . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of ACL-98 , pp. 414-420 . \n\t', '\n\t\t T. Gollins and M. Sanderson . \n\t', '\n\t\t 2001. Improving cross language information with triangulated translation . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of ACM SIGIR-2001 , pp. 90-95 . \n\t', '\n\t\t J. Halpern . \n\t', '\n\t\t 2000. Lexicon-based orthographic disambiguation in CJK intelligent information retrieval . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of Workshop on Asian Language Resources and International Standardization . \n\t', '\n\t\t A. Kilgarriff and G. Grefenstette . \n\t', '\n\t\t 2003. Introduction to the special issue on the web as corpus . \n\t', '\n\t\t Computational Linguistics 29(3) : 333-348 . \n\t', '\n\t\t J. M. Kupiec . \n\t', '\n\t\t 1993. An algorithm for finding noun phrase correspondences in bilingual corpora . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of ACL93 , pp. 17-22 . \n\t', '\n\t\t W.-H. Lu , L.-F. Chien , and H.-J. Lee . \n\t', '\n\t\t 2004. Anchor text mining for translation of web queries : a transitive translation Approach . \n\t', '\n\t\t ACM TOIS 22(2) : 242-269 . \n\t', '\n\t\t W.-H. Lu , L.-F. Chien , and H.-J. Lee . \n\t', '\n\t\t 2002. Translation of Web queries using anchor text mining . \n\t', '\n\t\t ACM TALIP : 159-172 . \n\t', '\n\t\t I. D. Melamed . \n\t', '\n\t\t 2000. Models of translational equivalence among words . \n\t', '\n\t\t Computational Linguistics , 26(2) : 221- 249 . \n\t', '\n\t\t J.-Y. Nie , P. Isabelle , M. Simard , and R. Durand . \n\t', '\n\t\t 1999. Cross-language information retrieval based on parallel texts and automatic mining of parallel texts from the Web. . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of ACM SIGIR-99 , pp. 74-81 . \n\t', '\n\t\t R. Rapp . \n\t', '\n\t\t 1999. Automatic identification of word translations from unrelated English and German corpora , In Proc . \n\t', '\n\t\t of ACL-99 , pp. 519-526 . \n\t', '\n\t\t P. Resnik . \n\t', '\n\t\t 1999. Mining the Web for bilingual text . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of ACL-99 , pp. 527-534 . \n\t', '\n\t\t M. Simard . \n\t', '\n\t\t 2000 . \n\t', '\n\t\t Multilingual Text Alignment . \n\t', '\n\t\t In \x93Parallel Text Processing\x94 , J. Veronis , ed. , pages 49-67 , Kluwer Academic Publishers , Netherlands . \n\t', '\n\t\t F. Smadja , K. McKeown , and V. Hatzivassiloglou . \n\t', '\n\t\t 1996 . \n\t', '\n\t\t Translating collocations for bilingual lexicons : a statistical approach . \n\t', '\n\t\t Computational Linguistics , 22(1) : 1-38 . \n\t', '\n\t\t B. K. Tsou , T. B. Y. Lai , and K. Chow . \n\t', '\n\t\t 2004. Comparing entropies within the Chinese language . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of IJCNLP-2004 . \n\t', '\n\t\t C. C. Yang and K.-W. Li . \n\t', '\n\t\t 2003. Automatic construction of English/Chinese parallel corpora . \n\t', '\n\t\t JASIST 54(8) : 730-742 . \n\t', '\n\t\t Probabilistic Parsing Strategies Mark-Jan Nederhof Faculty of Arts University of Groningen P.O. Box 716 NL-9700 AS Groningen The Netherlands markjan@let.rug.nl Giorgio Satta Dept. of Information Engineering University of Padua via Gradenigo , 6/A I-35131 Padova Italy satta@dei.unipd.it Abstract We present new results on the relation between context-free parsing strategies and their probabilistic counter-parts . \n\t', '\n\t\t We provide a necessary condition and a sufficient condition for the probabilistic extension of parsing strategies . \n\t', '\n\t\t These results generalize existing results in the literature that were obtained by considering parsing strategies in isolation . \n\t', '\n\t\t 1 Introduction Context-free grammars ( CFGs ) are standardly used in computational linguistics as formal models of the syntax of natural language , associating sentences with all their possible derivations . \n\t', '\n\t\t Other computational models with the same generative capacity as CFGs are also adopted , as for instance push-down automata ( PDAs ) . \n\t', '\n\t\t One of the advantages of the use of PDAs is that these devices provide an operational specification that determines which steps must be performed when parsing an input string , something that is not offered by CFGs . \n\t', '\n\t\t In other words , PDAs can be associated to parsing strategies for context- free languages . \n\t', '\n\t\t More precisely , parsing strategies are traditionally specified as constructions that map CFGs to language-equivalent PDAs . \n\t', '\n\t\t Popular examples of parsing strategies are the standard constructions of top-down PDAs \n\t\t']",Positive
"['\n\t\t CFGs and PDAs have probabilistic counterparts , called probabilistic CFGs ( PCFGs ) and probabilistic PDAs ( PPDAs ) . \n\t', '\n\t\t These models are very popular in natural language processing applications , where they are used to define a probability distribution function on the domain of all derivations for sentences in the language of interest . \n\t', '\n\t\t In PCFGs and PPDAs , probabilities are assigned to rules or transitions , respectively . \n\t', '\n\t\t However , these probabilities cannot be chosen entirely arbitrarily . \n\t', '\n\t\t For example , for a given nonterminal A in a PCFG , the sum of the probabilities of all rules rewriting A must be 1 . \n\t', '\n\t\t This means that , out of a total of say m rules rewriting A , only m \x97 1 rules represent \x93free\x94 parameters . \n\t', '\n\t\t Depending on the choice of the parsing strategy , the constructed PDA may allow different probability distributions than the underlying CFG , since the set of free parameters may differ between the CFG and the PDA , both quantitatively and qualitatively . \n\t', '\n\t\t For example , \n\t\t']",Positive
['\n\t\t Also the results from \n\t\t'],Positive
"['\n\t\t The question arises of whether parsing strategies can be extended probabilistically , i.e. , whether a given construction of PDAs from CFGs can be \x93augmented\x94 with a function defining the probabilities for the target PDA , given the probabilities associated with the input CFG , in such a way that the obtained probabilistic distributions on the CFG derivations and the corresponding PDA computations are equivalent . \n\t', '\n\t\t Some first results on this issue have been presented by \n\t\t']",Positive
"['\n\t\t One might think that any \x93practical\x94 parsing strategy can be probabilistically extended , but this turns out not to be the case . \n\t', '\n\t\t We briefly discuss here a counter-example , in order to motivate the approach we have taken in this paper . \n\t', '\n\t\t Probabilistic LR parsing has been investigated in the literature \n\t\t']",Negative
"['\n\t\t However , this is not the case in general . \n\t', '\n\t\t rule/probability pairs : Consider a PCFG with 5 AB , 1 B bC , 32 A aC , 3 1 B bD , 31 A aD , 23 C xc , 1 D xd , 1 There are two key transitions in the associated LR automaton , which represent shift actions over c and d ( we denote LR states by their sets of kernel items and encode these states into stack symbols ) : Tc:{C x\x95c,D x\x95d}c ~ { C x\x95c,D x\x95d}{C xc\x95 } Td : { C x\x95c,D x\x95d}d ~ { C x\x95c,D x\x95d}{D xd\x95 } Assume a proper assignment of probabilities to the transitions of the LR automaton , i.e. , the sum of transition probabilities for a given LR state is 1 . \n\t', '\n\t\t It can be easily seen that we must assign probability 1 to all transitions except Tc and Td , since this is the only pair of distinct transitions that can be applied for one and the same top-of-stack symbol , viz . \n\t', '\n\t\t { C x \x95 c , D x \x95 d } . \n\t', '\n\t\t However , in the PCFG model we have whereas in the LR PPDA model we have Pr(Tc)\x95Pr(Td) 1 1 Pr(Td)\x95Pr(T = c ) T_ 4 . \n\t', '\n\t\t Thus we conclude that there is no proper assignment of probabilities to the transitions of the LR automaton that would result in a distribution on the generated language that is equivalent to the one induced by the source PCFG . \n\t', '\n\t\t Therefore the LR strategy does not allow probabilistic extension . \n\t', '\n\t\t One may seemingly solve this problem by dropping the constraint of properness , letting each transition that outputs a rule have the same probability as that rule in the PCFG , and letting other transitions have probability 1 . \n\t', '\n\t\t However , the properness condition for PDAs has been heavily exploited in parsing applications , in doing incremental left-to-right probability computation for beam search \n\t\t']",Positive
"['\n\t\t Furthermore , commonly used training algorithms for PCFGS/PPDAs always produce proper probability assignments , and many desired mathematical properties of these methods are based on such an assumption ( Chi and Geman , 1998 ; S´anchez and Benedi , 1997 ) . \n\t', '\n\t\t We may therefore discard non-proper probability assignments in the current study . \n\t', '\n\t\t However , such probability assignments are outside the reach of the usual training algorithms for PDAs , which always produce proper PDAs . \n\t', '\n\t\t Therefore , we may discard such assignments in the current study , which investigates aspects of the potential of training algorithms for CFGs and PDAs . \n\t', '\n\t\t What has been lacking in the literature is a theoretical framework to relate the parameter space of a CFG to that of a PDA constructed from the CFG by a particular parsing strategy , in terms of the set of allowable probability distributions over derivations . \n\t', '\n\t\t Note that the number of free parameters alone is not a satisfactory characterization of the parameter space . \n\t', '\n\t\t In fact , if the \x93nature\x94 of the parameters is ill-chosen , then an increase in the number of parameters may lead to a deterioration of the accuracy of the model , due to sparseness of data . \n\t', '\n\t\t In this paper we extend previous results , where only a few specific parsing strategies were considered in isolation , and provide some general characterization of parsing strategies that can be probabilistically extended . \n\t', '\n\t\t Our main contribution can be stated as follows . \n\t', '\n\t\t \x95 We define a theoretical framework to relate the parameter space defined by a CFG and that defined by a PDA constructed from the CFG by a particular parsing strategy . \n\t', '\n\t\t \x95 We provide a necessary condition and a sufficient condition for the probabilistic extension of parsing strategies . \n\t', '\n\t\t We use the above findings to establish new results about probabilistic extensions of parsing strategies that are used in standard practice in computational linguistics , as well as to provide simpler proofs of already known results . \n\t', '\n\t\t We introduce our framework in Section 3 and report our main results in Sections 4 and 5 . \n\t', '\n\t\t We discuss applications of our results in Section 6 . \n\t', '\n\t\t 2 Preliminaries In this paper we assume some familiarity with definitions of (P)CFGs and (P)PDAs . \n\t', '\n\t\t We refer the reader to standard textbooks and publications as for instance \n\t\t']",Positive
"['\n\t\t A CFG G is a tuple ( E , N , 5 , R ) , with E and N the sets of terminals and nonterminals , respectively , 5 the start symbol and R the set of rules . \n\t', '\n\t\t In this paper we only consider left-most derivations , represented as strings d E R* and simply called deriva- Pr(axcbxd) Pr(axdbxc) = Pr(A\x97>aC)\x95Pr(B\x97>bD) Pr(A\x97>aD)\x95Pr(B\x97>bC) = 1 1 \x95 3 3 2 = 2 \x95 1 4 3 3 Pr(axcbxd) Pr(axdbxc) = tions . \n\t', '\n\t\t For a , ^ E ( E U N)* , we write a -d ^ with the usual meaning . \n\t', '\n\t\t If a = S and ^ = w E E* , we call d a complete derivation of w . \n\t', '\n\t\t We say a CFG is reduced if each rule in R occurs in some complete derivation . \n\t', '\n\t\t A PCFG is a pair ( !9 , p ) consisting of a CFG !9 and a probability function p from R to real numbers in the interval [ 0 , 1 ] . \n\t', '\n\t\t A PCFG is proper if E^=(A-->a)ER p(7r) = 1 for each A E N. The probability of a ( left-most ) derivation d = 7r1 · · · 7rryn , 7ri E R for 1 G i G m , is p(d) = Hryni=1 p(7ri) . \n\t', '\n\t\t The probability of a string w E E* is p(w) = ES~dw p(d) . \n\t', '\n\t\t A PCFG is consistent if EwE^* p(w) = 1 . \n\t', '\n\t\t A PCFG ( !9 , p ) is reduced if !9 is reduced . \n\t', '\n\t\t In this paper we will mainly consider push-down transducers rather than push-down automata . \n\t', '\n\t\t Pushdown transducers not only compute derivations of the grammar while processing an input string , but they also explicitly produce output strings from which these derivations can be obtained . \n\t', '\n\t\t We use transducers for two reasons . \n\t', '\n\t\t First , constraints on the output strings allow us to restrict our attention to \x93reasonable\x94 parsing strategies . \n\t', '\n\t\t Those strategies that cannot be formalized within these constraints are unlikely to be of practical interest . \n\t', '\n\t\t Secondly , mappings from input strings to derivations , such as those realized by push-down transducers , turn out to be a very powerful abstraction and allow direct proofs of several general results . \n\t', '\n\t\t Contrary to many textbooks , our push-down devices do not possess states next to stack symbols . \n\t', '\n\t\t This is without loss of generality , since states can be encoded into the stack symbols , given the types of transitions that we allow . \n\t', '\n\t\t Thus , a PDT A is a 6-tuple ( El , E^ , Q , Xin , Xfin , ^ ) , with El and E^ the input and output alphabets , respectively , Q the set of stack symbols , including the initial and final stack symbols Xin and Xfin , respectively , and ^the set of transitions . \n\t', '\n\t\t Each transition has one of the following three forms : X ~^ XY , called a push transition , YX ~^ Z , called a pop transition , or X H Y , called a swap transition ; here X , Y , Z E Q , x E El U { ^ } is the input read by the transition and y E E*^ is the written output . \n\t', '\n\t\t Note that in our notation , stacks grow from left to right , i.e. , the top-most stack symbol will be found at the right end . \n\t', '\n\t\t A configuration of a PDT is a triple ( a , w , v ) , where a E Q* is a stack , w E E*l is the remaining input , and v E E*^ is the output generated so far . \n\t', '\n\t\t Computations are represented as strings c E ^* . \n\t', ""\n\t\t For configurations ( a , w , v ) and ( ^ , w ' , v ' ) , we write ( a , w , v ) ~-c ( ^ , w ' , v ' ) with the usual meaning , and write ( a , w , v ) ~-* ( ^ , w ' , v ' ) when c is of no importance . \n\t"", '\n\t\t If ( Xin , w , ^ ) ~-c ( Xfin , ^ , v ) , then c is a complete computation of w , and the output string v is denoted out(c) . \n\t', '\n\t\t A PDT is reduced if each transition in ^ occurs in some complete computation . \n\t', '\n\t\t Without loss of generality , we assume that combinations of different types of transitions are not allowed for a given stack symbol . \n\t', '\n\t\t More precisely , for each stack symbol X =~ Xfin , the PDA can only take transitions of a single type ( push , pop or swap ) . \n\t', '\n\t\t A PDT can easily be brought in this form by introducing for each X three new stack symbols Xpush , Xpop and Xswap and new swap transitions X ^,^ ~^ Xpush , X ^,^ ~^ Xpop and X ^,^ ~^ Xswap . \n\t', '\n\t\t In each existing transition that operates on top-of-stack X , we then replace X by one from Xpush , Xpop or X swap , depending on the type of that transition . \n\t', '\n\t\t We also assume that Xfin does not occur in the left- hand side of a transition , again without loss of generality . \n\t', '\n\t\t A PPDT is a pair ( A , p ) consisting of a PDT A and a probability function p from ^ to real numbers in the interval [ 0 , 1 ] . \n\t', ""\n\t\t A PPDT is proper if \x95 E^=(X -->XY)E^ p(T) = 1 for each X E Q such that there is at least one transition X ~^ XY,YEQ ; \x95 E^=(X x,_1,'Y)E^ p(T) = 1 for each X E Q such that there is at least one transition X H Y , xEElU{^},yEE*^,YEQ;and \x95 E^=(YX -->Z)E ^ p(T) = 1 , for each X , Y E Q such that there is at least one transition YX ~^ Z,ZEQ . \n\t"", '\n\t\t The probability of a computation c = T1 · · · Tryn , Ti E ^ for 1 G i G m , is p(c) = Hz ` 1 p(Ti) . \n\t', '\n\t\t The probability of a string w is p(w) E(Xin,w,^)-c(Xfin,^,v) p(c) . \n\t', '\n\t\t A PPDT is consistent if EwE^* p(w) = 1 . \n\t', '\n\t\t A PPDT ( A , p ) is reduced if A is reduced . \n\t', '\n\t\t 3 Parsing Strategies The term \x93parsing strategy\x94 is often used informally to refer to a class of parsing algorithms that behave similarly in some way . \n\t', '\n\t\t In this paper , we assign a formal meaning to this term , relying on the observation by \n\t\t']",Positive
"['\n\t\t The first is a construction of push-down devices from CFGs , and the second is a method for handling nondeterminism ( e.g. backtracking or dynamic programming ) . \n\t', '\n\t\t Parsing algorithms that handle nondeterminism in different ways but apply the same construction of push-down devices from CFGs are seen as realizations of the same parsing strategy . \n\t', '\n\t\t Thus , we define a parsing strategy to be a function S that maps a reduced CFG 9 = ( El , N , S , R ) to a pair S(9) = ( A , f ) consisting of a reduced PDT A = ( E1 , E2 , Q , Xin , Xfin , ^ ) , and a function f that maps a subset of E^2 to a subset of R^ , with the following properties : \x95 R C E2 . \n\t', '\n\t\t \x95 For each string w E E^1 and each complete computation c on w , f ( out ( c ) ) = d is a ( leftmost ) derivation of w . \n\t', '\n\t\t Furthermore , each symbol from R occurs as often in out(c) as it occurs in d. \x95 Conversely , for each string w E E^1 and each derivation d of w , there is precisely one complete computation c on w such that f ( out ( c ) ) = d . \n\t', '\n\t\t If c is a complete computation , we will write f ( c ) to denote f ( out ( c ) ) . \n\t', '\n\t\t The conditions above then imply that f is a bijection from complete computations to complete derivations . \n\t', '\n\t\t Note that output strings of ( complete ) computations may contain symbols that are not in R , and the symbols that are in R may occur in a different order in v than in f ( v ) = d . \n\t', '\n\t\t The purpose of the symbols in E2 ^ R is to help this process of reordering of symbols from R in v , as needed for instance in the case of the left-corner parsing strategy ( see ( Nijholt , 1980 , pp. 22\x9623 ) for discussion ) . \n\t', '\n\t\t A probabilistic parsing strategy is defined to be a function S that maps a reduced , proper and consistent PCFG ( 9 , pg ) to a triple S(9 , pg ) = ( A , pA , f ) , where ( A , pA ) is a reduced , proper and consistent PPDT , with the same properties as a ( non-probabilistic ) parsing strategy , and in addition : \x95 For each complete derivation d and each complete computation c such that f ( c ) = d , pg ( d ) equals pA(c) . \n\t', '\n\t\t In other words , a complete computation has the same probability as the complete derivation that it is mapped to by function f . \n\t', '\n\t\t An implication of this property is that for each string w E E^1 , the probabilities assigned to that string by ( 9 , pg ) and ( A , pA ) are equal . \n\t', ""\n\t\t We say that probabilistic parsing strategy S ' is an extension of parsing strategy S if for each reduced CFG 9 and probability function pg we have S(9) = ( A , f ) if and only if S'(9 , pg ) = ( A , pA , f ) for some pA . \n\t"", '\n\t\t 4 Correct-Prefix Property In this section we present a necessary condition for the probabilistic extension of a parsing strategy . \n\t', '\n\t\t For a given PDT , we say a computation c is dead if ( Xin , w1 , ^ ) ~-c ( a , ^ , v1 ) , for some a E Q^ , w1 E E^1 and v1 E E^2 , and there are no w2 E E^1 and v2 E E^2 such that ( a , w2 , ^ ) ~^ ( Xfin , ^ , v2 ) . \n\t', '\n\t\t Informally , a dead computation is a computation that cannot be continued to become a complete computation . \n\t', '\n\t\t We say that a PDT has the correct-prefix property ( CPP ) if it does not allow any dead computations . \n\t', '\n\t\t We also say that a parsing strategy has the CPP if it maps each reduced CFG to a PDT that has the CPP . \n\t', '\n\t\t Lemma 1 For each reduced CFG 9 , there is a probability function pg such that PCFG ( 9 , pg ) is proper and consistent , and pg ( d ) > 0 for all complete derivations d . \n\t', '\n\t\t Proof . \n\t', '\n\t\t Since 9 is reduced , there is a finite set D consisting of complete derivations d , such that for each rule 7r in 9 there is at least one d E D in which 7r occurs . \n\t', '\n\t\t Let n,r,d be the number of occurrences of rule 7r in derivation d E D , and let n,r be EdED n,r,d , the total number of occurrences of 7r in D . \n\t', '\n\t\t Let nA be the sum of n,r for all rules 7r with A in the left-hand side . \n\t', '\n\t\t A probability function pg can be defined through \x93maximum-likelihood estimation\x94 such that pg ( 7r ) = nA for each rule 7r = A ^ a . \n\t', '\n\t\t For all nonterminals A , E,r=A^a pg ( 7r ) = nA = nA = 1 , which means that the E,r=A^a PCFG ( 9,pg ) is proper . \n\t', '\n\t\t Furthermore , it has been shown in ( Chi and Geman , 1998 ; S´anchez and Benedi , 1997 ) that a PCFG ( 9,pg ) is consistent if pg was obtained by maximum-likelihood estimation using a set of derivations . \n\t', '\n\t\t Finally , since n,r > 0 for each 7r , also pg ( 7r ) > 0 for each 7r , and pg ( d ) > 0 for all complete derivations d . \n\t', '\n\t\t We say a computation is a shortest dead computation if it is dead and none of its proper prefixes is dead . \n\t', '\n\t\t Note that each dead computation has a unique prefix that is a shortest dead computation . \n\t', '\n\t\t For a PDT A , let TA be the union of the set of all complete computations and the set of all shortest dead computations . \n\t', '\n\t\t Lemma 2 For each proper PPDT ( A , pA ) , EIETA pA(c) G 1 . \n\t', '\n\t\t Proof . \n\t', '\n\t\t The proof is a trivial variant of the proof that for a proper PCFG ( 9 , pg ) , the sum of pg ( d ) for all derivations d cannot exceed 1 , which is shown by \n\t\t']",Positive
"['\n\t\t From this , the main result of this section follows . \n\t', '\n\t\t Theorem 3 A parsing strategy that lacks the CPP cannot be extended to become a probabilistic parsing strategy . \n\t', '\n\t\t Proof . \n\t', '\n\t\t Take a parsing strategy S that does not have the CPP . \n\t', '\n\t\t Then there is a reduced CFG 9 = ( El , N , S , R ) , with S(9) = ( A , f ) for some A and f , and a shortest dead computation c allowed by A . \n\t', '\n\t\t It follows from Lemma 1 that there is a probability function pg such that ( 9 , pg ) is a proper and consistent PCFG and pg ( d ) > 0 for all complete derivations d . \n\t', ""\n\t\t Assume we also have a probability function pA such that ( A , pA ) is a proper and consistent PPDT and pA ( c ' ) = pg ( f ( c ' ) ) for each complete computation c ' . \n\t"", ""\n\t\t Since A is reduced , each transition T must occur in some complete computation c ' . \n\t"", ""\n\t\t Furthermore , for each complete computation c ' there is a complete derivation d such that f ( c ' ) = d , and pA ( c ' ) = pg ( d ) > 0 . \n\t"", '\n\t\t Therefore , pA ( T ) > 0 for each transition T , and pA(c) > 0 , where c is the above-mentioned dead computation . \n\t', ""\n\t\t Due to Lemma 2 , 1 > Ec,ETA pA(c') > EwEE*^ pA(w) + pA(c) > EwEE*^ pA(w) = EwEE*^ pg ( w ) . \n\t"", '\n\t\t This is in contradiction with the consistency of ( 9 , pg ) . \n\t', '\n\t\t Hence , a probability function pA with the properties we required above cannot exist , and therefore S cannot be extended to become a probabilistic parsing strategy . \n\t', '\n\t\t 5 Strong Predictiveness In this section we present our main result , which is a sufficient condition allowing the probabilistic extension of a parsing strategy . \n\t', '\n\t\t We start with a technical result that was proven in \n\t\t']",Positive
"[""\n\t\t Lemma 4 Given a non-proper PCFG ( 9 , pg ) , 9 = ( E , N , S , R ) , there is a probabilityfunction p'g such that PCFG ( 9 , p'g ) is proper and , for every com- plete derivation d , p ' g(d) = 1C · pg ( d ) , where C = PS-d,w,wEE* pg ( d ' ) . \n\t"", ""\n\t\t Note that if PCFG ( 9,pg ) in the above lemma is consistent , then C = 1 and ( 9 , p'g ) and ( 9 , pg ) define the same distribution on derivations . \n\t"", '\n\t\t The normalization procedure underlying Lemma 4 makes use of quantities PA-dw,wEE* pg ( d ) for each A E N. These quantities can be computed to any degree of precision , as discussed for instance in \n\t\t']",Positive
"['\n\t\t Thus normalization of a PCFG can be effectively computed . \n\t', ""\n\t\t For a fixed PDT , we define the binary relation --* on stack symbols by : Y --* Y ' if and only if ( Y , w , ^ ) ~^ ( Y ' , ^ , v ) for some w E E^l and v E E^^ . \n\t"", ""\n\t\t In words , some subcomputation of the PDT may start with stack Y and end with stack Y ' . \n\t"", '\n\t\t Note that all stacks that occur in such a subcomputation must have height of 1 or more . \n\t', '\n\t\t We say that a (P)PDA or a (P)PDT has the strong predictiveness property ( SPP ) if the existence of three transitions X ~\x97* XY , X Y1 ~\x97* Z1 and X Y2 ~\x97* Z2 such that Y Y1 and Y Y2 implies Z1 = Z2 . \n\t', '\n\t\t Informally , this means that when a subcomputation starts with some stack a and some push transition T , then solely on the basis of T we can uniquely determine what stack symbol Z1 = Z2 will be on top of the stack in the firstly reached configuration with stack height equal to lal . \n\t', '\n\t\t Another way of looking at it is that no information may flow from higher stack elements to lower stack elements that was not already predicted before these higher stack elements came into being , hence the term \x93strong predictiveness\x94 . \n\t', '\n\t\t We say that a parsing strategy has the SPP if it maps each reduced CFG to a PDT with the SPP . \n\t', '\n\t\t Theorem 5 Any parsing strategy that has the CPP and the SPP can be extended to become a probabilistic parsing strategy . \n\t', '\n\t\t Proof . \n\t', '\n\t\t Consider a parsing strategy S that has the CPP and the SPP , and a proper , consistent and reduced PCFG ( 9 , pg ) , 9 = ( El , N , S , R ) . \n\t', '\n\t\t Let S(9) = ( A , f ) , A = ( El , E^ , Q , Xin , Xfin , ^ ) . \n\t', '\n\t\t We will show that there is a probability function pA such that ( A , pA ) is a proper and consistent PPDT , and pA ( c ) = pg ( f ( c ) ) for all complete computations c . \n\t', ""\n\t\t We first construct a PPDT ( A , p'A ) as follows . \n\t"", ""\n\t\t For each scan transition T = X H Y in ^ , let p'A(T) = pg ( y ) in case y E R , and p'A(T) = 1 otherwise . \n\t"", ""\n\t\t For all remaining transitions T E ^ , let p'A(T) = 1 . \n\t"", ""\n\t\t Note that ( A , p'A ) may be non-proper . \n\t"", ""\n\t\t Still , from the definition of f it follows that , for each complete computation c , we have p'A ( c ) = pg(f(c)) , ( 1 ) and so our PPDT is consistent . \n\t"", ""\n\t\t We now map ( A , p'A ) to a language-equivalent PCFG ( 9 ' , pg , ) , 9 ' = ( El , Q , Xin , R ' ) , where R ' contains the following rules with the specified associated probabilities : \x95 X \x97* YZ with pg , ( X \x97* YZ ) = p'A ( X ~\x97* XY ) , for each X ~\x97* XY E ^ with Z the unique stack symbol such that there is at least one transition XY ' ~\x97* Z with Y. > Y ' ; \x95 X \x97* xY with pg , ( X \x97* xY ) = p'A ( X x ~\x97* Y ) , for each transition X x ~\x97* Y E ^ ; \x95 Y \x97* E with pg , ( X \x97* E ) = 1 , for each stack symbol Y such that there is at least one transition XY ~\x97* Z E A or such that Y= Xfin . \n\t"", ""\n\t\t It is not difficult to see that there exists a bijection f ' from complete computations of A to complete derivations of g ' , and that we have pg,(f' ( c ) ) = p'A ( c ) , ( 2 ) for each complete computation c . \n\t"", ""\n\t\t Thus ( g',pg , ) is consistent . \n\t"", ""\n\t\t However , note that ( g',pg , ) is not proper . \n\t"", ""\n\t\t By Lemma 4 , we can construct a new PCFG ( g',p'g , ) that is proper and consistent , and such that pg , ( d ) = p'g , ( d ) , for each complete derivation d of g ' . \n\t"", ""\n\t\t Thus , for each complete computation c of A , we have p'g,(f' ( c ) ) = pg,(f'(c)) . \n\t"", ""\n\t\t ( 3 ) We now transfer back the probabilities of rules of ( g',p'g , ) to the transitions of A . \n\t"", ""\n\t\t Formally , we define a new probability function pA such that , for each T E A , pA ( T ) = p ' g , ( 7r ) , where 7r is the rule in R ' that has been constructed from T as specified above . \n\t"", '\n\t\t It is easy to see that PPDT ( A , pA ) is now proper . \n\t', ""\n\t\t Furthermore , for each complete computation c of A we have pA(c) = p'g,(f' ( c ) ) , ( 4 ) and so ( A , pA ) is also consistent . \n\t"", ""\n\t\t By combining equations ( 1 ) to ( 4 ) we conclude that , for each complete computation c of A , pA ( c ) = p'g,(f'(c)) = pg,(f'(c)) = p'A(c) = pg(f ( c ) ) . \n\t"", '\n\t\t Thus our parsing strategy S can be probabilistically extended . \n\t', '\n\t\t Note that the construction in the proof above can be effectively computed ( see discussion in Section 4 for effective computation of normalized PCFGs ) . \n\t', ""\n\t\t The definition of p'A in the proof of Theorem 5 relies on the strings output by A . \n\t"", '\n\t\t This is the main reason why we needed to consider PDTs rather than PDAs . \n\t', '\n\t\t Now assume an appropriate probability function pA has been computed , such that the source PCFG and ( A , pA ) define equivalent distributions on derivations/computations . \n\t', '\n\t\t Then the probabilities assigned to strings over the input alphabet are also equal . \n\t', '\n\t\t We may subsequently ignore the output strings if the application at hand merely requires probabilistic recognition rather than probabilistic transduction , or in other words , we may simplify PDTs to PDAs . \n\t', '\n\t\t The proof of Theorem 5 also leads to the observation that parsing strategies with the CPP and the SPP as well as their probabilistic extensions can be described as grammar transformations , as follows . \n\t', '\n\t\t A given (P)CFG is mapped to an equivalent (P)PDT by a ( probabilistic ) parsing strategy . \n\t', '\n\t\t By ignoring the output components of swap transitions we obtain a (P)PDA , which can be mapped to an equivalent (P)CFG as shown above . \n\t', '\n\t\t This observation gives rise to an extension with probabilities of the work on covers by \n\t\t']",Positive
"['\n\t\t 6 Applications Many well-known parsing strategies with the CPP also have the SPP . \n\t', '\n\t\t This is for instance the case for top-down parsing and left-corner parsing . \n\t', '\n\t\t As discussed in the introduction , it has already been shown that for any PCFG g , there are equivalent PPDTs implementing these strategies , as reported in \n\t\t']",Positive
"['\n\t\t Those results more simply follow now from our general characterization . \n\t', '\n\t\t Furthermore , PLR parsing \n\t\t']",Positive
"['\n\t\t The above strategies are in contrast to the LR parsing strategy , which has the CPP but lacks the SPP , and therefore falls outside our sufficient condition . \n\t', '\n\t\t As we have already seen in the introduction , it turns out that LR parsing cannot be extended to become a probabilistic parsing strategy . \n\t', '\n\t\t Related to LR parsing is ELR parsing \n\t\t']",Negative
"['\n\t\t By an argument similar to the one provided for LR , we can show that also ELR parsing cannot be extended to become a probabilistic parsing strategy . \n\t', '\n\t\t ( See \n\t\t']",Positive
"['\n\t\t ) These two cases might suggest that the sufficient condition in Theorem 5 is tight in practice . \n\t', '\n\t\t Decidability of the CPP and the SPP obviously depends on how a parsing strategy is specified . \n\t', '\n\t\t As far as we know , in all practical cases of parsing strategies these properties can be easily decided . \n\t', '\n\t\t Also , observe that our results do not depend on the general behaviour of a parsing strategy S , but just on its \x93point-wise\x94 behaviour on each input CFG . \n\t', '\n\t\t Specifically , if S does not have the CPP and the SPP , but for some fixed CFG g of interest we obtain a PDT A that has the CPP and the SPP , then we can still apply the construction in Theorem 5 . \n\t', '\n\t\t In this way , any probability function pg associated with g can be converted into a probability function pA , such that the resulting PCFG and PPDT induce equivalent distributions . \n\t', '\n\t\t We point out that decidability of the CPP and the SPP for a fixed PDT can be efficiently decided using dynamic programming . \n\t', '\n\t\t One more consequence of our results is this . \n\t', '\n\t\t As discussed in the introduction , the properness condition reduces the number of parameters of a PPDT . \n\t', '\n\t\t However , our results show that if the PPDT has the CPP and the SPP then the properness assumption is not restrictive , i.e. , by lifting properness we do not gain new distributions with respect to those induced by the underlying PCFG . \n\t', '\n\t\t 7 Conclusions We have formalized the notion of CFG parsing strategy as a mapping from CFGs to PDTs , and have investigated the extension to probabilities . \n\t', '\n\t\t We have shown that the question of which parsing strategies can be extended to become probabilistic heavily relies on two properties , the correct-prefix property and the strong predictiveness property . \n\t', '\n\t\t As far as we know , this is the first general characterization that has been provided in the literature for probabilistic extension of CFG parsing strategies . \n\t', '\n\t\t We have also shown that there is at least one strategy of practical interest with the CPP but without the SPP , namely LR parsing , that cannot be extended to become a probabilistic parsing strategy . \n\t', '\n\t\t Acknowledgements The first author is supported by the PIONIER Project Algorithms for Linguistic Processing , funded by NWO ( Dutch Organization for Scientific Research ) . \n\t', '\n\t\t The second author is partially supported by MIUR under project PRIN No. 2003091149 005 . \n\t', '\n\t\t References S. Abney , D. McAllester , and F. Pereira . \n\t', '\n\t\t 1999. Re- lating probabilistic grammars and automata . \n\t', '\n\t\t In 37th Annual Meeting of the Association for Com- putational Linguistics , Proceedings of the Con- ference , pages 542\x96549 , Maryland , USA , June . \n\t', '\n\t\t A.V. Aho and J.D. Ullman . \n\t', '\n\t\t 1972. Parsing , vol- ume 1 of The Theory ofParsing , Translation and Compiling . \n\t', '\n\t\t Prentice-Hall . \n\t', '\n\t\t S. Billot and B. Lang . \n\t', '\n\t\t 1989. The structure of shared forests in ambiguous parsing . \n\t', '\n\t\t In 27th Annual Meeting of the Association for Computational Linguistics , Proceedings of the Conference , pages 143\x96151 , Vancouver , British Columbia , Canada , June . \n\t', '\n\t\t T.L. Booth and R.A. Thompson . \n\t', '\n\t\t 1973. Applying probabilistic measures to abstract languages . \n\t', '\n\t\t IEEE Transactions on Computers , C-22(5):442\x96 450 , May . \n\t', '\n\t\t T. Briscoe and J. Carroll . \n\t', '\n\t\t 1993. Generalized probabilistic LR parsing of natural language ( corpora ) with unification-based grammars . \n\t', '\n\t\t Computational Linguistics , 19(1):25\x9659 . \n\t', '\n\t\t E. Charniak and G. Carroll . \n\t', '\n\t\t 1994. Context- sensitive statistics for improved grammatical language models . \n\t', '\n\t\t In Proceedings Twelfth National Conference on Artificial Intelligence , volume 1 , pages 728\x96733 , Seattle , Washington . \n\t', '\n\t\t Z. Chi and S. Geman . \n\t', '\n\t\t 1998. Estimation of probabilistic context-free grammars . \n\t', '\n\t\t Computational Linguistics , 24(2):299\x96305 . \n\t', '\n\t\t Z. Chi . \n\t', '\n\t\t 1999. Statistical properties of probabilistic context-free grammars . \n\t', '\n\t\t Computational Linguistics , 25(1):131\x96160 . \n\t', '\n\t\t M.V. Chitrao and R. Grishman . \n\t', '\n\t\t 1990. Statistical parsing of messages . \n\t', '\n\t\t In Speech and Natural Language , Proceedings , pages 263\x96266 , Hidden Valley , Pennsylvania , June . \n\t', '\n\t\t M.A. Harrison . \n\t', '\n\t\t 1978. Introduction to Formal Language Theory . \n\t', '\n\t\t Addison-Wesley . \n\t', '\n\t\t K. Inui , V. Sornlertlamvanich , H. Tanaka , and T. Tokunaga . \n\t', '\n\t\t 2000. Probabilistic GLR parsing . \n\t', '\n\t\t In H. Bunt and A. Nijholt , editors , Advances in Probabilistic and other Parsing Technologies , chapter 5 , pages 85\x96104 . \n\t', '\n\t\t Kluwer Academic Publishers . \n\t', '\n\t\t B. Lang . \n\t', '\n\t\t 1974. Deterministic techniques for efficient non-deterministic parsers . \n\t', '\n\t\t In Automata , Languages and Programming , 2nd Colloquium , volume 14 of Lecture Notes in Computer Science , pages 255\x96269 , Saarbr¨ucken . \n\t', '\n\t\t Springer-Verlag . \n\t', '\n\t\t R. Leermakers . \n\t', '\n\t\t 1989. How to cover a grammar . \n\t', '\n\t\t In 27th Annual Meeting of the Association for Computational Linguistics , Proceedings of the Conference , pages 135\x96142 , Vancouver , British Columbia , Canada , June . \n\t', '\n\t\t C.D. Manning and B. Carpenter . \n\t', '\n\t\t 2000. Probabilistic parsing using left corner language models . \n\t', '\n\t\t In H. Bunt and A. Nijholt , editors , Advances in Probabilistic and other Parsing Technologies , chapter 6 , pages 105\x96124 . \n\t', '\n\t\t Kluwer Academic Publishers . \n\t', '\n\t\t M.-J. Nederhof and G. Satta . \n\t', '\n\t\t 2003. Probabilistic parsing as intersection . \n\t', '\n\t\t In 8th International Workshop on Parsing Technologies , pages 137\x96 148 , LORIA , Nancy , France , April . \n\t', '\n\t\t M.-J. Nederhof . \n\t', '\n\t\t 1994. An optimal tabular parsing algorithm . \n\t', '\n\t\t In 32nd Annual Meeting ofthe Association for Computational Linguistics , Proceedings of the Conference , pages 117\x96124 , Las Cruces , New Mexico , USA , June . \n\t', '\n\t\t A. Nijholt . \n\t', '\n\t\t 1980. Context-Free Grammars : Covers , Normal Forms , and Parsing , volume 93 of Lecture Notes in Computer Science . \n\t', '\n\t\t SpringerVerlag . \n\t', '\n\t\t P.W. Purdom , Jr. and C.A. Brown . \n\t', '\n\t\t 1981. Parsing extended LR(k) grammars . \n\t', '\n\t\t Acta Informatica , 15:115\x96127 . \n\t', '\n\t\t B. Roark and M. Johnson . \n\t', '\n\t\t 1999. Efficient probabilistic top-down and left-corner parsing . \n\t', '\n\t\t In 37th Annual Meeting of the Association for Computational Linguistics , Proceedings of the Conference , pages 421\x96428 , Maryland , USA , June . \n\t', '\n\t\t D.J. Rosenkrantz and P.M. Lewis II . \n\t', '\n\t\t 1970. Deterministic left corner parsing . \n\t', '\n\t\t In IEEE Conference Record of the 11 th Annual Symposium on Switching and Automata Theory , pages 139\x96152 . \n\t', '\n\t\t J.-A. S´anchez and J.-M. Benedi . \n\t', '\n\t\t 1997 . \n\t', '\n\t\t Consistency of stochastic context-free grammars from probabilistic estimation based on growth transformations . \n\t', '\n\t\t IEEE Transactions on Pattern Analysis and Machine Intelligence , 19(9):1052\x961055 , September . \n\t', '\n\t\t E.S. Santos . \n\t', '\n\t\t 1972. Probabilistic grammars and au- tomata . \n\t', '\n\t\t Information and Control , 21:27\x9647 . \n\t', '\n\t\t S. Sippu and E. Soisalon-Soininen . \n\t', '\n\t\t 1990. Parsing Theory , Vol. II : LR(k) and LL(k) Parsing , volume 20 of EATCS Monographs on Theoretical Computer Science . \n\t', '\n\t\t Springer-Verlag . \n\t', '\n\t\t E. Soisalon-Soininen and E. Ukkonen . \n\t', '\n\t\t 1979. A method for transforming grammars into LL(k) form . \n\t', '\n\t\t Acta Informatica , 12:339\x96369 . \n\t', '\n\t\t V. Sornlertlamvanich , K. Inui , H. Tanaka , T. Tokunaga , and T. Takezawa . \n\t', '\n\t\t 1999. Empirical support for new probabilistic generalized LR parsing . \n\t', '\n\t\t Journal of Natural Language Processing , 6(3):3\x9622 . \n\t', '\n\t\t A. Stolcke . \n\t', '\n\t\t 1995. An efficient probabilistic context-free parsing algorithm that computes prefix probabilities . \n\t', '\n\t\t Computational Linguistics , 21(2):167\x96201 . \n\t', '\n\t\t F. Tendeau . \n\t', '\n\t\t 1995. Stochastic parse-tree recognition by a pushdown automaton . \n\t', '\n\t\t In Fourth International Workshop on Parsing Technologies , pages 234\x96249 , Prague and Karlovy Vary , Czech Republic , September . \n\t', '\n\t\t F. Tendeau . \n\t', '\n\t\t 1997. Analyse syntaxique et s´emantique avec ´evaluation d\x92attributs dans un demi-anneau . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , University of Orl´eans . \n\t', '\n\t\t J.H. Wright and E.N. Wrigley . \n\t', '\n\t\t 1991. GLR parsing with probability . \n\t', '\n\t\t In M. Tomita , editor , Generalized LR Parsing , chapter 8 , pages 113\x96128 . \n\t', '\n\t\t Kluwer Academic Publishers . \n\t', '\n\t\t An alternative method of training probabilistic LR parsers Mark-Jan Nederhof Faculty of Arts University of Groningen P.O. Box 716 NL-9700 AS Groningen The Netherlands markjan@let.rug.nl Giorgio Satta Dept. of Information Engineering University of Padua via Gradenigo , 6/A I-35131 Padova Italy satta@dei.unipd.it Abstract We discuss existing approaches to train LR parsers , which have been used for statistical resolution of structural ambiguity . \n\t', '\n\t\t These approaches are non- optimal , in the sense that a collection of probability distributions cannot be obtained . \n\t', '\n\t\t In particular , some probability distributions expressible in terms of a context-free grammar cannot be expressed in terms of the LR parser constructed from that grammar , under the restrictions of the existing approaches to training of LR parsers . \n\t', '\n\t\t We present an alternative way of training that is provably optimal , and that allows all probability distributions expressible in the context-free grammar to be carried over to the LR parser . \n\t', '\n\t\t We also demonstrate empirically that this kind of training can be effectively applied on a large treebank . \n\t', '\n\t\t 1 Introduction The LR parsing strategy was originally devised for programming languages \n\t\t']",Positive
"['\n\t\t The main difference between the application to programming languages and the application to natural languages is that in the latter case the parsers should be nondeterministic , in order to deal with ambiguous context-free grammars ( CFGs ) . \n\t', '\n\t\t Nondeterminism can be handled in a number of ways , but the most efficient is tabulation , which allows processing in polynomial time . \n\t', '\n\t\t Tabular LR parsing is known from the work by \n\t\t']",Positive
"['\n\t\t In this context , the LR parsing strategy can be seen as a particular mapping from context-free grammars to PDTs . \n\t', '\n\t\t The acronym \x91LR\x92 stands for \x91Left-to-right processing of the input , producing a Right-most derivation ( in reverse)\x92 . \n\t', '\n\t\t When we construct a PDT A from a CFG !9 by the LR parsing strategy and apply it on an input sentence , then the set of output strings of A represents the set of all right-most derivations that !9 allows for that sentence . \n\t', '\n\t\t Such an output string enumerates the rules ( or labels that identify the rules uniquely ) that occur in the corresponding right-most derivation , in reversed order . \n\t', '\n\t\t If LR parsers do not use lookahead to decide between alternative transitions , they are called LR(0) parsers . \n\t', '\n\t\t More generally , if LR parsers look ahead k symbols , they are called LR(k) parsers ; some simplified LR parsing models that use lookahead are called SLR(k) and LALR(k) parsing \n\t\t']",Positive
"['\n\t\t In order to simplify the discussion , we abstain from using lookahead in this article , and \x91LR parsing\x92 can further be read as \x91LR(0) parsing\x92 . \n\t', '\n\t\t We would like to point out however that our observations carry over to LR parsing with lookahead . \n\t', '\n\t\t The theory of probabilistic pushdown automata \n\t\t']",Positive
"['\n\t\t A probability is then assigned to each transition , by a function that we will call the probability function pA , and the probability of an accepting computation of A is the product of the probabilities of the applied transitions . \n\t', '\n\t\t As each accepting computation produces a right-most derivation as output string , a probabilistic LR parser defines a probability distribution on the set of parses , and thereby also a probability distribution on the set of sentences generated by grammar !9 . \n\t', '\n\t\t Disambiguation of an ambiguous sentence can be achieved on the basis of a comparison between the probabilities assigned to the respective parses by the probabilistic LR model . \n\t', '\n\t\t The probability function can be obtained on the basis of a treebank , as proposed by \n\t\t']",Positive
['\n\t\t The model by \n\t\t'],Negative
"['\n\t\t As we will not discuss lookahead here , this matter does not play a significant role in the current study . \n\t', '\n\t\t Noteworthy is that \n\t\t']",Positive
"['\n\t\t In other words , the resulting probability function pA on transitions of the PDT allows better disambiguation than the corresponding function pg on rules of the original grammar . \n\t', '\n\t\t A plausible explanation of this is that stack symbols of an LR parser encode some amount of left context , i.e. information on rules applied earlier , so that the probability function on transitions may encode dependencies between rules that cannot be encoded in terms of the original CFG extended with rule probabilities . \n\t', '\n\t\t The explicit use of left context in probabilistic context-free models was investigated by e.g. \n\t\t']",Positive
"['\n\t\t Note that the probability distributions of language may be beyond the reach of a given context-free grammar , as pointed out by e.g. \n\t\t']",Positive
"['\n\t\t Therefore , the use of left context , and the resulting increase in the number of parameters of the model , may narrow the gap between the given grammar and ill-understood mechanisms underlying actual language . \n\t', '\n\t\t One important assumption that is made by \n\t\t']",Positive
"['\n\t\t This assumption may be motivated by pragmatic considerations , as such a proper model is easy to train by relative frequency estimation : count the number of times a transition is applied with respect to a treebank , and divide it by the number of times the relevant stack symbol ( or pair of stack symbols ) occurs at the top of the stack . \n\t', '\n\t\t Let us call the resulting probability function pTfe . \n\t', '\n\t\t This function is provably optimal in the sense that the likelihood it assigns to the training corpus is maximal among all probability functions pA that are proper in the above sense . \n\t', '\n\t\t However , properness restricts the space of probability distributions that a PDT allows . \n\t', '\n\t\t This means that a ( consistent ) probability function pA may exist that is not proper and that assigns a higher likelihood to the training corpus than pTfe does . \n\t', '\n\t\t ( By \x91consistent\x92 we mean that the probabilities of all strings that are accepted sum to 1 . \n\t', '\n\t\t ) It may even be the case that a ( proper and consistent ) probability function pg on the rules of the input grammar !9 exists that assigns a higher likelihood to the corpus than pTfe , and therefore it is not guaranteed that LR parsers allow better probability estimates than the CFGs from which they were constructed , if we constrain probability functions pA to be proper . \n\t', '\n\t\t In this respect , LR parsing differs from at least one other well-known parsing strategy , viz . \n\t', '\n\t\t left-corner parsing . \n\t', '\n\t\t See \n\t\t']",Positive
"['\n\t\t As main contribution of this paper we establish that this restriction on expressible probability distributions can be dispensed with , without losing the ability to perform training by relative frequency estimation . \n\t', '\n\t\t What comes in place of properness is reverse -properness , which can be seen as properness of the reversed pushdown automaton that processes input from right to left instead of from left to right , interpreting the transitions of A backwards . \n\t', '\n\t\t As we will show , reverse-properness does not restrict the space of probability distributions expressible by an LR automaton . \n\t', '\n\t\t More precisely , assume some probability distribution on the set of derivations is specified by a probability function pA on transitions of PDT A that realizes the LR strategy for a given grammar !9 . \n\t', '\n\t\t Then the same probability distribution can be specified by an alternative such function p~A that is reverse-proper . \n\t', '\n\t\t In addition , for each probability distribution on derivations expressible by a probability function pg for !9 , there is a reverse-proper probability function pA for A that expresses the same probability distribution . \n\t', '\n\t\t Thereby we ensure that LR parsers become at least as powerful as the original CFGs in terms of allowable probability distributions . \n\t', '\n\t\t This article is organized as follows . \n\t', '\n\t\t In Section 2 we outline our formalization of LR parsing as a construction of PDTs from CFGs , making some superficial changes with respect to standard formulations . \n\t', '\n\t\t Properness and reverse-properness are discussed in Section 3 , where we will show that reverse-properness does not restrict the space of probability distributions . \n\t', '\n\t\t Section 4 reports on experiments , and Section 5 concludes this article . \n\t', '\n\t\t 2 LR parsing As LR parsing has been extensively treated in existing literature , we merely recapitulate the main definitions here . \n\t', '\n\t\t For more explanation , the reader is referred to standard literature such as \n\t\t']",Positive
"['\n\t\t An LR parser is constructed on the basis of a CFG that is augmented with an additional rule 5t ^ ~- 5 , where 5 is the former start symbol , and the new nonterminal 5t becomes the start symbol of the augmented grammar . \n\t', '\n\t\t The new terminal ~- acts as an imaginary start-of-sentence marker . \n\t', '\n\t\t We denote the set of terminals by ^ and the set of nonterminals by N . \n\t', '\n\t\t We assume each rule has a unique label r . \n\t', '\n\t\t As explained before , we construct LR parsers as pushdown transducers . \n\t', '\n\t\t The main stack symbols of these automata are sets of dotted rules , which consist of rules from the augmented grammar with a distinguished position in the right-hand side indicated by a dot \x91\x95\x92 . \n\t', '\n\t\t The initial stack symbol is pinit = { 5t \x97* ~- \x95 5 } . \n\t', '\n\t\t We define the closure of a set p of dotted rules as the smallest set closure(p) such that : 1. p C closure(p) ; and 2. for ( B \x97* ^ \x95 A^ ) E closure(p) and A \x97* ^ a rule in the grammar , also ( A \x97* \x95 ^ ) E closure(p) . \n\t', '\n\t\t We define the operation goto on a set p of dotted rules and a grammar symbol X E ^ U N as : goto ( p , X ) = { A \x97* ^X \x95 ^ | ( A \x97* ^ \x95 X^ ) E closure(p) } The set of LR states is the smallest set such that : 1. pinit is an LR state ; and 2. if p is an LR state and goto ( p , X ) = q =~ ^ , for some X E ^ U N , then q is an LR state . \n\t', '\n\t\t We will assume that PDTs consist of three types of transitions , of the form P H P Q ( a push transition ) , of the form P H Q ( a swap transition ) , and of the form P Q H R ( a pop transition ) . \n\t', '\n\t\t Here P , Q and R are stack symbols , a is one input terminal or is the empty string ^ , and b is one output terminal or is the empty string ^ . \n\t', '\n\t\t In our notation , stacks grow from left to right , so that P H P Q means that Q is pushed on top of P . \n\t', '\n\t\t We do not have internal states next to stack symbols . \n\t', '\n\t\t For the PDT that implements the LR strategy , the stack symbols are the LR states , plus symbols of the form [ p ; X ] , where p is an LR state and X is a grammar symbol , and symbols of the form ( p , A , m ) , where p is an LR state , A is the left-hand side of some rule , and m is the length of some prefix of the right-hand side of that rule . \n\t', '\n\t\t More explanation on these additional stack symbols will be given below . \n\t', '\n\t\t The stack symbols and transitions are simultaneously defined in Figure 1 . \n\t', '\n\t\t The final stack symbol is pfinal = ( pinit , 5t , 0 ) . \n\t', '\n\t\t This means that an input a1 · · · an is accepted if and only if it is entirely read by a sequence of transitions that take the stack consisting only of pinit to the stack consisting only of pfinal . \n\t', ""\n\t\t The computed output consists of the string of terminals b1 · · · bn ' from the output components of the applied transitions . \n\t"", '\n\t\t For the PDTs that we will use , this output string will consist of a sequence of rule labels expressing a right-most derivation of the input . \n\t', '\n\t\t On the basis of the original grammar , the cor- responding parse tree can be constructed from such an output string . \n\t', '\n\t\t There are a few superficial differences with LR parsing as it is commonly found in the literature . \n\t', '\n\t\t The most obvious difference is that we divide reductions into \x91binary\x92 steps . \n\t', '\n\t\t The main reason is that this allows tabular interpretation with a time complexity cubic in the length of the input . \n\t', '\n\t\t Otherwise , the time complexity would be O(nm+1) , where m is the length of the longest right-hand side of a rule in the CFG . \n\t', '\n\t\t This observation was made before by \n\t\t']",Positive
['\n\t\t See also a related formulation of tabular LR parsing by \n\t\t'],Positive
"['\n\t\t To be more specific , instead of one step of the PDT taking stack : Up0p1 · · · pm immediately to stack : Up0q where ( A \x97* X1 · · · Xm \x95 ) E pm , U is a string of stack symbols and goto ( p0 , A ) = q , we have a number of smaller steps leading to a series of stacks : Up0p1 · · · pm-1pm Up0p1 ··· pm-1(A , m-1 ) Up0p1 · · · ( A , m-2 ) ... \n\t', '\n\t\t Up0(A , 0 ) Up0q There are two additional differences . \n\t', '\n\t\t First , we want to avoid steps of the form : Up0(A , 0 ) Up0q by transitions p0 ( A , 0 ) H p0 q , as such transitions complicate the generic definition of \x91properness\x92 for PDTs , to be discussed in the following section . \n\t', ""\n\t\t For this reason , we use stack symbols of the form [ p ; X ] next to p , and split up p0 ( A , 0 ) E'i p0 q into pop [ p0 ; X0 ] ( A , 0 ) H [ p0 ; A ] and push [ p0 ; A ] EH [ p0 ; A ] q . \n\t"", '\n\t\t This is a harmless modification , which increases the number of steps in any computation by at most a factor 2 . \n\t', '\n\t\t Secondly , we use stack symbols of the form ( p , A , m ) instead of ( A , m ) . \n\t', ""\n\t\t This concerns the conditions of reverse-properness to be discussed in the \x95 For LR state p and a E ^ such that goto(p , a ) =~ 0 : p H [ p ; a ] ( 1 ) \x95 For LR state p and ( A \x97* \x95 ) E p , where A \x97* ^ has label r : p H [ p ; A ] ( 2 ) \x95 For LR state p and ( A \x97* ^ \x95 ) E p , where I ^ I = m > 0 and A \x97* ^ has label r : p ~'> ( p , A , m \x97 1 ) ( 3 ) \x95 For LR state p and ( A \x97* ^ \x95 XQ ) E p , where I^I = m > 0 , such that goto(p , X ) = q =~ 0 : [ p ; X ] ( q , A , m ) H ( p , A , m \x97 1 ) ( 4 ) \x95 For LR state p and ( A \x97* \x95 XQ ) E p , such that goto(p , X ) = q =~ 0 : [ p ; X ] ( q , A , 0 ) H [ p ; A ] ( 5 ) \x95 For LR state p and X E ^ U N such that goto(p , X ) = q =~ 0 : [ p ; X ] E ' , [ p ; X ] q ( 6 ) Figure 1 : The transitions of a PDT implementing LR(0) parsing . \n\t"", '\n\t\t following section . \n\t', '\n\t\t By this condition , we consider LR parsing as being performed from right to left , so backwards with regard to the normal processing order . \n\t', '\n\t\t If we were to omit the first components p from stack symbols ( p , A , m ) , we may obtain \x91dead ends\x92 in the computation . \n\t', '\n\t\t We know that such dead ends make a (reverse-)proper PDT inconsistent , as probability mass lost in dead ends causes the sum of probabilities of all computations to be strictly smaller than 1 . \n\t', '\n\t\t ( See also \n\t\t']",Positive
"['\n\t\t ) It is interesting to note that the addition of the components p to stack symbols ( p , A , m ) does not increase the number of transitions , and the nature of LR parsing in the normal processing order from left to right is preserved . \n\t', ""\n\t\t With all these changes together , reductions are implemented by transitions resulting in the following sequence of stacks : U'[p0 ; X0 ] [ p1 ; X1 ] ... [ pm-1 ; Xm-1]pm U'[p0 ; X0 ] [ p1 ; X1 ] ... [ pm-1 ; Xm-1 ] ( pm , A , m\x971 ) U'[p0 ; X0 ] [ p1 ; X1 ] ... ( pm-1 , A , m\x972 ) ... \n\t"", ""\n\t\t U'[p0 ; X0](p1 , A , 0 ) U'[p0 ; A ] U'[p0 ; A]q Please note that transitions of the form [ p ; X ] ( q , A , m ) E i ( p , A , m \x97 1 ) may correspond to several dotted rules ( A \x97* ^ \x95 XQ ) E p , with different ^ of length m and different Q . \n\t"", '\n\t\t If we were to multiply such transitions for different ^ and Q , the PDT would become prohibitively large . \n\t', '\n\t\t 3 Properness and reverse-properness If a PDT is regarded to process input from left to right , starting with a stack consisting only of pinit , and ending in a stack consisting only of pfinal , then it seems reasonable to cast this process into a probabilistic framework in such a way that the sum of probabilities of all choices that are possible at any given moment is 1 . \n\t', '\n\t\t This is similar to how the notion of \x91properness\x92 is defined for probabilistic context- free grammars ( PCFGs ) ; we say a PCFG is proper if for each nonterminal A , the probabilities of all rules with left-hand side A sum to 1 . \n\t', '\n\t\t Properness for PCFGs does not restrict the space of probability distributions on the set of parse trees . \n\t', '\n\t\t In other words , if a probability distribution can be defined by attaching probabilities to rules , then we may reassign the probabilities such that that PCFG becomes proper , while preserving the probability distribution . \n\t', '\n\t\t This even holds if the input grammar is non-tight , meaning that probability mass is lost in \x91infinite derivations\x92 ( S´anchez and Benedi , 1997 ; Chi and Geman , 1998 ; Chi , 1999 ; Nederhof and Satta , 2003 ) . \n\t', '\n\t\t Although CFGs and PDTs are weakly equivalent , they behave very differently when they are extended with probabilities . \n\t', '\n\t\t In particular , there seems to be no notion similar to PCFG properness that can be imposed on all types of PDTs without losing generality . \n\t', '\n\t\t Below we will discuss two constraints , which we will call properness and reverse- properness . \n\t', '\n\t\t Neither of these is suitable for all types of PDTs , but as we will show , the second is more suitable for probabilistic LR parsing than the first . \n\t', '\n\t\t This is surprising , as only properness has been described in existing literature on probabilistic PDTs ( PPDTs ) . \n\t', '\n\t\t In particular , all existing approaches to probabilistic LR parsing have assumed properness rather than anything related to reverse-properness . \n\t', '\n\t\t For properness we have to assume that for each stack symbol P , we either have one or more tran- sitions of the form P H P Q or P H Q , or one or more transitions of the form Q P H R , but no combination thereof . \n\t', '\n\t\t In the first case , properness demands that the sum of probabilities of all transi- tions P H P Q and P H Q is 1 , and in the second case properness demands that the sum of probabili- ties of all transitions Q P H R is 1 for each Q . \n\t', '\n\t\t Note that our assumption above is without loss of generality , as we may introduce swap transitions P EH P1 and P H P2 , where P1 and P2 are new stack symbols , and replace transitions P H P Q and P H Q by P1 H P1 Q and P1 H Q , and replace transitions Q P H R by Q P2 H R. The notion of properness underlies the normal training process for PDTs , as follows . \n\t', '\n\t\t We assume a corpus of PDT computations . \n\t', '\n\t\t In these computations , we count the number of occurrences for each transition . \n\t', '\n\t\t For each P we sum the total number of all occurrences of transitions P Hp Q or P H Q. The probability of , say , a transition P H P Q is now estimated by dividing the number of occurrences thereof in the corpus by the above total number of occurrences of transitions with P in the left- hand side . \n\t', '\n\t\t Similarly , for each pair ( Q , P ) we sum the total number of occurrences of all transitions of the form Q P H R , and thereby estimate the proba- bility of a particular transition Q P H R by relative frequency estimation . \n\t', '\n\t\t The resulting PPDT is proper . \n\t', '\n\t\t It has been shown that imposing properness is without loss of generality in the case of PDTs constructed by a wide range of parsing strategies , among which are top-down parsing and left-corner parsing . \n\t', '\n\t\t This does not hold for PDTs constructed by the LR parsing strategy however , and in fact , properness for such automata may reduce the expressive power in terms of available probability distributions to strictly less than that offered by the original CFG . \n\t', '\n\t\t This was formally proven by \n\t\t']",Positive
['\n\t\t The same difficulty for ELR parsing was suggested by \n\t\t'],Positive
"['\n\t\t For this reason , we investigate a practical alternative , viz . \n\t', '\n\t\t reverse-properness . \n\t', '\n\t\t Now we have to assume that for each stack symbol R , we either have one or more transitions of the form P H R or Q P H R , or one or more transitions of the form P H P R , but no combination thereof . \n\t', '\n\t\t In the first case , reverse-properness demands that the sum of probabilities of all transitions P H R or Q P H R is 1 , and in the second case reverse-properness demands that the sum of probabilities of transitions P H P R is 1 for each P . \n\t', '\n\t\t Again , our assumption above is without loss of generality . \n\t', '\n\t\t In order to apply relative frequency estimation , we now sum the total number of occurrences of transitions P H R or Q P H R for each R , and we sum the total number of occurrences of transitions P H P R for each pair ( P , R ) . \n\t', '\n\t\t We now prove that reverse-properness does not restrict the space of probability distributions , by means of the construction of a \x91cover\x92 grammar from an input CFG , as reported in Figure 2 . \n\t', '\n\t\t This cover CFG has almost the same structure as the PDT resulting from Figure 1 . \n\t', '\n\t\t Rules and transitions almost stand in a one-to-one relation . \n\t', '\n\t\t The only noteworthy difference is between transitions of type ( 6 ) and rules of type ( 12 ) . \n\t', '\n\t\t The right-hand sides of those rules can be a because the corresponding transitions are deterministic if seen from right to left . \n\t', '\n\t\t Now it becomes clear why we needed the components p in stack symbols of the form ( p , A , m ) . \n\t', '\n\t\t Without it , one could obtain an LR state q that does not match the underlying [ p ; X ] in a reversed computation . \n\t', '\n\t\t We may assume without loss of generality that rules of type ( 12 ) are assigned probability 1 , as a probability other than 1 could be moved to corresponding rules of types ( 10 ) or ( 11 ) where state q was introduced . \n\t', '\n\t\t In the same way , we may assume that transitions of type ( 6 ) are assigned probability 1 . \n\t', '\n\t\t After making these assumptions , we obtain a bij ection between probability functions pA for the PDT and probability functions pq for the cover CFG . \n\t', '\n\t\t As was shown by e.g. \n\t\t']",Positive
"['\n\t\t It is now also clear that a reverse-proper LR parser can describe any probability distribution that the original CFG can . \n\t', '\n\t\t The proof is as follows . \n\t', '\n\t\t Given a probability function pq for the input CFG , we define a probability function pA for the LR parser , by letting transitions of types ( 2 ) and ( 3 ) \x95 For LR state p and a E ^ such that goto(p , a ) =~ 0 : [ p ; a ] \x97* p ( 7 ) \x95 For LR state p and ( A \x97* \x95 ) E p , where A \x97* ^ has label r : [ p ; A ] \x97* p r ( 8 ) \x95 For LR state p and ( A \x97* ^ \x95 ) E p , where I ^ I = m > 0 and A \x97* ^ has label r : ( p , A , m ^ 1 ) \x97* p r ( 9 ) \x95 For LR state p and ( A \x97* ^ \x95 XQ ) E p , where I ^ I = m > 0 , such that goto(p , X ) = q =~ 0 : ( p , A , m ^ 1 ) \x97* [ p ; X ] ( q , A , m ) ( 10 ) \x95 For LR state p and ( A \x97* \x95 XQ ) E p , such that goto(p , X ) = q =~ 0 : [ p ; A ] \x97* [ p ; X ] ( q , A , 0 ) ( 11 ) \x95 For LR state q : q\x97*^ ( 12 ) Figure 2 : A grammar that describes the set of computations of the LR(0) parser . \n\t', '\n\t\t Start symbol is pfinal = ( pinit , S\x86 , 0 ) . \n\t', '\n\t\t Terminals are rule labels . \n\t', '\n\t\t Generated language consists of right-most derivations in reverse . \n\t', '\n\t\t have probability pG ( r ) , and letting all other transitions have probability 1 . \n\t', '\n\t\t This gives us the required probability distribution in terms of a PPDT that is not reverse-proper in general . \n\t', '\n\t\t This PPDT can now be recast into reverse-proper form , as proven by the above . \n\t', '\n\t\t 4 Experiments We have implemented both the traditional training method for LR parsing and the novel one , and have compared their performance , with two concrete objectives : 1 . \n\t', '\n\t\t We show that the number of free parameters is significantly larger with the new training method . \n\t', '\n\t\t ( The number of free parameters is the number of probabilities of transitions that can be freely chosen within the constraints of properness or reverse-properness . \n\t', '\n\t\t ) 2 . \n\t', '\n\t\t The larger number of free parameters does not make the problem of sparse data any worse , and precision and recall are at least comparable to , if not better than , what we would obtain with the established method . \n\t', '\n\t\t The experiments were performed on the Wall Street Journal ( WSJ ) corpus , from the Penn Tree- bank , version II . \n\t', '\n\t\t Training was done on sections 02- 21 , i.e. , first a context-free grammar was derived from the \x91stubs\x92 of the combined trees , taking parts of speech as leaves of the trees , omitting all affixes from the nonterminal names , and removing ^generating subtrees . \n\t', '\n\t\t Such preprocessing of the WSJ corpus is consistent with earlier attempts to derive CFGs from that corpus , as e.g. by \n\t\t']",Positive
"['\n\t\t The obtained CFG has 10,035 rules . \n\t', '\n\t\t The dimensions of the LR parser constructed from this grammar are given in Table 1 . \n\t', '\n\t\t The PDT was then trained on the trees from the same sections 02-21 , to determine the number of times that transitions are used . \n\t', '\n\t\t At first sight it is not clear how to determine this on the basis of the tree- bank , as the structure of LR parsers is very different from the structure of the grammars from which they are constructed . \n\t', '\n\t\t The solution is to construct a second PDT from the PDT to be trained , replacing each transition ^ H Q with label r by transition ^ tH Q . \n\t', '\n\t\t By this second PDT we parse the tree- bank , encoded as a series of right-most derivations in reverse . \n\t', ""\n\t\t ' For each input string , there is exactly one parse , of which the output is the list of used transitions . \n\t"", '\n\t\t The same method can be used for other parsing strategies as well , such as left-corner parsing , replacing right-most derivations by a suitable alternative representation of parse trees . \n\t', '\n\t\t By the counts of occurrences of transitions , we may then perform maximum likelihood estimation to obtain probabilities for transitions . \n\t', '\n\t\t This can be done under the constraints of properness or of reverse-properness , as explained in the previous section . \n\t', ""\n\t\t We have not applied any form of smooth- ' We have observed an enormous gain in computational ef- ficiency when we also incorporate the \x91shifts\x92 next to \x91reductions\x92 in these right-most derivations , as this eliminates a considerable amount of nondeterminism . \n\t"", '\n\t\t total # transitions 8,340,315 # push transitions 753,224 # swap transitions 589,811 # pop transitions 6,997,280 Table 1 : Dimensions of PDT implementing LR strategy for CFG derived from WSJ , sect . \n\t', '\n\t\t 02-21 . \n\t', '\n\t\t proper rev.-prop. 577,650 6,589,716 137,134 137,134 0.772 0.777 0.747 0.749 Table 2 : The two methods of training , based on properness and reverse-properness . \n\t', '\n\t\t ing or back-off , as this could obscure properties inherent in the difference between the two discussed training methods . \n\t', '\n\t\t ( Back-off for probabilistic LR parsing has been proposed by \n\t\t']",Positive
"['\n\t\t ) All transitions that were not seen during training were given probability 0 . \n\t', '\n\t\t The results are outlined in Table 2 . \n\t', '\n\t\t Note that the number of free parameters in the case of reverse- properness is much larger than in the case of normal properness . \n\t', '\n\t\t Despite of this , the number of transitions that actually receive non-zero probabilities is ( predictably ) identical in both cases , viz . \n\t', '\n\t\t 137,134 . \n\t', '\n\t\t However , the potential for fine-grained probability estimates and for smoothing and parameter-tying techniques is clearly greater in the case of reverse- properness . \n\t', '\n\t\t That in both cases the number of non-zero probabilities is lower than the total number of parameters can be explained as follows . \n\t', '\n\t\t First , the treebank contains many rules that occur a small number of times . \n\t', '\n\t\t Secondly , the LR automaton is much larger than the CFG ; in general , the size of an LR automaton is bounded by a function that is exponential in the size of the input CFG . \n\t', '\n\t\t Therefore , if we use the same treebank to estimate the probability function , then many transitions are never visited and obtain a zero probability . \n\t', '\n\t\t We have applied the two trained LR automata on section 22 of the WSJ corpus , measuring labelled precision and recall , as done by e.g. \n\t\t']",Positive
"['\n\t\t 2We excluded all sentences with more than 30 words however , as some required prohibitive amounts of memory . \n\t', '\n\t\t Only one of the remaining 1441 sentences was not accepted by the parser . \n\t', '\n\t\t The most important conclusion that can be drawn from this is that the substantially larger space of obtainable probability distributions offered by the reverse-properness method does not come at the expense of a degradation of accuracy for large grammars such as those derived from the WSJ . \n\t', '\n\t\t For comparison , with a standard PCFG we obtain labelled precision and recall of 0.725 and 0.670 , respectively.3 We would like to stress that our experiments did not have as main objective the improvement of state-of-the-art parsers , which can certainly not be done without much additional fine-tuning and the incorporation of some form of lexicalization . \n\t', '\n\t\t Our main objectives concerned the relation between our newly proposed training method for LR parsers and the traditional one . \n\t', '\n\t\t 5 Conclusions We have presented a novel way of assigning probabilities to transitions of an LR automaton . \n\t', '\n\t\t Theoretical analysis and empirical data reveal the following . \n\t', '\n\t\t \x95 The efficiency of LR parsing remains unaffected . \n\t', '\n\t\t Although a right-to-left order of reading input underlies the novel training method , we may continue to apply the parser from left to right , and benefit from the favourable computational properties of LR parsing . \n\t', '\n\t\t \x95 The available space of probability distributions is significantly larger than in the case of the methods published before . \n\t', '\n\t\t In terms of the number of free parameters , the difference that we found empirically exceeds one order of magnitude . \n\t', '\n\t\t By the same criteria , we can now guarantee that LR parsers are at least as powerful as the CFGs from which they are constructed . \n\t', '\n\t\t \x95 Despite the larger number of free parameters , no increase of sparse data problems was observed , and in fact there was a small increase in accuracy . \n\t', '\n\t\t Acknowledgements Helpful comments from John Carroll and anonymous reviewers are gratefully acknowledged . \n\t', '\n\t\t The first author is supported by the PIONIER Project Algorithms for Linguistic Processing , funded by NWO ( Dutch Organization for Scientific Research ) . \n\t', '\n\t\t The second author is partially supported by MIUR under project PRIN No. 2003091149 005 . \n\t', '\n\t\t 3In this case , all 1441 sentences were accepted . \n\t', '\n\t\t # free parameters # non-zero probabilities labelled precision labelled recall References S. Billot and B. Lang . \n\t', '\n\t\t 1989. The structure of shared forests in ambiguous parsing . \n\t', '\n\t\t In 27th Annual Meeting of the Association for Computational Linguistics , pages 143\x96151 , Vancouver , British Columbia , Canada , June . \n\t', '\n\t\t T. Briscoe and J. Carroll . \n\t', '\n\t\t 1993. Generalized probabilistic LR parsing of natural language ( corpora ) with unification-based grammars . \n\t', '\n\t\t Computational Linguistics , 19(1):25\x9659 . \n\t', '\n\t\t Z. Chi and S. Geman . \n\t', '\n\t\t 1998. Estimation of probabilistic context-free grammars . \n\t', '\n\t\t Computational Linguistics , 24(2):299\x96305 . \n\t', '\n\t\t Z. Chi . \n\t', '\n\t\t 1999. Statistical properties of probabilistic context-free grammars . \n\t', '\n\t\t Computational Linguistics , 25(1):131\x96160 . \n\t', '\n\t\t M.V. Chitrao and R. Grishman . \n\t', '\n\t\t 1990. Statistical parsing of messages . \n\t', '\n\t\t In Speech and Natural Language , Proceedings , pages 263\x96266 , Hidden Valley , Pennsylvania , June . \n\t', '\n\t\t M. Collins . \n\t', '\n\t\t 2001. Parameter estimation for statistical parsing models : Theory and practice of distribution-free methods . \n\t', '\n\t\t In Proceedings of the Seventh International Workshop on Parsing Technologies , Beijing , China , October . \n\t', '\n\t\t M.A. Harrison . \n\t', '\n\t\t 1978. Introduction to Formal Language Theory . \n\t', '\n\t\t Addison-Wesley . \n\t', '\n\t\t K. Inui , V. Sornlertlamvanich , H. Tanaka , and T. Tokunaga . \n\t', '\n\t\t 2000. Probabilistic GLR parsing . \n\t', '\n\t\t In H. Bunt and A. Nijholt , editors , Advances in Probabilistic and other Parsing Technologies , chapter 5 , pages 85\x96104 . \n\t', '\n\t\t Kluwer Academic Publishers . \n\t', '\n\t\t M. Johnson . \n\t', '\n\t\t 1998. PCFG models of linguistic tree representations . \n\t', '\n\t\t Computational Linguistics , 24(4):613\x96632 . \n\t', '\n\t\t J.R. Kipps . \n\t', '\n\t\t 1991. GLR parsing in time 0(n3) . \n\t', '\n\t\t In M. Tomita , editor , Generalized LR Parsing , chapter 4 , pages 43\x9659 . \n\t', '\n\t\t Kluwer Academic Publishers . \n\t', '\n\t\t B. Lang . \n\t', '\n\t\t 1974. Deterministic techniques for efficient non-deterministic parsers . \n\t', '\n\t\t In Automata , Languages and Programming , 2nd Colloquium , volume 14 of Lecture Notes in Computer Science , pages 255\x96269 , Saarbr¨ucken . \n\t', '\n\t\t Springer-Verlag . \n\t', '\n\t\t A. Lavie and M. Tomita . \n\t', '\n\t\t 1993. GLR* \x96 an efficient noise-skipping parsing algorithm for context free grammars . \n\t', '\n\t\t In Third International Workshop on Parsing Technologies , pages 123\x96134 , Tilburg ( The Netherlands ) and Durbuy ( Belgium ) , August . \n\t', '\n\t\t M.-J. Nederhof and G. Satta . \n\t', '\n\t\t 1996. Efficient tabular LR parsing . \n\t', '\n\t\t In 34th Annual Meeting of the Association for Computational Linguistics , pages 239\x96246 , Santa Cruz , California , USA , June . \n\t', '\n\t\t M.-J. Nederhof and G. Satta . \n\t', '\n\t\t 2003. Probabilistic parsing as intersection . \n\t', '\n\t\t In 8th International Workshop on Parsing Technologies , pages 137\x96 148 , LORIA , Nancy , France , April . \n\t', '\n\t\t M.-J. Nederhof and G. Satta . \n\t', '\n\t\t 2004. Probabilistic parsing strategies . \n\t', '\n\t\t In 42nd Annual Meeting of the Association for Computational Linguistics , Barcelona , Spain , July . \n\t', '\n\t\t S.-K. Ng and M. Tomita . \n\t', '\n\t\t 1991. Probabilistic LR parsing for general context-free grammars . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the Second International Workshop on Parsing Technologies , pages 154\x96163 , Cancun , Mexico , February . \n\t', '\n\t\t T. Ruland . \n\t', '\n\t\t 2000. A context-sensitive model for probabilistic LR parsing of spoken language with transformation-based postprocessing . \n\t', '\n\t\t In The 18th International Conference on Computational Linguistics , volume 2 , pages 677\x96683 , Saarbr¨ucken , Germany , July\x96August . \n\t', '\n\t\t J.-A. S´anchez and J.-M. Benedi . \n\t', '\n\t\t 1997 . \n\t', '\n\t\t Consistency of stochastic context-free grammars from probabilistic estimation based on growth transformations . \n\t', '\n\t\t IEEE Transactions on Pattern Analysis and Machine Intelligence , 19(9):1052\x961055 , September . \n\t', '\n\t\t E.S. Santos . \n\t', '\n\t\t 1972. Probabilistic grammars and au- tomata . \n\t', '\n\t\t Information and Control , 21:27\x9647 . \n\t', '\n\t\t S. Sippu and E. Soisalon-Soininen . \n\t', '\n\t\t 1990. Parsing Theory , Vol. II : LR(k) and LL(k) Parsing , volume 20 of EATCS Monographs on Theoretical Computer Science . \n\t', '\n\t\t Springer-Verlag . \n\t', '\n\t\t V. Sornlertlamvanich , K. Inui , H. Tanaka , T. Tokunaga , and T. Takezawa . \n\t', '\n\t\t 1999. Empirical support for new probabilistic generalized LR parsing . \n\t', '\n\t\t Journal of Natural Language Processing , 6(3):3\x9622 . \n\t', '\n\t\t K.-Y. Su , J.-N. Wang , M.-H. Su , and J.-S. Chang . \n\t', '\n\t\t 1991. GLR parsing with scoring . \n\t', '\n\t\t In M. Tomita , editor , Generalized LR Parsing , chapter 7 , pages 93\x96112 . \n\t', '\n\t\t Kluwer Academic Publishers . \n\t', '\n\t\t F. Tendeau . \n\t', '\n\t\t 1997. Analyse syntaxique et s´emantique avec ´evaluation d\x92attributs dans un demi-anneau . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , University of Orl´eans . \n\t', '\n\t\t M. Tomita . \n\t', '\n\t\t 1986. Efficient Parsing for Natural Language . \n\t', '\n\t\t Kluwer Academic Publishers . \n\t', '\n\t\t J.H. Wright and E.N. Wrigley . \n\t', '\n\t\t 1991. GLR parsing with probability . \n\t', '\n\t\t In M. Tomita , editor , Generalized LR Parsing , chapter 8 , pages 113\x96128 . \n\t', '\n\t\t Kluwer Academic Publishers . \n\t', '\n\t\t Wrapping of Trees James Rogers Department of Computer Science Earlham College Richmond , IN 47374 , USA jrogers@cs.earlham.edu Abstract We explore the descriptive power , in terms of syntactic phenomena , of a formalism that extends Tree- Adjoining Grammar ( TAG ) by adding a fourth level of hierarchical decomposition to the three levels TAG already employs . \n\t', '\n\t\t While extending the descriptive power minimally , the additional level of decomposition allows us to obtain a uniform account of a range of phenomena that has heretofore been difficult to encompass , an account that employs unitary elementary structures and eschews synchronized derivation operations , and which is , in many respects , closer to the spirit of the intuitions underlying TAG-based linguistic theory than previously considered extensions to TAG . \n\t', '\n\t\t 1 Introduction Tree-Adjoining Grammar ( TAG ) \n\t\t']",Positive
"['\n\t\t There are , however , a number of constructions , many in the core of language , which present difficulties for the linguistic underpinnings of TAG systems , although not necessarily for the implemented systems themselves . \n\t', '\n\t\t Most of these involve the combining of trees in ways that are more complicated than the simple embedding provided by the tree-adjunction operation . \n\t', '\n\t\t The most widely studied way of addressing these constructions within TAG-based linguistic theory \n\t\t']",Positive
"['\n\t\t Depending on the restrictions placed on where this adjoining can occur the effect of such extensions range from no increase in complexity of either the licensed tree sets or the computational complexity of parsing , to substantial increases in both . \n\t', '\n\t\t In this paper we explore these issues within the framework of an extension of TAG that is conservative in the sense that it preserves the unitary nature of the elementary structures and of the adjunction operation and extends the descriptive power minimally . \n\t', '\n\t\t While the paper is organized around particular syntactic phenomena , it is not a study of syntax itself . \n\t', '\n\t\t We make no attempt to provide a comprehensive theory of syntax . \n\t', '\n\t\t In fact , we attempt to simply instantiate the foundations of existing theory \n\t\t']",Positive
"['\n\t\t Our primary focus is the interplay between the linguistic theory and the formal language theory . \n\t', '\n\t\t All of the phenomena we consider can be ( and in practice are \n\t\t']",Positive
"['\n\t\t From a practical perspective , the role of the underlying linguistic theory is , at least in part , to insure consistent and comprehensive implementation of ad hoc mechanisms . \n\t', '\n\t\t From a theoretical perspective , the role of the formal language framework is , at least in part , to insure coherent and computationally well-grounded theories . \n\t', '\n\t\t Our overall goal is to find formal systems that are as close as possible to being a direct embodiment of the principles guiding the linguistic theory and which are maximally constrained in their formal and computational complexity . \n\t', '\n\t\t 2 Hierarchical Decomposition of Strings and Trees Like many approaches to formalization of natural language syntax , TAG is based on a hierarchical decomposition of strings which is represented by ordered trees . \n\t', '\n\t\t ( Figure 1. ) These trees are , in essence , graphs representing two relationships\x97the left-toright ordering of the structural components of the string and the relationship between a component and its immediate constituents . \n\t', '\n\t\t The distinguishing characteristic of TAG is that it identifies an additional hierarchical decomposition of these trees . \n\t', '\n\t\t This shows up , for instance when a clause which has the form of a wh-question is embedded as an argument within another clause . \n\t', '\n\t\t In the Alice DP IP I I\x92 does like V VP DP t t V DP CP DP who I does DP Alice I^ C\x92 IP I\x92 VP d subj-aux inversion . \n\t', '\n\t\t Figure 2 : Bridge verbs and subj-aux inversion . \n\t', '\n\t\t of the tree for the embedded clause , an operation known as tree-adjunction . \n\t', '\n\t\t In effect , the tree for the embedded clause is wrapped around that of the matrix clause . \n\t', '\n\t\t This process may iterate , with adjunction of arbitrarily many instances of bridge verb trees : form and the canonical configuration . \n\t', '\n\t\t The Who does Bob believe ...Carol thinks that Alice likes . \n\t', '\n\t\t \x91\x92s Bob in the wh-form ( as in the right-hand tree of Figure 1 ) , one of the arguments of the verb is fronted as a wh-word and the inflectional element ( does , in this case ) precedes the subject . \n\t', ""\n\t\t This is generally known in the literature as wh-movement and subj-aux inversion , but TAG does not necessarily assume there is any actual transformational movement involved , only that there is a systematic relationship between the wh- trees mark the position of the corresponding compo- nents in the canonical trees.l When such a clause occurs as the argument of matrix clause between the upper an a bridge verb ( such as think or believe ) it is split , ' This systematic relationship between the wh-form and the with the wh-word appearing to the left of the matrix has been a fundamental component of clause and the rest of the subordinate clause occur- the ring to the right ( Figure 2 ) . \n\t"", '\n\t\t Standardly , TAG ac- like counts analyze this as insertion of the tree for the Figure 1 : Wh-movement an lower portions One of the key advantages of this approach is that the wh-word is introduced into the derivation within the same elementary structure as the verb it is an argument of . \n\t', '\n\t\t Hence these structures are semantically coherent\x97they express all and only the structural relationships between the elements of a single functional domain \n\t\t']",Positive
"['\n\t\t The adjoined structures are similarly coherent and the derivation preserves that coherence at all stages . \n\t', '\n\t\t Following \n\t\t']",Positive
"['\n\t\t This gives us canonical configuration syntactic theories dating back , at least , to the work of Harris in \x9250\x92s . \n\t', '\n\t\t IP DP Alice VP DP who CP I^ does IP Carol I DPt think V VP C that IP DP Alice I does V VP DP DP who C CP does I I t Carol DP VP IP like t I V think does V DP like t Alice DP IP I does seem V VP I to like V DP VP Bob DP who CP I does Alice DP IP I t seem V VP I to like V VP DP t Figure 3 : Raising verbs . \n\t', '\n\t\t structures that we usually conceptualize as three- dimensional trees , but which can simply be regarded as graphs with three sorts of edges , one for each of the hierarchical relations expressed by the structures . \n\t', '\n\t\t Within this context , tree-adjunction is a process of concatenating these structures , identifying the root of the adjoined structure with the point at which it is adjoined.2 The resulting complex structures are formally equivalent to the derivation trees in standard formalizations of TAG . \n\t', '\n\t\t The derived tree is obtained by concatenating the tree yield of the structure analogously to the way that the string yield of a derivation tree is concatenated to form the derived string of a context-free grammar . \n\t', '\n\t\t Note that in this case it is essential to identify the point in the frontier of each tree component at which the components it dominates will be attached . \n\t', '\n\t\t This point is referred to as the foot of the tree and the path to it from the root is referred to as the ( principal ) spine of the tree . \n\t', '\n\t\t Here we have marked the spines by doubling the corresponding edges of the graphs . \n\t', '\n\t\t Following \n\t\t']",Positive
"['\n\t\t At this point , this is for purely theory-internal reasons\x97it will allow us to exploit the additional formal power we will shortly bring to bear . \n\t', '\n\t\t It should be noted that it does not represent ordinary adjunction . \n\t', '\n\t\t The subject originates in the same elementary structure as the rest of the clause , it is just a somewhat richer structure than the more standard tree . \n\t', '\n\t\t 3 Raising Verbs and Subj-Aux Inversion A problem arises , for this account , when the matrix verb is a raising verb , such as seems or appears as in 2Context-free derivation can be viewed as a similar process of concatenating trees . \n\t', '\n\t\t Alice seems to like Bob Who does Alice seem to like Here the matrix clause and the embedded clause share , in some sense , the same subject argument . \n\t', '\n\t\t ( Figure 3. ) Raising verbs are distinguished , further , from the control verbs ( such as want or promise ) in the fact that they may realize their subject as an expletive it : It seems Alice likes Bob . \n\t', '\n\t\t Note , in particular , that in each of these cases the inflection is carried by the matrix clause . \n\t', '\n\t\t In order to maintain semantic coherence , we will assume that the subject originates in the elementary structure of the embedded clause . \n\t', '\n\t\t This , then , interprets the raising verb as taking an to an , adjoining at the between the subject and the inflectional element of the embedded clause ( as in the left-hand side of Figure 3 ) . \n\t', '\n\t\t For the declarative form this provides a nesting of the trees similar to that of the bridge verbs ; the embedded clause tree is wrapped around that of the matrix clause . \n\t', '\n\t\t For the wh-form , however , the wrapping pattern is more complex . \n\t', '\n\t\t Since who and Alice must originate in the same elementary structure as like , while does must originate in the same elementary structure as seem , the trees evidently must factor and be interleaved as shown in the right-hand side of the figure . \n\t', '\n\t\t Such a wrapping pattern is not possible in ordinary TAG . \n\t', '\n\t\t The sequences of labels occurring along the spines of TAG tree sets must form context- free languages \n\t\t']",Positive
"['\n\t\t Hence the \x93center- embedded\x94 wrapping patterns of the bridge verbs and the declarative form of the raising verbs are possible but the \x93cross-serial\x94 pattern of the wh-form of the raising verbs is not . \n\t', '\n\t\t Figure 4 : An higher-order account . \n\t', '\n\t\t CP DP^ IP who IP I DP DP VP who does I does I t Alice CP I V DP VP to seem Alice VP V IP I DP like t t CP I DP V does Alice seem DP who I to VP I t seem DP V VP to I like V VP t V DP like t 4 Higher-order Decomposition One approach to obtaining the more complicated wrapping pattern that occurs in the wh-form of the raising verb trees is to move to a formalism in which the spine languages of the derived trees are TALs ( the string languages derived by TAGs ) , which can describe such patterns . \n\t', '\n\t\t One such formalism is the third level of Weir\x92s Control Language Hierarchy \n\t\t']",Negative
"['\n\t\t It turns out , however , that one can generate exactly the same tree sets if one moves to a formalism in which another level of hierarchical decomposition is introduced \n\t\t']",Positive
"['\n\t\t This now gives structures which employ four hierarchical relations\x97the fourth representing the constituency relation encoding a hierarchical decomposition of the third-level structures . \n\t', '\n\t\t In this framework , the seem structure can be taken to be inserted between the subject and the rest of the like structure as shown in Figure 4 . \n\t', '\n\t\t Again , spines are marked by doubling 3TAG is equivalent to the second level of this hierarchy , in which the spine languages are Context-Free . \n\t', '\n\t\t the edges . \n\t', '\n\t\t The third-order yield of the corresponding derived structure now wraps the third-order like structure around that of the seem structure , with the fragment of like that contains the subject attaching at the third-order \x93foot\x94 node in the tree-yield of the seem structure ( the ) as shown at the bottom of the figure . \n\t', '\n\t\t The center-embedding wrapping pattern of these third-order spines guarantees that the wrapping pattern of spines of the tree yield will be a TAL , in particular , the \x93cross-serial\x94 pattern needed by raising of wh-form structures . \n\t', '\n\t\t The fourth-order structure has the added benefit of clearly justifying the status of the like structure as a single elementary structure despite of the apparent extraction of the subject along the third relation . \n\t', '\n\t\t 5 Locality Effects Note that it is the to recursion along the third- order spine of the seem structure that actually does the raising of the subject . \n\t', '\n\t\t One of the consequences of this is that that-trace violations , such as Who does Alice seem that does like . \n\t', '\n\t\t cannot occur . \n\t', '\n\t\t If the complementizer originates in the seem structure , it will occur under the . \n\t', '\n\t\t If it originates in the like tree it will occur in a similar position between the CP and the . \n\t', '\n\t\t In either case , Figure 5 : Expletive it . \n\t', '\n\t\t CP IP that C IP DP Alice does I DP it VP I V seem does V DP like Bob the complementizer must precede the raised subject in the derived string . \n\t', '\n\t\t If we fill the subject position of the seem structure with expletive it , as in Figure 5 , the position in the yield of the structure is occupied and we no longer have to recursion . \n\t', '\n\t\t This motivates analyzing these structures as to recursion , similar to bridge verbs , rather than to . \n\t', '\n\t\t ( Figure 5. ) More importantly the presence of the expletive subject in the seem tree rules out super-raising violations such as Alice does appear it seems does like Bob . \n\t', '\n\t\t No matter how the seem structure is interpreted , if it is to raise Alice then the Alice structure will have to settle somewhere in its yield . \n\t', '\n\t\t Without extending the seem structure to include the position , none of the possible positions will yield the correct string ( and all can be ruled out on simple structural grounds ) . \n\t', '\n\t\t If the seem structure is extended to include the , the raising will be ruled out on the assumption that the structure must attach at . \n\t', '\n\t\t 6 Subject-Object Asymmetry Another phenomenon that has proved problematic for standard TAG accounts is extraction from nominals , such as Who did Alice publish a picture of . \n\t', '\n\t\t Here the wh-word is an argument of the prepositional phrase in the object nominal picture of . \n\t', '\n\t\t Apparently , the tree structure involves wrapping of the picture tree around the publish tree . \n\t', '\n\t\t ( See Figure 6. ) The problem , as normally analyzed \n\t\t']",Positive
"['\n\t\t We will take a somewhat less strict view and rule out the adjunction of the publish tree simply on the grounds that it would involve attaching a structure rooted in ( or possibly CP ) to a DP node . \n\t', '\n\t\t The usual way around this difficulty has been to assume that the who is introduced in the publish tree , corresponding , presumably , to the as yet missing DP . \n\t', '\n\t\t The picture tree is then factored into two components , an isolated DP node which adjoins at the wh-DP , establishing its connection to the argument trace , and the picture DP which combines at the object position of publish . \n\t', '\n\t\t This seems to at least test the spirit of the semantic coherence requirement . \n\t', '\n\t\t If the who is not extraneous in the publish tree then it must be related in some way to the object position . \n\t', '\n\t\t But the identity of who is ultimately not the object of publish ( a picture ) but rather the object of the embedded preposition ( the person the picture is of ) . \n\t', '\n\t\t If we analyze this in terms of a fourth hierarchical relation , we can allow the who to originate in the picture structure , which would now be rooted in CP . \n\t', '\n\t\t This could be allowed to attach at the root of the publish structure on the assumption that it is a C-node of some sort , providing the wrapping of its tree-yield around that of the publish . \n\t', '\n\t\t ( See Figure 6. ) Thus we get an account with intact elementary structures which are unquestionably semantically coherent . \n\t', '\n\t\t One of the striking characteristics of extraction of this sort is the asymmetry between extraction from the object , which is acceptable , and extraction from the subject , which is not : Who did a picture of illustrate the point . \n\t', '\n\t\t In the account under consideration , we might contemplate a similar combination of structures , but in this case the picture DP has to somehow migrate up to combine at the subject position . \n\t', '\n\t\t Under our assumption that the subject structure is attached to the illustrate tree via the third relation , this would require the subject structure to , in effect , have two Alice does it seems does like Bob . \n\t', '\n\t\t CP CP DP DP DP IP who who IP DP PP IP IP did I t DP VP a picture of P DP t did DP V VP IP DP I PP illustrate the point DP CP IP DP a picture P t V did I VP DP illustrate the point P t Figure 7 : Extraction from subject nominal . \n\t', '\n\t\t of DP t who V DP PP P of D a picture DP DP illustrate t the point CP DP DP CP DP who who IP CP DP IP IP a picture DP Alice VP DP P of IP did did Alice I I VP t t V DP CP V DP publish publish DP VP DP PP who a picture DP P PP DP P IP P did DP t IP DP Alice I t V apicture publish of DP P t DP of t Figure 6 : Extraction from object nominal . \n\t', '\n\t\t feet , an extension that strictly increases the generative power of the formalism . \n\t', '\n\t\t Alternatively , we might assume that the picture structure attaches in the yield of the illustrate structure or between the main part of the structure and the subject tree , but either of these would fail to promote the who to the root of the yield structure . \n\t', '\n\t\t 7 Processing As with any computationally oriented formalism , the ability to define the correct set of structures is only one aspect of the problem . \n\t', '\n\t\t Just as important is the question of the complexity of processing language relative to that definition . \n\t', '\n\t\t Fortunately , the languages of the Control Language Hierarchy are well understood and recognition algorithms , based on a CKY-style dynamic programming approach , are know for each level . \n\t', '\n\t\t The time complexity of the algorithm for the level , as a function of the length of the input ( ) , is \n\t\t']",Positive
"['\n\t\t In the case of the fourth-order grammars , which correspond to the third level of the CLH , this gives an upper bound of . \n\t', '\n\t\t While , strictly speaking , this is a feasible time complexity , in practice we expect that approaches with better average-case complexity , such as Early- style algorithms , will be necessary if these grammars are to be parsed directly . \n\t', '\n\t\t But , as we noted in the introduction , grammars of this complexity are not necessarily intended to be used as working grammars . \n\t', '\n\t\t Rather they are mechanisms for expressing the linguistic theory serving as the foundation of working grammars of more practical complexity . \n\t', '\n\t\t Since all of our proposed use of the higher-order relations involve either combining at a root ( without properly embedding ) or embedding with finitely bounded depth of nesting , the effect of the higher- dimensional combining operations are expressible using a finite set of features . \n\t', '\n\t\t Hence , the sets of derived trees can be generated by adding finitely many features to ordinary TAGs and the theory entailed by our accounts of these phenomena ( as expressed in the sets of derived trees ) is expressible in FTAG . \n\t', '\n\t\t Thus , a complete theory of syntax incorporating them would be ( not necessarily not ) compatible with implementation within existing TAG-based systems . \n\t', '\n\t\t A more long term goal is to implement a compilation mechanism which will translate the linguistic theory , stated in terms of the hierarchical relations , directly into grammars stated in terms of the existing TAG-based systems . \n\t', '\n\t\t 8 Conclusion In many ways the formalism we have working with is a minimal extension of ordinary TAGs . \n\t', '\n\t\t Formally , the step from TAG to add the fourth hierarchical relation is directly analogous to the step from CFG to TAG . \n\t', '\n\t\t Moreover , while the graphs describing the derived structures are often rather complicated , conceptually they involve reasoning in terms of only a single additional relation . \n\t', '\n\t\t The benefit of the added complexity is a uniform account of a range of phenomena that has heretofore been difficult to encompass , an account that employs unitary elementary structures and eschews synchronized derivation operations , and which is , in many respects , closer to the spirit of the intuitions underlying TAG-based linguistic theory than previously considered extensions to TAG . \n\t', '\n\t\t While it is impossible to determine how comprehensive the coverage of a more fully developed theory of syntax based on this formalism will be without actually completing such a theory , we believe that the results presented here suggest that the uniformity provided by adding this fourth level of decomposition to our vocabulary is likely to more than compensate for the added complexity of the fourth level elementary structures . \n\t', '\n\t\t References Robert Evan Frank . \n\t', '\n\t\t 1992. Syntactic Locality and Tree Adjoining Grammar : Grammatical , Acquisition and Processing Perspectives . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t dissertation , Univ . \n\t', '\n\t\t of Penn. Robert Frank . \n\t', '\n\t\t 2002. Phrase Structure Composition and Syntactic Dependencies . \n\t', '\n\t\t MIT Press . \n\t', '\n\t\t The XTAG Research Group . \n\t', '\n\t\t 1998. A lexicalized tree adjoining grammar for english . \n\t', '\n\t\t Technical Report IRCS-98-18 , Institute for Research in Cognitive Science . \n\t', '\n\t\t Aravind K. Joshi and Yves Schabes . \n\t', '\n\t\t 1997. Tree- adjoining grammars . \n\t', '\n\t\t In Handbook of Formal Languages and Automata , volume 3 , pages 69\x96 123 . \n\t', '\n\t\t Springer-Verlag . \n\t', '\n\t\t Aravind K. Joshi , Leon Levy , and Masako Takahashi . \n\t', '\n\t\t 1975. Tree adjunct grammars . \n\t', '\n\t\t Journal of the Computer and Systems Sciences , 10:136\x96163 . \n\t', '\n\t\t Anthony Kroch and Aravind K. Joshi . \n\t', '\n\t\t 1985. The linquistic relevance of tree adjoining grammar . \n\t', '\n\t\t Technical Report MS-CS-85-16 , Dept. of Computer and Information Sciences . \n\t', '\n\t\t Anthony S. Kroch and Aravind K. Joshi . \n\t', '\n\t\t 1987. Analyzing extraposition in a tree adjoining grammar . \n\t', '\n\t\t In Syntax and Semantics , pages 107\x96149 . \n\t', '\n\t\t Academic Press . \n\t', '\n\t\t Vol. 20 . \n\t', '\n\t\t Anthony Kroch . \n\t', '\n\t\t 1989. Asymmetries in long distance extraction in a tree adjoining grammar . \n\t', '\n\t\t In Mark Baltin and Anthony Kroch , editors , Alternative Conceptions of Phrase Structure , pages 66\x9698 . \n\t', '\n\t\t University of Chicago Press . \n\t', '\n\t\t Michael A. Palis and Sunil M. Shende . \n\t', '\n\t\t 1992. Upper bounds on recognition of a hierarchy of noncontext-free languages . \n\t', '\n\t\t Theoretical Computer Science , 98:289\x96319 . \n\t', '\n\t\t James Rogers . \n\t', '\n\t\t 2002. One more perspective on semantic relations in TAG . \n\t', '\n\t\t In Proceedings of the Sixth International Workshop on Tree Adjoining Grammars and Related Frameworks , Venice , IT , May . \n\t', '\n\t\t James Rogers . \n\t', '\n\t\t 2003. Syntactic structures as multidimensional trees . \n\t', '\n\t\t Research on Language and Computation , 1(3\x964):265\x96305 . \n\t', '\n\t\t K. Vijay-Shanker and Aravind K. Joshi . \n\t', '\n\t\t 1991. Unification based tree adjoining grammars . \n\t', '\n\t\t In J. Wedekind , editor , Unification-based Grammars . \n\t', '\n\t\t MIT Press , Cambridge , MA . \n\t', '\n\t\t David J. Weir . \n\t', '\n\t\t 1988 . \n\t', '\n\t\t Characterizing Mildly Context-Sensitive Grammar Formalisms . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , University of Pennsylvania . \n\t', '\n\t\t David J. Weir . \n\t', '\n\t\t 1992. A geometric hierarchy beyond context-free languages . \n\t', '\n\t\t Theoretical Computer Science , 104:235\x96261 . \n\t', '\n\t\t Splitting Complex Temporal Questions for Question Answering systems E. Saquete , P. Martinez-Barco , R. Mu\x98noz , J.L. Vicedo Grupo de investigaci´on del Procesamiento del Lenguaje y Sistemas de Informaci´on . \n\t', '\n\t\t Departamento de Lenguajes y Sistemas Inform´aticos . \n\t', '\n\t\t Universidad de Alicante . \n\t', '\n\t\t Alicante , Spain stela,patricio,rafael,vicedo @dlsi.ua.es Abstract This paper presents a multi-layered Question Answering ( Q.A. ) architecture suitable for enhancing current Q.A. capabilities with the possibility of processing complex questions . \n\t', '\n\t\t That is , questions whose answer needs to be gathered from pieces of factual information scattered in different documents . \n\t', '\n\t\t Specifically , we have designed a layer oriented to process the different types of temporal questions . \n\t', '\n\t\t Complex temporal questions are first decomposed into simpler ones , according to the temporal relationships expressed in the original question . \n\t', '\n\t\t In the same way , the answers of each simple question are re-composed , fulfilling the temporal restrictions of the original complex question . \n\t', '\n\t\t Using this architecture , a Temporal Q.A. system has been developed . \n\t', '\n\t\t In this paper , we focus on explaining the first part of the process : the decomposition of the complex questions . \n\t', '\n\t\t Furthermore , it has been evaluated with the TERQAS question corpus of 112 temporal questions . \n\t', '\n\t\t For the task of question splitting our system has performed , in terms of precision and recall , 85 % and 71 % , respectively . \n\t', '\n\t\t 1 Introduction Question Answering could be defined as the process of computer-answering to precise or arbitrary questions formulated by users . \n\t', '\n\t\t Q.A. systems are especially useful to obtain a specific piece of information without the need of manually going through all the available documentation related to the topic . \n\t', '\n\t\t Research in Question Answering mainly focuses on the treatment offactual questions . \n\t', '\n\t\t These require as an answer very specific items of data , such as dates , names of entities or quantities , e.g. , \x93What is the capital ofBrazil?\x94 . \n\t', '\n\t\t This paper has been supported by the Spanish government , projects FIT-150500-2002-244 , FIT-150500-2002-416 , TIC- 2003-07158-C04-01 and TIC2000-0664-C02-02 . \n\t', '\n\t\t Temporal Q.A. is not a trivial task due to the complexity temporal questions may reach . \n\t', '\n\t\t Current operational Q.A. systems can deal with simple factual temporal questions . \n\t', '\n\t\t That is , questions requiring to be answered with a date , e.g. \x93When did Bob Marley die ? \n\t', '\n\t\t \x94 . \n\t', '\n\t\t or questions that include simple temporal expressions in their formulation , e.g. , \x93Who won the U.S. Open in 1999 ? \n\t', '\n\t\t \x94 . \n\t', '\n\t\t Processing this sort of questions is usually performed by identifying explicit temporal expressions in questions and relevant documents , in order to gather the necessary information to answer the queries . \n\t', '\n\t\t Even though , it seems necessary to emphasize that the system described in \n\t\t']",Negative
['\n\t\t It does so by applying the temporal tagger developed by \n\t\t'],Positive
"['\n\t\t However , issues like addressing the temporal properties or the ordering of events in questions , remain beyond the scope of current Q.A. systems : \x93Who was spokesman of the Soviet Embassy in Baghdad during the invasion of Kuwait?\x94 \x93Is Bill Clinton currently the President of the United States?\x94 This work presents a Question Answering system capable of answering complex temporal questions . \n\t', '\n\t\t This approach tries to imitate human behavior when responding this type of questions . \n\t', '\n\t\t For example , a human that wants to answer the question : \x93Who was spokesman of the Soviet Embassy in Baghdad during the invasion of Kuwait?\x94 would follow this process : 1 . \n\t', '\n\t\t First , he would decompose this question into two simpler ones : \x93Who was spokesman of the Soviet Embassy in Baghdad?\x94 and \x93When did the invasion ofKuwait occur?\x94 . \n\t', '\n\t\t 2. He would look for all the possible answers to the first simple question : \x93Who was spokesman of the Soviet Embassy in Baghdad ? \n\t', '\n\t\t \x94 . \n\t', '\n\t\t 3. After that , he would look for the answer to the second simple question : \x93When did the invasion ofKuwait occur?\x94 4 . \n\t', '\n\t\t Finally , he would give as a final answer one of the answers to the first question ( if there is any ) , whose associated date stays within the period of dates implied by the answer to the second question . \n\t', '\n\t\t That is , he would obtain the final answer by discarding all answers to the simple questions which do not accomplish the restrictions imposed by the temporal signal provided by the original question ( during ) . \n\t', '\n\t\t Therefore , the treatment of complex question is based on the decomposition of these questions into simpler ones , to be resolved using conventional Question Answering systems . \n\t', '\n\t\t Answers to simple questions are used to build the answer to the original question . \n\t', '\n\t\t This paper has been structured in the following fashion : first of all , section 2 presents our proposal of a taxonomy for temporal questions . \n\t', '\n\t\t Section 3 describes the general architecture of our temporal Q.A. system . \n\t', '\n\t\t Section 4 deepens into the first part of the system : the decomposition unit . \n\t', '\n\t\t Finally , the evaluation of the decomposition unit and some conclusions are shown . \n\t', '\n\t\t 2 Proposal of a Temporal Questions Taxonomy Before explaining how to answer temporal questions , it is necessary to classify them , since the way to solve them will be different in each case . \n\t', '\n\t\t Our classification distinguishes first between simple questions and complex questions . \n\t', '\n\t\t We will consider as simple those questions that can be solved directly by a current General Purpose Question Answering system , since they are formed by a single event . \n\t', '\n\t\t On the other hand , we will consider as complex those questions that are formed by more than one event related by a temporal signal which establishes an order relation between these events . \n\t', '\n\t\t Simple Temporal Questions : Type 1 : Single event temporal questions without temporal expression ( TE ) . \n\t', '\n\t\t This kind of questions are formed by a single event and can be directly resolved by a Q.A. System , without pre- or post- processing them . \n\t', '\n\t\t There are not temporal expressions in the question . \n\t', '\n\t\t Example : \x93When did Jordan close the port ofAqaba to Kuwait?\x94 Type 2 : Single event temporal questions with temporal expression . \n\t', '\n\t\t There is a single event in the ques tion , but there are one or more temporal expressions that need to be recognized , resolved and annotated . \n\t', '\n\t\t Each piece of temporal information could help to search for an answer . \n\t', '\n\t\t Example : \x93Who won the 1988 New Hampshire republican primary?\x94 . \n\t', '\n\t\t TE : 1988 Complex Temporal Questions : Type 3 : Multiple events temporal questions with temporal expression . \n\t', '\n\t\t Questions that contain two or more events , related by a temporal signal . \n\t', '\n\t\t This signal establishes the order between the events in the question . \n\t', '\n\t\t Moreover , there are one or more temporal expressions in the question . \n\t', '\n\t\t These temporal expressions need to be recognized , resolved and annotated , and they introduce temporal constraints to the answers of the question . \n\t', '\n\t\t Example : \x93What did George Bush do after the U.N. Security Council ordered a global embargo on trade with Iraq in August 90 ? \n\t', '\n\t\t \x94 In this example , the temporal signal is after and the temporal constraint is \x93between 8/1/1990 and 8/31/1990\x94 . \n\t', '\n\t\t This question can be divided into the following ones : Q1 : What did George Bush do ? \n\t', '\n\t\t Q2 : When the U.N. Security Council ordered a global embargo on trade with Iraq ? \n\t', '\n\t\t Type 4 : Multiple events temporal questions without temporal expression . \n\t', '\n\t\t Questions that consist of two or more events , related by a temporal signal . \n\t', '\n\t\t This signal establishes the order between the events in the question . \n\t', '\n\t\t Example : \x93What happened to world oil prices after the Iraqi annexation of Kuwait?\x94 . \n\t', '\n\t\t In this example , the temporal signal is after and the question would be decomposed into : Q1 : What happened to world oil prices ? \n\t', '\n\t\t Q2 : When did the Iraqi \x93annexation\x94 of Kuwait occur ? \n\t', '\n\t\t How to process each type will be explained in detail in the following sections . \n\t', '\n\t\t 3 Multi-layered Question-Answering System Architecture Current Question Answering system architectures do not allow to process complex questions . \n\t', '\n\t\t That is , questions whose answer needs to be gathered from pieces of factual information that is scattered in a document or through different documents . \n\t', '\n\t\t In order to be able to process these complex questions , we propose a multi-layered architecture . \n\t', '\n\t\t This architecture increases the functionality of the current Question-Answering systems , allowing us to solve any type of temporal questions . \n\t', '\n\t\t Moreover , this system could be easily augmented with new layers to cope with questions that need complex processing and are not temporal oriented . \n\t', '\n\t\t Some examples of complex questions are : Temporal questions like \x93Where did Michael Milken study before going to the University of Pennsylvania?\x94 . \n\t', '\n\t\t This kind of questions needs to use temporal information and event ordering to obtain the right answer . \n\t', '\n\t\t Script questions like \x93How do I assemble a bicycle?\x94 . \n\t', '\n\t\t In these questions , the final answer is a set of ordered answers . \n\t', '\n\t\t Template-based questions like \x93Which are the main biographical data ofNelson Mandela?\x94 . \n\t', '\n\t\t This question should be divided in a number of factual questions asking for different aspects of Nelson Mandela\x92s biography . \n\t', '\n\t\t Gathering their respective answers will make it possible to answer the original question . \n\t', '\n\t\t These three types of question have in common the necessity of an additional processing in order to be solved . \n\t', '\n\t\t Our proposal to deal with them is to superpose an additional processing layer , one by each type , to a current General Purpose Question Answering system , as it is shown in Figure 1 . \n\t', '\n\t\t This layer will perform the following steps : Decomposition of the question into simple events to generate simple questions ( sub- questions ) and the ordering of the sub- questions . \n\t', '\n\t\t Sending simple questions to a current General Purpose Question Answering system . \n\t', '\n\t\t Receiving the answers to the simple questions from the current General Purpose Question Answering system . \n\t', '\n\t\t Filtering and comparison between sub-answers to build the final complex answer . \n\t', '\n\t\t Figure 1 : Multi-layered Architecture of a Q.A. . \n\t', '\n\t\t The main advantages of performing this multi- layered system are : It allows you to use any existing general Q.A. system , with the only effort of adapting the output of the processing layer to the type of input that the Q.A. system uses . \n\t', '\n\t\t Due to the fact that the process of complex questions is performed at an upper layer , it is not necessary to modify the Q.A. system when you want to deal with more complex questions . \n\t', '\n\t\t Each additional processing layer is independent from each other and only processes those questions within the type accepted by that layer . \n\t', '\n\t\t Next , we present a layer oriented to process temporal questions according to the taxonomy shown in section 2 . \n\t', '\n\t\t 3.1 Architecture of a Question Answering System applied to Temporality The main components of the Temporal Question Answering System are ( c.f. figure 2 ) top-down : Question Decomposition Unit , General purpose Q.A. system and Answer , Recomposition Unit . \n\t', '\n\t\t Figure 2 : Temporal Question Answering System These components work all together for the obtainment of a final answer . \n\t', '\n\t\t The Question Decomposition Unit and the Answer Recomposition Unit are the units that conform the Temporal Q.A. layer Complex Answ Complex Question ~~~~~~AC~ ~~~~~~A~ Q~ A~ ~~~C~SS~~~ SC~~~~ Q~ A~ ~~~C~SS~~~ ~~~~~A~~ Q~ A~ ~~~C~SS~~~ Simple Questions Simple Answers ~~~~~A~ ~~~~~S~ Q~~S~~~~ A~S~~~~~~ S~S~~~ ~~~~~~ ~ ~ ~ ~n S ~ ~ ~ gewi- Qu..nen ~~~e~~e~~~en ~n~~ ~~n~~~~ ~u~~e~~ Q~ S~~~~~ Q Q~ ... \n\t', '\n\t\t Q. ~~~~e~~~ ~~~ Sn ~~ ~n~~~~ ~~~e~~e~~~~en ~n~~ ~~ ~~~e~n~~en ~n~ ~~~e~u~~en ~~~~ ~~~n~~~~~~en Qu~~~en S~~~~~~ ~ ~n~~~~~u~~ ~n~~~ ~~~~n~ ~n~~~e~~~~~~en ~n~ ~e~~e~~~en which process the temporal questions , before and after using a General Purpose Q.A. system . \n\t', '\n\t\t The Question Decomposition Unit is a preprocessing unit which performs three main tasks . \n\t', '\n\t\t First of all , the recognition and resolution of temporal expressions in the question . \n\t', '\n\t\t Secondly , there are different types of questions , according to the taxonomy shown in section 2 . \n\t', '\n\t\t Each type of them needs to be treated in a different manner . \n\t', '\n\t\t For this reason , type identification must be done . \n\t', '\n\t\t After that , complex questions of types 3 and 4 only , are split into simple ones , which are used as the input of a General Purpose Question-Answering system . \n\t', '\n\t\t For example , the question \x93Where did Bill Clinton study before going to Oxford University?\x94 , is divided into two sub-questions related through the temporal signal before : \x96 Q1 : Where did Bill Clinton study ? \n\t', '\n\t\t \x96 Q2 : When did Bill Clinton go to Oxford University ? \n\t', '\n\t\t A General Purpose Question Answering system . \n\t', '\n\t\t Simple factual questions generated are processed by a General Purpose Question Answering system . \n\t', '\n\t\t Any Question Answering system could be used here . \n\t', '\n\t\t In this case , the SEMQA system ( Vicedo and Ferr´andez , 2000 ) has been used . \n\t', '\n\t\t The only condition is to know the output format of the Q.A. system to accordingly adapt the layer interface . \n\t', '\n\t\t For the example above , a current Q.A. system returns the following answers : \x96 Q1 Answers : Georgetown University ( 1964-68 ) // Oxford University ( 1968-70 ) // Yale Law School ( 1970-73 ) \x96 Q2 Answer : 1968 The Answer Recomposition Unit is the last stage in the process . \n\t', '\n\t\t This unit builds the answer to the original question from the answers to the sub-questions and the temporal information extracted from the questions ( temporal signals or temporal expressions ) . \n\t', '\n\t\t As a result , the correct answer to the original question is returned . \n\t', '\n\t\t Apart from proposing a taxonomy of temporal questions , we have presented a multi- layered Q.A. architecture suitable for enhancing current Q.A. capabilities with the possibility of adding new layers for processing different kinds of complex questions . \n\t', '\n\t\t Moreover , we have proposed a specific layer oriented to process each type of temporal questions . \n\t', '\n\t\t The final goal of this paper is to introduce and evaluate the first part of the temporal question processing layer : the Question Decomposition Unit . \n\t', '\n\t\t Next section shows the different parts of the unit together with some examples of their behavior . \n\t', '\n\t\t 4 Question Decomposition Unit The main task of this unit is the decomposition of the question , which is divided in three main tasks or modules : Type Identification ( according to the taxonomy proposed in section 2 ) Temporal Expression Recognition and Resolution Question Splitter These modules are fully explained below . \n\t', '\n\t\t Once the decomposition of the question has been made , the output of this unit is : A set of sub-questions , that are the input of the General Purpose Question-Answering system . \n\t', '\n\t\t Temporal tags , containing concrete dates returned by TERSEO system \n\t\t']",Positive
"['\n\t\t A set of temporal signals that are part of the input of the Answer Recomposition Unit as well , because this information is necessary in order to compose the final answer . \n\t', '\n\t\t Once the decomposition has been made , the General Purpose Question-Answering system is used to treat with simple questions . \n\t', '\n\t\t The temporal information goes directly to the Answer Recomposition unit . \n\t', '\n\t\t 4.1 Type Identification The Type Identification Unit classifies the question in one of the four types of the taxonomy proposed in section 2 . \n\t', '\n\t\t This identification is necessary because each type of question causes a different behavior ( scenario ) in the system . \n\t', '\n\t\t Type 1 and Type 2 questions are classified as simple , and the answer can be obtained without splitting the original question . \n\t', '\n\t\t However , Type 3 and Type 4 questions need to be split in a set of simple sub-questions . \n\t', '\n\t\t The types of these sub-questions are always Type 1 or Type 2 or a non-temporal question , which are considered simple questions . \n\t', '\n\t\t The question type is established according to the rules in figure 3 : Figure 3 : Decision tree for Type Identification 4.2 Temporal Expression Recognition and Resolution This module uses TERSEO system \n\t\t']",Positive
"['\n\t\t The tags this module returns exhibit the following structure : Explicit dates : <DATE_TIME ID=""value"" TYPE=""value"" VALDATE1=""value""VALTIME1=""value"" VALDATE2=""value"" VALTIME2=""value""> expression </DATE_TIME> Implicit dates : <DATE_TIME_REF ID=""value"" TYPE=""value"" VALDATE1=""value""VALTIME1=""value"" VALDATE2=""value"" VALTIME2=""value""> expression </DATE_TIME_REF> Every expression is identified by a numeric ID . \n\t', '\n\t\t VALDATE# and VALTIME# store the range of dates and times obtained from the system , where VALDATE2 and VALTIME2 are only used to establish ranges . \n\t', '\n\t\t Furthermore , VALTIME1 could be omitted if a single date is specified . \n\t', '\n\t\t VALDATE2 , VALTIME1 and VALTIME2 are optional attributes . \n\t', '\n\t\t These temporal tags are the output of this module and they are used in the Answer Recomposition Unit in order to filter the individual answers obtained by the General Purpose Question-Answering system . \n\t', '\n\t\t The tags are working as temporal constraints . \n\t', '\n\t\t Following , a working example is introduced . \n\t', '\n\t\t Given the next question \x93Which U.S. ship was attacked by Israeli forces during the Six Day war in the sixties?\x94 : 1 . \n\t', '\n\t\t Firstly , the unit recognizes the temporal expression in the question , resolves and tags it , resulting in : <DATETIMEREF valdate1=""01/01/1960"" valdate2=""31/12/1969""> in the sixties </DATETIMEREF> 2 . \n\t', '\n\t\t The temporal constraint is that the date of the answers should be between the values valdate1 and valdate2 . \n\t', '\n\t\t 4.3 Question Splitter This task is only necessary when the type of the question , obtained by the Type Identification Module , is 3 or 4 . \n\t', '\n\t\t These questions are considered complex questions and need to be divided into simple ones ( Type 1 , Type 2 ) . \n\t', '\n\t\t The decomposition of a complex question is based on the identification of temporal signals , which relate simple events in the question and establish an order between the answers of the sub-questions . \n\t', '\n\t\t Finally , these signals are the output of this module and are described in next subsection . \n\t', '\n\t\t 4.3.1 Temporal Signals Temporal signals denote the relationship between the dates of the related events . \n\t', '\n\t\t Assuming that F1 is the date related to the first event in the question and F2 is the date related to the second event , the signal will establish an order between them . \n\t', '\n\t\t This we have named the ordering key . \n\t', '\n\t\t An example of some ordering keys is introduced in table 1 . \n\t', '\n\t\t SIGNAL ORDERING KEY After F1 > F2 When F1 = F2 Before F1 < F2 During F2i F2f <= F1 <= From F2 to F3 F2 <= F1 <= F3 About F2 -- -- F3 F2 <= F1 <= F3 On / in F1 = F2 While F2i F2f <= F1 <= For F2i F2f <= F1 <= At the time of F1 = F2 Since F1 > F2 Table 1 : Example of signals and ordering keys ~Y~E~ ~Y~E ~ ~Y~E~ ~Y~E~ ~~ES~~ON ~N~~YS~S ~~ES~ON ~~~ NO ~~O~ YES E~~~ESS~ON~ NO YES NO YES ~E~~O~~~ S~~N~~~ ~E~~O~~~ S~~N~~~ 4.3.2 Implementation One have divided each complex question into two parts , based on the temporal signal . \n\t', '\n\t\t The former is a simple question , therefore , no transformation is required . \n\t', '\n\t\t However , the latter ( the bit after the temporal signal ) needs transformation into a correct question pattern , always corresponding to a \x93When\x94 type-question . \n\t', '\n\t\t Moreover , three different kinds of question structures have been determined , being the transformation different for each of them . \n\t', '\n\t\t The implementation of this module is shown in figure 4 . \n\t', '\n\t\t Figure 4 : Decision tree for the Question Splitter The three possible cases are : The question that follows the temporal signal does not contain any verb , for example : \x93What happened to the world oil prices after the Iraqi annexation of Kuwait?\x94 In this case , our system returns the following transformation : \x93When did the Iraqi annexation of Kuwait occur?\x94 This case is the simplest , since the only transformation needed is adding the words \x93When did ... occur?\x94 to the second sentence . \n\t', '\n\t\t The question that follows the temporal signal contains a verb , but this verb is a gerund tense , for example : \x93Where did Bill Clinton study before going to Oxford University?\x94 In this case two previous steps to the transformation are necessary : 1 . \n\t', '\n\t\t Extracting the subject of the previous question . \n\t', '\n\t\t Converting the verb of the second sentence to infinitive tense . \n\t', '\n\t\t The final question returned by the system is : \x93When did Bill Clinton go to Oxford University ? \n\t', '\n\t\t \x94 . \n\t', '\n\t\t In the last type of transformation the second sentence in the question contains a tensed verb and its own subject , e.g. , \x93What did George Bush do after the U.N. Security Council ordered a global embargo on trade with Iraq?\x94 In this case , the infinitive and the tense of the sentence are obtained . \n\t', '\n\t\t Hence , the question results in the following form : \x93When did the U.N. Security Council order a global embargo on trade with Iraq?\x94 . \n\t', '\n\t\t 4.3.3 Example In the following example a part of the returned file of our Decomposition Unit is shown . \n\t', '\n\t\t 1.Where did Bill Clinton study before going to Oxford University ? \n\t', '\n\t\t Temporal Signal : before Q1 : Where did Bill Clinton study ? \n\t', '\n\t\t Q2 : When did Bill Clinton go to Oxford University ? \n\t', '\n\t\t 2.What did George Bush do after the U.N. Security Council ordered a global embargo on trade with Iraq in August 90 ? \n\t', '\n\t\t Temporal Signal : after Temporal Expression : in August 90 Q1 : What did George Bush do ? \n\t', '\n\t\t Q2 : When did the U.N. Security Council order a global embargo on trade with Iraq in August 90 ? \n\t', '\n\t\t DateQ2:[01/08/1990--31/08/1990] 3.When did Iraq invade Kuwait ? \n\t', '\n\t\t Temporal Signal : - Temporal Expression : - Q1 : When did Iraq invade Kuwait ? \n\t', '\n\t\t 4.Who became governor of New Hampshire in 1949 ? \n\t', '\n\t\t Temporal Signal : - Temporal Expression : in 1949 Q1 : Who became governor of New Hampshire in 1949 ? \n\t', '\n\t\t DateQ1:[01/01/1949--31/12/1949] 4.4 Decomposition Unit Evaluation This section presents an evaluation of the Decomposition Unit for the treatment of complex questions . \n\t', '\n\t\t For the evaluation a corpus of questions containing as many simple as complex questions is required . \n\t', '\n\t\t Due to the fact that question corpora used in TREC ( TREC , ) and CLEF ( CLEF , ) do not contain complex questions , the TERQAS question corpus has been chosen \n\t\t']",Positive
"['\n\t\t It consists of 123 temporal questions . \n\t', '\n\t\t YES NO ~RAN~OR~A~~ON ~~~ ~~~~ ~~~ ~~~~~ ~~EVERB ~SA GER~ND~ ~~ES~~ON ~~~ NO VERB ~N YES SECOND ~AR~~ OB~A~N~NG SUB]ECY O~ ~~ OB~A~N~NG ~~ ~N~~N~~~VE VERB ~RAN~OR~A~~ON ~~~ ~~~~ S~~~~~ ~~~~~ ~~~~~~~~ ~~~ ~RAN~OR~A~~ON ~~~ ~~~~ ~~ ~~~~ ~~~~~~~~ OB~A~N~NG ~~ ~N~~N~~~VE VERB TOTAL TREATED SUCCESSES PRECISION RECALL F- MEASURE TE Recognition and Resolu- tion 62 52 47 90 % 75 % 86 % Type Identification 112 112 104 92 % 100 % 93 % Signal Detection 17 14 14 100 % 82 % 95 % Question Splitter 17 14 12 85 % 71 % 81 % DECOMPOSITION UNIT 112 112 93 83 % 83 % 83 % Table 2 : Evaluation of the system From these , 11 were discarded due to requiring the need of a treatment beyond the capabilities of the system introduced hereby . \n\t', '\n\t\t Questions of the type : \x93Who was the second man on the moon\x94 can not be answered by applying the question decomposition . \n\t', '\n\t\t They need a special treatment . \n\t', '\n\t\t For the aforementioned phrase , this would consist of obtaining the names of all the men having been on the moon , ordering the dates and picking the second in the ordered list of names . \n\t', '\n\t\t Therefore , for this evaluation , we have just been focusing on trying to resolve the 112 left . \n\t', '\n\t\t The evaluation has been made manually by three annotators . \n\t', '\n\t\t Four different aspects of the unit have been considered : Recognition and resolution of Temporal Expressions : In this corpus , there were 62 temporal expressions and our system was able to recognize 52 , from which 47 were properly resolved by this module . \n\t', '\n\t\t Type Identification : There were 112 temporal questions in the corpus . \n\t', '\n\t\t Each of them was processed by the module , resulting in 104 properly identified according to the taxonomy proposed in section 2 . \n\t', '\n\t\t Signal Detection : In the corpus , there were 17 questions that were considered complex ( Type 3 and Type 4 ) . \n\t', '\n\t\t Our system was able to treat and recognize correctly the temporal signal of 14 of these questions . \n\t', '\n\t\t Question Splitter : From this set of 17 complex questions , the system was able to process 14 questions and divided properly 12 of them . \n\t', '\n\t\t The results , in terms of precision and recall are shown in Table 2 . \n\t', '\n\t\t In the evaluation , only 19 questions are wrongly pre-processed . \n\t', '\n\t\t Errors provoking a wrong pre-processing have been analyzed thoroughly : There were 8 errors in the identification of the type of the question and they were due to : \x96 Not treated TE or wrong TE recognition : 6 questions . \n\t', '\n\t\t \x96 Wrong Temporal Signal detection : 2 questions . \n\t', '\n\t\t There were 5 errors in the Question Splitter module : \x96 Wrong Temporal Signal detection : 3 questions . \n\t', '\n\t\t \x96 Syntactic parser problems : 2 questions . \n\t', '\n\t\t There were 15 errors not affecting the treatment of the question by the General Purpose Question Answering system . \n\t', '\n\t\t Nevertheless , they do affect the recomposition of the final answer . \n\t', '\n\t\t They are due to : \x96 Not treated TE or wrong TE recognition : 6 questions . \n\t', '\n\t\t \x96 Wrong temporal expression resolution : 9 questions . \n\t', '\n\t\t Some of these questions provoke more than one problem , causing that both , type identification and division turn to be wrong . \n\t', '\n\t\t 5 Conclusions This paper presents a new and intuitive method for answering complex temporal questions using an embedded current factual-based Q.A. system . \n\t', '\n\t\t The method proposed is based on a new procedure for the decomposition of temporal questions , where complex questions are divided into simpler ones by means of the detection of temporal signals . \n\t', '\n\t\t The TERSEO system , a temporal information extraction system applied to event ordering has been used to detect and resolve temporal expressions in questions and answers . \n\t', '\n\t\t Moreover , this work proposes a new multi- layered architecture that enables to solve complex questions by enhancing current Q.A. capabilities . \n\t', '\n\t\t The multi-layered approach can be applied to any kind of complex questions that allow question decomposition such as script questions , e.g. , \x93How do I assemble a bicycle?\x94 , or template-like questions , e.g. , \x93Which are the main biographical data ofNelson Mandela?\x94 . \n\t', '\n\t\t This paper has specifically focused on a process of decomposition of complex temporal questions and on its evaluation on a temporal question corpus . \n\t', '\n\t\t In the future , our work is directed to fine tune this system and increase its capabilities towards processing questions of higher complexity . \n\t', '\n\t\t References E. Breck , J. Burger , L. Ferro , W. Greiff , M . \n\t', '\n\t\t Light , I. Mani , and J. Rennie . \n\t', '\n\t\t 2000. Another sys called quanda . \n\t', '\n\t\t In Ninth Text REtrieval Conference , volume 500-249 of NIST Special Publication , pages 369\x96378 , Gaithersburg , USA , nov . \n\t', '\n\t\t National Institute of Standards and Technology . \n\t', '\n\t\t CLEF . \n\t', '\n\t\t Cross-language evaluation forum . \n\t', '\n\t\t http://clef.iei.pi.cnr.it/ . \n\t', '\n\t\t I. Mani and G. Wilson . \n\t', '\n\t\t 2000 . \n\t', '\n\t\t Robust temporal processing of news . \n\t', '\n\t\t In ACL , editor , Proceedings of the 38th Meeting of the Association of Computational Linguistics ( ACL 2000 ) , Hong Kong , October . \n\t', '\n\t\t J. Pustejovsky . \n\t', '\n\t\t 2002. Terqas:time and event recognition for question answering systems . \n\t', '\n\t\t http://time2002.org/ . \n\t', '\n\t\t D. Radev and B. Sundheim . \n\t', '\n\t\t 2002. Us- ing timeml in question answering . \n\t', '\n\t\t http://www.cs.brandeis.edu/ jamesp/ arda/ time/ documentation/ TimeML-use-in-qa-v1.0.pdf . \n\t', '\n\t\t E. Saquete , R. Munoz , and P. Mart´ínez-Barco . \n\t', '\n\t\t 2003. Terseo : Temporal expression resolution system applied to event ordering . \n\t', '\n\t\t In TSD , editor , Proceedings of the 6th International Conference , TSD 2003 , Text , Speech and Dialogue , pages 220\x96228 , Ceske Budejovice,Czech Republic , September . \n\t', '\n\t\t TREC . \n\t', '\n\t\t Text retrieval conference . \n\t', '\n\t\t http://trec.nist.gov/ . \n\t', '\n\t\t J. L. Vicedo and A. Ferr´andez . \n\t', '\n\t\t 2000. A semantic approach to question answering systems . \n\t', '\n\t\t In Ninth Text REtrieval Conference , volume 500- 249 of NIST Special Publication , pages 13\x9616 , Gaithersburg , USA , nov . \n\t', '\n\t\t National Institute of Standards and Technology . \n\t', '\n\t\t Question Answering using Constraint Satisfaction : QA-by-Dossier-with-Constraints John Prager T.J. Watson Research Ctr. Yorktown Heights N.Y. 10598 jprager@us.ibm.com Jennifer Chu-Carroll T.J. Watson Research Ctr. Yorktown Heights N.Y. 10598 jencc@us.ibm.com Krzysztof Czuba T.J. Watson Research Ctr. Yorktown Heights N.Y. 10598 kczuba@us.ibm.com Abstract QA-by-Dossier-with-Constraints is a new approach to Question Answering whereby candidate answers\x92 confidences are adjusted by asking auxiliary questions whose answers constrain the original answers . \n\t', '\n\t\t These constraints emerge naturally from the domain of interest , and enable application of real-world knowledge to QA . \n\t', '\n\t\t We show that our approach significantly improves system performance ( 75 % relative improvement in F-measure on select question types ) and can create a \x93dossier\x94 of information about the subject matter in the original question . \n\t', '\n\t\t 1 Introduction Traditionally , Question Answering ( QA ) has drawn on the fields of Information Retrieval , Natural Language Processing ( NLP ) , Ontologies , Data Bases and Logical Inference , although it is at heart a problem of NLP . \n\t', '\n\t\t These fields have been used to supply the technology with which QA components have been built . \n\t', '\n\t\t We present here a new methodology which attempts to use QA holistically , along with constraint satisfaction , to better answer questions , without requiring any advances in the underlying fields . \n\t', '\n\t\t Because NLP is still very much an error-prone process , QA systems make many mistakes ; accordingly , a variety of methods have been developed to boost the accuracy of their answers . \n\t', '\n\t\t Such methods include redundancy ( getting the same answer from multiple documents , sources , or algorithms ) , deep parsing of questions and texts ( hence improving the accuracy of confidence measures ) , inferencing ( proving the answer from information in texts plus background knowledge ) and sanity-checking ( veri fying that answers are consistent with known facts ) . \n\t', '\n\t\t To our knowledge , however , no QA system deliberately asks additional questions in order to derive constraints on the answers to the original questions . \n\t', '\n\t\t We have found empirically that when our own QA system\x92s \n\t\t']",Negative
"['\n\t\t In other words , the correct answer is in the passages retrieved by the search engine , but the system was unable to sufficiently promote the correct answer and/or deprecate the incorrect ones . \n\t', '\n\t\t Our new approach of QA-by-Dossier-with-Constraints ( QDC ) uses the answers to additional questions to provide more information that can be used in ranking candidate answers to the original question . \n\t', '\n\t\t These auxiliary questions are selected such that natural constraints exist among the set of correct answers . \n\t', '\n\t\t After issuing both the original question and auxiliary questions , the system evaluates all possible combinations of the candidate answers and scores them by a simple function of both the answers\x92 intrinsic confidences , and how well the combination satisfies the aforementioned constraints . \n\t', '\n\t\t Thus we hope to improve the accuracy of an essentially NLP task by making an end-run around some of the more difficult problems in the field . \n\t', '\n\t\t We describe QDC and experiments to evaluate its effectiveness . \n\t', '\n\t\t Our results show that on our test set , substantial improvement is achieved by using constraints , compared with our baseline system , using standard evaluation metrics . \n\t', '\n\t\t 2 Related Work Logic and inferencing have been a part of Question-Answering since its earliest days . \n\t', '\n\t\t The first such systems employed natural-language interfaces to expert systems , e.g. SHRDLU \n\t\t']",Positive
"['\n\t\t CHAT-80 ( Warren & Pereira , 1982 ) was a DCG-based NLquery system about world geography , entirely in Prolog . \n\t', '\n\t\t In these systems , the NL question is transformed into a semantic form , which is then processed further ; the overall architecture and system operation is very different from today\x92s systems , however , primarily in that there is no text corpus to process . \n\t', '\n\t\t Inferencing is used in at least two of the more visible systems of the present day . \n\t', '\n\t\t The LCC system ( Moldovan & Rus , 2001 ) uses a Logic Prover to establish the connection between a candidate answer passage and the question . \n\t', '\n\t\t Text terms are converted to logical forms , and the question is treated as a goal which is \x93proven\x94 , with real-world knowledge being provided by Extended WordNet . \n\t', '\n\t\t The IBM system PIQUANT \n\t\t']",Positive
"['\n\t\t Cyc can in some cases confirm or reject candidate answers based on its own store of instance information ; in other cases , primarily of a numerical nature , Cyc can confirm whether candidates are within a reasonable range established for their subtype . \n\t', '\n\t\t At a more abstract level , the use of constraints discussed in this paper can be viewed as simply an example of finding support ( or lack of it ) for candidate answers . \n\t', '\n\t\t Many current systems ( see , e.g. \n\t\t']",Positive
"['\n\t\t Finally , our approach is somewhat reminiscent of the scripts introduced by Schank ( Schank et al. , 1975 , and see also Lehnert , 1978 ) . \n\t', '\n\t\t In order to generate meaningful auxiliary questions and constraints , we need a model ( \x93script\x94 ) of the situation the question is about . \n\t', '\n\t\t Among others , we have identified one such script modeling the human life cycle that seems common to different question types regarding people . \n\t', '\n\t\t 3 Introducing QDC QA-by-Dossier-with-Constraints is an extension of on-going work of ours called QA-by-Dossier ( QbD ) \n\t\t']",Positive
"['\n\t\t In the latter , definitional questions of the form \x93Who/What is X\x94 are answered by asking a set of specific factoid questions about properties of X . \n\t', '\n\t\t So if X is a person , for example , these auxiliary questions may be about important dates and events in the person\x92s life-cycle , as well as his/her achievement . \n\t', '\n\t\t Likewise , question sets can be developed for other entities such as organizations , places and things . \n\t', '\n\t\t QbD employs the notion of follow-on questions . \n\t', '\n\t\t Given an answer to a first-round question , the system can ask more specific questions based on that knowledge . \n\t', '\n\t\t For example , on discovering a person\x92s profession , it can ask occupation-specific follow-on questions : if it finds that people are musicians , it can ask what they have composed , if it finds they are explorers , then what they have discovered , and so on . \n\t', '\n\t\t QA-by-Dossier-with-Constraints extends this approach by capitalizing on the fact that a set of answers about a subject must be mutually consistent , with respect to constraints such as time and geography . \n\t', '\n\t\t The essence of the QDC approach is to initially return instead of the best answer to appropriately selected factoid questions , the top n answers ( we use n=5 ) , and to choose out of this top set the highest confidence answer combination that satisfies consistency constraints . \n\t', '\n\t\t We illustrate this idea by way of the example , \x93When did Leonardo da Vinci paint the Mona Lisa?\x94 . \n\t', '\n\t\t Table 1 shows our system\x92s top answers to this question , with associated scores in the range 0-1 . \n\t', '\n\t\t Score Painting Date 1 .64 2000 2 .43 1988 3 .34 1911 4 .31 1503 5 .30 1490 Table 1 . \n\t', '\n\t\t Answers for \x93When did Leonardo da Vinci paint the Mona Lisa?\x94 The correct answer is \x931503\x94 , which is in 4th place , with a low confidence score . \n\t', '\n\t\t Using QA-byDossier , we ask two related questions \x93When was Leonardo da Vinci born?\x94 and \x93When did Leonardo da Vinci die?\x94 The answers to these auxiliary questions are shown in Table 2 . \n\t', '\n\t\t Given common knowledge about a person\x92s life expectancy and that a painting must be produced while its author is alive , we observe that the best dates proposed in Table 2 consistent with one another are that Leonardo da Vinci was born in 1452 , died in 1519 , and painted the Mona Lisa in 1503 . \n\t', '\n\t\t [ The painting date of 1490 also satisfies the constraints , but with a lower confidence . \n\t', '\n\t\t ] We will examine the exact constraints used a little later . \n\t', '\n\t\t This example illustrates how the use of auxiliary questions helps constrain answers to the original question , and promotes correct answers with initial low confidence scores . \n\t', '\n\t\t As a side-effect , a short dossier is produced . \n\t', '\n\t\t Score Born Score Died 1 .66 1452 .99 1519 2 .12 1519 .98 1989 3 .04 1920 .96 1452 4 .04 1987 .60 1988 5 .04 1501 .60 1990 Table 2 . \n\t', '\n\t\t Answers for auxiliary questions \x93When was Leonardo da Vinci born?\x94 and \x93When did Leonardo da Vinci die?\x94 . \n\t', '\n\t\t 3.1 Reciprocal Questions QDC also employs the notion of reciprocal questions . \n\t', '\n\t\t These are a type of follow-on question used solely to provide constraints , and do not add to the dossier . \n\t', '\n\t\t The idea is simply to double-check the answer to a question by inverting it , substituting the first-round answer and hoping to get the original subject back . \n\t', '\n\t\t For example , to double-check \x93Sacramento\x94 as the answer to \x93What is the capital of California?\x94 we would ask \x93Of what state is Sacramento the capital?\x94 . \n\t', '\n\t\t The reciprocal question would be asked of all of the candidate answers , and the confidences of the answers to the reciprocal questions would contribute to the selection of the optimum answer . \n\t', '\n\t\t We will discuss later how this reciprocation may be done automatically . \n\t', '\n\t\t In a separate study of reciprocal questions \n\t\t']",Positive
"['\n\t\t Although the reciprocal questions seem to be symmetrical and thus redundant , their power stems from the differences in the search for answers inherent in our system . \n\t', '\n\t\t The search is primarily based on the expected answer type ( STATE vs. CAPITAL in the above example ) . \n\t', '\n\t\t This results in different document sets being passed to the answer selection module . \n\t', '\n\t\t Subsequently , the answer selection module works with a different set of syntactic and semantic relationships , and the process of asking a reciprocal question ends up looking more like the process of asking an independent one . \n\t', '\n\t\t The only difference between this and the \x93regular\x94 QDC case is in the type of constraint applied to resolve the resulting answer set . \n\t', '\n\t\t 3.2 Applying QDC In order to automatically apply QDC during question answering , several problems need to be addressed . \n\t', '\n\t\t First , criteria must be developed to determine when this process should be invoked . \n\t', '\n\t\t Second , we must identify the set of question types that would potentially benefit from such an ap proach , and , for each question type , develop a set of auxiliary questions and appropriate constraints among the answers . \n\t', '\n\t\t Third , for each question type , we must determine how the results of applying constraints should be utilized . \n\t', '\n\t\t 3.2.1 When to apply QDC To address these questions we must distinguish between \x93planned\x94 and \x93ad-hoc\x94 uses of QDC . \n\t', '\n\t\t For answering definitional questions ( \x93Who/what is X?\x94 ) of the sort used in TREC2003 , in which collections of facts can be gathered by QA-by-Dossier , we can assume that QDC is always appropriate . \n\t', '\n\t\t By defining broad enough classes of entities for which these questions might be asked ( e.g. people , places , organizations and things , or major subclasses of these ) , we can for each of these classes manually establish once and for all a set of auxiliary questions for QbD and constraints for QDC . \n\t', '\n\t\t This is the approach we have taken in the experiments reported here . \n\t', '\n\t\t We are currently working on automatically learning effective auxiliary questions for some of these classes . \n\t', '\n\t\t In a more ad-hoc situation , we might imagine that a simple variety of QDC will be invoked using solely reciprocal questions whenever the difference between the scores of the first and second answer is below a certain threshold . \n\t', '\n\t\t 3.2.2 How to apply QDC We will posit three methods of generating auxiliary question sets : o By hand o Through a structured repository , such as a knowledge-base of real-world information o Through statistical techniques tied to a machine- learning algorithm , and a text corpus . \n\t', '\n\t\t We think that all three methods are appropriate , but we initially concentrate on the first for practical reasons . \n\t', '\n\t\t Most TREC-style factoid questions are about people , places , organizations , and things , and we can generate generic auxiliary question sets for each of these classes . \n\t', '\n\t\t Moreover , the purpose of this paper is to explain the QDC methodology and to investigate its value . \n\t', '\n\t\t 3.2.3 Constraint Networks The constraints that apply to a given situation can be naturally represented in a network , and we find it useful for visualization purposes to depict the constraints graphically . \n\t', '\n\t\t In such a graph the entities and values are represented as nodes , and the constraints and questions as edges . \n\t', '\n\t\t It is not clear how possible , or desirable , it is to automatically develop such constraint networks ( other than the simple one for reciprocal questions ) , since so much real-world knowledge seems to be required . \n\t', '\n\t\t To illustrate , let us look at the constraints required for the earlier example . \n\t', '\n\t\t A more complex constraint system is used in our experiments described later . \n\t', '\n\t\t For our Leonardo da Vinci example , the set of constraints applied can be expressed as follows1 : Date(Died) <= Date(Born) + 100 Date(Painting) >= Date(Born) + 7 Date(Painting) <= Date(Died) The corresponding graphical representation is in Figure 1 . \n\t', '\n\t\t Although the numerical constants in these constraints betray a certain arbitrariness , we found it a useful practice to find a middle ground between absolute minima or maxima that the values can achieve and their likely values . \n\t', '\n\t\t Furthermore , although these constraints are manually derived for our prototype system , they are fairly general for the human life-cycle and can be easily reused for other , similar questions , or for more complex dossiers , as described below . \n\t', '\n\t\t Figure 1. Constraint Network for Leonardo example . \n\t', '\n\t\t Dashed lines represent question-answer pairs , solid lines constraints between the answers . \n\t', '\n\t\t We also note that even though a constraint network might have been inspired by and centered around a particular question , once the network is established , any question employed in it could be the end-user question that triggers it . \n\t', '\n\t\t There exists the ( general ) problem of when more than one set of answers satisfies our constraints . \n\t', '\n\t\t Our approach is to combine the first-round scores of the individual answers to provide a score for the dossier as a whole . \n\t', '\n\t\t There are several ways to do this , and we found experimentally that it does not appear critical exactly how this is done . \n\t', '\n\t\t In the example in the evaluation we mention one particular combination algorithm . \n\t', '\n\t\t 3.2.4 Kinds of constraint network There are an unlimited number of possible constraint networks that can be constructed . \n\t', '\n\t\t We have experimented with the following : Timelines . \n\t', '\n\t\t People and even artifacts have life- cycles . \n\t', '\n\t\t The examples in this paper exploit these . \n\t', '\n\t\t 1 Painting is only an example of an activity in these constraints . \n\t', '\n\t\t Any other achievement that is usually associated with adulthood can be used . \n\t', '\n\t\t Geographic ( \x93Where is X\x94 ) . \n\t', '\n\t\t Neighboring entities are in the same part of the world . \n\t', '\n\t\t Kinship ( \x93Who is married to X\x94 ) . \n\t', '\n\t\t Most kinship relationships have named reciprocals e.g. husband- wife , parent-child , and cousin-cousin . \n\t', '\n\t\t Even though these are not in practice one-one relationships , we can take advantage of sufficiency even if necessity is not entailed . \n\t', '\n\t\t Definitional ( \x93What is X?\x94 , \x93What does XYZ stand for?\x94 ) For good definitions , a term and its definition are interchangeable . \n\t', '\n\t\t Part-whole . \n\t', '\n\t\t Sizes of parts are no bigger than sizes of wholes . \n\t', '\n\t\t This fact can be used for populations , areas , etc. 3.2.5 QDC potential We performed a manual examination of the 500 TREC2002 questions2 to see for how many of these questions the QDC framework would apply . \n\t', '\n\t\t Being a manual process , these numbers provide an upper bound on how well we might expect a future automatic process to work . \n\t', '\n\t\t We noted that for 92 questions ( 18 % ) a nontrivial constraint network of the above kinds would apply . \n\t', '\n\t\t For a total of 454 questions ( 91 % ) , a simple reciprocal constraint could be generated . \n\t', '\n\t\t However , for 61 of those , the reciprocal question was sufficiently non-specific that the sought reciprocal answer was unlikely to be found in a reasonably-sized hit-list . \n\t', '\n\t\t For example , the reciprocal question to \x93How did Mickey Mantle die?\x94 would be \x93Who died of cancer?\x94 However , we can imagine using other facts in the dossier to craft the question , giving us \x93What famous baseball player ( or Yankees player ) died of cancer?\x94 , giving us a much better chance of success . \n\t', '\n\t\t For the simple reciprocation , though , subtracting these doubtful instances leaves 79 % of the questions appearing to be good candidates for QDC . \n\t', '\n\t\t 4 Experimental Setup 4.1 Test set generation To evaluate QDC , we had our system develop dossiers of people in the creative arts , unseen in previous TREC questions . \n\t', '\n\t\t However , we wanted to use the personalities in past TREC questions as independent indicators of appropriate subject matter . \n\t', '\n\t\t Therefore we collected all of the \x93creative\x94 people in the TREC9 question set , and divided them up into classes by profession , so we had , for example , male singers Bob Marley , Ray Charles , Billy Joel and Alice Cooper ; poets William Wordsworth and Langston Hughes ; painters Picasso , Jackson Pollock 2 This set did not contain definition questions , which , by our inspection , lend themselves readily to reciprocation . \n\t', '\n\t\t Birthdate Leonardo Painting Deathdate and Vincent Van Gogh , etc. \x96 twelve such groupings in all . \n\t', '\n\t\t For each set , we entered the individuals in the \x93Google Sets\x94 interface ( http://labs.google.com/sets ) , which finds \x93similar\x94 entities to the ones entered . \n\t', '\n\t\t For example , from our set of male singers it found : Elton John , Sting , Garth Brooks , James Taylor , Phil Collins , Melissa Etheridge , Alanis Morissette , Annie Lennox , Jackson Browne , Bryan Adams , Frank Sinatra and Whitney Houston . \n\t', '\n\t\t Altogether , we gathered 276 names of creative individuals this way , after removing duplicates , items that were not names of individuals , and names that did not occur in our test corpus ( the AQUAINT corpus ) . \n\t', '\n\t\t We then used our system manually to help us develop \x93ground truth\x94 for a randomly selected subset of 109 names . \n\t', '\n\t\t This ground truth served both as training material and as an evaluation key . \n\t', '\n\t\t We split the 109 names randomly into a set of 52 for training and 57 for testing . \n\t', '\n\t\t The training process used a hill-climbing method to find optimal values for three internal rejection thresholds . \n\t', '\n\t\t In developing the ground truth we might have missed some instances of assertions we were looking for , so the reported recall ( and hence F-measure ) figures should be considered to be upper bounds , but we believe the calculated figures are not far from the truth . \n\t', '\n\t\t 4.2 QDC Operation The system first asked three questions for each subject X : In what year was X born ? \n\t', '\n\t\t In what year did X die ? \n\t', '\n\t\t What compositions did X have ? \n\t', '\n\t\t The third of these triggers our named-entity type COMPOSITION that is used for all kinds of titled works \x96 books , films , poems , music , plays and so on , and also quotations . \n\t', '\n\t\t Our named-entity recognizer has rules to detect works of art by phrases that are in apposition to \x93the film ...\x94 or the \x93the book ...\x94 etc. , and also captures any short phrase in quotes beginning with a capital letter . \n\t', '\n\t\t The particular question phrasing we used does not commit us to any specific creative verb . \n\t', '\n\t\t This is of particular importance since it very frequently happens in text that titled works are associated with their creators by means of a possessive or parenthetical construction , rather than subject-verb-object . \n\t', '\n\t\t The top five answers , with confidences , are returned for the born and died questions ( subject to also passing a confidence threshold test ) . \n\t', '\n\t\t The compositions question is treated as a list question , meaning that all answers that pass a certain threshold are returned . \n\t', '\n\t\t For each such returned work Wi , two additional questions are asked : What year did X have Wi ? \n\t', '\n\t\t Who had Wi ? \n\t', '\n\t\t The top 5 answers to each of these are returned , again as long as they pass a confidence threshold . \n\t', '\n\t\t We added a sixth answer \x93NIL\x94 to each of the date sets , with a confidence equal to the rejection threshold . \n\t', '\n\t\t ( NIL is the code used in TREC ever since TREC10 to indicate the assertion that there is no answer in the corpus . \n\t', '\n\t\t ) We used a two stage constraint-satisfaction process : Stage 1 : For each work Wi for subject X , we added together its original confidence to the confidence of the answer X in the answer set of the reciprocal question ( if it existed \x96 otherwise we added zero ) . \n\t', '\n\t\t If the total did not exceed a learned threshold ( .50 ) the work was rejected . \n\t', '\n\t\t Stage 2 . \n\t', '\n\t\t For each subject , with the remaining candidate works we generated all possible combinations of the date answers . \n\t', '\n\t\t We rejected any combination that did not satisfy the following constraints : DIED >= BORN + 7 DIED <= BORN + 100 WORK >= BORN + 7 WORK <= BORN + 100 WORK <= DIED DIED <= WORK + 100 The apparent redundancy here is because of the potential NIL answers for some of the date slots . \n\t', '\n\t\t We also rejected combinations of works whose years spanned more than 100 years ( in case there were no BORN or DIED dates ) . \n\t', '\n\t\t In performing these constraint calculations , NIL satisfied every test by fiat . \n\t', '\n\t\t The constraint network we used is depicted in Figure 2 . \n\t', '\n\t\t Figure 2. Constraint Network for evaluation example . \n\t', '\n\t\t Dashed lines represent question-answer pairs , solid lines constraints between the answers . \n\t', '\n\t\t We used as a test corpus the AQUAINT corpus used in TREC-QA since 2002 . \n\t', '\n\t\t Since this was not the same corpus from which the test questions were generated ( the Web ) , we acknowledged that there might be some difference in the most common spelling of certain names , but we made no attempt to correct for this . \n\t', '\n\t\t Neither did we attempt to normalize , translate or aggregate names of the titled works that were returned , so that , for example , \x93Well- Author X Xi = Author of Wi Work Wi Deathdate of X Date of Wi Birthdate of X Tempered Klavier\x94 and \x93Well-Tempered Clavier\x94 were treated as different . \n\t', '\n\t\t Since only individuals were used in the question set , we did not have instances of problems we saw in training , such as where an ensemble ( such as The Beatles ) created a certain piece , which in turn via the reciprocal question was found to have been written by a single person ( Paul McCartney ) . \n\t', '\n\t\t The reverse situation was still possible , but we did not handle it . \n\t', '\n\t\t We foresee a future version of our system having knowledge of ensembles and their composition , thus removing this restriction . \n\t', '\n\t\t In general , a variety of ontological relationships could occur between the original individual and the discovered performer(s) of the work . \n\t', '\n\t\t We generated answer keys by reading the passages that the system had retrieved and from which the answers were generated , to determine \x93truth\x94 . \n\t', '\n\t\t In cases of absent information in these passages , we did our own corpus searches . \n\t', '\n\t\t This of course made the issue of evaluation of recall only relative , since we were not able to guarantee we had found all existing instances . \n\t', '\n\t\t We encountered some grey areas , e.g. , if a painting appeared in an exhibition or if a celebrity endorsed a product , then should the exhibition\x92s or product\x92s name be considered an appropriate \x93work\x94 of the artist ? \n\t', '\n\t\t The general perspective adopted was that we were not establishing or validating the nature of the relationship between an individual and a creative work , but rather its existence . \n\t', '\n\t\t We answered \x93yes\x94 if we subjectively felt the association to be both very strong and with the individual\x92s participation \x96 for example , Pamela Anderson and Playboy . \n\t', '\n\t\t However , books/plays about a person or dates of performances of one\x92s work were considered incorrect . \n\t', '\n\t\t As we shall see , these decisions would not have a big impact on the outcome . \n\t', '\n\t\t 4.3 Effect of Constraints The answers collected from these two rounds of questions can be regarded as assertions about the subject X . \n\t', '\n\t\t By applying constraints , two possible effects can occur to these assertions : 1 . \n\t', '\n\t\t Some works can get thrown out . \n\t', '\n\t\t 2. An asserted date ( which was the top candidate from its associated question ) can get replaced by a candidate date originally in positions 2-6 ( where sixth place is NIL ) Effect #1 is expected to increase precision at the risk of worsening recall ; effect #2 can go either way . \n\t', '\n\t\t We note that NIL , which is only used for dates , can be the correct answer if the desired date assertion is absent from the corpus ; NIL is considered a \x93value\x94 in this evaluation . \n\t', '\n\t\t By inspection , performances and other indirect works ( discussed in the previous section ) were usu ally associated with the correct artist , so our decision to remove them from consideration resulted in a decrease in both the numerator and denominator of the precision and recall calculations , resulting in a minimal effect . \n\t', '\n\t\t The results of applying QDC to the 57 test individuals are summarized in Table 3 . \n\t', '\n\t\t The baseline assertions for individual X were : \x95 Top-ranking birthdate/NIL \x95 Top-ranking deathdate/NIL \x95 Set of works WZ that passed threshold \x95 Top-ranking date for WZ /NIL The sets of baseline assertions ( by individual ) are in effect the results of QA-by-Dossier WITHOUT Constraints ( QbD ) . \n\t', '\n\t\t Assertions Micro-Average Macro-Average Total Cor- rect Tru- th Prec Rec F Prec Rec F Base-1671 line 517 933 .309 .554 .396 .331 .520 .386 QDC 1417 813 933 .573 .871 .691 .603 .865 .690 Table 3 . \n\t', '\n\t\t Results of Performance Evaluation . \n\t', '\n\t\t Two calculations of P/R/F are made , depending on whether the averaging is done over the whole set , or first by individual ; the results are very similar . \n\t', '\n\t\t The QDC assertions were the same as those for QbD , but reflecting the following effects : \x95 Some { WZ , date } pairs were thrown out ( 3 out of 14 on average ) \x95 Some dates in positions 2-6 moved up ( applicable to birth , death and work dates ) The results show improvement in both precision and recall , in turn determining a 75-80 % relative increase in F-measure . \n\t', '\n\t\t 5 Discussion This exposition of QA-by-Dossier-with- Constraints is very short and undoubtedly leaves may questions unanswered . \n\t', '\n\t\t We have not presented a precise method for computing the QDC scores . \n\t', '\n\t\t One way to formalize this process would be to treat it as evidence gathering and interpret the results in a Bayesian-like fashion . \n\t', '\n\t\t The original system confidences would represent prior probabilities reflecting the system\x92s belief that the answers are correct . \n\t', '\n\t\t As more evidence is found , the confidences would be updated to reflect the changed likelihood that an answer is correct . \n\t', '\n\t\t We do not know a priori how much \x93slop\x94 should be allowed in enforcing the constraints , since auxiliary questions are as likely to be answered incor- rectly as the original ones . \n\t', '\n\t\t A further problem is to determine the best metric for evaluating such approaches , which is a question for QA in general . \n\t', '\n\t\t The task of generating auxiliary questions and constraint sets is a matter of active research . \n\t', '\n\t\t Even for simple questions like the ones considered here , the auxiliary questions and constraints we looked at were different and manually chosen . \n\t', '\n\t\t Hand-crafting a large number of such sets might not be feasible , but it is certainly possible to build a few for common situations , such as a person\x92s life-cycle . \n\t', '\n\t\t More generally , QDC could be applied to situations in which a certain structure is induced by natural temporal ( our Leonardo example ) and/or spatial constraints , or by properties of the relation mentioned in the question ( evaluation example ) . \n\t', '\n\t\t Temporal and spatial constraints appear general to all relevant question types , and include relations of precedence , inclusion , etc. . \n\t', '\n\t\t For certain relationships , there are naturally- occurring reciprocals ( if X is married to Y , then Y is married to X ; if X is a child of Y then Y is a parent of X ; compound-term to acronym and vice versa ) . \n\t', '\n\t\t Transitive relationships ( e.g. greater-than , located- in , etc. ) offer the immediate possibility of constraints , but this avenue has not yet been explored . \n\t', '\n\t\t 5.1 Automatic Generation of Reciprocal Questions While not done in the work reported here , we are looking at generating reciprocal questions automatically . \n\t', '\n\t\t Consider the following transformations : \x93What is the capital of California?\x94 -> \x93Of what state is <candidate> the capital?\x94 \x93What is Frank Sinatra\x92s nickname?\x94 -> \x93Whose ( or what person\x92s ) nickname is <candidate>?\x94 \x93How deep is Crater Lake?\x94 -> \x93What ( or what lake ) is <candidate> deep?\x94 \x93Who won the Oscar for best actor in 1970?\x94 -> \x93In what year did <candidate> win the Oscar for best actor?\x94 ( and/or \x93What award did <candidate> win in 1970?\x94 ) These are precisely the transformations necessary to generate the auxiliary reciprocal questions from the given original questions and candidate answers to them . \n\t', '\n\t\t Such a process requires identifying an entity in the question that belongs to a known class , and substituting the class name for the entity . \n\t', '\n\t\t This entity is made the subject of the question , the previous subject ( or trace ) being replaced by the candidate answer . \n\t', '\n\t\t We are looking at parse-tree rather than string transformations to achieve this . \n\t', '\n\t\t This work will be reported in a future paper . \n\t', '\n\t\t 5.2 Final Thoughts Despite these open questions , initial trials with QA-by-Dossier-with-Constraints have been very encouraging , whether it is by correctly answering previously missed questions , or by improving confidences of correct answers . \n\t', '\n\t\t An interesting question is when it is appropriate to apply QDC . \n\t', '\n\t\t Clearly , if the base QA system is too poor , then the answers to the auxiliary questions will be useless ; if the base system is highly accurate , the increase in accuracy will be negligible . \n\t', '\n\t\t Thus our approach seems most beneficial to middle-performance levels , which , by inspection of TREC results for the last 5 years , is where the leading systems currently lie . \n\t', '\n\t\t We had initially thought that use of constraints would obviate the need for much of the complexity inherent in NLP . \n\t', '\n\t\t As mentioned earlier , with the case of \x93The Beatles\x94 being the reciprocal answer to the auxiliary composition question to \x93Who is Paul McCartney?\x94 , we see that structured , ontological information would benefit QDC . \n\t', '\n\t\t Identifying alternate spellings and representations of the same name ( e.g. Clavier/Klavier , but also taking care of variations in punctuation and completeness ) is also necessary . \n\t', '\n\t\t When we asked \x93Who is Ian Anderson?\x94 , having in mind the singer-flautist for the Jethro Tull rock band , we found that he is not only that , but also the community investment manager of the English conglomerate Whitbread , the executive director of the U.S. Figure Skating Association , a writer for New Scientist , an Australian medical advisor to the WHO , and the general sales manager of Houseman , a supplier of water treatment systems . \n\t', '\n\t\t Thus the problem of word sense disambiguation has returned in a particularly nasty form . \n\t', '\n\t\t To be fully effective , QDC must be configured not just to find a consistent set of properties , but a number of independent sets that together cover the highest-confidence returned answers 3 . \n\t', '\n\t\t Altogether , we see that some of the very problems we aimed to skirt are still present and need to be addressed . \n\t', '\n\t\t However , we have shown that even disregarding these issues , QDC was able to provide substantial improvement in accuracy . \n\t', '\n\t\t 6 Summary We have presented a method to improve the accuracy of a QA system by asking auxiliary questions for which natural constraints exist . \n\t', '\n\t\t Using these constraints , sets of mutually consistent answers can be generated . \n\t', '\n\t\t We have explored questions in the biographical areas , and identified other areas of applicability . \n\t', '\n\t\t We have found that our methodology exhibits a double advantage : not only can it im- 3 Possibly the smallest number of sets that provide such coverage . \n\t', '\n\t\t prove QA accuracy , but it can return a set of mutually-supporting assertions about the topic of the original question . \n\t', '\n\t\t We have identified many open questions and areas of future work , but despite these gaps , we have shown an example scenario where QA-by-Dossier-with-Constraints can improve the F- measure by over 75 % . \n\t', '\n\t\t 7 Acknowledgements We wish to thank Dave Ferrucci , Elena Filatova and Sasha Blair-Goldensohn for helpful discussions . \n\t', ""\n\t\t This work was supported in part by the Advanced Research and Development Activity (ARDA) 's Advanced Question Answering for Intelligence ( AQUAINT ) Program under contract number MDA904-01-C-0988 . \n\t"", '\n\t\t References Chu-Carroll , J. , J. Prager , C. Welty , K. Czuba and D. Ferrucci . \n\t', '\n\t\t \x93A Multi-Strategy and Multi-Source Approach to Question Answering\x94 , Proceedings of the 11 th TREC , 2003 . \n\t', '\n\t\t Clarke , C. , Cormack , G. , Kisman , D .. and Lynam , T. \x93Question answering by passage selection ( Multitext experiments for TREC-9)\x94 in Proceedings of the 9th TREC , pp. 673-683 , 2001 . \n\t', '\n\t\t Hendrix , G. , E. Sacerdoti , D. Sagalowicz , J. Slocum : Developing a Natural Language Interface to Complex Data . \n\t', '\n\t\t VLDB 1977 : 292 Lehnert , W . \n\t', '\n\t\t The Process of Question Answering . \n\t', '\n\t\t A Computer Simulation of Cognition . \n\t', '\n\t\t Lawrence Erlbaum Associates , Publishers , 1978 . \n\t', '\n\t\t Lenat , D. 1995 . \n\t', '\n\t\t "" Cyc : A Large-Scale Investment in Knowledge Infrastructure . \n\t', '\n\t\t "" Communications of the ACM 38 , no . \n\t', '\n\t\t 11. Moldovan , D. and V. Rus , \x93Logic Form Transformation of WordNet and its Applicability to Question Answering\x94 , Proceedings of the ACL , 2001 . \n\t', '\n\t\t Prager , J. , E. Brown , A. Coden , and D. Radev . \n\t', '\n\t\t 2000. "" Question-Answering by Predictive Annotation\x94 . \n\t', '\n\t\t In Proceedings of SIGIR 2000 , pp. 184-191 . \n\t', '\n\t\t Prager , J. , J. Chu-Carroll and K. Czuba , "" A Multi- Agent Approach to using Redundancy and Reinforcement in Question Answering "" in New Directions in Question -Answering , Maybury , M. ( Ed . \n\t', '\n\t\t ) , to appear in 2004 . \n\t', '\n\t\t Schank , R. and R. Abelson . \n\t', '\n\t\t \x93Scripts , Plans and Knowledge\x94 , Proceedings of IJCAI\x9275 . \n\t', '\n\t\t Voorhees , E. \x93Overview of the TREC 2002 Question Answering Track\x94 , Proceedings of the 11th TREC , 2003 . \n\t', '\n\t\t Warren , D. , and F. Pereira "" An efficient easily adaptable system for interpreting natural language queries , "" Computational Linguistics , 8:3-4 , 110- 122 , 1982 . \n\t', '\n\t\t Winograd , T. Procedures as a representation for data in a computer program for under-standing natural language . \n\t', '\n\t\t Cognitive Psychology , 3(1) , 1972 . \n\t', '\n\t\t Woods , W. Progress in natural language understanding --- an application in lunar geology . \n\t', '\n\t\t Proceedings of the 1973 National Computer Conference , AFIPS Conference Proceedings , Vol. 42 , 441-- 450 , 1973 . \n\t', '\n\t\t Applying Machine Learning to Chinese Temporal Relation Resolution Wenjie Li Department of Computing The Hong Kong Polytechnic University , Hong Kong cswjli@comp.polyu.edu.hk Guihong Cao Department of Computing The Hong Kong Polytechnic University , Hong Kong csghcao@comp.polyu.edu.hk Kam-Fai Wong Department of Systems Engineering and Engineering Management The Chinese University of Hong Kong , Hong Kong kfwong@se.cuhk.edu.hk Chunfa Yuan Department of Computer Science and Technology Tsinghua University , Beijing , China . \n\t', '\n\t\t cfyuan@tsinghua.edu.cn Abstract Temporal relation resolution involves extraction of temporal information explicitly or implicitly embedded in a language . \n\t', '\n\t\t This information is often inferred from a variety of interactive grammatical and lexical cues , especially in Chinese . \n\t', '\n\t\t For this purpose , inter-clause relations ( temporal or otherwise ) in a multiple-clause sentence play an important role . \n\t', '\n\t\t In this paper , a computational model based on machine learning and heterogeneous collaborative bootstrapping is proposed for analyzing temporal relations in a Chinese multiple-clause sentence . \n\t', '\n\t\t The model makes use of the fact that events are represented in different temporal structures . \n\t', '\n\t\t It takes into account the effects of linguistic features such as tense/aspect , temporal connectives , and discourse structures . \n\t', '\n\t\t A set of experiments has been conducted to investigate how linguistic features could affect temporal relation resolution . \n\t', '\n\t\t 1 Introduction In language studies , temporal information describes changes and time of changes expressed in a language . \n\t', '\n\t\t Such information is critical in many typical natural language processing ( NLP ) applications , e.g. language generation and machine translation , etc. . \n\t', '\n\t\t Modeling temporal aspects of an event in a written text is more complex than capturing time in a physical time-stamped system . \n\t', ""\n\t\t Event time may be specified explicitly in a sentence , e.g. \x93JAIM , , 1997 ~M &-T=Arin ' ZA Ip7E \n\t\t""]",Positive
"[""\n\t\t For example , one may know that \x93~~~'Z~~~~JAIM&-T=A ri n'ZAIp7E ( after the street bridge had been built , they solved the traffic problem of the city)\x94 , yet without knowing the exact time when the street bridge was built . \n\t"", '\n\t\t As reported by Partee \n\t\t']",Positive
"['\n\t\t The objective of relative temporal relation resolution is to determine the type of relative relation embedded in a sentence . \n\t', '\n\t\t In English , temporal expressions have been widely studied . \n\t', '\n\t\t Lascarides and Asher \n\t\t']",Positive
"['\n\t\t They investigated various contextual effects on five discourse relations ( namely narration , elaboration , explanation , background and result ) and then corresponded each of them to a kind of temporal relations . \n\t', '\n\t\t Hitzeman et al . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t They argued that rhetorical relations could be further constrained by event temporal classification . \n\t', '\n\t\t Later , Dorr and Gaasterland \n\t\t']",Positive
"['\n\t\t Their works , however , are theoretical in nature and have not investigated computational aspects . \n\t', '\n\t\t The pioneer work on Chinese temporal relation extraction was first reported by Li and Wong \n\t\t']",Negative
"['\n\t\t To discover temporal relations embedded in a sentence , they devised a set of simple rules to map the combined effects of temporal indicators , which are gathered from different grammatical categories , to their corresponding relations . \n\t', '\n\t\t However , their work did not focus on relative temporal relations . \n\t', '\n\t\t Given a sentence describing two temporally related events , Li and Wong only took the temporal position words ( including before , after and when , which serve as temporal connectives ) and the tense/aspect markers of the second event into consideration . \n\t', '\n\t\t The proposed rule-based approach was simple ; but it suffered from low coverage and was particularly ineffective when the interaction between the linguistic elements was unclear . \n\t', '\n\t\t This paper studies how linguistic features in Chinese interact to influence relative relation resolution . \n\t', '\n\t\t For this purpose , statistics-based machine learning approaches are applied . \n\t', '\n\t\t The remainder of the paper is structured as follows : Section 2 summarizes the linguistic features , which must be taken into account in temporal relation resolution , and introduces how these features are expressed in Chinese . \n\t', '\n\t\t In Section 3 , the proposed machine learning algorithms to identify temporal relations are outlined ; furthermore , a heterogeneous collaborative bootstrapping technique for smoothing is presented . \n\t', '\n\t\t Experiments designed for studying the impact of different approaches and linguistic features are described in Section 4 . \n\t', '\n\t\t Finally , Section 5 concludes the paper . \n\t', '\n\t\t 2 Modeling Temporal Relations 2.1 Temporal Relation Representations As the importance of temporal information processing has become apparent , a variety of temporal systems have been introduced , attempting to accommodate the characteristics of relative temporal information . \n\t', '\n\t\t Among those who worked on temporal relation representations , many took the work of Reichenbach \n\t\t']",Positive
"['\n\t\t Reichenbach proposed a point-based temporal theory . \n\t', '\n\t\t This was later enhanced by Bruce who defined seven relative temporal relations ( Bruce . \n\t', '\n\t\t 1972 ) . \n\t', '\n\t\t Given two durative events , the interval relations between them were modeled by the order between the greatest lower bounding points and least upper bounding points of the two events . \n\t', '\n\t\t In the other camp , instead of adopting time points , Allen took intervals as temporal primitives and introduced thirteen basic binary relations . \n\t', '\n\t\t In this interval-based theory , points are relegated to a subsidiary status as \x91meeting places\x92 of intervals . \n\t', '\n\t\t An extension to Allen\x92s theory , which treated both points and intervals as primitives on an equal footing , was later investigated by Ma and Knight \n\t\t']",Positive
"['\n\t\t In natural language , events can either be punctual ( e.g. 1 ~ ( explore ) ) or durative ( e.g. 1~ ( built a house ) ) in nature . \n\t', '\n\t\t Thus Ma and Knight\x92s model is adopted in our work ( see Figure 1 ) . \n\t', '\n\t\t Taking the sen- tence \x931MhAcA-Z~fir , f 417~~~i rinZA1p79 ( after the street bridge had been built , they solved the traffic problem of the city)\x94 as an example , the relation held between building the bridge ( i.e. an interval ) and solving the problem ( i.e. a point ) is BEFORE . \n\t', '\n\t\t BEFORE/AFTER MEETS/MET-BY OVERLAPS/OVERLAPPED-BY STARTS/STARTED-BY DURING/CONTAINS FINISHES/FINISHED-BY SAME-AS A punctual event ( i.e. represented in time point ) A durative event ( i.e. represented in time interval ) Figure 1. Thirteen temporal relations between points and intervals 2.2 Linguistic Features for Determining Relative Relations Relative relations are generally determined by tense/aspect , connecting words ( temporal or otherwise ) and event classes . \n\t', '\n\t\t Tense/Aspect in English is manifested by verb inflections . \n\t', '\n\t\t But such morphological variations are inapplicable to Chinese verbs ; instead , they are conveyed lexically \n\t\t']",Positive
"['\n\t\t In other words , tense and aspect in Chinese are expressed using a combination of time words , auxiliaries , temporal position words , adverbs and prepositions , and particular verbs . \n\t', '\n\t\t Temporal Connectives in English primarily involve conjunctions , e.g. after , before and when \n\t\t']",Positive
"['\n\t\t They are key components in discourse structures . \n\t', '\n\t\t In Chinese , however , conjunctions , conjunctive adverbs , prepositions and position words are required to represent connectives . \n\t', '\n\t\t A few verbs which express cause and effect also imply a forward movement of event time . \n\t', '\n\t\t The words , which contribute to the tense/aspect and temporal connective expressions , are explicit in a sentence and generally known as Temporal Indicators . \n\t', '\n\t\t Event Class is implicit in a sentence . \n\t', '\n\t\t Events can be classified according to their inherent temporal characteristics , such as the degree of telicity and/or atomicity \n\t\t']",Positive
"['\n\t\t The four widespread accepted temporal classes1 are state , process , punctual event and developing event . \n\t', '\n\t\t Based on their classes , events interact with the tense/aspect of verbs to define the temporal relations between two events . \n\t', '\n\t\t Temporal indicators and event classes are together referred to as Linguistic Features ( see Table 1 ) . \n\t', '\n\t\t For example , linguistic features are underlined in the sentence \x93(~~)1MhAcA-Z~(fir),f 417~~~i ri nZA1p7 9 after/because the street bridge had been built ( i.e. a developing event ) , they solved the traffic problem of the city ( i.e. a punctual event)\x94 . \n\t', '\n\t\t 1 Temporal classification refers to aspectual classification . \n\t', '\n\t\t Linguistic Feature Symbol POS Tag Effect Example With/Without punctuations PT Not Applica- Not Applicable Not Applicable ble Speech verbs VS TI _vs Tense ^^ , ^^ , ^ Trend verbs TR TI_tr Aspect ^^ , ^^ Preposition words P TI_p Discourse Structure/Aspect ^ , ^ , ^ Position words PS TI _f Discourse Structure ^ , ^ , ^^ Verbs with verb objects VV TI_vv Tense/Aspect ^^ , ^^ , ^ Verbs expressing wish/hope VA TI_va Tense ^^ , ^ , ^ Verbs related to causality VC TI_vc Discourse Structure ^^ , ^^ , ^^ Conjunctive words C TI_c Discourse Structure ^ , ^^ , ^^ Auxiliary words U TI_u Aspect ^ , ^ , ^ Time words T TI _t Tense ^^ , ^^ , ^^ Adverbs D TI_d Tense/Aspect/Discourse Structure ^ , ^ , ^^ , ^ Event class EC E0/E1/E2/E3 Event Classification State , Punctual Event , Developing Event , Process Table 1 . \n\t', '\n\t\t Linguistic features : eleven temporal indicators and one event class Table 1 shows the mapping between a temporal indicator and its effects . \n\t', '\n\t\t Notice that the mapping is not one-to-one . \n\t', '\n\t\t For example , adverbs affect tense/aspect as well as discourse structure . \n\t', '\n\t\t For another example , tense/aspect can be affected by auxiliary words , trend verbs , etc. . \n\t', '\n\t\t This shows that classification of temporal indicators based on partof-speech ( POS ) information alone cannot determine relative temporal relations . \n\t', '\n\t\t 3 Machine Learning Approaches for Relative Relation Resolution Previous efforts in corpus-based natural language processing have incorporated machine learning methods to coordinate multiple linguistic features for example in accent restoration \n\t\t']",Positive
"['\n\t\t Relative relation resolution can be modeled as a relation classification task . \n\t', '\n\t\t We model the thirteen relative temporal relations ( see Figure 1 ) as the classes to be decided by a classifier . \n\t', '\n\t\t The resolution process is to assign an event pair ( i.e. the two events under concern)2 to one class according to their linguistic features . \n\t', '\n\t\t For this purpose , we train two classifiers , a Probabilistic Decision Tree Classifier ( PDT ) and a Naïve Bayesian Classifier ( NBC ) . \n\t', '\n\t\t We then combine the results by the Collaborative Bootstrappi,,g ( CB ) technique which is used to mediate the sparse data problem arose due to the limited number of training cases . \n\t', '\n\t\t 3.1 Probabilistic Decision Tree ( PDT ) Due to two domain-specific characteristics , we encounter some difficulties in classification . \n\t', '\n\t\t ( a ) Unknown values are common , for many events are modified by less than three linguistic features . \n\t', '\n\t\t ( b ) Both training and testing data are noisy . \n\t', '\n\t\t For this reason , it is impossible to obtain a tree which can completely classify all training examples . \n\t', '\n\t\t To overcome this predicament , we aim to obtain more adjusted probability distributions of event pairs over their possible classes . \n\t', '\n\t\t Therefore , a probabilistic decision tree approach is preferred over conventional decision tree approaches ( e.g. C4.5 , ID3 ) . \n\t', '\n\t\t We adopt a non-incremental supervised learning algorithm in TDIDT ( Top Down Induction of Decision Trees ) family . \n\t', '\n\t\t It constructs a tree top-down and the process is guided by distributional information learned from examples \n\t\t']",Positive
"['\n\t\t 3.1.1 Parameter Estimation Based on probabilities , each object in the PDT approach can belong to a number of classes . \n\t', '\n\t\t These probabilities could be estimated from training cases with Maximum Likelihood Estimation ( MLE ) . \n\t', '\n\t\t Let l be the decision sequence , z the object and c the class . \n\t', '\n\t\t The probability of z belonging to c is : Ac|z)=^ Al , c| z ) ^ ^Ac|OpY| z ) l l let l= B1 B2 ... \n\t', '\n\t\t B , , , by MLE we have : p(c |l ) ^ p(c | B ) , , = f ( c , B,,)(2) f(B,,) f ( c , B , , ) is the count of the items whose leaf nodes are B , , and belonging to class c . \n\t', '\n\t\t And ( 1 ) 2 It is an object in machine learning algorithms . \n\t', '\n\t\t where p ( l | z ) = p(B1 | z)p(B2 ) | B1 , z)p(B3 | B1 , B2 , z ) ( 3 ) ...p ( Bn | Bn^1 ...B1,z p(BmBm^1Bm_2 ... \n\t', '\n\t\t B1 | z ) p(Bm_1Bm2 ... \n\t', '\n\t\t B1 | z ) p(Bm Bm\x97 2 ...B1 , z ) tance-based measurement is unbiased towards the attributes with a large number of values and is capable of generating smaller trees with no loss of accuracy \n\t\t']",Positive
"['\n\t\t This characteristic makes it an ideal choice for our work , where most attributes have more than 200 values . \n\t', '\n\t\t f ( BmBm^1Bm_2 ... \n\t', '\n\t\t B1 | z ) f ( Bm_1Bm_2 ...B1 | z ) An object might traverse more than one decision path if it has unknown attribute values . \n\t', '\n\t\t f(BmBm^1Bm^2 ...B1 | z ) is the count of the item z , which owns the decision paths from B1 to Bm. 3.1.2 Classification Attributes Objects are classified into classes based on their attributes . \n\t', '\n\t\t In the context of temporal relation resolution , how to categorize linguistic features into classification attributes is a major design issue . \n\t', '\n\t\t We extract all temporal indicators surrounding an event . \n\t', '\n\t\t Assume m and n are the anterior and posterior window size . \n\t', '\n\t\t They represent the numbers of the indicators BEFORE and AFTER respectively . \n\t', '\n\t\t Consider the most extreme case where an event consists of at most 4 temporal indicators before and 2 after . \n\t', '\n\t\t We set m and n to 4 and 2 initially . \n\t', '\n\t\t Experiments show that learning performance drops when m>4 and n>2 and there is only very little difference otherwise ( i.e. when m^4 and n^2 ) . \n\t', '\n\t\t In addition to temporal indicators alone , the position of the punctuation mark separating the two clauses describing the events and the classes of the events are also useful classification attributes . \n\t', '\n\t\t We will outline why this is so in Section 4.1 . \n\t', '\n\t\t Altogether , the following 15 attributes are used to train the PDT and NBC classifiers : TI ~ , TI ~~ , TI ~ , TI ~~ , class ( e1 ) , TI ~~ , TI ~~ , wi / wo punC , TI l4 , TI l3 , TI l2 , TI l1 , class ( e2 ) , TI2 , , TI r2 e2 e2 e2 e2 e , , e2 li ( i=1,2,3,4 ) and rj ( j=1,2 ) are the ith indictor before and the jth indicator after the event ek ( k=1,2 ) . \n\t', '\n\t\t Given a sentence , for example , 5""c/TI_d ~/E0 _T/TI_u ~T/n , /w A-/TI_d T,0/E2 _T/TI_u IRiI/n o /w , the at- tribute vector could be represented as : [ 0 , 0 , 0 , 5""c , E0 , _T , 0 , 1 , 0 , 0 , 0 , A- , E2 , _T , 0 ] . \n\t', '\n\t\t 3.1.3 Attribute Selection Function Many similar attribute selection functions were used to construct a decision tree \n\t\t']",Positive
['\n\t\t These included information gain and information gain ratio \n\t\t'],Positive
['\n\t\t We adopt the one proposed by Lopez de Mantaraz \n\t\t'],Positive
"['\n\t\t Compared with Quinlan\x92s information gain ratio , Lopez\x92s dis- 3.2 Naïve Bayesian Classifier ( NBC ) NBC assumes independence among features . \n\t', '\n\t\t Given the class label c , NBC learns from training data the conditional probability of each attribute Ai ( see Section 3.1.2 ) . \n\t', '\n\t\t Classification is then performed by applying Bayes rule to compute the probability of c given the particular instance of A1,...,An , and then predicting the class with the highest posterior probability ratio . \n\t', ""\n\t\t c = arg max score(c | A1 , A2 , A3 , ... , An ) ( 4 ) c score(c | A1 , A2 , 3,\x94' A A ) = Ac | A1 , A2 , A3 , ... , An ) ( 5 ) , n Ac | A1 , A2 , A3 , ... , An ) Apply Bayesian rule to ( 5 ) , we have : An ) A1 , A2 , A3 , ... , An p(c | A1 , A2 , A3 , . \n\t"", '\n\t\t ..,An ) ) ( c A 1 , A 2 , A 3 , ... , n |c ( c ) ( 6 ) )p p(A1 , A2 , A3 , ... , An | c)p(c) p(Ai | c ) and p(Ai | c ) are estimated by MLE from training data with Dirichlet Smoothing method : p(Ai | c ) = n ( 7 ) j=1 ^ c c(Ai,c)+u ( A j , c )+ u × n l 1 p(A i | c ) = n ( 8 ) j=1 ^ c c(Ai,c)+u ( A j , c )+ u × n l 1 3.3 Collaborative Bootstrapping ( CB ) PDT and NB are both supervised learning approach . \n\t', '\n\t\t Thus , the training processes require many labeled cases . \n\t', '\n\t\t Recent results \n\t\t']",Positive
"['\n\t\t In previous works , CB trained two homogeneous classifiers based on different independent feature spaces . \n\t', '\n\t\t However , this approach is not applicable to our work since only a few temporal indicators occur in each case . \n\t', '\n\t\t Therefore , we develop an alternative CB algorithm , i.e. to train two different classifiers based on the same feature spaces . \n\t', '\n\t\t PDT ( a non-linear classifier ) and NBC ( a linear classifier ) are under consideration . \n\t', '\n\t\t This is inspired by Blum and Mitchell\x92s theory that two collaborative classifiers should be conditionally , ( m = 2,3 , ... , n ) . \n\t', '\n\t\t score(c | A , ... , An |c )p( c p(A1 , A2 , A3 ^ p(Ai fl 11 p(Ai i=1 | c)p(c) independent so that each classifier can make its own contribution \n\t\t']",Positive
"['\n\t\t The learning steps are outlined in Figure 2. Inputs : A collection of the labeled cases and unlabeled cases is prepared . \n\t', '\n\t\t The labeled cases are separated into three parts , training cases , test cases and held-out cases . \n\t', '\n\t\t Loop : While the breaking criteria is not satisfied 1 Build the PDT and NBC classifiers us- ing training cases 2 Use PDT and NBC to classify the unla- beled cases , and exchange with the selected cases which have higher Classification Confidence ( i.e. the uncertainty is less than a threshold ) . \n\t', '\n\t\t 3 Evaluate the PDT and NBC classifiers with the held-out cases . \n\t', '\n\t\t If the error rate increases or its reduction is below a threshold break the loop ; else go to step 1 . \n\t', '\n\t\t Output : Use the optimal classifier to label the test cases Figure 2. Collaborative bootstrapping algorithm 3.4 Classification Confidence Measurement Classification confidence is the metric used to measure the correctness of each labeled case automatically ( see Step 2 in Figure 2 ) . \n\t', '\n\t\t The desirable metric should satisfy two principles : \x95 It should be able to measure the uncertainty/ certainty of the output of the classifiers ; and \x95 It should be easy to calculate . \n\t', '\n\t\t We adopt entropy , i.e. an information theory based criterion , for this purpose . \n\t', '\n\t\t Let x be the classi- fied object , and C = { c1 , c2 , c3 , ... , cn } the set of output . \n\t', '\n\t\t x is classified as ci with the probability p(ci | x ) i =1,2,3 , .. , n . \n\t', '\n\t\t The entropy of the output is then calculated as : n e(C | x)=^^p(ci | x ) log p(ci | x ) ( 9 ) i = 1 Once p(ci | x ) is known , the entropy can be deter- mined . \n\t', '\n\t\t These parameters can be easily determined in PDT , as each incoming case is classified into each class with a probability . \n\t', '\n\t\t However , the incoming cases in NBC are grouped into one class which is assigned the highest score . \n\t', '\n\t\t We then have to estimate p(ci | x ) from those scores . \n\t', '\n\t\t Without loss of general- ity , the probability is estimated as : p(ci | x ) = nscore(ci | x ) ( 10 ) ^ score c | x ) j where score(ci | x ) is the ranking score of x belonging to ci . \n\t', '\n\t\t 4 Experiment Setup and Evaluation Several experiments have been designed to evaluate the proposed learning approaches and to reveal the impact of linguistic features on learning performance . \n\t', '\n\t\t 700 sentences are extracted from Ta Kong Pao ( a local Hong Kong Chinese newspaper ) financial version . \n\t', '\n\t\t 600 cases are labeled manually and 100 left unlabeled . \n\t', '\n\t\t Among those labeled , 400 are used as training data , 100 as test data and the rest as held-out data . \n\t', '\n\t\t 4.1 Use of Linguistic Features As Classification Attributes The impact of a temporal indicator is determined by its position in a sentence . \n\t', '\n\t\t In PDT and NBC , we consider an indicator located in four positions : ( 1 ) BEFORE the first event ; ( 2 ) AFTER the first event and BEFORE the second and it modifies the first event ; ( 3 ) the same as ( 2 ) but it modifies the second event ; and ( 4 ) AFTER the second event . \n\t', '\n\t\t Cases ( 2 ) and ( 3 ) are ambiguous . \n\t', '\n\t\t The positions of the temporal indicators are the same . \n\t', '\n\t\t But it is uncertain whether these indicators modify the first or the second event if there is no punctuation separating their roles . \n\t', '\n\t\t We introduce two methods , namely NA and SAP to check if the ambiguity affects the two learning approaches . \n\t', '\n\t\t N(atural) O(rder) : the temporal indicators between the two events are extracted and compared according to their occurrence in the sentences regardless which event they modify . \n\t', '\n\t\t S(eparate) A(uxiliary) and P(osition) words : we try to resolve the above ambiguity with the grammatical features of the indicators . \n\t', '\n\t\t In this method , we assume that an indicator modifies the first event if it is an auxiliary word ( e.g. T ) , a trend verb ( e.g. jt~* ) or a position word ( e.g. u ) ; otherwise it modifies the second event . \n\t', '\n\t\t Temporal indicators are either tense/aspect or connectives ( see Section 2.2 ) . \n\t', '\n\t\t Intuitively , it seems that classification could be better achieved if connective features are isolated from tense/ aspect features , allowing like to be compared with like . \n\t', '\n\t\t Methods SC1 and SC2 are designed based on this assumption . \n\t', '\n\t\t Table 2 shows the effect the different classification methods . \n\t', '\n\t\t SC1 ( Separate Connecting words 1 ) : it separates conjunctions and verbs relating to causality from others . \n\t', '\n\t\t They are assumed to contribute to discourse structure ( intra- or inter-sentence structure ) , and the others contribute to the tense/aspect expressions for each individual event . \n\t', '\n\t\t They are built into 2 separate attributes , one for each event . \n\t', '\n\t\t j=1 SC2 ( Separate Connecting words 2 ) : it is the same as SC1 except that it combines the connecting word pairs ( i.e. as a single pattern ) into one attribute . \n\t', '\n\t\t EC ( Event Class ) : it takes event classes into consideration . \n\t', '\n\t\t Method Accuracy PDT NBC NO 82.00 % 81.00 % SAP 82.20 % 81.50 % SAP +SC1 80.20 % 78.00 % SAP +SC2 81.70 % 79.20 % SAP +EC 85.70 % 82.25 % Table 2 . \n\t', '\n\t\t Effect of encoding linguistic features in the dif- ferent ways 4.2 Impact of Individual Features From linguistic perspectives , 13 features ( see Table 1 ) are useful for relative relation resolution . \n\t', '\n\t\t To examine the impact of each individual feature , we feed a single linguistic feature to the PDT learning algorithm one at a time and study the accuracy of the resultant classifier . \n\t', '\n\t\t The experimental results are given in Table 3 . \n\t', '\n\t\t It shows that event classes have greatest accuracy , followed by conjunctions in the second place , and adverbs in the third . \n\t', '\n\t\t Feature Accuracy Feature Accuracy PT 50.5 % VA 56.5 % VS 54 % C 62 % VC 54 % U 51.5 % TR 50.5 % T 57.2 % P 52.2 % D 61.7 % PS 58.7 % EC 68.2 % VS 51.2 % None 50.5 % Table 3 . \n\t', '\n\t\t Impact of individual linguistic features 4.3 Discussions Analysis of the results in Tables 2 and 3 reveals some linguistic insights : 1 . \n\t', '\n\t\t In a situation where temporal indicators appear between two events and there is no punctuation mark separating them , POS information help reduce the ambiguity . \n\t', '\n\t\t Compared with NO , SAP shows a slight improvement from 82 % to 82.2 % . \n\t', '\n\t\t But the improvement seems trivial and is not as good as our prediction . \n\t', '\n\t\t This might due to the small percent of such cases in the corpus . \n\t', '\n\t\t Separating conjunctions and verbs relating to causality from others is ineffective . \n\t', '\n\t\t This reveals the complexity of Chinese in connecting expressions . \n\t', '\n\t\t It is because other words ( such as adverbs , proposition and position words ) also serve such a function . \n\t', '\n\t\t Meanwhile , experiments based on SC1 and SC2 suggest that the connecting ex- pressions generally involve more than one word or phrase . \n\t', '\n\t\t Although the words in a connecting expression are separated in a sentence , the action is indeed interactive . \n\t', '\n\t\t It would be more useful to regard them as one attribute . \n\t', '\n\t\t 3. The effect of event classification is striking . \n\t', '\n\t\t Taking this feature into account , the accuracies of both PDT and NB improved significantly . \n\t', '\n\t\t As a matter of fact , different event classes may introduce different relations even if they are constrained by the same temporal indicators . \n\t', '\n\t\t 4.4 Collaborative Bootstrapping Table 4 presents the evaluation results of the four different classification approaches . \n\t', '\n\t\t DM is the default model , which classifies all incoming cases as the most likely class . \n\t', '\n\t\t It is used as evaluation baseline . \n\t', '\n\t\t Compare with DM , PDT and NBC show improvement in accuracy ( i.e. above 60 % improvement ) . \n\t', '\n\t\t And CB in turn outperforms PDT and NBC . \n\t', '\n\t\t This proves that using unlabeled data to boost the performance of the two classifiers is effective . \n\t', '\n\t\t Approach Accuracy Close test Open test DM 50.50 % 55.00 % NBC 82.25 % 72.00 % PDT 85.70 % 74.00 % CB 88.70 % 78.00 % Table 4 . \n\t', '\n\t\t Evaluation of NBC , PDT and CB approaches 5 Conclusions Relative temporal relation resolution received growing attentions in recent years . \n\t', '\n\t\t It is important for many natural language processing applications , such as information extraction and machine translation . \n\t', '\n\t\t This topic , however , has not been well studied , especially in Chinese . \n\t', '\n\t\t In this paper , we propose a model for relative temporal relation resolution in Chinese . \n\t', '\n\t\t Our model combines linguistic knowledge and machine learning approaches . \n\t', '\n\t\t Two learning approaches , namely probabilistic decision tree ( PDT ) and naive Bayesian classifier ( NBC ) and 13 linguistic features are employed . \n\t', '\n\t\t Due to the limited labeled cases , we also propose a collaborative bootstrapping technique to improve learning performance . \n\t', '\n\t\t The experimental results show that our approaches are encouraging . \n\t', '\n\t\t To our knowledge , this is the first attempt of collaborative bootstrapping , which involves two heterogeneous classifiers , in NLP application . \n\t', '\n\t\t This lays down the main contribution of our research . \n\t', '\n\t\t In this pilot work , temporal indicators are selected based on linguistic knowledge . \n\t', '\n\t\t It is time-consuming and could be error-prone . \n\t', '\n\t\t This suggests two directions for future studies . \n\t', '\n\t\t We will try to automate or at least semi-automate feature selection process . \n\t', '\n\t\t An- other future work worth investigating is temporal indicator clustering . \n\t', '\n\t\t There are two methods we could investigate , i.e. clustering the recognized indicators which occur in training corpus according to co-occurrence information or grouping them into two semantic roles , one related to tense/aspect expressions and the other to connecting expressions between two events . \n\t', '\n\t\t Acknowledgements The work presented in this paper is partially supported by Research Grants Council of Hong Kong ( RGC reference number PolyU5085/02E ) and CUHK Strategic Grant ( account number 4410001 ) . \n\t', '\n\t\t References Allen J. , 1981 . \n\t', '\n\t\t An Interval-based Represent Action of Temporal Knowledge . \n\t', '\n\t\t In Proceedings of 7th International Joint Conference on Artificial Intelligence , pages 221-226 . \n\t', '\n\t\t Los Altos , CA . \n\t', '\n\t\t Blum , A. and Mitchell T. , 1998 . \n\t', '\n\t\t Combining Labeled and Unlabeled Data with Co-Training . \n\t', '\n\t\t In Proceedings of the Eleventh Annual Conference on Computational Learning Theory , Madison , Wisconsin , pages 92-100 Bruce B. , 1972 . \n\t', '\n\t\t A Model for Temporal References and its Application in Question-Answering Program . \n\t', '\n\t\t Artificial Intelligence , 3(1):1-25 . \n\t', '\n\t\t Collins M. and Singer Y , 1999 . \n\t', '\n\t\t Unsupervised Models for Named Entity Classification . \n\t', '\n\t\t In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora , pages 189-196 . \n\t', '\n\t\t University of Maryland . \n\t', '\n\t\t Dorr B. and Gaasterland T. , 2002 . \n\t', '\n\t\t Constraints on the Generation of Tense , Aspect , and Connecting Words from Temporal Expressions . \n\t', '\n\t\t ( submitted to JAIR ) Hitzeman J. , Moens M. and Grover C. , 1995 . \n\t', '\n\t\t Algorithms for Analyzing the Temporal Structure of Discourse . \n\t', '\n\t\t In Proceedings of the 7th European Meeting of the Association for Computational Linguistics , pages 253-260 . \n\t', '\n\t\t Dublin , Ireland . \n\t', '\n\t\t Lascarides A. , Asher N. and Oberlander J. , 1992 . \n\t', '\n\t\t Inferring Discourse Relations in Context . \n\t', '\n\t\t In Proceedings of the 30th Meeting of the Association for Computational Linguistics , pages 1-8 , Newark , Del. Li W.J. and Wong K.F. , 2002 . \n\t', '\n\t\t A Word-based Approach for Modeling and Discovering Temporal Relations Embedded in Chinese Sentences , ACM Transaction on Asian Language Processing , 1(3):173-206 . \n\t', '\n\t\t Ma J. and Knight B. , 1994 . \n\t', '\n\t\t A General Temporal Theory . \n\t', '\n\t\t The Computer Journal , 37(2):114- 123 . \n\t', '\n\t\t Màntaras L. , 1991 . \n\t', '\n\t\t A Distance-based Attribute Selection Measure for Decision Tree Induction . \n\t', '\n\t\t Machine Learning , 6(1) : 81\x9692 . \n\t', '\n\t\t Màrquez L. , Padró L. and Rodríguez H. , 2000 . \n\t', '\n\t\t A Machine Learning Approach to POS Tagging . \n\t', '\n\t\t Machine Learning , 39(1):59-91 . \n\t', '\n\t\t Kluwer Academic Publishers . \n\t', '\n\t\t Partee , B. , 1984 . \n\t', '\n\t\t Nominal and Temporal Anaphora . \n\t', '\n\t\t Linguistics and Philosophy , 7(3):287-324 . \n\t', '\n\t\t Quinlan J. , 1993 . \n\t', '\n\t\t C4.5 Programs for Machine Learning . \n\t', '\n\t\t Morgan Kauman Press . \n\t', '\n\t\t Reichenbach H. , 1947 . \n\t', '\n\t\t Elements of Symbolic Logic . \n\t', '\n\t\t Berkeley CA , University of California Press . \n\t', '\n\t\t Siegel E. and McKeown K. , 2000 . \n\t', '\n\t\t Learning Methods to Combine Linguistic Indicators : Improving Aspectual Classification and Revealing Linguistic Insights . \n\t', '\n\t\t Computational Linguistics , 26(4) : 595- 627 . \n\t', ""\n\t\t Wiebe , J.M. , O'Hara , T.P. , Ohrstrom-Sandgren , T. and McKeever , K.J , 1998 . \n\t"", '\n\t\t An Empirical Approach to Temporal Reference Resolution . \n\t', '\n\t\t Journal of Artificial Intelligence Research , 9:247-293 . \n\t', '\n\t\t Wong F. , Li W. , Yuan C. , etc. , 2002 . \n\t', '\n\t\t Temporal Representation and Classification in Chinese . \n\t', '\n\t\t International Journal of Computer Processing of Oriental Languages , 15(2):211-230 . \n\t', '\n\t\t Yarowsky D. , 1994 . \n\t', '\n\t\t Decision Lists for Lexical Ambiguity Resolution : Application to the Accent Restoration in Spanish and French . \n\t', '\n\t\t In Proceeding of the 32rd Annual Meeting of ACL , San Francisco , CA . \n\t', '\n\t\t Zhou X. , Dillon T. , 1991 . \n\t', '\n\t\t A Statistical-heuristic Feature Selection Criterion for Decision Tree Induction . \n\t', '\n\t\t IEEE Transaction on Pattern Analysis and Machine Intelligence , 13(8) : 834-841 . \n\t', '\n\t\t Multi-Criteria- based Active Learning for Named Entity Recognition Dan Shen\x86\x871 Jie Zhang\x86\x87 Jian Su\x86 Guodong Zhou\x86 Chew-Lim Tan\x87 \x86 Institute for Infocomm Technology \x87 Department of Computer Science 21 Heng Mui Keng Terrace National University of Singapore Singapore 119613 3 Science Drive 2 , Singapore 117543 {shendan,zhangjie,sujian,zhougd}@i2r.a-star.edu.sg {shendan,zhangjie,tancl}@comp.nus.edu.sg Abstract In this paper , we propose a multi-criteriabased active learning approach and effectively apply it to named entity recognition . \n\t', '\n\t\t Active learning targets to minimize the human annotation efforts by selecting examples for labeling . \n\t', '\n\t\t To maximize the contribution of the selected examples , we consider the multiple criteria : informativeness , representativeness and diversity and propose measures to quantify them . \n\t', '\n\t\t More comprehensively , we incorporate all the criteria using two selection strategies , both of which result in less labeling cost than single-criterion-based method . \n\t', '\n\t\t The results of the named entity recognition in both MUC-6 and GENIA show that the labeling cost can be reduced by at least 80 % without degrading the performance . \n\t', '\n\t\t 1 Introduction In the machine learning approaches of natural language processing ( NLP ) , models are generally trained on large annotated corpus . \n\t', '\n\t\t However , annotating such corpus is expensive and time- consuming , which makes it difficult to adapt an existing model to a new domain . \n\t', '\n\t\t In order to overcome this difficulty , active learning ( sample selection ) has been studied in more and more NLP applications such as POS tagging \n\t\t']",Positive
"['\n\t\t Active learning is based on the assumption that a small number of annotated examples and a large number of unannotated examples are available . \n\t', '\n\t\t This assumption is valid in most NLP tasks . \n\t', '\n\t\t Different from supervised learning in which the entire corpus are labeled manually , active learning is to select the most useful example for labeling and add the labeled example to training set to retrain model . \n\t', '\n\t\t This procedure is repeated until the model achieves a certain level of performance . \n\t', '\n\t\t Practically , a batch of examples are selected at a time , called batched- based sample selection \n\t\t']",Positive
['\n\t\t Many existing work in the area focus on two approaches : certainty-based methods \n\t\t'],Positive
"['\n\t\t Being the first piece of work on active learning for name entity recognition ( NER ) task , we target to minimize the human annotation efforts yet still reaching the same level of performance as a supervised learning approach . \n\t', '\n\t\t For this purpose , we make a more comprehensive consideration on the contribution of individual examples , and more importantly maximizing the contribution of a batch based on three criteria : informativeness , representativeness and diversity . \n\t', '\n\t\t First , we propose three scoring functions to quantify the informativeness of an example , which can be used to select the most uncertain examples . \n\t', '\n\t\t Second , the representativeness measure is further proposed to choose the examples representing the majority . \n\t', '\n\t\t Third , we propose two diversity considerations ( global and local ) to avoid repetition among the examples of a batch . \n\t', '\n\t\t Finally , two combination strategies with the above three criteria are proposed to reach the maximum effectiveness on active learning for NER . \n\t', '\n\t\t 1 Current address of the first author : Universität des Saarlandes , Computational Linguistics Dept. , 66041 Saarbrü cken , Germany dshen@coli.uni-sb.de We build our NER model using Support Vector Machines ( SVM ) . \n\t', '\n\t\t The experiment shows that our active learning methods achieve a promising result in this NER task . \n\t', '\n\t\t The results in both MUC6 and GENIA show that the amount of the labeled training data can be reduced by at least 80 % without degrading the quality of the named entity recognizer . \n\t', '\n\t\t The contributions not only come from the above measures , but also the two sample selection strategies which effectively incorporate informativeness , representativeness and diversity criteria . \n\t', '\n\t\t To our knowledge , it is the first work on considering the three criteria all together for active learning . \n\t', '\n\t\t Furthermore , such measures and strategies can be easily adapted to other active learning tasks as well . \n\t', '\n\t\t 2 Multi-criteria for NER Active Learning Support Vector Machines ( SVM ) is a powerful machine learning method , which has been applied successfully in NER tasks , such as \n\t\t']",Positive
"['\n\t\t In this paper , we apply active learning methods to a simple and effective SVM model to recognize one class of names at a time , such as protein names , person names , etc. . \n\t', '\n\t\t In NER , SVM is to classify a word into positive class \x931\x94 indicating that the word is a part of an entity , or negative class \x93-1\x94 indicating that the word is not a part of an entity . \n\t', '\n\t\t Each word in SVM is represented as a high-dimensional feature vector including surface word information , orthographic features , POS feature and semantic trigger features \n\t\t']",Positive
"['\n\t\t The semantic trigger features consist of some special head nouns for an entity class which is supplied by users . \n\t', '\n\t\t Furthermore , a window ( size = 7 ) , which represents the local context of the target word w , is also used to classify w . \n\t', '\n\t\t However , for active learning in NER , it is not reasonable to select a single word without context for human to label . \n\t', '\n\t\t Even if we require human to label a single word , he has to make an addition effort to refer to the context of the word . \n\t', '\n\t\t In our active learning process , we select a word sequence which consists of a machine-annotated named entity and its context rather than a single word . \n\t', '\n\t\t Therefore , all of the measures we propose for active learning should be applied to the machine- annotated named entities and we have to further study how to extend the measures for words to named entities . \n\t', '\n\t\t Thus , the active learning in SVMbased NER will be more complex than that in simple classification tasks , such as text classification on which most SVM active learning works are conducted \n\t\t']",Positive
"['\n\t\t In the next part , we will introduce informativeness , representativeness and diversity measures for the SVM-based NER . \n\t', '\n\t\t 2.1 Informativeness The basic idea of informativeness criterion is similar to certainty-based sample selection methods , which have been used in many previous works . \n\t', '\n\t\t In our task , we use a distance-based measure to evaluate the informativeness of a word and extend it to the measure of an entity using three scoring functions . \n\t', '\n\t\t We prefer the examples with high informative degree for which the current model are most uncertain . \n\t', '\n\t\t 2.1.1 Informativeness Measure for Word In the simplest linear form , training SVM is to find a hyperplane that can separate the positive and negative examples in training set with maximum margin . \n\t', '\n\t\t The margin is defined by the distance of the hyperplane to the nearest of the positive and negative examples . \n\t', '\n\t\t The training examples which are closest to the hyperplane are called support vectors . \n\t', '\n\t\t In SVM , only the support vectors are useful for the classification , which is different from statistical models . \n\t', '\n\t\t SVM training is to get these support vectors and their weights from training set by solving quadratic programming problem . \n\t', '\n\t\t The support vectors can later be used to classify the test data . \n\t', '\n\t\t Intuitively , we consider the informativeness of an example as how it can make effect on the support vectors by adding it to training set . \n\t', '\n\t\t An example may be informative for the learner if the distance of its feature vector to the hyperplane is less than that of the support vectors to the hyper- plane ( equal to 1 ) . \n\t', '\n\t\t This intuition is also justified by \n\t\t']",Positive
"['\n\t\t They state that labeling an example that lies on or close to the hyperplane is guaranteed to have an effect on the solution . \n\t', '\n\t\t In our task , we use the distance to measure the informativeness of an example . \n\t', '\n\t\t The distance of a word\x92s feature vector to the hyperplane is computed as follows : where w is the feature vector of the word , ai , yi , si corresponds to the weight , the class and the feature vector of the ith support vector respectively . \n\t', '\n\t\t N is the number of the support vectors in current model . \n\t', '\n\t\t We select the example with minimal Dist , which indicates that it comes closest to the hyper- plane in feature space . \n\t', '\n\t\t This example is considered most informative for current model . \n\t', '\n\t\t 2.1.2 Informativeness Measure for Named Entity N = ^ Dist(w) a iyik(si,w)+b = 1 i Based on the above informativeness measure for a word , we compute the overall informativeness degree of a named entity NE . \n\t', '\n\t\t In this paper , we propose three scoring functions as follows . \n\t', '\n\t\t Let NE = w1...wN in which wi is the feature vector of the ith word of NE . \n\t', '\n\t\t \x95 Info_Avg : The informativeness of NE is scored by the average distance of the words in NE to the hyperplane . \n\t', '\n\t\t Info(NE) = 1^ ^ Dist(wi) wi^ NE where , wi is the feature vector of the ith word in NE . \n\t', '\n\t\t \x95 Info_Min : The informativeness of NE is scored by the minimal distance of the words in NE . \n\t', '\n\t\t Info(NE) = 1^ Min{Dist(wi)} wi ^ NE \x95 Info_S/N : If the distance of a word to the hy- perplane is less than a threshold a ( = 1 in our task ) , the word is considered with short distance . \n\t', '\n\t\t Then , we compute the proportion of the number of words with short distance to the to- tal number of words in the named entity and use this proportion to quantify the informativeness of the named entity . \n\t', '\n\t\t NUM ( Dist ( wi ) < a ) NE N In Section 4.3 , we will evaluate the effectiveness of these scoring functions . \n\t', '\n\t\t 2.2 Representativeness In addition to the most informative example , we also prefer the most representative example . \n\t', '\n\t\t The representativeness of an example can be evaluated based on how many examples there are similar or near to it . \n\t', '\n\t\t So , the examples with high representative degree are less likely to be an outlier . \n\t', '\n\t\t Adding them to the training set will have effect on a large number of unlabeled examples . \n\t', '\n\t\t There are only a few works considering this selection criterion \n\t\t']",Positive
"['\n\t\t text classification and statistical parsing . \n\t', '\n\t\t In this section , we compute the similarity between words using a general vector-based measure , extend this measure to named entity level using dynamic time warping algorithm and quantify the representativeness of a named entity by its density . \n\t', '\n\t\t 2.2.1 Similarity Measure between Words In general vector space model , the similarity between two vectors may be measured by computing the cosine value of the angle between them . \n\t', '\n\t\t The smaller the angle is , the more similar between the vectors are . \n\t', '\n\t\t This measure , called cosine-similarity measure , has been widely used in information retrieval tasks \n\t\t']",Positive
"['\n\t\t In our task , we also use it to quantify the similarity between two words . \n\t', '\n\t\t Particularly , the calculation in SVM need be projected to a higher dimensional space by using a certain kernel function K ( w i , w j ) . \n\t', '\n\t\t Therefore , we adapt the cosine-similarity measure to SVM as follows : k(wi , wj ) (wi,wj)=k(wi,wi)k(wj,wj) where , wi and wj are the feature vectors of the words i and j . \n\t', '\n\t\t This calculation is also supported by \n\t\t']",Positive
"['\n\t\t Furthermore , if we use the linear kernel k(wi , wj ) = wi ^ w j , the measure is the same as the traditional cosine similarity meas- ure cosq = wi ^ wj and may be regarded as a wi ^ wj general vector-based similarity measure . \n\t', '\n\t\t 2.2.2 Similarity Measure between Named Entities In this part , we compute the similarity between two machine-annotated named entities given the similarities between words . \n\t', '\n\t\t Regarding an entity as a word sequence , this work is analogous to the alignment of two sequences . \n\t', '\n\t\t We employ the dynamic time warping ( DTW ) algorithm \n\t\t']",Positive
"['\n\t\t Here , we adapt it to our task . \n\t', '\n\t\t A sketch of the modified algorithm is as follows . \n\t', '\n\t\t Let NE1 = w11w12 ... w1n...w1N , ( n = 1 , ... , N ) and NE2 = w21w22 ... w2m...w2M , ( m = 1 , ... , M ) denote two word sequences to be matched . \n\t', '\n\t\t NE1 and NE2 consist of M and N words respectively . \n\t', '\n\t\t NE1 ( n ) = w1n and NE2(m) = w2m . \n\t', '\n\t\t A similarity value Sim(w1n , w2m ) has been known for every pair of words ( w1n , w2m ) within NE1 and NE2 . \n\t', '\n\t\t The goal of DTW is to find a path , m = map(n) , which map n onto the corresponding m such that the accumulated similarity Sim* along the path is maximized . \n\t', '\n\t\t N Sim*= Max { ^ Sim ( NE1(n) , NE2 ( map(n)) } { map(n) } n=1 A dynamic programming method is used to determine the optimum path map(n) . \n\t', '\n\t\t The accumulated similarity SimA to any grid point ( n , m ) can be recursively calculated as SimA (n,m)= Sim (w1n,w2m)+MaxSimA(n^1,q) q^m Finally , Sim* = Sim A(N,M ) Certainly , the overall similarity measure Sim* has to be normalized as longer sequences normally give higher similarity value . \n\t', '\n\t\t So , the similarity between two sequences NE1 and NE2 is calculated as Info ( NE ) = wi^ Sim 2.2.3 Representativeness Measure for Named Entity Given a set of machine-annotated named entities NESet = { NE1 , ... , NEN } , the representativeness of a named entity NE ; in NESet is quantified by its density . \n\t', '\n\t\t The density of NE ; is defined as the average similarity between NE ; and all the other entities NEj in NESet as follows . \n\t', '\n\t\t ( NE ; , NEj ) Dens;ty ( NE ; ) = j^ ; N^1 If NE ; has the largest density among all the entities in NESet , it can be regarded as the centroid of NE- Set and also the most representative examples in NESet . \n\t', '\n\t\t 2.3 Diversity Diversity criterion is to maximize the training utility of a batch . \n\t', '\n\t\t We prefer the batch in which the examples have high variance to each other . \n\t', '\n\t\t For example , given the batch size 5 , we try not to select five repetitious examples at a time . \n\t', '\n\t\t To our knowledge , there is only one work \n\t\t']",Positive
"['\n\t\t In our task , we propose two methods : local and global , to make the examples diverse enough in a batch . \n\t', '\n\t\t 2.3.1 Global Consideration For a global consideration , we cluster all named entities in NESet based on the similarity measure proposed in Section 2.2.2 . \n\t', '\n\t\t The named entities in the same cluster may be considered similar to each other , so we will select the named entities from different clusters at one time . \n\t', '\n\t\t We employ a K- means clustering algorithm \n\t\t']",Positive
"['\n\t\t Given : NESet = { NE1 , ... , NEN } Suppose : The number of clusters is K Initialization : Randomly equally partition { NE1 , ... , NEN } into K initial clusters Cj ( j = 1 , ... , K ) . \n\t', '\n\t\t Loop until the number of changes for the centroids of all clusters is less than a threshold \x95 Find the centroid of each cluster Cj ( j = 1 , ... , K ) . \n\t', '\n\t\t NECent j = arg max( ^ S;m(NE ; , NE ) ) NE^ Cj NE;^ Cj \x95 Repartition { NE1 , ... , NEN } into K clusters . \n\t', '\n\t\t NE ; will be assigned to Cluster Cj if S;m(NE;,NECentj)^ S;m(NE;,NECentw),w ^ j Figure 1 : Global Consideration for Diversity : K- Means Clustering algorithm In each round , we need to compute the pair- wise similarities within each cluster to get the centroid of the cluster . \n\t', '\n\t\t And then , we need to compute the similarities between each example and all centroids to repartition the examples . \n\t', '\n\t\t So , the algorithm is time-consuming . \n\t', '\n\t\t Based on the assumption that N examples are uniformly distributed between the K clusters , the time complexity of the algorithm is about O(N2/K+NK) \n\t\t']",Positive
"['\n\t\t In one of our experiments , the size of the NESet ( N ) is around 17000 and K is equal to 50 , so the time complexity is about O(106) . \n\t', '\n\t\t For efficiency , we may filter the entities in NESet before clustering them , which will be further discussed in Section 3 . \n\t', '\n\t\t 2.3.2 Local Consideration When selecting a machine-annotated named entity , we compare it with all previously selected named entities in the current batch . \n\t', '\n\t\t If the similarity between them is above a threshold 8 , this example cannot be allowed to add into the batch . \n\t', '\n\t\t The order of selecting examples is based on some measure , such as informativeness measure , representativeness measure or their combination . \n\t', '\n\t\t This local selection method is shown in Figure 2 . \n\t', '\n\t\t In this way , we avoid selecting too similar examples ( similarity value ^ 8 ) in a batch . \n\t', '\n\t\t The threshold 8 may be the average similarity between the examples in NESet . \n\t', '\n\t\t Given : NESet = { NE1 , ... , NEN } BatchSet with the maximal size K. Initialization : BatchSet = empty Loop until BatchSet is full \x95 Select NE ; based on some measure from NESet . \n\t', '\n\t\t \x95 RepeatFlag = false ; \x95 Loop from j = 1 to CurrentSize(BatchSet) If S;m ( NE;,NEj ) ^ b Then RepeatFlag = true ; Stop the Loop ; \x95 If RepeatFlag == false Then add NE ; into BatchSet \x95 remove NE ; from NESet Figure 2 : Local Consideration for Diversity This consideration only requires O(NK+K2) computational time . \n\t', '\n\t\t In one of our experiments ( N \x98 17000 and K = 50 ) , the time complexity is about O(105) . \n\t', '\n\t\t It is more efficient than clustering algorithm described in Section 2.3.1 . \n\t', '\n\t\t 3 Sample Selection strategies In this section , we will study how to combine and strike a proper balance between these criteria , viz . \n\t', '\n\t\t informativeness , representativeness and diversity , S;m(NE\x84 NE2 ) = S;m Max(N , M ) ^ S;m to reach the maximum effectiveness on NER active learning . \n\t', '\n\t\t We build two strategies to combine the measures proposed above . \n\t', '\n\t\t These strategies are based on the varying priorities of the criteria and the varying degrees to satisfy the criteria . \n\t', '\n\t\t \x95 Strategy 1 : We first consider the informativeness criterion . \n\t', '\n\t\t We choose m examples with the most informativeness score from NESet to an intermediate set called INTERSet . \n\t', '\n\t\t By this preselecting , we make the selection process faster in the later steps since the size of INTERSet is much smaller than that of NESet . \n\t', '\n\t\t Then we cluster the examples in INTERSet and choose the centroid of each cluster into a batch called BatchSet . \n\t', '\n\t\t The centroid of a cluster is the most representative example in that cluster since it has the largest density . \n\t', '\n\t\t Furthermore , the examples in different clusters may be considered diverse to each other . \n\t', '\n\t\t By this means , we consider representativeness and diversity criteria at the same time . \n\t', '\n\t\t This strategy is shown in Figure 3 . \n\t', '\n\t\t One limitation of this strategy is that clustering result may not reflect the distrib ution of whole sample space since we only cluster on INTERSet for efficiency . \n\t', '\n\t\t The other is that since the representativeness of an example is only evaluated on a cluster . \n\t', '\n\t\t If the cluster size is too small , the most representative example in this cluster may not be representative in the whole sample space . \n\t', '\n\t\t Given : NESet = { NE1 , ... , NEN } BatchSet with the maximal size K. INTERSet with the maximal size M Steps : \x95 BatchSet = ^ \x95 INTERSet = ^ \x95 Select M entities with most Info score from NESet to INTERSet . \n\t', '\n\t\t \x95 Cluster the entities in INTERSet into K clusters \x95 Add the centroid entity of each cluster to BatchSet Figure 3 : Sample Selection Strategy 1 \x95 Strategy 2 : ( Figure 4 ) We combine the infor- mativeness and representativeness criteria using the functio l Info(NE) + ( 1^ l )Density(NEi) , in which the Info and Density value of NEi are normalized first . \n\t', '\n\t\t The individual importance of each criterion in this function is adjusted by the trade- off parameter l ( 0 ^ l ^1 ) ( set to 0.6 in our experiment ) . \n\t', '\n\t\t First , we select a candidate example NEi with the maximum value of this function from NESet . \n\t', '\n\t\t Second , we consider diversity criterion using the local method in Section 3.3.2 . \n\t', '\n\t\t We add the candidate example NEi to a batch only if NEi is different enough from any previously selected example in the batch . \n\t', '\n\t\t The threshold ß is set to the average pair-wise similarity of the entities in NE- Set . \n\t', '\n\t\t Given : NESet = { NE1 , ... , NEN } BatchSet with the maximal size K. Initialization : BatchSet = ^ Loop until BatchSet is full \x95 Select NEi which have the maximum value for the combination function between Info score and Density socre from NESet . \n\t', '\n\t\t NEi = argMax(l Info (NEi)+ ( 1^ l )Density(NEi)) NEi^ NESet \x95 RepeatFlag = false ; \x95 Loop from j = 1 to CurrentSize(BatchSet) If Sim ( NEi , NEj ) ^ b Then RepeatFlag = true ; Stop the Loop ; \x95 If RepeatFlag == false Then add NEi into BatchSet \x95 remove NEi from NESet Figure 4 : Sample Selection Strategy 2 4 Experimental Results and Analysis 4.1 Experiment Settings In order to evaluate the effectiveness of our selection strategies , we apply them to recognize protein ( PRT ) names in biomedical domain using GENIA corpus V 1.1 \n\t\t']",Positive
"['\n\t\t First , we randomly split the whole corpus into three parts : an initial training set to build an initial model , a test set to evaluate the performance of the model and an unlabeled set to select examples . \n\t', '\n\t\t The size of each data set is shown in Table 1 . \n\t', '\n\t\t Then , iteratively , we select a batch of examples following the selection strategies proposed , require human experts to label them and add them into the training set . \n\t', '\n\t\t The batch size K = 50 in GENIA and 10 in MUC-6 . \n\t', '\n\t\t Each example is defined as a machine-recognized named entity and its context words ( previous 3 words and next 3 words ) . \n\t', '\n\t\t Domain Class Corpus Initial Training Set Test Set Unlabeled Set Biomedical PRT GENIA1.1 10 sent . \n\t', '\n\t\t ( 277 words ) 900 sent . \n\t', '\n\t\t ( 26K words ) 8004 sent . \n\t', '\n\t\t ( 223K words ) Newswire PER MUC-6 5 sent . \n\t', '\n\t\t ( 131 words ) 602 sent . \n\t', '\n\t\t ( 14K words ) 7809 sent . \n\t', '\n\t\t ( 157K words ) LOC 5 sent . \n\t', '\n\t\t ( 130 words ) 7809 sent . \n\t', '\n\t\t ( 157K words ) ORG 5 sent . \n\t', '\n\t\t ( 113 words ) 7809 sent . \n\t', '\n\t\t ( 157K words ) Table 1 : Experiment settings for active learning using GENIA1 . \n\t', '\n\t\t 1(PRT) and MUC-6(PER,LOC,ORG) The goal of our work is to minimize the human annotation effort to learn a named entity recognizer with the same performance level as supervised learning . \n\t', '\n\t\t The performance of our model is evaluated using \x93precision/recall/F-measure\x94 . \n\t', '\n\t\t 4.2 Overall Result in GENIA and MUC-6 In this section , we evaluate our selection strategies by comparing them with a random selection method , in which a batch of examples is randomly selected iteratively , on GENIA and MUC-6 corpus . \n\t', '\n\t\t Table 2 shows the amount of training data needed to achieve the performance of supervised learning using various selection methods , viz . \n\t', '\n\t\t Random , Strategy1 and Strategy2 . \n\t', '\n\t\t In GENIA , we find : \x95 The model achieves 63.3 F-measure using 223K words in the supervised learning . \n\t', '\n\t\t \x95 The best performer is Strategy2 ( 31K words ) , requiring less than 40 % of the training data that Random ( 83K words ) does and 14 % of the training data that the supervised learning does . \n\t', '\n\t\t \x95 Strategy1 ( 40K words ) performs slightly worse than Strategy2 , requiring 9K more words . \n\t', '\n\t\t It is probably because Strategy1 cannot avoid selecting outliers if a cluster is too small . \n\t', '\n\t\t \x95 Random ( 83K words ) requires about 37 % of the training data that the supervised learning does . \n\t', '\n\t\t It indicates that only the words in and around a named entity are useful for classification and the words far from the named entity may not be helpful . \n\t', '\n\t\t Class Supervised Random Strategy1 Strategy2 PRT 223K ( F=63.3 ) 83K 40K 31K PER 157K ( F=90.4 ) 11.5K 4.2K 3.5K LOC 157K ( F=73.5 ) 13.6K 3.5K 2.1K ORG 157K ( F=86.0 ) 20.2K 9.5K 7.8K Table 2 : Overall Result in GENIA and MUC-6 Furthermore , when we apply our model to newswire domain ( MUC-6 ) to recognize person , location and organization names , Strategy1 and Strategy2 show a more promising result by comparing with the supervised learning and Random , as shown in Table 2 . \n\t', '\n\t\t On average , about 95 % of the data can be reduced to achieve the same performance with the supervised learning in MUC-6 . \n\t', '\n\t\t It is probably because NER in the newswire domain is much simpler than that in the biomedical domain \n\t\t']",Positive
"['\n\t\t 4.3 Effectiveness of Informativeness-based Selection Method In this section , we investigate the effectiveness of informativeness criterion in NER task . \n\t', '\n\t\t Figure 5 shows a plot of training data size versus F-measure achieved by the informativeness-based measures in Section 3.1.2 : Info_Avg , Info_Min and Info_S/N as well as Random . \n\t', '\n\t\t We make the comparisons in GENIA corpus . \n\t', '\n\t\t In Figure 5 , the horizontal line is the performance level ( 63.3 F-measure ) achieved by supervised learning ( 223K words ) . \n\t', '\n\t\t We find that the three informativeness-based measures perform similarly and each of them outperforms Random . \n\t', '\n\t\t Table 3 highlights the various data sizes to achieve the peak performance using these selection methods . \n\t', '\n\t\t We find that Random ( 83K words ) on average requires over 1.5 times as much as data to achieve the same performance as the informativeness-based selection methods ( 52K words ) . \n\t', '\n\t\t Figure 5 : Active learning curves : effectiveness of the three informativeness-criterion-based selections comparing with the Random selection . \n\t', '\n\t\t Supervised Random Info_Avg Info_Min Info_ S/N 223K 83K 52.0K 51.9K 52.3K Table 3 : Training data sizes for various selection methods to achieve the same performance level as the supervised learning 4.4 Effectiveness of Two Sample Selection Strategies In addition to the informativeness criterion , we further incorporate representativeness and diversity criteria into active learning using two strategies described in Section 3 . \n\t', '\n\t\t Comparing the two strategies with the best result of the single-criterionbased selection methods Info_Min , we are to justify that representativeness and diversity are also important factors for active learning . \n\t', '\n\t\t Figure 6 shows the learning curves for the various methods : Strategy1 , Strategy2 and Info_Min . \n\t', '\n\t\t In the beginning iterations ( F-measure < 60 ) , the three methods performed similarly . \n\t', '\n\t\t But with the larger training set , the efficiencies of Stratety1 and Strategy2 begin to be evident . \n\t', '\n\t\t Table 4 highlights the final result of the three methods . \n\t', '\n\t\t In order to reach the performance of supervised learning , Strategy1 ( 40K words ) and Strategyy2 ( 31K words ) require about 80 % and 60 % of the data that Info_Min ( 51.9K ) does . \n\t', '\n\t\t So we believe the effective combinations of informativeness , representativeness and diversity will help to learn the NER model more quickly and cost less in annotation . \n\t', '\n\t\t 0 20 40 60 K words 80 0.65 0.55 F 0.5 0.6 Supervised Random Info_Min Info_S/N Info_Avg 0 20 40 60 K words Figure 6 : Active learning curves : effectiveness of the two multi-criteria-based selection strategies comparing with the informativeness-criterion-based selection ( Info_Min ) . \n\t', '\n\t\t Info_Min Strategy 1 Strategy2 51.9K 40K 31K Table 4 : Comparisons of training data sizes for the multicriteria-based selection strategies and the informativenesscriterion-based selection ( Info_Min ) to achieve the same performance level as the supervised learning . \n\t', '\n\t\t 5 Related Work Since there is no study on active learning for NER task previously , we only introduce general active learning methods here . \n\t', '\n\t\t Many existing active learning methods are to select the most uncertain examples using various measures \n\t\t']",Negative
"['\n\t\t Our informativeness-based measure is similar to these works . \n\t', '\n\t\t However these works just follow a single criterion . \n\t', '\n\t\t \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t Moreover , the representativeness measure we use is relatively general and easy to adapt to other tasks , in which the example selected is a sequence of words , such as text chunking , POS tagging , etc. . \n\t', '\n\t\t On the other hand , \n\t\t']",Positive
"['\n\t\t Their work is similar to our local consideration in Section 2.3.2 . \n\t', '\n\t\t However , he didn\x92t further explore how to avoid selecting outliers to a batch . \n\t', '\n\t\t So far , we haven\x92t found any previous work integrating the informativeness , representativeness and diversity all together . \n\t', '\n\t\t 6 Conclusion and Future Work In this paper , we study the active learning in a more complex NLP task , named entity recognition . \n\t', '\n\t\t We propose a multi-criteria-based approach to select examples based on their informativeness , representativeness and diversity , which are incorporated all together by two strategies ( local and global ) . \n\t', '\n\t\t Experiments show that , in both MUC6 and GENIA , both of the two strategies combining the three criteria outperform the single criterion ( informativeness ) . \n\t', '\n\t\t The labeling cost can be significantly reduced by at least 80 % comparing with the supervised learning . \n\t', '\n\t\t To our best knowledge , this is not only the first work to report the empirical results of active learning for NER , but also the first work to incorporate the three criteria all together for selecting examples . \n\t', '\n\t\t Although the current experiment results are very promising , some parameters in our experi- ment , such as the batch size K and the l in the function of strategy 2 , are decided by our experience in the domain . \n\t', '\n\t\t In practical application , the optimal value of these parameters should be decided automatically based on the training process . \n\t', '\n\t\t Furthermore , we will study how to overcome the limitation of the strategy 1 discussed in Section 3 by using more effective clustering algorithm . \n\t', '\n\t\t Another interesting work is to study when to stop active learning . \n\t', '\n\t\t References R. Baeza-Yates and B. Ribeiro-Neto . \n\t', '\n\t\t 1999. Modern Information Retrieval . \n\t', '\n\t\t ISBN 0-201-39829-X. K. Brinker . \n\t', '\n\t\t 2003 . \n\t', '\n\t\t Incorporating Diversity in Active Learning with Support Vector Machines . \n\t', '\n\t\t In Proceedings of ICML , 2003 . \n\t', '\n\t\t S. A. Engelson and I. Dagan . \n\t', '\n\t\t 1999. Committee- Based Sample Selection for Probabilistic Classifiers . \n\t', '\n\t\t Journal of Artifical Intelligence Research . \n\t', '\n\t\t F. Jelinek . \n\t', '\n\t\t 1997. Statistical Methods for Speech Recognition . \n\t', '\n\t\t MIT Press . \n\t', '\n\t\t J. Kazama , T. Makino , Y. Ohta and J. Tsujii . \n\t', '\n\t\t 2002 . \n\t', '\n\t\t Tuning Support Vector Machines for Biomedical Named Entity Recognition . \n\t', '\n\t\t In Proceedings of the ACL2002 Workshop on NLP in Biomedicine . \n\t', '\n\t\t K. J. Lee , Y. S. Hwang and H. C. Rim . \n\t', '\n\t\t 2003 . \n\t', '\n\t\t Two- Phase Biomedical NE Recognition based on SVMs . \n\t', '\n\t\t In Proceedings of the ACL2003 Work- shop on NLP in Biomedicine . \n\t', '\n\t\t D. D. Lewis and J. Catlett . \n\t', '\n\t\t 1994. Heterogeneous Uncertainty Sampling for Supervised Learning . \n\t', '\n\t\t In Proceedings of ICML , 1994 . \n\t', '\n\t\t A. McCallum and K. Nigam . \n\t', '\n\t\t 1998. Employing EM in Pool-Based Active Learning for Text Classification . \n\t', '\n\t\t In Proceedings of ICML , 1998 . \n\t', '\n\t\t G. Ngai and D. Yarowsky . \n\t', '\n\t\t 2000. Rule Writing or Annotation : Cost-efficient Resource Usage for Base Noun Phrase Chunking . \n\t', '\n\t\t In Proceedings of ACL , 2000 . \n\t', '\n\t\t 0.65 F 0.55 0.6 0.5 Supervised Info_Min Strategy1 Strategy2 T. Ohta , Y. Tateisi , J. Kim , H. Mima and J. Tsujii . \n\t', '\n\t\t 2002. The GENIA corpus : An annotated research abstract corpus in molecular biology domain . \n\t', '\n\t\t In Proceedings of HLT 2002 . \n\t', '\n\t\t L. R. Rabiner , A. E. Rosenberg and S. E. Levinson . \n\t', '\n\t\t 1978. Considerations in Dynamic Time Warping Algorithms for Discrete Word Recognition . \n\t', '\n\t\t In Proceedings of IEEE Transactions on acoustics , speech and signal processing . \n\t', '\n\t\t Vol. ASSP-26 , NO.6 . \n\t', '\n\t\t D. Schohn and D. Cohn . \n\t', '\n\t\t 2000. Less is More : Active Learning with Support Vector Machines . \n\t', '\n\t\t In Proceedings of the 17th International Conference on Machine Learning . \n\t', '\n\t\t D. Shen , J. Zhang , G. D. Zhou , J. Su and C. L. Tan . \n\t', '\n\t\t 2003. Effective Adaptation of a Hidden Markov Model-based Named Entity Recognizer for Biomedical Domain . \n\t', '\n\t\t In Proceedings of the ACL2003 Workshop on NLP in Biomedicine . \n\t', '\n\t\t M. Steedman , R. Hwa , S. Clark , M. Osborne , A. Sarkar , J. Hockenmaier , P. Ruhlen , S. Baker and J. Crim . \n\t', '\n\t\t 2003 . \n\t', '\n\t\t Example Selection for Bootstrapping Statistical Parsers . \n\t', '\n\t\t In Proceedings of HLTNAACL , 2003 . \n\t', '\n\t\t M. Tang , X. Luo and S. Roukos . \n\t', '\n\t\t 2002. Active Learning for Statistical Natural Language Parsing . \n\t', '\n\t\t In Proceedings of the ACL 2002 . \n\t', '\n\t\t C. A. Thompson , M. E. Califf and R. J. Mooney . \n\t', '\n\t\t 1999. Active Learning for Natural Language Parsing and Information Extraction . \n\t', '\n\t\t In Proceedings of ICML 1999 . \n\t', '\n\t\t S. Tong and D. Koller . \n\t', '\n\t\t 2000. Support Vector Machine Active Learning with Applications to Text Classification . \n\t', '\n\t\t Journal of Machine Learning Research . \n\t', '\n\t\t V. Vapnik . \n\t', '\n\t\t 1998. Statistical learning theory . \n\t', '\n\t\t N.Y.:John Wiley . \n\t', '\n\t\t Automatic Evaluation of Machine Translation Quality Using Longest Com- mon Subsequence and Skip-Bigram Statistics Chin-Yew Lin and Franz Josef Och Information Sciences Institute University of Southern California 4676 Admiralty Way Marina del Rey , CA 90292 , USA {cyl,och}@isi.edu Abstract In this paper we describe two new objective automatic evaluation methods for machine translation . \n\t', '\n\t\t The first method is based on longest common subsequence between a candidate translation and a set of reference translations . \n\t', '\n\t\t Longest common subsequence takes into account sentence level structure similarity naturally and identifies longest co-occurring in- sequence n-grams automatically . \n\t', '\n\t\t The second method relaxes strict n-gram matching to skipbigram matching . \n\t', '\n\t\t Skip-bigram is any pair of words in their sentence order . \n\t', '\n\t\t Skip-bigram co- occurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations . \n\t', '\n\t\t The empirical results show that both methods correlate with human judgments very well in both adequacy and fluency . \n\t', '\n\t\t 1 Introduction Using objective functions to automatically evaluate machine translation quality is not new . \n\t', '\n\t\t \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t Nießen et al . ( 2000 ) calculated the length- normalized edit distance , called word error rate ( WER ) , between a candidate and multiple reference translations . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t Instead of error measures , we can also use accuracy measures that compute similarity between candidate and reference translations in proportion to the number of common words between them as suggested by \n\t\t']",Positive
"['\n\t\t An n-gram co-occurrence measure , BLEU , proposed by \n\t\t']",Positive
"['\n\t\t A variant of BLEU developed by NIST ( 2002 ) has been used in two recent large-scale machine translation evaluations . \n\t', '\n\t\t Recently , \n\t\t']",Positive
"['\n\t\t However , results based on their method , General Text Matcher ( GTM ) , showed that unigram F-measure correlated best with human judgments while assigning more weight to higher n-gram ( n > 1 ) matches achieved similar performance as Bleu . \n\t', '\n\t\t Since unigram matches do not distinguish words in consecutive positions from words in the wrong order , measures based on position-independent unigram matches are not sensitive to word order and sentence level structure . \n\t', '\n\t\t Therefore , systems optimized for these unigram-based measures might generate adequate but not fluent target language . \n\t', '\n\t\t Since BLEU has been used to report the performance of many machine translation systems and it has been shown to correlate well with human judgments , we will explain BLEU in more detail and point out its limitations in the next section . \n\t', '\n\t\t We then introduce a new evaluation method called ROUGE-L that measures sentence-to-sentence similarity based on the longest common subsequence statistics between a candidate translation and a set of reference translations in Section 3 . \n\t', '\n\t\t Section 4 describes another automatic evaluation method called ROUGE-S that computes skipbigram co-occurrence statistics . \n\t', '\n\t\t Section 5 presents the evaluation results of ROUGE-L , and ROUGES and compare them with BLEU , GTM , NIST , PER , and WER in correlation with human judgments in terms of adequacy and fluency . \n\t', '\n\t\t We conclude this paper and discuss extensions of the current work in Section 6 . \n\t', '\n\t\t 2 BLEU and N-gram Co-Occurrence To automatically evaluate machine translations the machine translation community recently adopted an n-gram co-occurrence scoring procedure BLEU \n\t\t']",Positive
"['\n\t\t In two recent large-scale machine translation evaluations sponsored by NIST , a closely related automatic evalua- tion method , simply called NIST score , was used . \n\t', '\n\t\t The NIST ( NIST 2002 ) scoring method is based on BLEU . \n\t', '\n\t\t The main idea of BLEU is to measure the similarity between a candidate translation and a set of reference translations with a numerical metric . \n\t', '\n\t\t They used a weighted average of variable length n- gram matches between system translations and a set of human reference translations and showed that the weighted average metric correlating highly with human assessments . \n\t', '\n\t\t BLEU measures how well a machine translation overlaps with multiple human translations using n- gram co-occurrence statistics . \n\t', '\n\t\t N-gram precision in BLEU is computed as follows : ^ ^ Countclip ( n^gram ) p = C^{Candidates}n^gram^C ( 1 ) n ^ ^Count(n gram ) C^ { Candidates } n^gram^ C Where Countclip(n-gram) is the maximum number of n-grams co-occurring in a candidate translation and a reference translation , and Count(ngram) is the number of n-grams in the candidate translation . \n\t', '\n\t\t To prevent very short translations that try to maximize their precision scores , BLEU adds a brevity penalty , BP , to the formula : 1 if c > r ^ e(1^Ir|/Ic|) if c ^ r Where |c| is the length of the candidate translation and |r| is the length of the reference translation . \n\t', '\n\t\t The BLEU formula is then written as follows : BLEU =BP\x95exp^^wn log pn ^ ^ n=1 ^ N The weighting factor , wn , is set at 1/N . \n\t', '\n\t\t Although BLEU has been shown to correlate well with human assessments , it has a few things that can be improved . \n\t', '\n\t\t First the subjective application of the brevity penalty can be replaced with a recall related parameter that is sensitive to reference length . \n\t', '\n\t\t Although brevity penalty will penalize candidate translations with low recall by a factor of e(1- |r|/|c| ) , it would be nice if we can use the traditional recall measure that has been a well known measure in NLP as suggested by \n\t\t']",Positive
"['\n\t\t Of course we have to make sure the resulting composite function of precision and recall is still correlates highly with human judgments . \n\t', '\n\t\t Second , although BLEU uses high order n-gram ( n>1 ) matches to favor candidate sentences with consecutive word matches and to estimate their fluency , it does not consider sentence level structure . \n\t', '\n\t\t For example , given the following sentences : S1 . \n\t', '\n\t\t police killed the gunman S2 . \n\t', '\n\t\t police kill the gunman1 S3 . \n\t', '\n\t\t the gunman kill police We only consider BLEU with unigram and bigram , i.e. N=2 , for the purpose of explanation and call this BLEU-2 . \n\t', '\n\t\t Using S 1 as the reference and S2 and S3 as the candidate translations , S2 and S3 would have the same BLEU-2 score , since they both have one bigram and three unigram matches2 . \n\t', '\n\t\t However , S2 and S3 have very different meanings . \n\t', '\n\t\t Third , BLEU is a geometric mean of unigram to N-gram precisions . \n\t', '\n\t\t Any candidate translation without a N-gram match has a per-sentence BLEU score of zero . \n\t', '\n\t\t Although BLEU is usually calculated over the whole test corpus , it is still desirable to have a measure that works reliably at sentence level for diagnostic and introspection purpose . \n\t', '\n\t\t To address these issues , we propose three new automatic evaluation measures based on longest common subsequence statistics and skip bigram co-occurrence statistics in the following sections . \n\t', '\n\t\t 3 Longest Common Subsequence 3.1 ROUGE-L A sequence Z = [ z1 , z2 , ... , zn ] is a subsequence of another sequence X = [ x1 , x2 , ... , xm ] , if there exists a strict increasing sequence [ i1 , i2 , ... , ik ] of indices of X such that for all j = 1 , 2 , ... , k , we have xij = zj \n\t\t']",Positive
"['\n\t\t Given two sequences X and Y , the longest common subsequence ( LCS ) of X and Y is a common subsequence with maximum length . \n\t', '\n\t\t We can find the LCS of two sequences of length m and n using standard dynamic programming technique in O(mn) time . \n\t', '\n\t\t LCS has been used to identify cognate candidates during construction of N-best translation lexicons from parallel text . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t He used as an approximate string matching algorithm . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t NP-LCS can be shown as a special case of Equation ( 6 ) with , B = 1 . \n\t', '\n\t\t However , they did not provide the correlation analysis of NP-LCS with 1 This is a real machine translation output . \n\t', '\n\t\t 2 The \x93kill\x94 in S2 or S3 does not match with \x93killed\x94 in S 1 in strict word-to-word comparison . \n\t', '\n\t\t BP = ^ ^ ^ ) 2 ( ) 3 ( human judgments and its effectiveness as an automatic evaluation measure . \n\t', '\n\t\t To apply LCS in machine translation evaluation , we view a translation as a sequence of words . \n\t', '\n\t\t The intuition is that the longer the LCS of two translations is , the more similar the two translations are . \n\t', '\n\t\t We propose using LCS-based F-measure to estimate the similarity between two translations X of length m and Y of length n , assuming X is a reference translation and Y is a candidate translation , as follows : LCS(X,Y) = ( 4 ) m n ( 1+^2 )R P lcs lcs = Rlcs + ^2 Plcs Where LCS(X,Y) is the length of a longest common subsequence of X and Y , and , B = Plcs/Rlcs when aFlcs/aRlcs = aFlcs/aPlcs . \n\t', '\n\t\t We call the LCS-based F- measure , i.e. Equation 6 , ROUGE-L . \n\t', '\n\t\t Notice that ROUGE-L is 1 when X = Y since LCS(X,Y) = m or n ; while ROUGE-L is zero when LCS(X,Y) = 0 , i.e. there is nothing in common between X and Y. F- measure or its equivalents has been shown to have met several theoretical criteria in measuring accuracy involving more than one factor \n\t\t']",Positive
"['\n\t\t The composite factors are LCS-based recall and precision in this case . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t One advantage of using LCS is that it does not require consecutive matches but in-sequence matches that reflect sentence level word order as n- grams . \n\t', '\n\t\t The other advantage is that it automatically includes longest in-sequence common n-grams , therefore no predefined n-gram length is necessary . \n\t', '\n\t\t ROUGE-L as defined in Equation 6 has the property that its value is less than or equal to the minimum of unigram F-measure of X and Y. Unigram recall reflects the proportion of words in X ( reference translation ) that are also present in Y ( candidate translation ) ; while unigram precision is the proportion of words in Y that are also in X. Unigram recall and precision count all co-occurring words regardless their orders ; while ROUGE-L counts only in-sequence co-occurrences . \n\t', '\n\t\t By only awarding credit to in-sequence unigram matches , ROUGE-L also captures sentence level structure in a natural way . \n\t', '\n\t\t Consider again the example given in Section 2 that is copied here for convenience : S1 . \n\t', '\n\t\t police killed the gunman S2 . \n\t', '\n\t\t police kill the gunman S3 . \n\t', '\n\t\t the gunman kill police As we have shown earlier , BLEU-2 cannot differentiate S2 from S3 . \n\t', '\n\t\t However , S2 has a ROUGE-L score of 3/4 = 0.75 and S3 has a ROUGE-L score of 2/4 = 0.5 , with ^ = 1 . \n\t', '\n\t\t Therefore S2 is better than S3 according to ROUGE-L . \n\t', '\n\t\t This example also illustrated that ROUGE-L can work reliably at sentence level . \n\t', '\n\t\t However , LCS only counts the main in-sequence words ; therefore , other longest common subsequences and shorter sequences are not reflected in the final score . \n\t', '\n\t\t For example , consider the following candidate sentence : S4 . \n\t', '\n\t\t the gunman police killed Using S 1 as its reference , LCS counts either \x93the gunman\x94 or \x93police killed\x94 , but not both ; therefore , S4 has the same ROUGE-L score as S3 . \n\t', '\n\t\t BLEU-2 would prefer S4 over S3 . \n\t', '\n\t\t In Section 4 , we will introduce skip-bigram co-occurrence statistics that do not have this problem while still keeping the advantage of in-sequence ( not necessary consecutive ) matching that reflects sentence level word order . \n\t', '\n\t\t 3.2 Multiple References So far , we only demonstrated how to compute ROUGE-L using a single reference . \n\t', '\n\t\t When multiple references are used , we take the maximum LCS matches between a candidate translation , c , of n words and a set of u reference translations of mj words . \n\t', '\n\t\t The LCS-based F-measure can be computed as follows : ^ ^ LCS(rj , c ) Rlcs-multi =max ~ =1 mj =max , - ^LCS(rj,c)^ Plcs-multi =1 J n ^2)Rlcs^multiPcs^multi ( 9 ) Rlcs^multi + ^2 Pcs^multi where fl = Plcs-multi/Rlcs-multi when aFlcs-multi/aRlcsmulti = aFlcs-multi/ aPlcs-multi . \n\t', '\n\t\t This procedure is also applied to computation of ROUGE-S when multiple references are used . \n\t', '\n\t\t In the next section , we introduce the skip-bigram co- occurrence statistics . \n\t', '\n\t\t In the next section , we describe how to extend ROUGE-L to assign more credits to longest common subsequences with consecutive words . \n\t', '\n\t\t = LCS(X , Y ) Rlcs Plcs Flcs ( 5 ) ( 6 ) ( 7 ) ( 8 ) Flcs-multi 1+ 3.3 ROUGE-W : Weighted Longest Common Subsequence LCS has many nice properties as we have described in the previous sections . \n\t', '\n\t\t Unfortunately , the basic LCS also has a problem that it does not differentiate LCSes of different spatial relations within their embedding sequences . \n\t', '\n\t\t For example , given a reference sequence X and two candidate sequences Y1 and Y2 as follows : X : [ A B C D E F G ] Y1 : [ A B C D H I K ] Y2 : [ A H B K C I D ] Y1 and Y2 have the same ROUGE-L score . \n\t', '\n\t\t However , in this case , Y1 should be the better choice than Y2 because Y1 has consecutive matches . \n\t', '\n\t\t To improve the basic LCS method , we can simply remember the length of consecutive matches encountered so far to a regular two dimensional dynamic program table computing LCS . \n\t', '\n\t\t We call this weighted LCS ( WLCS ) and use k to indicate the length of the current consecutive matches ending at words xi and yj . \n\t', '\n\t\t Given two sentences X and Y , the WLCS score of X and Y can be computed using the following dynamic programming procedure : ( 1 ) For ( i = 0 ; i <=m ; i++ ) c(i , j ) = 0 // initialize c-table w(i , j ) = 0 // initialize w-table ( 2 ) For ( i = 1 ; i <= m ; i++ ) For ( j = 1 ; j <= n ; j++ ) If xi = yj Then // the length of consecutive matches at // position i-1 and j-1 k = w(i-1 , j-1 ) c(i , j ) = c(i-1 , j-1 ) + f(k+ 1 ) \x96 f(k) // remember the length of consecutive // matches at position i , j w(ij) = k+1 Otherwise If c(i-1 , j ) > c(i , j-1 ) Then c(i , j ) = c(i-1 , j ) w(i , j ) = 0 // no match at i , j Else c(i , j ) = c(i , j-1 ) w(i , j ) = 0 // no match at i , j ( 3 ) WLCS(X,Y) = c(m,n) Where c is the dynamic programming table , c(i , j ) stores the WLCS score ending at word xi of X and yj of Y , w is the table storing the length of consecutive matches ended at c table position i and j , and f is a function of consecutive matches at the table position , c(i , j ) . \n\t', '\n\t\t Notice that by providing different weighting function f , we can parameterize the WLCS algorithm to assign different credit to consecutive in-sequence matches . \n\t', '\n\t\t The weighting function f must have the property that f(x+y) > f(x) + f(y) for any positive integers x and y . \n\t', '\n\t\t In other words , consecutive matches are awarded more scores than non-consecutive matches . \n\t', '\n\t\t For example , f(k) = ^k \x96 ^ when k >= 0 , and ^ , ^ > 0 . \n\t', '\n\t\t This function charges a gap penalty of \x96^ for each non-consecutive n-gram sequences . \n\t', '\n\t\t Another possible function family is the polynomial family of the form k^ where ^ > 1 . \n\t', '\n\t\t However , in order to normalize the final ROUGE-W score , we also prefer to have a function that has a close form inverse function . \n\t', '\n\t\t For example , f(k) = k2 has a close form inverse function f -1(k) = k1/2 . \n\t', '\n\t\t F-measure based on WLCS can be computed as follows , given two sequences X of length m and Y of length n : Rwlcs = f \x971 WLCS(X,Y)^(10) ^ f(m) ^ Pwlcs = f \x971 WLCS(X,Y)^(11) ^ f ( n ) ^ Where f -1 is the inverse function of f . \n\t', '\n\t\t We call the WLCS-based F-measure , i.e. Equation 12 , ROUGE-W . \n\t', '\n\t\t Using Equation 12 and f(k) = k2 as the weighting function , the ROUGE-W scores for sequences Y1 and Y2 are 0.571 and 0.286 respectively . \n\t', '\n\t\t Therefore , Y1 would be ranked higher than Y2 using WLCS . \n\t', '\n\t\t We use the polynomial function of the form k^ in the ROUGE evaluation package . \n\t', '\n\t\t In the next section , we introduce the skip-bigram co- occurrence statistics . \n\t', '\n\t\t 4 ROUGE-S : Skip-Bigram Co-Occurrence Statistics Skip-bigram is any pair of words in their sentence order , allowing for arbitrary gaps . \n\t', '\n\t\t Skipbigram co-occurrence statistics measure the overlap of skip-bigrams between a candidate translation and a set of reference translations . \n\t', '\n\t\t Using the example given in Section 3.1 : S1 . \n\t', '\n\t\t police killed the gunman S2 . \n\t', '\n\t\t police kill the gunman S3 . \n\t', '\n\t\t the gunman kill police S4 . \n\t', '\n\t\t the gunman police killed Each sentence has C(4,2)3 = 6 skip-bigrams . \n\t', '\n\t\t For example , S 1 has the following skip-bigrams : 3 Combination : C(4,2) = 4!/(2!*2!) = 6 . \n\t', '\n\t\t Rwlcs + ^2 Pw lcs ( 1+^2 )RwlcsPwlcs = Fwlcs ( 12 ) ( \x93police killed\x94 , \x93police the\x94 , \x93police gunman\x94 , \x93killed the\x94 , \x93killed gunman\x94 , \x93the gunman\x94 ) S2 has three skip-bigram matches with S1 ( \x93police the\x94 , \x93police gunman\x94 , \x93the gunman\x94 ) , S3 has one skip-bigram match with S1 ( \x93the gunman\x94 ) , and S4 has two skip-bigram matches with S 1 ( \x93police killed\x94 , \x93the gunman\x94 ) . \n\t', '\n\t\t Given translations X of length m and Y of length n , assuming X is a reference translation and Y is a candidate translation , we compute skip-bigram-based F-measure as follows : SKIP2(X,Y) ( 13 ) Rskip2 = C(m,2) Pskip2 SKIP2(X,Y) ( 14 ) = C(n,2) Where SKIP2(X,Y) is the number of skip-bigram matches between X and Y , , B = Pskip2/Rskip2 when ^fskip2/^Rskip2 = ^fskip2/^Pskip2 , and C is the combi- nation function . \n\t', '\n\t\t We call the skip-bigram-based F- measure , i.e. Equation 15 , ROUGE-S . \n\t', '\n\t\t Using Equation 15 with , B = 1 and S 1 as the reference , S2\x92s ROUGE-S score is 0.5 , S3 is 0.167 , and S4 is 0.333 . \n\t', '\n\t\t Therefore , S2 is better than S3 and S4 , and S4 is better than S3 . \n\t', '\n\t\t This result is more intuitive than using BLEU-2 and ROUGE-L . \n\t', '\n\t\t One advantage of skip-bigram vs. BLEU is that it does not require consecutive matches but is still sensitive to word order . \n\t', '\n\t\t Comparing skip-bigram with LCS , skip-bigram counts all in-order matching word pairs while LCS only counts one longest common subsequence . \n\t', '\n\t\t We can limit the maximum skip distance , dskip , between two in-order words that is allowed to form a skip-bigram . \n\t', '\n\t\t Applying such constraint , we limit skip-bigram formation to a fix window size . \n\t', '\n\t\t Therefore , computation time can be reduced and hopefully performance can be as good as the version without such constraint . \n\t', '\n\t\t For example , if we set dskip to 0 then ROUGE-S is equivalent to bigram overlap . \n\t', '\n\t\t If we set dskip to 4 then only word pairs of at most 4 words apart can form skip-bigrams . \n\t', '\n\t\t Adjusting Equations 13 , 14 , and 15 to use maximum skip distance limit is straightforward : we only count the skip-bigram matches , SKIP2(X,Y) , within the maximum skip distance and replace denominators of Equations 13 , C(m,2) , and 14 , C(n,2) , with the actual numbers of within distance skip-bigrams from the reference and the candidate respectively . \n\t', '\n\t\t In the next section , we present the evaluations of ROUGE-L , ROUGE-S , and compare their performance with other automatic evaluation measures . \n\t', '\n\t\t 5 Evaluations One of the goals of developing automatic evaluation measures is to replace labor-intensive human evaluations . \n\t', '\n\t\t Therefore the first criterion to assess the usefulness of an automatic evaluation measure is to show that it correlates highly with human judgments in different evaluation settings . \n\t', '\n\t\t However , high quality large-scale human judgments are hard to come by . \n\t', '\n\t\t Fortunately , we have access to eight MT systems\x92 outputs , their human assessment data , and the reference translations from 2003 NIST Chinese MT evaluation ( NIST 2002a ) . \n\t', '\n\t\t There were 919 sentence segments in the corpus . \n\t', '\n\t\t We first computed averages of the adequacy and fluency scores of each system assigned by human evaluators . \n\t', '\n\t\t For the input of automatic evaluation methods , we created three evaluation sets from the MT outputs : 1 . \n\t', '\n\t\t Case set : The original system outputs with case information . \n\t', '\n\t\t 2. NoCase set : All words were converted into lower case , i.e. no case information was used . \n\t', '\n\t\t This set was used to examine whether human assessments were affected by case information since not all MT systems generate properly cased output . \n\t', '\n\t\t 3. Stem set : All words were converted into lower case and stemmed using the Porter stemmer \n\t\t']",Positive
"['\n\t\t Since ROUGE computed similarity on surface word level , stemmed version allowed ROUGE to perform more lenient matches . \n\t', '\n\t\t To accommodate multiple references , we use a Jackknifing procedure . \n\t', '\n\t\t Given N references , we compute the best score over N sets of N-1 references . \n\t', '\n\t\t The final score is the average of the N best scores using N different sets of N-1 references . \n\t', '\n\t\t The Jackknifing procedure is adopted since we often need to compare system and human performance and the reference translations are usually the only human translations available . \n\t', '\n\t\t Using this procedure , we are able to estimate average human performance by averaging N best scores of one reference vs. the rest N-1 references . \n\t', '\n\t\t We then computed average BLEU1-124 , GTM with exponents of 1.0 , 2.0 , and 3.0 , NIST , WER , and PER scores over these three sets . \n\t', '\n\t\t Finally we applied ROUGE-L , ROUGE-W with weighting function k1.2 , and ROUGE-S without skip distance 4 BLEUN computes BLEU over n-grams up to length N . \n\t', '\n\t\t Only BLEU1 , BLEU4 , and BLEU12 are shown in Table 1 . \n\t', '\n\t\t = 2 ( 15 ) 2 fskip2 ( 1+ ^ 2 ) Rskip2Pskip2 Rshp2 + ^ Pkip Adequacy With Case Information ( Case ) Lower Case ( NoCase ) Lower Case & Stemmed ( Stem ) Method P 95%L 95%U S 95%L 95%U P 95%L 95%U S 95%L 95%U P 95%L 95%U S 95%L 95%U BLEU1 0.86 0.83 0.89 0.80 0.71 0.90 0.87 0.84 0.90 0.76 0.67 0.89 0.91 0.89 0.93 0.85 0.76 0.95 BLEU4 0.77 0.72 0.81 0.77 0.71 0.89 0.79 0.75 0.82 0.67 0.55 0.83 0.82 0.78 0.85 0.76 0.67 0.89 BLEU12 0.66 0.60 0.72 0.53 0.44 0.65 0.72 0.57 0.81 0.65 0.25 0.88 0.72 0.58 0.81 0.66 0.28 0.88 NIST 0.89 0.86 0.92 0.78 0.71 0.89 0.87 0.85 0.90 0.80 0.74 0.92 0.90 0.88 0.93 0.88 0.83 0.97 WER 0.47 0.41 0.53 0.56 0.45 0.74 0.43 0.37 0.49 0.66 0.60 0.82 0.48 0.42 0.54 0.66 0.60 0.81 PER 0.67 0.62 0.72 0.56 0.48 0.75 0.63 0.58 0.68 0.67 0.60 0.83 0.72 0.68 0.76 0.69 0.62 0.86 ROUGE-L 0.87 0.84 0.90 0.84 0.79 0.93 0.89 0.86 0.92 0.84 0.71 0.94 0.92 0.90 0.94 0.87 0.76 0.95 ROUGE-W 0.84 0.81 0.87 0.83 0.74 0.90 0.85 0.82 0.88 0.77 0.67 0.90 0.89 0.86 0.91 0.86 0.76 0.95 ROUGE-S* 0.85 0.81 0.88 0.83 0.76 0.90 0.90 0.88 0.93 0.82 0.70 0.92 0.95 0.93 0.97 0.85 0.76 0.94 ROUGE-S0 0.82 0.78 0.85 0.82 0.71 0.90 0.84 0.81 0.87 0.76 0.67 0.90 0.87 0.84 0.90 0.82 0.68 0.90 ROUGE-S4 0.82 0.78 0.85 0.84 0.79 0.93 0.87 0.85 0.90 0.83 0.71 0.90 0.92 0.90 0.94 0.84 0.74 0.93 ROUGE-S9 0.84 0.80 0.87 0.84 0.79 0.92 0.89 0.86 0.92 0.84 0.76 0.93 0.94 0.92 0.96 0.84 0.76 0.94 GTM10 0.82 0.79 0.85 0.79 0.74 0.83 0.91 0.89 0.94 0.84 0.79 0.93 0.94 0.92 0.96 0.84 0.79 0.92 GTM20 0.77 0.73 0.81 0.76 0.69 0.88 0.79 0.76 0.83 0.70 0.55 0.83 0.83 0.79 0.86 0.80 0.67 0.90 GTM30 0.74 0.70 0.78 0.73 0.60 0.86 0.74 0.70 0.78 0.63 0.52 0.79 0.77 0.73 0.81 0.64 0.52 0.80 Fluency With Case Information ( Case ) Lower Case ( NoCase ) Lower Case & Stemmed ( Stem ) Method P 95%L 95%U S 95%L 95%U P 95%L 95%U S 95%L 95%U P 95%L 95%U S 95%L 95%U 0.76 0.62 0.90 0.73 0.67 0.79 0.70 0.62 0.81 BLEU1 0.81 0.75 0.86 0.70 0.63 0.77 0.79 0.67 0.90 BLEU4 0.86 0.81 0.90 0.74 0.62 0.86 0.83 0.78 0.88 0.68 0.60 0.81 0.83 0.78 0.88 0.70 0.62 0.81 BLEU12 0.87 0.76 0.93 0.66 0.33 0.79 0.93 0.81 0.97 0.78 0.44 0.94 0.93 0.84 0.97 0.80 0.49 0.94 NIST 0.81 0.75 0.87 0.74 0.62 0.86 0.70 0.64 0.77 0.68 0.60 0.79 0.68 0.61 0.75 0.77 0.67 0.88 WER 0.69 0.62 0.75 0.68 0.57 0.85 0.59 0.51 0.66 0.70 0.57 0.82 0.60 0.52 0.68 0.69 0.57 0.81 PER 0.79 0.74 0.85 0.67 0.57 0.82 0.68 0.60 0.73 0.69 0.60 0.81 0.70 0.63 0.76 0.65 0.57 0.79 ROUGE-L 0.83 0.77 0.88 0.80 0.67 0.90 0.76 0.69 0.82 0.79 0.64 0.90 0.73 0.66 0.80 0.78 0.67 0.90 ROUGE-W 0.85 0.80 0.90 0.79 0.63 0.90 0.78 0.73 0.84 0.72 0.62 0.83 0.77 0.71 0.83 0.78 0.67 0.90 ROUGE-S* 0.84 0.78 0.89 0.79 0.62 0.90 0.80 0.74 0.86 0.77 0.64 0.90 0.78 0.71 0.84 0.79 0.69 0.90 ROUGE-S0 0.87 0.81 0.91 0.78 0.62 0.90 0.83 0.78 0.88 0.71 0.62 0.82 0.82 0.77 0.88 0.76 0.62 0.90 ROUGE-S4 0.84 0.79 0.89 0.80 0.67 0.90 0.82 0.77 0.87 0.78 0.64 0.90 0.81 0.75 0.86 0.79 0.67 0.90 ROUGE-S9 0.84 0.79 0.89 0.80 0.67 0.90 0.81 0.76 0.87 0.79 0.69 0.90 0.79 0.73 0.85 0.79 0.69 0.90 GTM10 0.73 0.66 0.79 0.76 0.60 0.87 0.71 0.64 0.78 0.80 0.67 0.90 0.66 0.58 0.74 0.80 0.64 0.90 GTM20 0.86 0.81 0.90 0.80 0.67 0.90 0.83 0.77 0.88 0.69 0.62 0.81 0.83 0.77 0.87 0.74 0.62 0.89 GTM30 0.87 0.81 0.91 0.79 0.67 0.90 0.83 0.77 0.87 0.73 0.62 0.83 0.83 0.77 0.88 0.71 0.60 0.83 Table 1 . \n\t', '\n\t\t Pearson\x92s p and Spearman\x92s p correlations of automatic evaluation measures vs. adequacy and fluency : BLEU1 , 4 , and 12 are BLEU with maximum of 1 , 4 , and 12 grams , NIST is the NIST score , ROUGE-L is LCS-based F-measure ( 0 = 1 ) , ROUGE-W is weighted LCS-based F-measure ( 0 = 1 ) . \n\t', '\n\t\t ROUGE-S* is skip-bigram-based co-occurrence statistics with any skip distance limit , ROUGESN is skip-bigram-based F-measure ( 0 = 1 ) with maximum skip distance of N , PER is position independent word error rate , and WER is word error rate . \n\t', '\n\t\t GTM 10 , 20 , and 30 are general text matcher with exponents of 1.0 , 2.0 , and 3.0 . \n\t', '\n\t\t ( Note , only BLEU1 , 4 , and 12 are shown here to preserve space . \n\t', '\n\t\t ) limit and with skip distant limits of 0 , 4 , and 9 . \n\t', '\n\t\t Correlation analysis based on two different correlation statistics , Pearson\x92s p and Spearman\x92s p , with respect to adequacy and fluency are shown in Table 1 . \n\t', '\n\t\t The Pearson\x92s correlation coefficient5 measures the strength and direction of a linear relationship between any two variables , i.e. automatic metric score and human assigned mean coverage score in our case . \n\t', '\n\t\t It ranges from +1 to -1 . \n\t', '\n\t\t A correlation of 1 means that there is a perfect positive linear relationship between the two variables , a correlation of -1 means that there is a perfect negative linear relationship between them , and a correlation of 0 means that there is no linear relationship between them . \n\t', '\n\t\t Since we would like to use automatic evaluation metric not only in comparing systems 5 For a quick overview of the Pearson\x92s coefficient , see : http://davidmlane.com/hyperstat/A34739.html . \n\t', '\n\t\t but also in in-house system development , a good linear correlation with human judgment would enable us to use automatic scores to predict corresponding human judgment scores . \n\t', '\n\t\t Therefore , Pearson\x92s correlation coefficient is a good measure to look at . \n\t', '\n\t\t Spearman\x92s correlation coefficient 6 is also a measure of correlation between two variables . \n\t', '\n\t\t It is a non-parametric measure and is a special case of the Pearson\x92s correlation coefficient when the values of data are converted into ranks before computing the coefficient . \n\t', '\n\t\t Spearman\x92s correlation coefficient does not assume the correlation between the variables is linear . \n\t', '\n\t\t Therefore it is a useful correlation indicator even when good linear correlation , for example , according to Pearson\x92s correlation coefficient between two variables could 6 For a quick overview of the Spearman\x92s coefficient , see : http://davidmlane.com/hyperstat/A62436.html . \n\t', '\n\t\t not be found . \n\t', '\n\t\t It also suits the NIST MT evaluation scenario where multiple systems are ranked according to some performance metrics . \n\t', '\n\t\t To estimate the significance of these correlation statistics , we applied bootstrap resampling , generating random samples of the 919 different sentence segments . \n\t', '\n\t\t The lower and upper values of 95 % confidence interval are also shown in the table . \n\t', '\n\t\t Dark ( green ) cells are the best correlation numbers in their categories and light gray cells are statistically equivalent to the best numbers in their categories . \n\t', '\n\t\t Analyzing all runs according to the adequacy and fluency table , we make the following observations : Applying the stemmer achieves higher correlation with adequacy but keeping case information achieves higher correlation with fluency except for BLEU7-12 ( only BLEU12 is shown ) . \n\t', '\n\t\t For example , the Pearson\x92s p ( P ) correlation of ROUGE-S* with adequacy increases from 0.85 ( Case ) to 0.95 ( Stem ) while its Pearson\x92s p correlation with fluency drops from 0.84 ( Case ) to 0.78 ( Stem ) . \n\t', '\n\t\t We will focus our discussions on the Stem set in adequacy and Case set in fluency . \n\t', ""\n\t\t The Pearson 's p correlation values in the Stem set of the Adequacy Table , indicates that ROUGE- L and ROUGE-S with a skip distance longer than 0 correlate highly and linearly with adequacy and outperform BLEU and NIST . \n\t"", '\n\t\t ROUGE-S* achieves that best correlation with a Pearson\x92s p of 0.95 . \n\t', '\n\t\t Measures favoring consecutive matches , i.e. BLEU4 and 12 , ROUGE-W , GTM20 and 30 , ROUGE-S0 ( bigram ) , and WER have lower Pearson\x92s p . \n\t', '\n\t\t Among them WER ( 0.48 ) that tends to penalize small word movement is the worst performer . \n\t', '\n\t\t One interesting observation is that longer BLEU has lower correlation with adequacy . \n\t', ""\n\t\t Spearman\x92s p values generally agree with Pear- son 's p but have more equivalents . \n\t"", ""\n\t\t The Pearson 's p correlation values in the Stem set of the Fluency Table , indicates that BLEU12 has the highest correlation ( 0.93 ) with fluency . \n\t"", '\n\t\t However , it is statistically indistinguishable with 95 % confidence from all other metrics shown in the Case set of the Fluency Table except for WER and GTM10 . \n\t', '\n\t\t GTM10 has good correlation with human judgments in adequacy but not fluency ; while GTM20 and GTM30 , i.e. GTM with exponent larger than 1.0 , has good correlation with human judgment in fluency but not adequacy . \n\t', '\n\t\t ROUGE-L and ROUGE-S* , 4 , and 9 are good automatic evaluation metric candidates since they perform as well as BLEU in fluency correlation analysis and outperform BLEU4 and 12 significantly in adequacy . \n\t', '\n\t\t Among them , ROUGE-L is the best metric in both adequacy and fluency correlation with human judgment according to Spear man\x92s correlation coefficient and is statistically indistinguishable from the best metrics in both adequacy and fluency correlation with human judgment according to Pearson\x92s correlation coefficient . \n\t', '\n\t\t 6 Conclusion In this paper we presented two new objective automatic evaluation methods for machine translation , ROUGE-L based on longest common subsequence ( LCS ) statistics between a candidate translation and a set of reference translations . \n\t', '\n\t\t Longest common subsequence takes into account sentence level structure similarity naturally and identifies longest co-occurring in-sequence n- grams automatically while this is a free parameter in BLEU . \n\t', '\n\t\t To give proper credit to shorter common sequences that are ignored by LCS but still retain the flexibility of non-consecutive matches , we proposed counting skip bigram co-occurrence . \n\t', ""\n\t\t The skip-bigram-based ROUGE-S* ( without skip distance restriction ) had the best Pearson 's p correlation of 0.95 in adequacy when all words were lower case and stemmed . \n\t"", '\n\t\t ROUGE-L , ROUGE-W , ROUGE-S* , ROUGE-S4 , and ROUGE-S9 were equal performers to BLEU in measuring fluency . \n\t', '\n\t\t However , they have the advantage that we can apply them on sentence level while longer BLEU such as BLEU12 would not differentiate any sentences with length shorter than 12 words ( i.e. no 12-gram matches ) . \n\t', '\n\t\t We plan to explore their correlation with human judgments on sentence-level in the future . \n\t', '\n\t\t We also confirmed empirically that adequacy and fluency focused on different aspects of machine translations . \n\t', '\n\t\t Adequacy placed more emphasis on terms co-occurred in candidate and reference translations as shown in the higher correlations in Stem set than Case set in Table 1 ; while the reverse was true in the terms of fluency . \n\t', '\n\t\t The evaluation results of ROUGE-L , ROUGE- W , and ROUGE-S in machine translation evaluation are very encouraging . \n\t', '\n\t\t However , these measures in their current forms are still only applying string-to-string matching . \n\t', '\n\t\t We have shown that better correlation with adequacy can be reached by applying stemmer . \n\t', '\n\t\t In the next step , we plan to extend them to accommodate synonyms and paraphrases . \n\t', '\n\t\t For example , we can use an existing thesaurus such as WordNet \n\t\t']",Positive
['\n\t\t Paraphrases can also be automatically acquired using statistical methods as shown by \n\t\t'],Positive
"['\n\t\t Once we have acquired synonym and paraphrase data , we then need to design a soft matching function that assigns partial credits to these approximate matches . \n\t', '\n\t\t In this scenario , statistically generated data has the advantage of being able to provide scores reflecting the strength of similarity between synonyms and paraphrased . \n\t', '\n\t\t ROUGE-L , ROUGE-W , and ROUGE-S have also been applied in automatic evaluation of summarization and achieved very promising results \n\t\t']",Positive
['\n\t\t In \n\t\t'],Positive
"['\n\t\t According to the results reported in that paper , ROUGE-L , ROUGE-W , and ROUGE-S also outperformed BLEU and NIST . \n\t', '\n\t\t References Akiba , Y. , K. Imamura , and E. Sumita . \n\t', '\n\t\t 2001. Using Multiple Edit Distances to Automatically Rank Machine Translation Output . \n\t', '\n\t\t In Proceedings of the MT Summit VIII , Santiago de Compostela , Spain . \n\t', '\n\t\t Barzilay , R. and L. Lee . \n\t', '\n\t\t 2003. Learning to Paraphrase : An Unsupervised Approach Using Multiple-Sequence Alignmen . \n\t', '\n\t\t In Proceeding of NAACL-HLT 2003 , Edmonton , Canada . \n\t', '\n\t\t Leusch , G. , N. Ueffing , and H. Ney . \n\t', '\n\t\t 2003. A Novel String-to-String Distance Measure with Applications to Machine Translation Evaluation . \n\t', '\n\t\t In Proceedings of MT Summit IY , New Orleans , U.S.A. . \n\t', '\n\t\t Levenshtein , V. I. 1966 . \n\t', '\n\t\t Binary codes capable of correcting deletions , insertions and reversals . \n\t', '\n\t\t Soviet Physics Doklady . \n\t', '\n\t\t Lin , C.Y. 2004 . \n\t', '\n\t\t ROUGE : A Package for Automatic Evaluation of Summaries . \n\t', '\n\t\t In Proceedings of the Workshop on Text Summarization Branches Out , post-conference workshop of ACL 2004 , Barcelona , Spain . \n\t', '\n\t\t Lin , C.-Y. and F. J. Och . \n\t', '\n\t\t 2004. ORANGE : a Method for Evaluating Automatic Evaluation Metrics for Machine Translation . \n\t', '\n\t\t In Proceedings of 20th International Conference on Computational Linguistic ( COLING 2004 ) , Geneva , Switzerland . \n\t', '\n\t\t Miller , G. 1990 . \n\t', '\n\t\t WordNet : An Online Lexical Database . \n\t', '\n\t\t International Journal of Lexicography , 3(4) . \n\t', '\n\t\t Melamed , I.D. 1995 . \n\t', '\n\t\t Automatic Evaluation and Uniform Filter Cascades for Inducing N-best Translation Lexicons . \n\t', '\n\t\t In Proceedings of the 3rd Workshop on Very Large Corpora ( WVLC3 ) . \n\t', '\n\t\t Boston , U.S.A. Melamed , I.D. , R. Green and J. P. Turian . \n\t', '\n\t\t 2003. Precision and Recall of Machine Translation . \n\t', '\n\t\t In Proceedings of NAACL/HLT 2003 , Edmonton , Canada . \n\t', '\n\t\t Nießen S. , F.J. Och , G , Leusch , H. Ney . \n\t', '\n\t\t 2000. An Evaluation Tool for Machine Translation : Fast Evaluation for MT Research . \n\t', '\n\t\t In Proceedings of the 2nd International Conference on Language Resources and Evaluation , Athens , Greece . \n\t', '\n\t\t NIST . \n\t', '\n\t\t 2002. Automatic Evaluation of Machine Translation Quality using N-gram Co- Occurrence Statistics . \n\t', '\n\t\t http://www.nist.gov/speech/tests/mt/doc/ngramstudy.pdf Pantel , P. and Lin , D. 2002 . \n\t', '\n\t\t Discovering Word Senses from Text . \n\t', '\n\t\t In Proceedings of SIGKDD02 . \n\t', '\n\t\t Edmonton , Canada . \n\t', '\n\t\t Papineni , K. , S. Roukos , T. Ward , and W.-J. Zhu . \n\t', '\n\t\t 2001. BLEU : a Method for Automatic Evaluation of Machine Translation . \n\t', '\n\t\t IBM Research Report RC22176 ( W0109-022 ) . \n\t', '\n\t\t Porter , M.F. 1980 . \n\t', '\n\t\t An Algorithm for Suffix Stripping . \n\t', '\n\t\t Program , 14 , pp. 130-137 . \n\t', '\n\t\t Saggion H. , D. Radev , S. Teufel , and W. Lam . \n\t', '\n\t\t 2002. Meta-Evaluation of Summaries in a Cross-Lingual Environment Using Content- Based Metrics . \n\t', '\n\t\t In Proceedings of COLING2002 , Taipei , Taiwan . \n\t', '\n\t\t Su , K.-Y. , M.-W. Wu , and J.-S. Chang . \n\t', '\n\t\t 1992. A New Quantitative Quality Measure for Machine Translation System . \n\t', '\n\t\t In Proceedings of COLING-92 , Nantes , France . \n\t', '\n\t\t Thompson , H. S. 1991 . \n\t', '\n\t\t Automatic Evaluation of Translation Quality : Outline of Methodology and Report on Pilot Experiment . \n\t', '\n\t\t In Proceedings of the Evaluator\x92s Forum , ISSCO , Geneva , Switzerland . \n\t', '\n\t\t Turian , J. P. , L. Shen , and I. D. Melamed . \n\t', '\n\t\t 2003. Evaluation of Machine Translation and its Evaluation . \n\t', '\n\t\t In Proceedings of MT Summit IY , New Orleans , U.S.A. . \n\t', '\n\t\t Van Rijsbergen , C.J. 1979 . \n\t', '\n\t\t Information Retrieval . \n\t', '\n\t\t Butterworths . \n\t', '\n\t\t London . \n\t', '\n\t\t A Unified Framework for Automatic Evaluation using N-gram Co-Occurrence Statistics Radu SORICUT Eric BRILL Information Sciences Institute Microsoft Research University of Southern California One Microsoft Way 4676 Admiralty Way Redmond , WA 98052 , USA Marina del Rey , CA 90292 , USA brill@microsoft.com radu@isi.edu Abstract In this paper we propose a unified framework for automatic evaluation of NLP applications using N-gram co-occurrence statistics . \n\t', '\n\t\t The automatic evaluation metrics proposed to date for Machine Translation and Automatic Summarization are particular instances from the family of metrics we propose . \n\t', '\n\t\t We show that different members of the same family of metrics explain best the variations obtained with human evaluations , according to the application being evaluated ( Machine Translation , Automatic Summarization , and Automatic Question Answering ) and the evaluation guidelines used by humans for evaluating such applications . \n\t', '\n\t\t 1 Introduction With the introduction of the BLEU metric for machine translation evaluation \n\t\t']",Positive
"['\n\t\t Recently , a second proposal for automatic evaluation has come from the Automatic Summarization community \n\t\t']",Positive
"['\n\t\t An automatic evaluation metric is said to be successful if it is shown to have high agreement with human-performed evaluations . \n\t', '\n\t\t Human evaluations , however , are subject to specific guidelines given to the human assessors when performing the evaluation task ; the variation in human judgment is therefore highly influenced by these guidelines . \n\t', '\n\t\t It follows that , in order for an automatic evaluation to agree with a human- performed evaluation , the evaluation metric used by the automatic method must be able to account , at least to some degree , for the bias induced by the human evaluation guidelines . \n\t', '\n\t\t None of the automatic evaluation methods proposed to date , however , explicitly accounts for the different criteria followed by the human assessors , as they are defined independently of the guidelines used in the human evaluations . \n\t', '\n\t\t In this paper , we propose a framework for automatic evaluation of NLP applications which is able to account for the variation in the human evaluation guidelines . \n\t', '\n\t\t We define a family of metrics based on N-gram co-occurrence statistics , for which the automatic evaluation metrics proposed to date for Machine Translation and Automatic Summarization can be seen as particular instances . \n\t', '\n\t\t We show that different members of the same family of metrics explain best the variations obtained with human evaluations , according to the application being evaluated ( Machine Translation , Automatic Summarization , and Question Answering ) and the guidelines used by humans when evaluating such applications . \n\t', '\n\t\t 2 An Evaluation Plane for NLP In this section we describe an evaluation plane on which we place various NLP applications evaluated using various guideline packages . \n\t', '\n\t\t This evaluation plane is defined by two orthogonal axes ( see Figure 1 ) : an Application Axis , on which we order NLP applications according to the faithfulness/compactness ratio that characterizes the application\x92s input and output ; and a Guideline Axis , on which we order various human guideline packages , according to the precision/recall ratio that characterizes the evaluation guidelines . \n\t', '\n\t\t Figure 1 : Evaluation plane for NLP applications high Application Axis low faithfulness compactness faithfulness compactness MT QA adequacy evaluation TIDES^MT(2002) AS fluency evaluation TIDES^MT(2002) correctness evaluation QA(2004) coverage evaluation DUC^AS ( 2001 ) lowprecision recall Guideline Axis high precision recall 2.1 An Application Axis for Evaluation When trying to define what translating and summarizing means , one can arguably suggest that a translation is some \x93as-faithful-as-possible\x94 rendering of some given input , whereas a summary is some \x93as-compact-as-possible\x94 rendering of some given input . \n\t', '\n\t\t As such , Machine Translation ( MT ) and Automatic Summarization ( AS ) are on the extremes of a faithfulness/compactness ( f/c ) ratio between inputs and outputs . \n\t', '\n\t\t In between these two extremes lie various other NLP applications : a high f/c ratio , although lower than MT\x92s , characterizes Automatic Paraphrasing ( paraphrase : To express , interpret , or translate with latitude ) ; close to the other extreme , a low f/c ratio , although higher than AS\x92s , characterizes Automatic Summarization with view-points ( summarization which needs to focus on a given point of view , extern to the document(s) to be summarized ) . \n\t', '\n\t\t Another NLP application , Automatic Question Answering ( QA ) , has arguably a close-to-1 f/c ratio : the task is to render an answer about the thing(s) inquired for in a question ( the faithfulness side ) , in a manner that is concise enough to be regarded as a useful answer ( the compactness side ) . \n\t', '\n\t\t 2.2 An Guideline Axis for Evaluation Formal human evaluations make use of various guidelines that specify what particular aspects of the output being evaluated are considered important , for the particular application being evaluated . \n\t', '\n\t\t For example , human evaluations of MT ( e.g. , TIDES 2002 evaluation , performed by NIST ) have traditionally looked at two different aspects of a translation : adequacy ( how much of the content of the original sentence is captured by the proposed translation ) and fluency ( how correct is the proposed translation sentence in the target language ) . \n\t', '\n\t\t In many instances , evaluation guidelines can be linearly ordered according to the precision/recall ( p/r ) ratio they specify . \n\t', '\n\t\t For example , evaluation guidelines for adequacy evaluation of MT have a low p/r ratio , because of the high emphasis on recall ( i.e. , content is rewarded ) and low emphasis on precision ( i.e. , verbosity is not penalized ) ; on the other hand , evaluation guidelines for fluency of MT have a high p/r ratio , because of the low emphasis on recall ( i.e. , content is not rewarded ) and high emphasis on wording ( i.e. , extraneous words are penalized ) . \n\t', '\n\t\t Another evaluation we consider in this paper , the DUC 2001 evaluation for Automatic Summarization ( also performed by NIST ) , had specific guidelines for coverage evaluation , which means a low p/r ratio , because of the high emphasis on recall ( i.e. , content is rewarded ) . \n\t', '\n\t\t Last but not least , the QA evaluation for correctness we discuss in Section 4 has a close-to-1 p/r ratio for evaluation guidelines ( i.e. , both correct content and precise answer wording are rewarded ) . \n\t', '\n\t\t When combined , the application axis and the guideline axis define a plane in which particular evaluations are placed according to their application/guideline coordinates . \n\t', '\n\t\t In Figure 1 we illustrate this evaluation plane , and the evaluation examples mentioned above are placed in this plane according to their coordinates . \n\t', '\n\t\t 3 A Unified Framework for Automatic Evaluation In this section we propose a family of evaluation metrics based on N-gram co-occurrence statistics . \n\t', '\n\t\t Such a family of evaluation metrics provides flexibility in terms of accommodating both various NLP applications and various values of precision/recall ratio in the human guideline packages used to evaluate such applications . \n\t', '\n\t\t 3.1 A Precision-focused Family of Metrics Inspired by the work of \n\t\t']",Positive
"['\n\t\t Part of the definition includes a list of stop- words ( SR ) and a function for extracting the stem of a given word ( ST ) . \n\t', '\n\t\t Suppose we have a given NLP application for which we want to evaluate the candidate answer set Candidates for some input sequences , given a reference answer set References . \n\t', '\n\t\t For each individual candidate answer C , we define S(C,n) as the multi-set of n-grams obtained from the candidate answer C after stemming the unigrams using ST and eliminating the unigrams found in SW . \n\t', '\n\t\t We therefore define a precision score : ^ ^ C^ { Candidates } ngram ^ S(C,n) ^ ^ C^ { Candidates } ngram ^ S(C,n) where Count(ngram) is the number of n-gram counts , and Countclip(ngram) is the maximum number of co-occurrences of ngram in the candidate answer and its reference answer . \n\t', '\n\t\t Because the denominator in the P(n) formula consists of a sum over the proposed candidate answers , this formula is a precision-oriented formula , penalizing verbose candidates . \n\t', '\n\t\t This precision score , however , can be made artificially higher when proposing shorter and shorter candidate answers . \n\t', '\n\t\t This is offset by adding a brevity penalty , BP : = e if B c r ( 1 | | / | | ) ^ r B c , | | | | ^ < clip ( ngram ) R^{Re ferences }ngram^S(R,n) Count ^ ^ R^ { Re ferences } ngram ^ S(R,n) 1 , e(1W j c j / j r| ) , if W^ | c |>| r | 1,if B^|c |r| ) Count clip ( ngram P(n) ) Count ( ngram BP ` t negative integerN , withalistofstop-words andafunction forextracting the stemofagiven word(ST) as partofthe definition . \n\t', '\n\t\t As before , suppose we have agiven NLP application forwhichwe want to evaluate the candidate answersetCandidates forsome input sequences , given areference answerset References . \n\t', '\n\t\t Foreachindividual reference answer R , we define S(R,n) as the multi-setofn-grams obtainedfromthe reference answerR after stemming the unigrams usingSTan R(n) ) Count ( ngram ~ t = WP ( N ) PS = BP N ^= n 1 ^exp( wn P(n))) N wn 1 ^= n RS (N)=WP ^exp( where |c| equals the sumofthe lengths ofthe proposed answers , |r| equals the sumofthe lengths ofthe reference answers , and B is abrevity constant . \n\t', '\n\t\t We define now a precision-focused family of metrics , parameteri zed by a non-negative integer N , as : log( This family ofmetrics can be interpreted as a weighted linearaverage ofprecision scores for increasingly longern-grams . \n\t', '\n\t\t As the values ofthe precision scores decrease roughly exponentially withthe increase ofN , the logarithmis needed to obtainalinearaverage . \n\t', '\n\t\t Note thatthe metrics of this familyare well-defined only for N\x92s small enough to yieldnon-zero P(n) scores . \n\t', '\n\t\t Fortest corporaofreasonable size , the metrics are usually well-definedforN54 . \n\t', '\n\t\t The BLEU proposedby\n\t\t']",Positive
['\n\t\t 3.2 A Recall-focused Family ofMetrics As proposedby Linand \n\t\t'],Positive
"['\n\t\t In a similar manner , we define a recall-focused family of metrics , using as parameter a non- ( SW ) d eliminating the unigrams found in SW . \n\t', '\n\t\t We therefore define a recall score as : ^ ^ where , as before , Count(ngram) is the numberof n-gramcounts , and Countclip(ngram) is the maximum numberofco-occurrences ofngram in the reference answerand its corresponding candidate answer . \n\t', '\n\t\t Because the denominatorinthe R(n) formulaconsists ofasumoverthe reference answers , this formulais essentiallyarecall- orientedformula , which penalizes incomplete candidates . \n\t', '\n\t\t This recall score , however , canbe made artificiallyhigherwhen proposing longerand longercan didate answers . \n\t', '\n\t\t This is offset by adding a wordiness penalty , WP : if W^|c|^|r| where |c| and |r| are definedas before , and Wis a wordiness constant . \n\t', '\n\t\t We define now arecall-focused family of metrics , parameteri zed by a non-negative integer N , as : log(R ( n ) ) ) This family ofmetrics can be interpreted as a weighted linearaverage ofrecall scores for increasingly longern-grams . \n\t', '\n\t\t Fortestcorporaof reasonable size , the metrics are usuallywell- definedforN<_4 . \n\t', '\n\t\t The ROUGE metric proposedby Linand \n\t\t']",Positive
"['\n\t\t 3.3 A Unified Framework for Automatic Evaluation The precision-focusedmetric family PS(N) and the recall-focusedmetri c family RS(N) defined in the previous sections are unified under the metric family AEv(a,N) , defined as : RS(N)PS(N) AEv(^,~ = ^ ^ RS(N) + ( 1^^ ) ^ PS(N) This formula extends the well-known F-measure that combines recall and precision numbers into a single number ( van Rijsbergen , 1979 ) , by combining recall and precision metric families into a single metric family . \n\t', '\n\t\t For a=0 , AEv(a,N) is the same as the recall-focused family of metrics RS(N) ; for a=1 , AEv(a,~N) is the same as the precision-focused family of metrics PS(N) . \n\t', '\n\t\t For a in between 0 and 1 , AEv(a,N) are metrics that balance recall and precision according to a . \n\t', '\n\t\t For the rest of the paper , we restrict the parameters of the AEv(a,N) family as follows : a varies continuously in [ 0 , 1 ] , N varies discretely in { 1,2,3,4 } , the linear weights wn are 1/N , the brevity constant is 1 , the wordiness constant is 2 , the list of stop-words SW is our own 626 stop-word list , and the stemming function ST is the one defined by the Porter stemmer \n\t\t']",Positive
"['\n\t\t We establish a correspondence between the parameters of the family of metrics AEv(a,N) and the evaluation plane in Figure 1 as follows : a parameterizes the guideline axis ( x-axis ) of the plane , such that a=0 corresponds to a low precision/recall ( p/r ) ratio , and a=1 corresponds to a high p/r ratio ; N parameterizes the application axis ( y-axis ) of the plane , such that N=1 corresponds to a low faithfulness/compactness ( f/c ) ratio ( unigram statistics allow for a low representation of faithfulness , but a high representation of compactness ) , and N=4 corresponds to a high f/c ratio ( n-gram statistics up to 4-grams allow for a high representation of faithfulness , but a low representation of compactness ) . \n\t', '\n\t\t This framework enables us to predict that a human-performed evaluation is best approximated by metrics that have similar f/c ratio as the application being evaluated and similar p/r ratio as the evaluation package used by the human assessors . \n\t', '\n\t\t For example , an application with a high f/c ratio , evaluated using a low p/r ratio evaluation guideline package ( an example of this is the adequacy evaluation for MT in TIDES 2002 ) , is best approximated by the automatic evaluation metric defined by a low a and a high N ; an application with a close-to-1 f/c ratio , evaluated using an evaluation guideline package characterized by a close-to-1 p/r ratio ( such as the correctness evaluation for Question Answering in Section 4.3 ) is best approximated by an automatic metric defined by a median a and a median N. 4 Evaluating the Evaluation Framework In this section , we present empirical results regarding the ability of our family of metrics to approximate human evaluations of various applications under various evaluation guidelines . \n\t', '\n\t\t We measure the amount of approximation of a human evaluation by an automatic evaluation as the value of the coefficient of determination R2 between the human evaluation scores and the automatic evaluation scores for various systems implementing Machine Translation , Summarization , and Question Answering applications . \n\t', '\n\t\t In this framework , the coefficient of determination R2 is to be interpreted as the percentage from the total variation of the human evaluation ( that is , why some system\x92s output is better than some other system\x92s output , from the human evaluator\x92s perspective ) that is captured by the automatic evaluation ( that is , why some system\x92s output is better than some other system\x92s output , from the automatic evaluation perspective ) . \n\t', '\n\t\t The values of R2 vary between 0 and 1 , with a value of 1 indicating that the automatic evaluation explains perfectly the human evaluation variation , and a value of 0 indicating that the automatic evaluation explains nothing from the human evaluation variation . \n\t', '\n\t\t All the results for the values of R2 for the family of metrics AEv(a,N) are reported with a varying from 0 to 1 in 0.1 increments , and N varying from 1 to 4 . \n\t', '\n\t\t 4.1 Machine Translation Evaluation The Machine Translation evaluation carried out by NIST in 2002 for DARPA\x92s TIDES programme involved 7 systems that participated in the Chinese-English track . \n\t', '\n\t\t Each system was evaluated by a human judge , using one reference extracted from a list of 4 available reference translations . \n\t', '\n\t\t Each of the 878 test sentences was evaluated both for adequacy ( how much of the content of the original sentence is captured by the proposed translation ) and fluency ( how correct is the proposed translation sentence in the target language ) . \n\t', '\n\t\t From the publicly available data for this evaluation ( TIDES 2002 ) , we compute the values of R2 for 7 data points ( corresponding to the 7 systems participating in the Chinese-English track ) , using as a reference set one of the 4 sets of reference translations available . \n\t', '\n\t\t In Table 1 , we present the values of the coefficient of determination R2 for the family of metrics AEv(a,N) , when considering only the fluency scores from the human evaluation . \n\t', '\n\t\t As mentioned in Section 2 , the evaluation guidelines for fluency have a high precision/recall ratio , whereas MT is an application with a high 4 76.10 76.45 76.78 77.10 77.40 77.69 77.96 78.21 78.45 78.67 78.87 3 76.11 76.6 77.04 77.44 77.80 78.11 78.38 78.61 78.80 78.94 79.04 2 73.19 74.21 75.07 75.78 76.32 76.72 76.96 77.06 77.03 76.87 76.58 1 31.71 38.22 44.82 51.09 56.59 60.99 64.10 65.90 66.50 66.12 64.99 N/^ 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Table 1 : R2 values for the family of metrics AEv(^,N) , for fluency scores in MT evaluation 4 83.04 82.58 82.11 81.61 81.10 80.56 80.01 79.44 78.86 78.26 77.64 3 81.80 81.00 80.16 79.27 78.35 77.39 76.40 75.37 74.31 73.23 72.11 2 80.84 79.46 77.94 76.28 74.51 72.63 70.67 68.64 66.55 64.42 62.26 1 62.16 66.26 69.18 70.59 70.35 68.48 65.24 60.98 56.11 50.98 45.88 N/^ 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Table 2 : R2 values for the family of metrics AEv(^,N) , for adequacy scores in MT evaluation faithfulness/compactness ratio . \n\t', '\n\t\t In this case , our evaluation framework predicts that the automatic evaluation metrics that explain most of the variation in the human evaluation must have a high a and a high N . \n\t', '\n\t\t As seen in Table 1 , our evaluation framework correctly predicts the automatic evaluation metrics that explain most of the variation in the human evaluation : metrics AEv(1,3) , AEv(0.9,3) , and AEv(1,4) capture most of the variation : 79.04 % , 78.94 % , and 78.87 % , respectively . \n\t', '\n\t\t Since metric AEv(1,4) is almost the same as the BLEU metric ( modulo stemming and stop word elimination for unigrams ) , our results confirm the current practice in the Machine Translation community , which commonly uses BLEU for automatic evaluation . \n\t', '\n\t\t For comparison purposes , we also computed the value of R2 for fluency using the BLEU score formula given in \n\t\t']",Positive
"['\n\t\t In Table 2 , we present the values of the coefficient of determination R2 for the family of metrics AEv(a,N) , when considering only the adequacy scores from the human evaluation . \n\t', '\n\t\t As mentioned in Section 2 , the evaluation guidelines for adequacy have a low precision/recall ratio , whereas MT is an application with high faithfulness/compactness ratio . \n\t', '\n\t\t In this case , our evaluation framework predicts that the automatic evaluation metrics that explain most of the variation in the human evaluation must have a low a and a high N . \n\t', '\n\t\t As seen in Table 2 , our evaluation framework correctly predicts the automatic evaluation metric that explains most of the variation in the human evaluation : metric AEv(0,4) captures most of the variation , 83.04 % . \n\t', '\n\t\t For comparison purposes , we also computed the value of R2 for adequacy using the BLEU score formula given in \n\t\t']",Positive
"['\n\t\t 4.2 Automatic Summarization Evaluation The Automatic Summarization evaluation carried out by NIST for the DUC 2001 conference involved 15 participating systems . \n\t', '\n\t\t We focus here on the multi-document summarization task , in which 4 generic summaries ( of 50 , 100 , 200 , and 400 words ) were required for a given set of documents on a single subject . \n\t', '\n\t\t For this evaluation 30 test sets were used , and each system was evaluated by a human judge using one reference extracted from a list of 2 reference summaries . \n\t', '\n\t\t One of the evaluations required the assessors to judge the coverage of the summaries . \n\t', '\n\t\t The coverage of a summary was measured by comparing a system\x92s units versus the units of a reference summary , and assessing whether each system unit expresses all , most , some , hardly any , or none of the current reference unit . \n\t', '\n\t\t A final evaluation score for coverage was obtained using a coverage score computed as a weighted recall score ( see \n\t\t']",Positive
"['\n\t\t From the publicly available data for this evaluation ( DUC 2001 ) , we compute the values of R2 for 15 data points available ( corresponding to the 15 participating systems ) . \n\t', '\n\t\t In Tables 3-4 we present the values of the coefficient of determination R2 for the family of metrics AEv(a,N) , when considering the coverage 4 67.10 66.51 65.91 65.29 64.65 64.00 63.34 62.67 61.99 61.30 60.61 3 69.55 68.81 68.04 67.24 66.42 65.57 64.69 63.79 62.88 61.95 61.00 2 74.43 73.29 72.06 70.74 69.35 67.87 66.33 64.71 63.03 61.30 59.51 1 90.77 90.77 90.66 90.42 90.03 89.48 88.74 87.77 86.55 85.05 83.21 N/^ 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Table 3 : R2 for the family of metrics AEv(^,N) , for coverage scores in AS evaluation ( 200 words ) 4 81.24 81.04 80.78 80.47 80.12 79.73 79.30 78.84 78.35 77.84 77.31 3 84.72 84.33 83.86 83.33 82.73 82.08 81.39 80.65 79.88 79.07 78.24 2 89.54 88.56 87.47 86.26 84.96 83.59 82.14 80.65 79.10 77.53 75.92 1 92.28 91.11 89.70 88.07 86.24 84.22 82.05 79.74 77.30 74.77 72.15 N/^ 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Table 4 : R2 for the family of metrics AEv(^,N) , for coverage scores in AS evaluation ( 400 words ) scores from the human evaluation , for summaries of 200 and 400 words , respectively ( the values of R2 for summaries of 50 and 100 words show similar patterns ) . \n\t', '\n\t\t As mentioned in Section 2 , the evaluation guidelines for coverage have a low precision/recall ratio , whereas AS is an application with low faithfulness/compactness ratio . \n\t', '\n\t\t In this case , our evaluation framework predicts that the automatic evaluation metrics that explain most of the variation in the human evaluation must have a low a and a low N . \n\t', '\n\t\t As seen in Tables 3-4 , our evaluation framework correctly predicts the automatic evaluation metric that explain most of the variation in the human evaluation : metric AEv(0,1) explains 90.77 % and 92.28 % of the variation in the human evaluation of summaries of length 200 and 400 , respectively . \n\t', '\n\t\t Since metric AEv(0 , 1 ) is almost the same as the ROUGE metric proposed by \n\t\t']",Positive
"['\n\t\t 4.3 Question Answering Evaluation One of the most common approaches to automatic question answering ( QA ) restricts the domain of questions to be handled to so-called factoid questions . \n\t', '\n\t\t Automatic evaluation of factoid QA is often straightforward , as the number of correct answers is most of the time limited , and exhaustive lists of correct answers are available . \n\t', '\n\t\t When removing the factoid constraint , however , the set of possible answer to a ( complex , beyondfactoid ) question becomes unfeasibly large , and consequently automatic evaluation becomes a challenge . \n\t', '\n\t\t In this section , we focus on an evaluation carried out in order to assess the performance of a QA system for answering questions from the Frequently-Asked-Question ( FAQ ) domain \n\t\t']",Positive
"['\n\t\t These are generally questions requiring a more elaborated answer than a simple factoid ( e.g. , questions such as : \x93How does a film qualify for an Academy Award?\x94 ) . \n\t', '\n\t\t In order to evaluate such a system a human- performed evaluation was performed , in which 11 versions of the QA system ( various modules were implemented using various algorithms ) were separately evaluated . \n\t', '\n\t\t Each version was evaluated by a human evaluator , with no reference answer available . \n\t', '\n\t\t For this evaluation 115 test questions were used , and the human evaluator was asked to assess whether the proposed answer was correct , somehow related , or wrong . \n\t', '\n\t\t A unique ranking number was achieved using a weighted average of the scored answers . \n\t', '\n\t\t ( See \n\t\t']",Positive
"['\n\t\t ) One important aspect in the evaluation procedure was devising criteria for assigning a rating to an answer which was not neither correct nor wrong . \n\t', '\n\t\t One of such cases involved so-called flooded answers : answers which contain the correct information , along with several other unrelated pieces of information . \n\t', '\n\t\t A first evaluation has been carried with a guideline package asking the human assessor to assign the rating correct to flooded answers . \n\t', '\n\t\t In Table 5 , we present the values of the coefficient of determination R2 for the family of metrics AEv(a,N) for this first QA evaluation . \n\t', '\n\t\t On the guideline side , the guideline package used in this first QA evaluation has a low precision/recall ratio , because the human judge is asked to evaluate based on the content provided by a given answer ( high recall ) , but is asked to disregard the conciseness ( or lack thereof ) of the answer ( low precision ) ; consequently , systems that focus on 4 63.40 57.62 51.86 46.26 40.96 3 81.39 76.38 70.76 64.76 58.61 2 91.72 89.21 85.54 80.78 75.14 1 61.61 58.83 55.25 51.04 46.39 N/^ 0 0.1 0.2 0.3 0.4 36.02 31.51 27.43 23.78 20.54 17.70 52.51 46.63 41.09 35.97 31.33 27.15 68.87 62.25 55.56 49.04 42.88 37.20 41.55 36.74 32.12 27.85 23.97 20.54 0.5 0.6 0.7 0.8 0.9 1 Table 5 : R2 for the family of metrics AEv(^,N) , for correctness scores , first QA evaluation 4 79.94 79.18 75.80 70.63 64.58 3 76.15 80.44 81.19 78.45 73.07 2 67.76 77.48 84.34 86.26 82.75 1 56.55 60.81 59.60 53.56 45.38 N/^ 0 0.1 0.2 0.3 0.4 58.35 52.39 46.95 42.11 37.87 34.19 66.27 59.11 52.26 46.08 40.68 36.04 75.24 65.94 56.65 48.32 41.25 35.42 37.40 30.68 25.36 21.26 18.12 15.69 0.5 0.6 0.7 0.8 0.9 1 Table 6 : R2 for the family of metrics AEv(^,N) , for correctness scores , second QA evaluation giving correct and concise answers are not distinguished from systems that give correct answers , but have no regard for concision . \n\t', '\n\t\t On the application side , as mentioned in Section 2 , QA is arguably an application characterized by a closeto-1 faithfulness/compactness ratio . \n\t', '\n\t\t In this case , our evaluation framework predicts that the automatic evaluation metrics that explain most of the variation in the human evaluation must have a low a and a median N . \n\t', '\n\t\t As seen in Table 5 , our evaluation framework correctly predicts the automatic evaluation metric that explain most of the variation in the human evaluation : metric AEv(0,2) explains most of the human variation , 91.72 % . \n\t', '\n\t\t Note that other members of the AEv(~^,N) family do not explain nearly as well the variation in the human evaluation . \n\t', '\n\t\t For example , the ROUGE-like metric AEv(0,1) explains only 61.61 % of the human variation , while the BLEU- like metric AEv(1,4) explains a mere 17.7 % of the human variation ( to use such a metric in order to automatically emulate the human QA evaluation is close to performing an evaluation assigning random ratings to the output answers ) . \n\t', '\n\t\t In order to further test the prediction power of our evaluation framework , we carried out a second QA evaluation , using a different evaluation guideline package : a flooded answer was rated only somehow-related . \n\t', '\n\t\t In Table 6 , we present the values of the coefficient of determination R2 for the family of metrics AEv(a,N) for this second QA evaluation . \n\t', '\n\t\t Instead of performing this second evaluation from scratch , we actually simulated it using the following methodology : 2/3 of the output answers rated correct of the systems ranked 1st , 2nd , 3rd , and 6th by the previous human evaluation have been intentionally over-flooded using two long and out-of-context sentences , while their ratings were changed from correct to somehow-related . \n\t', '\n\t\t Such a change simulated precisely the change in the guideline package , by downgrading flooded answers . \n\t', '\n\t\t This means that , on the guideline side , the guideline package used in this second QA evaluation has a close-to-1 precision/recall ratio , because the human judge evaluates now based both on the content and the conciseness of a given answer . \n\t', '\n\t\t At the same time , the application remains unchanged , which means that on the application side we still have a close-to-1 faithfulness/compactness ratio . \n\t', '\n\t\t In this case , our evaluation framework predicts that the automatic evaluation metrics that explain most of the variation in the human evaluation must have a median a and a median N . \n\t', '\n\t\t As seen in Table 6 , our evaluation framework correctly predicts the automatic evaluation metric that explain most of the variation in the human evaluation : metric AEv(0.3,2) explains most of the variation in the human evaluation , 86.26 % . \n\t', '\n\t\t Also note that , while the R2 values around AEv(0.3,2) are still reasonable , evaluation metrics that are further and further away from it have increasingly lower R2 values , meaning that they are more and more unreliable for this task . \n\t', '\n\t\t The high correlation of metric AEv(0.3,2) with human judgment , however , suggests that such a metric is a good candidate for performing automatic evaluation of QA systems that go beyond answering factoid questions . \n\t', '\n\t\t 5 Conclusions In this paper , we propose a unified framework for automatic evaluation based on N-gram co- occurrence statistics , for NLP applications for which a correct answer is usually an unfeasibly large set ( e.g. , Machine Translation , Paraphrasing , Question Answering , Summarization , etc. ) . \n\t', '\n\t\t The success of BLEU in doing automatic evaluation of machine translation output has often led researchers to blindly try to use this metric for evaluation tasks for which it was more or less appropriate ( see , e.g. , the paper of \n\t\t']",Negative
"['\n\t\t Our unifying framework facilitates the understanding of when various automatic evaluation metrics are able to closely approximate human evaluations for various applications . \n\t', '\n\t\t Given an application app and an evaluation guideline package eval , the faithfulness/compactness ratio of the application and the precision/recall ratio of the evaluation guidelines determine a restricted area in the evaluation plane in Figure 1 which best characterizes the ( app , eval ) pair . \n\t', '\n\t\t We have empirically demonstrated that the metrics from the AEv(~^,N) family that best approximate human judgment are those that have the a and N parameters in the determined restricted area . \n\t', '\n\t\t To our knowledge , this is the first proposal regarding automatic evaluation in which the automatic evaluation metrics are able to account for the variation in human judgment due to specific evaluation guidelines . \n\t', '\n\t\t References DUC . \n\t', '\n\t\t 2001. The Document Understanding Conference . \n\t', '\n\t\t http://duc.nist.gov . \n\t', '\n\t\t C.Y. Lin and E. H. Hovy . \n\t', '\n\t\t 2003. Automatic Evaluation of Summaries Using N-gram Co- Occurrence Statistics . \n\t', '\n\t\t In Proceedings of the HLT/NAACL 2003 : Main Conference , 150-156 . \n\t', '\n\t\t K. Papineni , S. Roukos , T. Ward , and W.J. Zhu . \n\t', '\n\t\t 2002. BLEU : a Method for Automatic Evaluation of Machine Translation . \n\t', '\n\t\t In Proceedings of the ACL 2002 , 311-318 . \n\t', '\n\t\t M. F. Porter . \n\t', '\n\t\t 1980. An algorithm for Suffix Stripping . \n\t', '\n\t\t Program , 14 : 130-137 . \n\t', '\n\t\t F. J. Och . \n\t', '\n\t\t 2003. Minimum Error Rate Training for Statistical Machine Translation . \n\t', '\n\t\t In Proceedings of the ACL 2003 , 160-167 . \n\t', '\n\t\t R. Soricut and E. Brill . \n\t', '\n\t\t 2004. Automatic Question Answering : Beyond the Factoid . \n\t', '\n\t\t In Proceedings of the HLT/NAACL 2004 : Main Conference , 57- 64 . \n\t', '\n\t\t TIDES . \n\t', '\n\t\t 2002. The Translingual Information Detection , Extraction , and Summarization programme . \n\t', '\n\t\t http://tides.nist.gov . \n\t', '\n\t\t C. J. van Rijsbergen . \n\t', '\n\t\t 1979. Information Retrieval . \n\t', '\n\t\t London : Butterworths . \n\t', '\n\t\t Second Edition . \n\t', '\n\t\t Extending the BLEU MT Evaluation Method with Frequency Weightings Bogdan Babych Anthony Hartley Centre for Translation Studies Centre for Translation Studies University of Leeds University of Leeds Leeds , LS2 9JT , UK Leeds , LS2 9JT , UK bogdan@comp.leeds.ac.uk a.hartley@leeds.ac.uk Abstract We present the results of an experiment on extending the automatic method of Machine Translation evaluation BLUE with statistical weights for lexical items , such as tf.idf scores . \n\t', '\n\t\t We show that this extension gives additional information about evaluated texts ; in particular it allows us to measure translation Adequacy , which , for statistical MT systems , is often overestimated by the baseline BLEU method . \n\t', '\n\t\t The proposed model uses a single human reference translation , which increases the usability of the proposed method for practical purposes . \n\t', '\n\t\t The model suggests a linguistic interpretation which relates frequency weights and human intuition about translation Adequacy and Fluency . \n\t', '\n\t\t 1. Introduction Automatic methods for evaluating different aspects of MT quality \x96 such as Adequacy , Fluency and Informativeness \x96 provide an alternative to an expensive and time-consuming process of human MT evaluation . \n\t', '\n\t\t They are intended to yield scores that correlate with human judgments of translation quality and enable systems ( machine or human ) to be ranked on this basis . \n\t', '\n\t\t Several such automatic methods have been proposed in recent years . \n\t', '\n\t\t Some of them use human reference translations , e.g. , the BLEU method \n\t\t']",Negative
"['\n\t\t However , a serious problem for the BLEU method is the lack of a model for relative importance of matched and mismatched items . \n\t', '\n\t\t Words in text usually carry an unequal informational load , and as a result are of differing importance for translation . \n\t', '\n\t\t It is reasonable to expect that the choices of right translation equivalents for certain key items , such as expressions denoting principal events , event participants and relations in a text are more important in the eyes of human evaluators then choices of function words and a syntactic perspective for sentences . \n\t', '\n\t\t Accurate rendering of these key items by an MT system boosts the quality of translation . \n\t', '\n\t\t Therefore , at least for evaluation of translation Adequacy ( Fidelity ) , the proper choice of translation equivalents for important pieces of information should count more than the choice of words which are used for structural purposes and without a clear translation equivalent in the source text . \n\t', '\n\t\t ( The latter may be more important for Fluency evaluation ) . \n\t', '\n\t\t The problem of different significance of N- gram matches is related to the issue of legitimate variation in human translations , when certain words are less stable than others across independently produced human translations . \n\t', '\n\t\t BLEU accounts for legitimate translation variation by using a set of several human reference translations , which are believed to be representative of several equally acceptable ways of translating any source segment . \n\t', '\n\t\t This is motivated by the need not to penalise deviations from the set of N- grams in a single reference , although the requirement of multiple human references makes automatic evaluation more expensive . \n\t', '\n\t\t However , the \x93significance\x94 problem is not directly addressed by the BLEU method . \n\t', '\n\t\t On the one hand , the matched items that are present in several human references receive the same weights as items found in just one of the references . \n\t', '\n\t\t On the other hand the model of legitimate translation variation cannot fully accommodate the issue of varying degrees of \x93salience\x94 for matched lexical items , since alternative synonymic translation equivalents may also be highly significant for an adequate translation from the human perspective \n\t\t']",Positive
"['\n\t\t Therefore it is reasonable to suggest that introduction of a model which approximates intuitions about the significance of the matched N-grams will improve the correlation between automatically computed MT evaluation scores and human evaluation scores for translation Adequacy . \n\t', '\n\t\t In this paper we present the result of an experiment on augmenting BLEU N-gram comparison with statistical weight coefficients which capture a word\x92s salience within a given document : the standard tf.idf measure used in the vector-space model for Information Retrieval \n\t\t']",Positive
['\n\t\t Both scores are computed for each term in each of the 100 human reference translations from French into English available in DARPA-94 MT evaluation corpus \n\t\t'],Positive
"['\n\t\t The proposed weighted N-gram model for MT evaluation is tested on a set of translations by four different MT systems available in the DARPA corpus , and is compared with the results of the baseline BLEU method with respect to their correlation with human evaluation scores . \n\t', '\n\t\t The scores produced by the N-gram model with tf.idf and S-Score weights are shown to be consistent with baseline BLEU evaluation results for Fluency and outperform the BLEU scores for Adequacy ( where the correlation for the S-score weighting is higher ) . \n\t', '\n\t\t We also show that the weighted model may still be reliably used if there is only one human reference translation for an evaluated text . \n\t', '\n\t\t Besides saving cost , the ability to dependably work with a single human translation has an additional advantage : it is now possible to create Recall-based evaluation measures for MT , which has been problematic for evaluation with multiple reference translations , since only one of the choices from the reference set is used in translation ( Papineni et al . 2002:314 ) . \n\t', '\n\t\t Notably , Recall of weighted N-grams is found to be a good estimation of human judgements about translation Adequacy . \n\t', '\n\t\t Using weighted N-grams is essential for predicting Adequacy , since correlation of Recall for non-weighted N-grams is much lower . \n\t', '\n\t\t It is possible that other automatic methods which use human translations as a reference may also benefit from an introduction of an explicit model for term significance , since so far these methods also implicitly assume that all words are equally important in human translation , and use all of them , e.g. , for measuring edit distances ( Akiba et al , 2001 ; 2003 ) . \n\t', '\n\t\t The weighted N-gram model has been implemented as an MT evaluation toolkit ( which includes a Perl script , example files and documentation ) . \n\t', '\n\t\t It computes evaluation scores with tf.idf and S-score weights for translation Adequacy and Fluency . \n\t', '\n\t\t The toolkit is available at http://www.comp.leeds.ac.uk/bogdan/evalMT.html 2 . \n\t', '\n\t\t Set-up of the experiment The experiment used French\x96English translations available in the DARPA-94 MT evaluation corpus . \n\t', '\n\t\t The corpus contains 100 French news texts ( each text is about 350 words long ) translated into English by 5 different MT systems : \x93Systran\x94 , \x93Reverso\x94 , \x93Globalink\x94 , \x93Metal\x94 , \x93Candide\x94 and scored by human evaluators ; there are no human scores for \x93Reverso\x94 , which was added to the corpus on a later stage . \n\t', '\n\t\t The corpus also contains 2 independent human translations of each text . \n\t', '\n\t\t Human evaluation scores are available for each of the 400 texts translated by the 4 MT systems for 3 parameters of translation quality : \x93Adequacy\x94 , \x93Fluency\x94 and \x93Informativeness\x94 . \n\t', '\n\t\t The Adequacy ( Fidelity ) scores are given on a 5-point scale by comparing MT with a human reference translation . \n\t', '\n\t\t The Adequacy parameter captures how much of the original content of a text is conveyed , regardless of how grammatically imperfect the output might be . \n\t', '\n\t\t The Fluency scores ( also given on a 5-point scale ) determine intelligibility of MT without reference to the source text , i.e. , how grammatical and stylistically natural the translation appears to be . \n\t', '\n\t\t The Informativeness scores ( which we didn\x92t use for our experiment ) determine whether there is enough information in MT out- put to enable evaluators to answer multiple- choice questions on its content ( White , 2003:237 ) In the first stage of the experiment , each of the two sets of human translations was used to compute tf.idf and S-scores for each word in each of the 100 texts . \n\t', '\n\t\t The tf.idf score was calculated as : tf.idf(ij) = ( 1 + log ( tfi,j ) ) log ( N / dfi ) , if tfij ^ 1 ; where : \x96 tfij is the number of occurrences of the word wi in the document dj ; \x96 dfi is the number of documents in the corpus where the word wi occurs ; \x96 N is the total number of documents in the corpus . \n\t', '\n\t\t where : \x96 Pdoc(ij) is the relative frequency of the word in the text ; ( \x93Relative frequency\x94 is the number of tokens of this word-type divided by the total number of tokens ) . \n\t', '\n\t\t \x96 Pcorp-doc(i) is the relative frequency of the same word in the rest of the corpus , without this text ; \x96 ( N \x96 df(i)) / N is the proportion of texts in the corpus , where this word does not occur ( number of texts , where it is not found , divided by number of texts in the corpus ) ; \x96 Pcorp(i) is the relative frequency of the word in the whole corpus , including this particular text . \n\t', '\n\t\t In the second stage we carried out N-gram based MT evaluation , measuring Precision and Recall of N-grams in MT output using a single human reference translation . \n\t', '\n\t\t N-gram counts were adjusted with the tf.idf weights and S-scores for every matched word . \n\t', '\n\t\t The following procedure was used to integrate the S-scores / tf.idf scores for a lexical item into N-gram counts . \n\t', '\n\t\t For every word in a given text which received an S-score and tf.idf score on the basis of the human reference corpus , all counts for the N-grams containing this word are increased by the value of the respective score ( not just by 1 , as in the baseline BLEU approach ) . \n\t', '\n\t\t The original matches used for BLEU and the weighted matches are both calculated . \n\t', '\n\t\t The following changes have been made to the Perl script of the BLEU tool : apart from the operator which increases counts for every matched N-gram $ ngr by 1 , i.e. : $ ngr .= $ words[$i+$j] . \n\t', '\n\t\t 11 11 ; $ $ hashNgr{$ngr}++ ; the following code was introduced : [ ... ] $ WORD = $ words[$i+$j] ; $ WEIGHT = 0 ; if ( exists $ WordWeight{$TxtN}{$WORD}){ $ WEIGHT= $ WordWeight{$TxtN}{$WORD} ; } $ ngr .= $ words[$i+$j] . \n\t', '\n\t\t 11 11 ; $ $ hashNgr{$ngr}++ ; $ $ hashNgrWEIGHTED{$ngr}+= $ WEIGHT ; [ ... ] \x96 where the hash data structure : $ WordWeight{$TxtN}{$WORD}=$WEIGHT represents the table of tf.idf scores or S-scores for words in every text in the corpus . \n\t', '\n\t\t The weighted N-gram evaluation scores of Precision , Recall and F-measure may be produced for a segment , for a text or for a corpus of translations generated by an MT system . \n\t', '\n\t\t In the third stage of the experiment the weighted Precision and Recall scores were tested for correlation with human scores for the same texts and compared to the results of similar tests for standard BLEU evaluation . \n\t', '\n\t\t Finally we addressed the question whether the proposed MT evaluation method allows us to use a single human reference translation reliably . \n\t', '\n\t\t In order to assess the stability of the weighted evaluation scores with a single reference , two runs of the experiment were carried out . \n\t', '\n\t\t The first run used the \x93Reference\x94 human translation , while the second run used the \x93Expert\x94 human translation ( each time a single reference translation was used ) . \n\t', '\n\t\t The scores for both runs were compared using a standard deviation measure . \n\t', '\n\t\t 3. The results of the MT evaluation with frequency weights With respect to evaluating MT systems , the correlation for the weighted N-gram model was found to be stronger , for both Adequacy and Fluency , the improvement being highest for Adequacy . \n\t', '\n\t\t These results are due to the fact that the weighted N-gram model gives much more accurate predictions about the statistical MT system The S-score was calculated as : System BLEU Prec . \n\t', '\n\t\t Recall Fscore [ ade ] / [ flu ] CANDIDE [ 1&2 ] 1/2 1/2 1/2 0.3561 0.4068 0.3806 0.3933 0.677 / 0.455 GLOBALINK 0.3199 0.4012 0.3790 0.3898 0.3429 0.3465 0.3447 0.710 / 0.381 MS 0.3003 0.3414 0.3484 0.3449 0.3289 0.3650 0.3460 0.718 / 0.382 REVERSO 0.3823 0.3286 0.3682 0.3473 0.3948 0.4012 0.3980 NA / NA SYSTRAN 0.4002 0.3923 0.4025 0.3973 0.4029 0.4129 0.4078 0.789 / 0.508 Corr r(2) with 0.5918 0.3981 0.4118 0.4049 0.1809 0.6691 0.4063 [ ade ] \x96 MT Corr r(2) with [ flu ] \x96 MT 0.9807 0.1871 0.9096 0.9124 0.6988 0.9540 0.9353 0.4270 0.9836 0.9869 Table 1 . \n\t', '\n\t\t Baseline non-weighted scores . \n\t', '\n\t\t Table 2 summarises the evaluation scores for BLEU as compared to tf.idf weighted scores , and Table 3 summarises the same scores as compared to S-score weighed evaluation . \n\t', '\n\t\t System BLEU Prec . \n\t', '\n\t\t Recall Fscore [ ade ] / [ flu ] CANDIDE [ 1&2 ] ( w ) 1/2 ( w ) 1/2 ( w ) 1/2 0.3561 0.5242 0.3094 0.3892 0.677 / 0.455 GLOBALINK 0.3199 0.5176 0.3051 0.3839 0.4905 0.2919 0.3660 0.710 / 0.381 MS 0.3003 0.4890 0.2911 0.3650 0.4919 0.3083 0.3791 0.718 / 0.382 REVERSO 0.3823 0.4902 0.3100 0.3798 0.5336 0.3400 0.4154 NA / NA SYSTRAN 0.4002 0.5342 0.3413 0.4165 0.5442 0.3521 0.4276 0.789 / 0.508 Corr r(2) with 0.5918 0.5375 0.3491 0.4233 0.5248 0.8354 0.7691 [ ade ] \x96 MT Corr r(2) with [ flu ] \x96 MT 0.9807 0.5561 0.9987 0.9998 0.8667 0.8849 0.8350 0.8119 0.9408 0.9070 Table 2 . \n\t', '\n\t\t BLEU vs tf.idf weighted scores . \n\t', '\n\t\t System BLEU Prec . \n\t', '\n\t\t Recall Fscore [ ade ] / [ flu ] CANDIDE [ 1&2 ] ( w ) 1/2 ( w ) 1/2 ( w ) 1/2 0.3561 0.5034 0.2553 0.3388 0.677 / 0.455 GLOBALINK 0.3199 0.4982 0.2554 0.3377 0.4677 0.2464 0.3228 0.710 / 0.381 MS 0.3003 0.4672 0.2493 0.3252 0.4766 0.2635 0.3394 0.718 / 0.382 REVERSO 0.3823 0.4793 0.2679 0.3437 0.5204 0.2930 0.3749 NA / NA SYSTRAN 0.4002 0.5214 0.2967 0.3782 0.5314 0.3034 0.3863 0.789 / 0.508 0.5918 0.5218 0.3022 0.3828 Corr r(2) with [ ade ] \x96 MT Corr r(2) with 0.9807 0.6055 0.6137 0.9912 0.9069 0.9215 0.8022 0.8574 0.8792 0.8715 [ flu ] \x96 MT 0.9769 0.7499 0.8247 Table 3 . \n\t', '\n\t\t BLEU vs S-score weights . \n\t', '\n\t\t It can be seen from the table that there is a strong positive correlation between the baseline BLEU scores and human scores for Fluency : r(2)=0.9807 , p <0.05 . \n\t', '\n\t\t However , the correlation with Adequacy is much weaker and is not statistically significant : r(2)= 0.5918 , p >0.05 . \n\t', '\n\t\t The most serious problem for BLEU is predicting scores for the statistical MT system Candide , which was judged to produce relatively fluent , but largely inadequate translation . \n\t', '\n\t\t For other MT systems ( developed with the knowledge-based MT architecture ) the scores for Adequacy and Fluency are consistent with each other : more fluent translations are also more adequate . \n\t', '\n\t\t BLEU scores go in line with Candide\x92s Fluency scores , but do not account for its Adequacy scores . \n\t', '\n\t\t When Candide is excluded from the evaluation \x93Candide\x94 , whereas the standard BLEU approach tends to over-estimate its performance for translation Adequacy . \n\t', '\n\t\t Table 1 present the baseline results for non- weighted Precision , Recall and F-score . \n\t', '\n\t\t It shows the following figures : \x96 Human evaluation scores for Adequacy and Fluency ( the mean scores for all texts produced by each MT system ) ; \x96 BLEU scores produced using 2 human reference translations and the default script settings ( N-gram size = 4 ) ; \x96 Precision , Recall and F-score for the weighted N-gram model produced with 1 human reference translation and N-gram size = 4 . \n\t', '\n\t\t \x96 Pearson\x92s correlation coefficient r for Precision , Recall and F-score correlated with human scores for Adequacy and Fluency r(2) ( with 2 degrees of freedom ) for the sets which include scores for the 4 MT systems . \n\t', '\n\t\t The scores at the top of each cell show the results for the first run of the experiment , which used the \x93Reference\x94 human translation ; the scores at the bottom of the cells represent the results for the second run with the \x93Expert\x94 human translation . \n\t', '\n\t\t set , r correlation goes up , but it is still lower than the correlation for Fluency and remains statistically insignificant : r(1)= 0. 9608 , p > 0.05 . \n\t', '\n\t\t Therefore , the baseline BLEU approach fails to consistently predict scores for Adequacy . \n\t', '\n\t\t Correlation figures between non-weighted N- gram counts and human scores are similar to the results for BLEU : the highest and statistically significant correlation is between the F-score and Fluency : r(2)=0.9836 , p<0.05 , r(2)=0.9869 , p<0.01 , and there is somewhat smaller and statistically significant correlation with Precision . \n\t', '\n\t\t This confirms the need to use modified Precision in the BLEU method that also in certain respect integrates Recall . \n\t', '\n\t\t The proposed weighted N-gram model outperforms BLEU and non-weighted N-gram evaluation in its ability to predict Adequacy scores : weighted Recall scores have much stronger correlation with Adequacy ( which for MT-only evaluation is still statistically insignificant at the level p<0.05 , but come very close to that point : t=3.729 and t=4.108 ; the required value for p<0.05 is t=4.303 ) . \n\t', '\n\t\t Correlation figures for S-score-based weights are higher than for tf.idf weights ( S-score : r(2)= 0. 9069 , p > 0.05 ; r(2)= 0. 9215 , p > 0 . \n\t', '\n\t\t 05 , tf. idf score : r(2)= 0.8354 , p >0.05 ; r(2)= 0.8667 , p > 0.05 ) . \n\t', '\n\t\t The improvement in the accuracy of evaluation for the weighted N-gram model can be illustrated by the following example of translating the French sentence : ORI-French : Les trente-huit chefs d\'entreprise mis en examen dans le dossier ont déjà fait l\'objet d\'auditions , mais trois d\'entre eux ont été confrontés , mercredi , dans la foulée de la confrontation "" politique "" . \n\t', '\n\t\t English translations of this sentence by the knowledge-based system Systran and statistical MT system Candide have an equal number of matched unigrams ( highlighted in italic ) , therefore conventional unigram Precision and Recall scores are the same for both systems . \n\t', '\n\t\t However , for each translation two of the matched unigrams are different ( underlined ) and receive different frequency weights ( shown in brackets ) : MT \x93Systran\x94 : The thirty-eight heads ( tf.idf=4.605 ; S=4.614 ) of undertaking put in examination in the file already were the subject of hearings , but three of them were confronted , Wednesday , in the tread of "" po- litical "" confrontation ( tf.idf=5.937 ; S=3.890 ) . \n\t', '\n\t\t Human translation \x93Expert\x94 : The thirty-eight heads of companies questioned in the case had already been heard , but three of them were brought together Wednesday following the "" political "" confrontation . \n\t', '\n\t\t MT \x93Candide\x94 : The thirty-eight counts of company put into consideration in the case ( tf.idf=3.719 ; S=2.199 ) already had ( tf.idf=0.562 ; S=0.000 ) the object of hearings , but three of them were checked , Wednesday , in the path of confrontal "" political . \n\t', '\n\t\t "" ( In the human translation the unigrams matched by the Systran output sentence are in italic , those matched by the Candide sentence are in bold ) . \n\t', '\n\t\t It can be seen from this example that the unigrams matched by Systran have higher term frequency weights ( both tf.idf and S-scores ) : heads ( tf.idf=4.605;S=4.614 ) confrontation ( tf.idf=5.937;S=3.890 ) The output sentence of Candide instead matched less salient unigrams : case ( tf. idf=3.719 ; S=2.199 ) had ( tf.idf=0.562;S=0.000 ) Therefore for the given sentence weighted unigram Recall ( i.e. , the ability to avoid under- generation of salient unigrams ) is higher for Systran than for Candide ( Table 4 ) : Systran Candide R 0.6538 0.6538 R * tf.idf 0.5332 0.4211 R * S-score 0.5517 0.3697 P 0.5484 0.5484 P * tf.idf 0.7402 0.9277 P * S-score 0.7166 0.9573 Table 4 . \n\t', '\n\t\t Recall , Precision , and weighted scores Weighted Recall scores capture the intuition that the translation generated by Systran is more adequate than the one generated by Candide , since it preserves more important pieces of information . \n\t', '\n\t\t On the other hand , weighted Precision scores are higher for Candide . \n\t', '\n\t\t This is due to the fact that Systran over-generates ( doesn\x92t match in the human translation ) much more \x93exotic\x94 , unordinary words , which on average have higher cumulative salience scores , e.g. , undertaking , examination , confronted , tread \x96 vs. the corresponding words \x93over-generated\x94 by Candide : company , consideration , checked , path . \n\t', '\n\t\t In some respect higher weighted precision can be interpreted as higher Fluency of the Candide\x92s output sentence , which intuitively is perceived as sounding more naturally ( although not making much sense ) . \n\t', '\n\t\t On the level of corpus statistics the weighted Recall scores go in line with Adequacy , and weighted Precision scores ( as well as the Precision-based BLEU scores ) \x96 with Fluency , which confirms such interpretation of weighted Precision and Recall scores in the example above . \n\t', '\n\t\t On the other hand , Precision-based scores and non- weighted Recall scores fail to capture Adequacy . \n\t', '\n\t\t The improvement in correlation for weighted Recall scores with Adequacy is achieved by reducing overestimation for the Candide system , moving its scores closer to human judgements about its quality in this respect . \n\t', '\n\t\t However , this is not completely achieved : although in terms of Recall weighted by the S-scores Candide is correctly ranked below MS ( and not ahead of it , as with the BLEU scores ) , it is still slightly ahead of Globalink , contrary to human evaluation results . \n\t', '\n\t\t For both methods \x96 BLEU and the Weighted N-gram evaluation \x96 Adequacy is found to be harder to predict than Fluency . \n\t', '\n\t\t This is due to the fact that there is no good linguistic model of translation adequacy which can be easily formalised . \n\t', '\n\t\t The introduction of S-score weights may be a useful step towards developing such a model , since correlation scores with Adequacy are much better for the Weighted N-gram approach than for BLEU . \n\t', '\n\t\t Also from the linguistic point of view , S-score weights and N-grams may only be reasonably good approximations of Adequacy , which involves a wide range of factors , like syntactic and semantic issues that cannot be captured by N- gram matches and require a thesaurus and other knowledge-based extensions . \n\t', '\n\t\t Accurate formal models of translation variation may also be useful for improving automatic evaluation of Adequacy . \n\t', '\n\t\t The proposed evaluation method also preserves the ability of BLEU to consistently predict scores for Fluency : Precision weighted by tf.idf scores has the strongest positive correlation with this aspect of MT quality , which is slightly better than the values for BLEU ; ( S-score : r(2)= 0.9912 , p<0.01 ; r(2)= 0.9769 , p<0.05 ; tf. idf score : r(2)= 0.9987 , p<0.001 ; r(2)= 0.9998 , p< 0.001 ) . \n\t', '\n\t\t The results suggest that weighted Precision gives a good approximation of Fluency . \n\t', '\n\t\t Similar results with non-weighted approach are only achieved if some aspect of Recall is integrated into the evaluation metric ( either as modified precision , as in BLEU , or as an aspect of the F- score ) . \n\t', '\n\t\t Weighted Recall ( especially with S- scores ) gives a reasonably good approximation of Adequacy . \n\t', '\n\t\t On the one hand using 1 human reference with uniform results is essential for our methodology , since it means that there is no more \x93trouble with Recall\x94 ( Papineni et al. , 2002:314 ) \x96 a system\x92s ability to avoid under-generation of N-grams can now be reliably measured . \n\t', '\n\t\t On the other hand , using a single human reference translation instead of multiple translations will certainly increase the usability of N-gram based MT evaluation tools . \n\t', '\n\t\t The fact that non-weighted F-scores also have high correlation with Fluency suggests a new linguistic interpretation of the nature of these two quality criteria : it is intuitively plausible that Fluency subsumes , i.e. presupposes Adequacy ( similarly to the way the F-score subsumes Recall , which among all other scores gives the best correlation with Adequacy ) . \n\t', '\n\t\t The non-weighted F- score correlates more strongly with Fluency than either of its components : Precision and Recall ; similarly Adequacy might make a contribution to Fluency together with some other factors . \n\t', '\n\t\t It is conceivable that people need adequate translations ( or at least translations that make sense ) in order to be able to make judgments about naturalness , or Fluency . \n\t', '\n\t\t Being able to make some sense out of a text could be the major ground for judging Adequacy : sensible mistranslations in MT are relatively rare events . \n\t', '\n\t\t This may be the consequence of a principle similar to the \x93second law of thermodynamics\x94 applied to text structure , \x96 in practice it is much rarer to some alternative sense to be created ( even if the number of possible error types could be significant ) , than to destroy the existing sense in translation , so the majority of inadequate translations are just nonsense . \n\t', '\n\t\t However , in con- trast to human translation , fluent mistranslations in MT are even rarer than disfluent ones , according to the same principle . \n\t', '\n\t\t A real difference in scores is made by segments which make sense and may or may not be fluent , and things which do not make any sense and about which it is hard to tell whether they are fluent . \n\t', '\n\t\t This suggestion may be empirically tested : if Adequacy is a necessary precondition for Fluency , there should be a greater inter-annotator disagreement in Fluency scores on texts or segments which have lower Adequacy scores . \n\t', '\n\t\t This will be a topic of future research . \n\t', '\n\t\t We note that for the DARPA corpus the correlation scores presented are highest if the evaluation unit is an entire corpus of translations produced by an MT system , and for text-level evaluation , correlation is much lower . \n\t', '\n\t\t A similar observation was made in ( Papineni et al. , 2002 : 313 ) . \n\t', '\n\t\t This may be due to the fact that human judges are less consistent , especially for puzzling segments that do not fit the scoring guidelines , like nonsense segments for which it is hard to decide whether they are fluent or even adequate . \n\t', '\n\t\t However , this randomness is leveled out if the evaluation unit increases in size \x96 from the text level to the corpus level . \n\t', '\n\t\t Automatic evaluation methods such as BLEU \n\t\t']",Negative
"['\n\t\t 4. Stability of weighted evaluation scores In this section we investigate how reliable is the use of a single human reference translation . \n\t', '\n\t\t The stability of the scores is central to the issue of computing Recall and reducing the cost of automatic evaluation . \n\t', '\n\t\t We also would like to compare the stability of our results with the stability of the baseline non-weighted N-gram model using a single reference . \n\t', '\n\t\t In this stage of the experiment we measured the changes that occur for the scores of MT systems if an alternative reference translation is used \x96 both for the baseline N-gram counts and for the weighted N-gram model . \n\t', '\n\t\t Standard deviation was computed for each pair of evaluation scores pro duced by the two runs of the system with alternative human references . \n\t', '\n\t\t An average of these standard deviations is the measure of stability for a given score . \n\t', '\n\t\t The results of these calculations are presented in Table 5. systems StDev- StDev- StDev- P R F candide basln 0.004 0.0011 0.0002 0.0018 0.0034 0.0021 0.0011 0.0013 0.0023 0.0009 0.0008 0.0013 0.0025 0.0001 0.0009 0.0005 0.0021 0.0012 tf.idf 0.0047 0.0011 0.0012 0.0004 0.0047 0.0024 0.003 0.0006 0.0012 0.0009 0.0021 0.0016 0.0037 0.0007 0.0005 0.0008 0.003 0.0018 S-score 0.0037 0.0004 0.0019 0.0007 0.0068 0.0027 0.0001 0.0021 0.0031 0.0026 0.0008 0.0017 0.0008 0.0017 0.003 0.0023 0.0025 0.0021 globalink ms reverso systran AVE SDEV candide globalink ms reverso systran AVE SDEV candide globalink ms reverso systran AVE SDEV Table 5 . \n\t', '\n\t\t Stability of scores Standard deviation for weighted scores is generally slightly higher , but both the baseline and the weighted N-gram approaches give relatively stable results : the average standard deviation was not greater than 0.0027 , which means that both will produce reliable figures with just a single human reference translation ( although interpretation of the score with a single reference should be different than with multiple references ) . \n\t', '\n\t\t Somewhat higher standard deviation figures for the weighted N-gram model confirm the suggestion that a word\x92s importance for translation cannot be straightforwardly derived from the model of the legitimate translation variation implemented in BLEU and needs the salience weights , such as tf.idf or S-scores . \n\t', '\n\t\t 5. Conclusion and future work The results for weighted N-gram models have a significantly higher correlation with human intuitive judgements about translation Adequacy and Fluency than the baseline N-gram evaluation measures which are used in the BLEU MT evaluation toolkit . \n\t', '\n\t\t This shows that they are a promising direction of research . \n\t', '\n\t\t Future work will apply our approach to evaluating MT into languages other than English , extending the experiment to a larger number of MT systems built on different architectures and to larger corpora . \n\t', '\n\t\t However , the results of the experiment may also have implications for MT development : significance weights may be used to rank the relative \x93importance\x94 of translation equivalents . \n\t', '\n\t\t At present all MT architectures ( knowledge-based , example-based , and statistical ) treat all translation equivalents equally , so MT systems cannot dynamically prioritise rule applications , and translations of the central concepts in texts are often lost among excessively literal translations of less important concepts and function words . \n\t', '\n\t\t For example , for statistical MT significance weights of lexical items may indicate which words have to be introduced into the target text using the translation model for source and target languages , and which need to be brought there by the language model for the target corpora . \n\t', '\n\t\t Similar ideas may be useful for the Example-based and Rule-based MT architectures . \n\t', '\n\t\t The general idea is that different pieces of information expressed in the source text are not equally important for translation : MT systems that have no means for prioritising this information often introduce excessive information noise into the target text by literally translating structural information , etymology of proper names , collocations that are unacceptable in the target language , etc. . \n\t', '\n\t\t This information noise often obscures important translation equivalents and prevents the users from focusing on the relevant bits . \n\t', '\n\t\t MT quality may benefit from filtering out this excessive information as much as from frequently recommended extension of knowledge sources for MT systems . \n\t', '\n\t\t The significance weights may schedule the priority for retrieving translation equivalents and motivate application of compensation strategies in translation , e.g. , adding or deleting implicitly inferable information in the target text , using non-literal strategies , such as transposition or modulation \n\t\t']",Positive
"['\n\t\t Such weights may allow MT systems to make an approximate distinction between salient words which require proper translation equivalents and structural material both in the source and in the target texts . \n\t', '\n\t\t Exploring applica bility of this idea to various MT architectures is another direction for future research . \n\t', '\n\t\t Acknowledgments We are very grateful for the insightful comments of the three anonymous reviewers . \n\t', '\n\t\t References Akiba , Y. , K. Imamura and E. Sumita . \n\t', '\n\t\t 2001. Using multiple edit distances to automatically rank machine translation output . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t MT Summit VIII . \n\t', '\n\t\t p. 15\x96 20 . \n\t', '\n\t\t Akiba , Y. , E. Sumita , H. Nakaiwa , S. Yamamoto and H.G. Okuno . \n\t', '\n\t\t 2003. Experimental Comparison of MT Evaluation Methods : RED vs. BLEU . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t MT Summit IX , URL : http://www.amtaweb.org/summit/ MTSummit/ FinalPapers/55-Akiba-final.pdf . \n\t', '\n\t\t Babych , B. , A. Hartley and E. Atwell . \n\t', '\n\t\t 2003. Statistical Modelling of MT output corpora for Information Extraction . \n\t', '\n\t\t In : Proceedings of the Corpus Linguistics 2003 conference , Lancaster University ( UK ) , 28 - 31 March 2003 , pp. 62-70 . \n\t', '\n\t\t Babych , B. and A. Hartley . \n\t', '\n\t\t 2004. Modelling legitimate translation variation for automatic evaluation of MT quality . \n\t', '\n\t\t In : Proceedings of LREC 2004 ( forthcoming ) . \n\t', '\n\t\t Papineni , K. , S. Roukos , T. Ward , W.-J. Zhu . \n\t', '\n\t\t 2002 BLEU : a method for automatic evaluation of machine translation . \n\t', '\n\t\t Proceedings of the 40th Annual Meeting of the Association for the Computational Linguistics ( ACL ) , Philadelphia , July 2002 , pp. 311-318 . \n\t', '\n\t\t Salton , G. and M.E. Lesk . \n\t', '\n\t\t 1968. Computer evaluation of indexing and text processing . \n\t', '\n\t\t Journal of the ACM , 15(1) , 8-36 . \n\t', '\n\t\t Vinay , J.P. and J.Darbelnet . \n\t', '\n\t\t 1995. Comparative stylistics of French and English : a methodology for translation / translated and edited by Juan C. Sager , M.-J. Hamel . \n\t', '\n\t\t J. Benjamins Pub. , Amsterdam , Philadelphia . \n\t', '\n\t\t White , J. , T. O\x92Connell and F. O\x92Mara . \n\t', '\n\t\t 1994. The ARPA MT evaluation methodologies : evolution , lessons and future approaches . \n\t', '\n\t\t Proceedings of the 1st Conference of the Association for Machine Translation in the Americas . \n\t', '\n\t\t Columbia , MD , October 1994. pp. 193-205 . \n\t', '\n\t\t White , J. 2003 . \n\t', '\n\t\t How to evaluate machine translation . \n\t', '\n\t\t In : H. Somers . \n\t', '\n\t\t ( Ed . \n\t', '\n\t\t ) Computers and Translation : a translator\x92s guide . \n\t', '\n\t\t Ed . \n\t', '\n\t\t J. Benjamins B.V. , Amsterdam , Philadelphia , pp. 211-244 . \n\t', '\n\t\t Learning Word Senses With Feature Selection and Order Identification Capabilities Zheng-Yu Niu , Dong-Hong Ji Institute for Infocomm Research 21 Heng Mui Keng Terrace 119613 Singapore { zniu , dhji}@i2r.a-star.edu.sg Chew-Lim Tan Department of Computer Science National University of Singapore 3 Science Drive 2 117543 Singapore tancl@comp.nus.edu.sg Abstract This paper presents an unsupervised word sense learning algorithm , which induces senses of target word by grouping its occurrences into a \x93natural\x94 number of clusters based on the similarity of their contexts . \n\t', '\n\t\t For removing noisy words in feature set , feature selection is conducted by optimizing a cluster validation criterion subject to some constraint in an unsupervised manner . \n\t', '\n\t\t Gaussian mixture model and Minimum Description Length criterion are used to estimate cluster structure and cluster number . \n\t', '\n\t\t Experimental results show that our algorithm can find important feature subset , estimate model order ( cluster number ) and achieve better performance than another algorithm which requires cluster number to be provided . \n\t', '\n\t\t 1 Introduction Sense disambiguation is essential for many language applications such as machine translation , information retrieval , and speech processing ( Ide and V´eronis , 1998 ) . \n\t', '\n\t\t Almost all of sense disambiguation methods are heavily dependant on manually compiled lexical resources . \n\t', '\n\t\t However these lexical resources often miss domain specific word senses , even many new words are not included inside . \n\t', '\n\t\t Learning word senses from free text will help us dispense of outside knowledge source for defining sense by only discriminating senses of words . \n\t', '\n\t\t Another application of word sense learning is to help enriching or even constructing semantic lexicons \n\t\t']",Positive
"['\n\t\t The solution of word sense learning is closely related to the interpretation of word senses . \n\t', '\n\t\t Different interpretations of word senses result in different solutions to word sense learning . \n\t', '\n\t\t One interpretation strategy is to treat a word sense as a set of synonyms like synset in WordNet . \n\t', '\n\t\t The committee based word sense discovery algorithm \n\t\t']",Positive
"['\n\t\t Their algorithm initially discovered tight clusters called committees by grouping top n words similar with target word using average link clustering . \n\t', '\n\t\t Then the target word was assigned to committees if the similarity between them was above a given threshold . \n\t', '\n\t\t Each committee that the target word belonged to was interpreted as one of its senses . \n\t', '\n\t\t There are two difficulties with this committee based sense learning . \n\t', '\n\t\t The first difficulty is about derivation of feature vectors . \n\t', '\n\t\t A feature for target word here consists of a contextual content word and its grammatical relationship with target word . \n\t', '\n\t\t Acquisition of grammatical relationship depends on the output of a syntactic parser . \n\t', '\n\t\t But for some languages , ex . \n\t', '\n\t\t Chinese , the performance of syntactic parsing is still a problem . \n\t', '\n\t\t The second difficulty with this solution is that two parameters are required to be provided , which control the number of committees and the number of senses of target word . \n\t', '\n\t\t Another interpretation strategy is to treat a word sense as a group of similar contexts of target word . \n\t', '\n\t\t The context group discrimination ( CGD ) algorithm presented in ( Sch¨utze , 1998 ) adopted this strategy . \n\t', '\n\t\t Firstly , their algorithm selected important contextual words using x2 or local frequency criterion . \n\t', '\n\t\t With the x2 based criterion , those contextual words whose occurrence depended on whether the ambiguous word occurred were chosen as features . \n\t', '\n\t\t When using local frequency criterion , their algorithm selected top n most frequent contextual words as features . \n\t', '\n\t\t Then each context of occurrences of target word was represented by second order co- occurrence based context vector . \n\t', '\n\t\t Singular value decomposition ( SVD ) was conducted to reduce the dimensionality of context vectors . \n\t', '\n\t\t Then the reduced context vectors were grouped into a pre-defined number of clusters whose centroids corresponded to senses of target word . \n\t', '\n\t\t Some observations can be made about their feature selection and clustering procedure . \n\t', '\n\t\t One observation is that their feature selection uses only first order information although the second order co- occurrence data is available . \n\t', '\n\t\t The other observation is about their clustering procedure . \n\t', '\n\t\t Similar with committee based sense discovery algorithm , their clustering procedure also requires the predefinition of cluster number . \n\t', '\n\t\t Their method can capture both coarse-gained and fine-grained sense distinction as the predefined cluster number varies . \n\t', '\n\t\t But from a point of statistical view , there should exist a partitioning of data at which the most reliable , \x93natural\x94 sense clusters appear . \n\t', '\n\t\t In this paper , we follow the second order representation method for contexts of target word , since it is supposed to be less sparse and more robust than first order information ( Sch¨utze , 1998 ) . \n\t', '\n\t\t We introduce a cluster validation based unsupervised feature wrapper to remove noises in contextual words , which works by measuring the consistency between cluster structures estimated from disjoint data subsets in selected feature space . \n\t', '\n\t\t It is based on the assumption that if selected feature subset is important and complete , cluster structure estimated from data subset in this feature space should be stable and robust against random sampling . \n\t', '\n\t\t After determination of important contextual words , we use a Gaussian mixture model ( GMM ) based clustering algorithm \n\t\t']",Positive
"['\n\t\t We construct several subsets from widely used benchmark corpus as test data . \n\t', '\n\t\t Experimental results show that our algorithm ( FSGMM ) can find important feature subset , estimate cluster number and achieve better performance compared with CGD algorithm . \n\t', '\n\t\t This paper is organized as follows . \n\t', '\n\t\t In section 2 we will introduce our word sense learning algorithm , which incorporates unsupervised feature selection and model order identification technique . \n\t', '\n\t\t Then we will give out the experimental results of our algorithm and discuss some findings from these results in section 3 . \n\t', '\n\t\t Section 4 will be devoted to a brief review of related efforts on word sense discrimination . \n\t', '\n\t\t In section 5 we will conclude our work and suggest some possible improvements . \n\t', '\n\t\t 2 Learning Procedure 2.1 Feature selection Feature selection for word sense learning is to find important contextual words which help to discriminate senses of target word without using class labels in data set . \n\t', '\n\t\t This problem can be generalized as selecting important feature subset in an unsupervised manner . \n\t', '\n\t\t Many unsupervised feature selection algorithms have been presented , which can be categorized as feature filter \n\t\t']",Positive
"['\n\t\t In this paper we propose a cluster validation based unsupervised feature subset evaluation method . \n\t', '\n\t\t Cluster validation has been used to solve model order identification problem \n\t\t']",Positive
"['\n\t\t Table 1 gives out our feature subset evaluation algorithm . \n\t', '\n\t\t If some features in feature subset are noises , the estimated cluster structure on data subset in selected feature space is not stable , which is more likely to be the artifact of random splitting . \n\t', '\n\t\t Then the consistency between cluster structures estimated from disjoint data subsets will be lower . \n\t', '\n\t\t Otherwise the estimated cluster structures should be more consistent . \n\t', '\n\t\t Here we assume that splitting does not eliminate some of the underlying modes in data set . \n\t', '\n\t\t For comparison of different clustering structures , predictors are constructed based on these clustering solutions , then we use these predictors to classify the same data subset . \n\t', '\n\t\t The agreement between class memberships computed by different predictors can be used as the measure of consistency between cluster structures . \n\t', '\n\t\t We use the stability measure \n\t\t']",Positive
"['\n\t\t For each occurrence , one strategy is to construct its second order context vector by summing the vectors of contextual words , then let the feature selection procedure start to work on these second order contextual vectors to select features . \n\t', '\n\t\t However , since the sense associated with a word\x92s occurrence is always determined by very few feature words in its contexts , it is always the case that there exist more noisy words than the real features in the contexts . \n\t', '\n\t\t So , simply summing the contextual word\x92s vectors together may result in noise-dominated second order context vectors . \n\t', '\n\t\t To deal with this problem , we extend the feature selection procedure further to the construction of second order context vectors : to select better feature words in contexts to construct better second order context vectors enabling better feature selection . \n\t', '\n\t\t Since the sense associated with a word\x92s occurrence is always determined by some feature words in its contexts , it is reasonable to suppose that the selected features should cover most of occurrences . \n\t', '\n\t\t Formally , let coverage(D,T) be the coverage rate of the feature set T with respect to a set of contexts D , i.e. , the ratio of the number of the occurrences with at least one feature in their local contexts against the total number of occurrences , then we assume that coverage(D,T) > ^ . \n\t', '\n\t\t In practice , we set ^ = 0.9 . \n\t', '\n\t\t This assumption also helps to avoid the bias toward the selection of fewer features , since with fewer features , there are more occurrences without features in contexts , and their context vectors will be zero valued , which tends to result in more stable cluster structure . \n\t', '\n\t\t Let D be a set of local contexts of occurrences of target word , then D = {di}Ni_1 , where di represents local context of the i-th occurrence , and N is the total number of this word\x92s occurrences . \n\t', '\n\t\t W is used to denote bag of words occurring in context set D , then W = {wi}Mi_1 , where wi denotes a word occurring in D , and M is the total number of different contextual words . \n\t', '\n\t\t Let V denote a M x M second-order co- occurrence symmetric matrix . \n\t', '\n\t\t Suppose that the i-th , 1 < i < M , row in the second order matrix corresponds to word wi and the j-th , 1 < j < M , column corresponds to word wj , then the entry specified by i-th row and j-th column records the number of times that word wi occurs close to wj in corpus . \n\t', '\n\t\t We use v(wi) to represent the word vector of contextual word wi , which is the i-th row in matrix V. HT is a weight matrix of contextual word subset T , T ^ W . \n\t', '\n\t\t Then each entry hi J represents the weight of word wj in di , wj E T , 1 < i < N . \n\t', '\n\t\t We use binary term weighting method to derive context vectors : hiJ = 1 if word wj occurs in di , otherwise zero . \n\t', '\n\t\t Let CT = { cTi }Ni_ 1 be a set of context vectors in feature space T , where cT i is the context vector of the i-th occurrence . \n\t', '\n\t\t cTi is defined as : ( hijv(wj)) , wj E T , 1 < i < N. ( 1 ) The feature subset selection in word set W can be formulated as : T\x88 = arg maxfcriterion(T , H , V , q)1 , T C W , ( 2 ) T subject to coverage(D , T ) ^ ^ , where T\x88 is the optimal feature subset , criterion is the cluster validation based evaluation function ( the function in Table 1 ) , q is the resampling frequency for estimate of stability , and coverage(D,T) is the proportion of contexts with occurrences of features in T . \n\t', '\n\t\t This constrained optimization results in a solution which maximizes the criterion and meets the given constraint at the same time . \n\t', '\n\t\t In this paper we use sequential greedy forward floating search \n\t\t']",Positive
"['\n\t\t We set l = 1 , m = 1 , where l is plus step , and m is take-away step . \n\t', '\n\t\t 2.2 Clustering with order identification After feature selection , we employ a Gaussian mixture modelling algorithm , Cluster ( Bouman et al. , Table 1 : Unsupervised Feature Subset Evaluation Algorithm . \n\t', '\n\t\t Intuitively , for a given feature subset T , we iteratively split data set into disjoint halves , and compute the agreement of clustering solutions estimated from these sets using stability measure . \n\t', '\n\t\t The average of stability over q resampling is the estimation of the score of T. Function criterion(T , H , V , q ) Input parameter : feature subset T , weight matrix H , second order co-occurrence matrix V , resampling frequency q ; ( 1 ) ST = 0 ; ( 2 ) For i = 1 to q do ( 2.1 ) Randomly split CT into disjoint halves , denoted as CTA and CTB ; ( 2.2 ) Estimate GMM parameter and cluster number on CTA using Cluster , and the parameter set is denoted as \x88BA ; The solution \x88BA can be used to construct a predictor PA ; ( 2.3 ) Estimate GMM parameter and cluster number on CTB using Cluster , and the parameter set is denoted as \x88BB , The solution \x88BB can be used to construct a predictor PB ; ( 2.4 ) Classify CTB using PA and PB ; The class labels assigned by PA and PB are denoted as LA and LB ; ( 2.5 ) ST+ = max^ CTBlPi 1 f ir(LA(cTBi)) = LB(cTBi)1 , where ir denotes possible permutation relating indices between LA and LB , and cTBi E CTB ; ( 3 ) ST = 1qST ; ( 4 ) Return ST ; 1998 ) , to estimate cluster structure and cluster number . \n\t', '\n\t\t Let Y = { yn}Nn_ 1 be a set of M dimensional vectors to be modelled by GMM . \n\t', '\n\t\t Assuming that this model has K subclasses , let Irk denote the prior probability of subclass k , µk denote the M dimensional mean vector for subclass k , Rk denote the M x M dimensional covariance matrix for subclass k , 1 < k < K . \n\t', '\n\t\t The subclass label for pixel yn is represented by xn. MDL criterion is used for GMM parameter estimation and order identification , which is given by : log (pvnlxn(ynIE)))+ 12Llog ( NM ) , ( 3 ) pvnlxn(ynIk,B)irk , ( 4 ) L = K(1 + M + ( M +21)M ) \x97 1 , ( 5 ) The log likelihood measures the goodness of fit of a model to data sample , while the second term penalizes complex model . \n\t', '\n\t\t This estimator works by attempting to find a model order with minimum code length to describe the data sample Y and parameter set O . \n\t', '\n\t\t If the cluster number is fixed , the estimation of GMM parameter can be solved using EM algorithm E T ci = j N E n_1 MDL(K , B ) = \x97 K E k_1 pvnlxn(ynIE)) = to address this type of incomplete data problem \n\t\t']",Positive
"['\n\t\t The initialization of mixture parameter 0M is given by : 1 7rk = ( 1 ) Ko ( 6 ) µk 1 ) = yn , where n = L(k \x97 1)(N \x97 1)/(Ko \x97 1)J + 1 ( 7 ) R(1) = 1NENn_1ynytn(8) Ko is a given initial subclass number . \n\t', '\n\t\t Then EM algorithm is used to estimate model parameters by minimizing MDL : E-step : re-estimate the expectations based on previous iteration : pXnvn(kJyn,0(i)) = pvnlXn(yn Jk , 0(i))7rk 9 t{1(PvnIXn(YnI""0(i))70 ( ) E M-step : estimate the model parameter 0(i) to maximize the log-likelihood in MDL : pXnvn(kJyn,0(i)) ( 10 ) 7rk = Nk ( 11 ) N ( 13 ) pvnXn ( yn Jk,0(i)) = ( 27r1M/2JRkJ^1/2exp{^} ( 14 ) ^= \x97 2 ( yn \x97 µk ) Rk ^1 ( yn\x97 µk ) ( 15 ) The EM iteration is terminated when the change of MDL(K , 0 ) is less than E : e= 100 ( 1 + M + ( M 21)M )log(NM) ( 16 ) For inferring the cluster number , EM algorithm is applied for each value of K , 1 G K G Ko , and the value K\x88 which minimizes the value of MDL is chosen as the correct cluster number . \n\t', '\n\t\t To make this process more efficient , two cluster pair l and m are selected to minimize the change in MDL crite- ria when reducing K to K \x97 1 . \n\t', '\n\t\t These two clusters l and m are then merged . \n\t', '\n\t\t The resulting parameter set is chosen as an initial condition for EM iteration with K \x97 1 subclasses . \n\t', '\n\t\t This operation will avoid a complete minimization with respect to ^ , µ , and R for each value of K. Table 2 : Four ambiguous words , their senses and frequency distribution of each sense . \n\t', '\n\t\t Word Sense Percentage hard not easy ( difficult ) 82.8 % ( adjective ) not soft ( metaphoric ) 9.6 % not soft ( physical ) 7.6 % interest money paid for the use of money 52.4 % a share in a company or business 20.4 % readiness to give attention 14 % advantage , advancement or favor 9.4 % activity that one gives attention to 3.6 % causing attention to be given to 0.2 % line product 56 % ( noun ) telephone connection 10.6 % written or spoken text 9.8 % cord 8.6 % division 8.2 % formation 6.8 % serve supply with food 42.6 % ( verb ) hold an office 33.6 % function as something 16 % provide a service 7.8 % 3 Experiments and Evaluation 3.1 Test data We constructed four datasets from hand-tagged corpus 1 by randomly selecting 500 instances for each ambiguous word - \x93hard\x94 , \x93interest\x94 , \x93line\x94 , and \x93serve\x94 . \n\t', '\n\t\t The details of these datasets are given in Table 2 . \n\t', '\n\t\t Our preprocessing included lowering the upper case characters , ignoring all words that contain digits or non alpha-numeric characters , removing words from a stop word list , and filtering out low frequency words which appeared only once in entire set . \n\t', '\n\t\t We did not use stemming procedure . \n\t', '\n\t\t The sense tags were removed when they were used by FSGMM and CGD . \n\t', '\n\t\t In evaluation procedure , these sense tags were used as ground truth classes . \n\t', '\n\t\t A second order co-occurrence matrix for English words was constructed using English version of Xinhua News ( Jan. 1998-Dec. 1999 ) . \n\t', '\n\t\t The window size for counting second order co-occurrence was 50 words . \n\t', '\n\t\t 3.2 Evaluation method for feature selection For evaluation of feature selection , we used mutual information between feature subset and class label set to assess the importance of selected feature subset . \n\t', '\n\t\t Our assessment measure is defined as : p(w , l)logp(( )p(l) , ( 17 ) where T is the feature subset to be evaluated , T C W , L is class label set , p(w , l ) is the joint distribution of two variables w and l , p(w) and p(l) are marginal probabilities . \n\t', '\n\t\t p(w , l ) is estimated based 1http://www.d.umn.edu/^tpederse/data.html ynpXnlvn(kJyn,0(i)) ( 12 ) 1 Nk ~N n_1 µk = 1 Rk = Nk ( yn \x97 µk ) ( yn \x97 µk)tpXn l vn ( k Jyn , 0(i) ) N E n_1 N E n_1 Nk = X M(T)= J1 J wET X lEL on contingency table of contextual word set W and class label set L. Intuitively , if M(T1) > M(T2) , T1 is more important than T2 since T1 contains more information about L. 3.3 Evaluation method for clustering result When assessing the agreement between clustering result and hand-tagged senses ( ground truth classes ) in benchmark data , we encountered the difficulty that there was no sense tag for each cluster . \n\t', '\n\t\t In \n\t\t']",Positive
"['\n\t\t In this paper , we applied their method to assign different sense tags to only min ( I UI,I CI ) clusters by maximizing the accuracy , where I U I is the number of clusters , and ICI is the number of ground truth classes . \n\t', '\n\t\t The underlying assumption here is that each cluster is considered as a class , and for any two clusters , they do not share same class labels . \n\t', '\n\t\t At most ICI clusters are assigned sense tags , since there are only ICI classes in benchmark data . \n\t', '\n\t\t Given the contingency table Q between clusters and ground truth classes , each entry Qj j gives the number of occurrences which fall into both the ith cluster and the j-th ground truth class . \n\t', '\n\t\t If I U I < ICI , we constructed empty clusters so that IUI = ICI . \n\t', '\n\t\t Let Q represent a one-to-one mapping function from C to U . \n\t', '\n\t\t It means that Q ( j 1 ) =~ Q ( j2 ) if j 1 =~ j2 and vice versa , 1 < j1 , j2 < ICI . \n\t', '\n\t\t Then Q(j) is the index of the cluster associated with the j-th class . \n\t', '\n\t\t Searching a mapping function to maximize the accuracy of U can be formulated as : IjiIj In fact , Ejj Qj j is equal to N , the number of occurrences of target word in test set . \n\t', '\n\t\t 3.4 Experiments and results For each dataset , we tested following procedures : CGDterr,t:We implemented the context group discrimination algorithm . \n\t', '\n\t\t Top max ( IWI x 20 % , 100 ) words in contextual word list was selected as features using frequency or k2 based ranking . \n\t', '\n\t\t Then k-means clustering2 was performed on context vector matrix using normalized Euclidean distance . \n\t', '\n\t\t K-means clustering was repeated 5 times 2We used k-means function in statistics toolbox ofMatlab . \n\t', '\n\t\t and the partition with best quality was chosen as final result . \n\t', '\n\t\t The number of clusters used by k-means was set to be identical with the number of ground truth classes . \n\t', '\n\t\t We tested CGDterr,t using various word vector weighting methods when deriving context vectors , ex . \n\t', '\n\t\t binary , idf , t f \x95 idf . \n\t', '\n\t\t CGDSVD : The context vector matrix was derived using same method in CGDterr,t . \n\t', '\n\t\t Then k- means clustering was conducted on latent semantic space transformed from context vector matrix , using normalized Euclidean distance . \n\t', '\n\t\t Specifically , context vectors were reduced to 100 dimensions using SVD . \n\t', '\n\t\t If the dimension of context vector was less than 100 , all of latent semantic vectors with non-zero eigenvalue were used for subsequent clustering . \n\t', '\n\t\t We also tested it using different weighting methods , ex . \n\t', '\n\t\t binary , idf , t f \x95 idf . \n\t', '\n\t\t F5GMM : We performed cluster validation based feature selection in feature set used by CGD . \n\t', '\n\t\t Then Cluster algorithm was used to group target word\x92s instances using Euclidean distance measure . \n\t', '\n\t\t ^ was set as 0.90 in feature subset search procedure . \n\t', '\n\t\t The random splitting frequency is set as 10 for estimation of the score of feature subset . \n\t', '\n\t\t The initial subclass number was 20 and full covariance matrix was used for parameter estimation of each subclass . \n\t', '\n\t\t For investigating the effect of different context window size on the performance of three procedures , we tested these procedures using various context window sizes : ±1 , ±5 , ±15 , ±25 , and all of contextual words . \n\t', '\n\t\t The average length of sentences in 4 datasets is 32 words before preprocessing . \n\t', '\n\t\t Performance on each dataset was assessed by equation 19 . \n\t', '\n\t\t The scores of feature subsets selected by F5GMM and CGD are listed in Table 3 and 4 . \n\t', '\n\t\t The average accuracy of three procedures with different feature ranking and weighting method is given in Table 5 . \n\t', '\n\t\t Each figure is the average over 5 different context window size and 4 datasets . \n\t', '\n\t\t We give out the detailed results of these three procedures in Figure 1 . \n\t', '\n\t\t Several results should be noted specifically : From Table 3 and 4 , we can find that F5GMM achieved better score on mutual information ( MI ) measure than CGD over 35 out of total 40 cases . \n\t', '\n\t\t This is the evidence that our feature selection procedure can remove noise and retain important features . \n\t', '\n\t\t As it was shown in Table 5 , with both k2 and f req based feature ranking , F5GMM algorithm performed better than CGDterr,t and CGDSVD if we used average accuracy to evaluate their performance . \n\t', '\n\t\t Specifically , with k2 based feature ^\x88 = arg max ICI QO(j)Ij . \n\t', '\n\t\t ( 18 ) O L j=1 Then the accuracy of solution U is given by Accuracy(U) = Ej Q\x88O(j)Ij . \n\t', '\n\t\t Q ( 19 ) ranking , FSGMM attained 55.4 % average accuracy , while the best average accuracy of CGDterm and CGDSVD were 40.9 % and 51.3 % respectively . \n\t', '\n\t\t With f req based feature ranking , FSGMM achieved 51.2 % average accuracy , while the best average accuracy of CGDterm and CGDSVD were 45.1 % and 50.2 % . \n\t', '\n\t\t The automatically estimated cluster numbers by FSGMM over 4 datasets are given in Table 6 . \n\t', '\n\t\t The estimated cluster number was 2 - 4 for \x93hard\x94 , 3 - 6 for \x93interest\x94 , 3 - 6 for \x93line\x94 , and 2 - 4 for \x93serve\x94 . \n\t', '\n\t\t It is noted that the estimated cluster number was less than the number of ground truth classes in most cases . \n\t', '\n\t\t There are some reasons for this phenomenon . \n\t', '\n\t\t First , the data is not balanced , which may lead to that some important features cannot be retrieved . \n\t', '\n\t\t For example , the fourth sense of \x93serve\x94 , and the sixth sense of \x93line\x94 , their corresponding features are not up to the selection criteria . \n\t', '\n\t\t Second , some senses can not be distinguished using only bag-of-words information , and their difference lies in syntactic information held by features . \n\t', '\n\t\t For example , the third sense and the sixth sense of \x93interest\x94 may be distinguished by syntactic relation of feature words , while the bag of feature words occurring in their context are similar . \n\t', '\n\t\t Third , some senses are determined by global topics , rather than local contexts . \n\t', '\n\t\t For example , according to global topics , it may be easier to distinguish the first and the second sense of \x93interest\x94 . \n\t', '\n\t\t Figure 2 shows the average accuracy over three procedures in Figure 1 as a function of context window size for 4 datasets . \n\t', '\n\t\t For \x93hard\x94 , the performance dropped as window size increased , and the best accuracy(77.0%) was achieved at window size 1 . \n\t', '\n\t\t For \x93interest\x94 , sense discrimination did not benefit from large window size and the best accuracy(40 . \n\t', '\n\t\t 1 % ) was achieved at window size 5 . \n\t', '\n\t\t For \x93line\x94 , accuracy dropped when increasing window size and the best accuracy(50.2%) was achieved at window size 1 . \n\t', '\n\t\t For \x93serve\x94 , the performance benefitted from large window size and the best accuracy(46.8%) was achieved at window size 15 . \n\t', '\n\t\t In \n\t\t']",Positive
"['\n\t\t They observed that local context was more reliable than topical context as an indicator of senses for this verb and adjective , but slightly less reliable for this noun . \n\t', '\n\t\t Compared with their conclusion , we can find that our result is consistent with it for \x93hard\x94 . \n\t', '\n\t\t But there is some differences for verb \x93serve\x94 and noun \x93line\x94 . \n\t', '\n\t\t For Table 3 : Mutual information between feature subset and class label with x2 based feature ranking . \n\t', '\n\t\t Word Cont . \n\t', '\n\t\t Size of MI Size of MI wind . \n\t', '\n\t\t size feature subset of CGD x 10-2 feature X10-2 subset of FSGMM hard 1 18 6.4495 14 8.1070 5 100 0.4018 80 0.4300 15 100 0.1362 80 0.1416 25 133 0.0997 102 0.1003 all 145 0.0937 107 0.0890 interest 1 64 1.9697 55 2.0639 5 100 0.3234 89 0.3355 15 157 0.1558 124 0.1531 25 190 0.1230 138 0.1267 all 200 0.1163 140 0.1191 line 1 39 4.2089 32 4.6456 5 100 0.4628 84 0.4871 15 183 0.1488 128 0.1429 25 263 0.1016 163 0.0962 all 351 0.0730 192 0.0743 serve 1 22 6.8169 20 6.7043 5 100 0.5057 85 0.5227 15 188 0.2078 164 0.2094 25 255 0.1503 225 0.1536 all 320 0.1149 244 0.1260 Table 4 : Mutual information between feature subset and class label with f req based feature ranking . \n\t', '\n\t\t Word Cont . \n\t', '\n\t\t Size of MI Size of MI wind . \n\t', '\n\t\t size feature subset of CGD x 10-2 feature X10-2 subset of FSGMM hard 1 18 6.4495 14 8.1070 5 100 0.4194 80 0.4832 15 100 0.1647 80 0.1774 25 133 0.1150 102 0.1259 all 145 0.1064 107 0.1269 interest 1 64 1.9697 55 2.7051 5 100 0.6015 89 0.8309 15 157 0.2526 124 0.3495 25 190 0.1928 138 0.2982 all 200 0.1811 140 0.2699 line 1 39 4.2089 32 4.4606 5 100 0.6895 84 0.7816 15 183 0.2301 128 0.2929 25 263 0.1498 163 0.2181 all 351 0.1059 192 0.1630 serve 1 22 6.8169 20 7.0021 5 100 0.7045 85 0.8422 15 188 0.2763 164 0.3418 25 255 0.1901 225 0.2734 all 320 0.1490 244 0.2309 \x93serve\x94 , the possible reason is that we do not use position of local word and part of speech information , which may deteriorate the performance when local context(<_ 5 words ) is used . \n\t', '\n\t\t For \x93line\x94 , the reason might come from the feature subset , which is not good enough to provide improvement when Table 5 : Average accuracy of three procedures with various settings over 4 datasets . \n\t', '\n\t\t Algorithm Feature Feature Average ranking method weighting method accuracy FSGMM x2 binary 0.554 CGDte ,. , x2 binary 0.404 CGDte ,. , x2 idf 0.407 CGDte ,. , x2 t f \x95 idf 0.409 CGDSVD x2 binary 0.513 CGDSVD x2 idf 0.512 CGDSVD x2 t f \x95 idf 0.508 FSGMM f req binary 0.512 CGDte ,. , f req binary 0.451 CGDte ,. , f req idf 0.437 CGDte ,. , f req t f \x95 idf 0.447 CGDSVD f req binary 0.502 CGDSVD f req idf 0.498 CGDSVD f req t f \x95 idf 0.485 Table 6 : Automatically determined mixture component num- ber . \n\t', '\n\t\t Word Context Model Model window order order size with x2 with f req hard 1 3 4 5 2 2 15 2 3 25 2 3 all 2 3 interest 1 5 4 5 3 4 15 4 6 25 4 6 all 3 4 line 1 5 6 5 4 3 15 5 4 25 5 4 all 3 4 serve 1 3 3 5 3 4 15 3 3 25 3 3 all 2 4 context window size is no less than 5 . \n\t', '\n\t\t 4 Related Work Besides the two works ( Pantel and Lin , 2002 ; Sch¨utze , 1998 ) , there are other related efforts on word sense discrimination \n\t\t']",Positive
['\n\t\t In \n\t\t'],Positive
"['\n\t\t Their feature sets included morphology of target word , part of speech of contextual words , absence or presence of particular contextual words , and collocation of fre- Figure 1 : Results for three procedures over 4 datases.aThe horizontal axis corresponds to the context window size . \n\t', '\n\t\t Solid line represents the result of FSGMM + binary , dashed line denotes the result of CGDSVD + idf , and dotted line is the result of CGDte ,. , + idf . \n\t', '\n\t\t Square marker denotes x2 based feature ranking , while cross marker denotes f req based feature ranking . \n\t', '\n\t\t Figure 2 : Average accuracy over three procedures in Figure 1 as a function of context window size ( horizontal axis ) for 4 datasets . \n\t', '\n\t\t quent words . \n\t', '\n\t\t Then occurrences of target word were grouped into a pre-defined number of clusters . \n\t', '\n\t\t Similar with many other algorithms , their algorithm also required the cluster number to be provided . \n\t', '\n\t\t In \n\t\t']",Negative
"['\n\t\t The weakness of their method is to assume that nouns co-occurring with verbs are disambiguated in advance and the number of senses of target verb is no less than two . \n\t', '\n\t\t The algorithm in \n\t\t']",Positive
"['\n\t\t Then senses of target word were iteratively learned by clustering the local graph of similar words around target word . \n\t', '\n\t\t Their algorithm required a threshold as input , which controlled the number of senses . \n\t', '\n\t\t 5 Conclusion and Future Work Our word sense learning algorithm combined two novel ingredients : feature selection and order identification . \n\t', '\n\t\t Feature selection was formalized as a constrained optimization problem , the output of which was a set of important features to determine word senses . \n\t', '\n\t\t Both cluster structure and cluster number were estimated by minimizing a MDL criterion . \n\t', '\n\t\t Experimental results showed that our algorithm can retrieve important features , estimate cluster number automatically , and achieve better performance in terms of average accuracy than CGD algorithm which required cluster number as input . \n\t', '\n\t\t Our word sense learning algorithm is unsupervised in two folds : no requirement of sense tagged data , and no requirement of predefinition of sense number , which enables the automatic discovery of word senses from free text . \n\t', '\n\t\t In our algorithm , we treat bag of words in local contexts as features . \n\t', '\n\t\t It has been shown that local collocations and morphology of target word play important roles in word sense disambiguation or discrimination \n\t\t']",Positive
"['\n\t\t It is necessary to incorporate these more structural information to improve the performance of word sense learning . \n\t', '\n\t\t References Bouman , C. A. , Shapiro , M. , Cook , G. W. , Atkins , C. B. , & Cheng , H. ( 1998 ) Cluster : An Unsupervsied Algorithm for Modeling Gaussian Mixtures . \n\t', '\n\t\t http://dynamo.ecn.purdue.edu/ \x97 bouman/software/cluster/ . \n\t', '\n\t\t Dash , M. , Choi , K. , Scheuermann , P. , & Liu , H. ( 2002 ) Feature Selection for Clustering - A Filter Solution . \n\t', '\n\t\t Proc . \n\t', '\n\t\t of IEEE Int. Conf . \n\t', '\n\t\t on Data Mining(pp . \n\t', '\n\t\t 115\x96 122 ) . \n\t', '\n\t\t Dempster , A. P. , Laird , N. M. , & Rubin , D. B. ( 1977 ) Maximum likelihood from incomplete data using the EM algorithm . \n\t', '\n\t\t Journal of the Royal Statistical Society , 39(B) . \n\t', '\n\t\t Dorow , B , & Widdows , D. ( 2003 ) Discovering Corpus- Specific Word Senses . \n\t', '\n\t\t Proc . \n\t', '\n\t\t of the 10th Conf . \n\t', '\n\t\t of the European Chapter of the Association for Computa- tional Linguistics , Conference Companion ( research notes and demos)(pp.79\x9682) . \n\t', '\n\t\t Dy , J. G. , & Brodley , C. E. ( 2000 ) Feature Subset Selection and Order Identification for Unsupervised Learning . \n\t', '\n\t\t Proc . \n\t', '\n\t\t of the 17th Int. Conf . \n\t', '\n\t\t on Machine Learning(pp . \n\t', '\n\t\t 247\x96254 ) . \n\t', '\n\t\t Fukumoto , F. , & Suzuki , Y. ( 1999 ) Word Sense Disambiguation in Untagged Text Based on Term Weight Learning . \n\t', '\n\t\t Proc . \n\t', '\n\t\t of the 9th Conf . \n\t', '\n\t\t ofEuropean Chapter of the Association for Computational Linguistics(pp . \n\t', '\n\t\t 209\x96216 ) . \n\t', '\n\t\t Ide , N. , & V´eronis , J. ( 1998 ) Word Sense Disambiguation : The State of the Art . \n\t', '\n\t\t Computational Linguistics , 24:1,1\x9641 . \n\t', '\n\t\t Lange , T. , Braun , M. , Roth , V. , & Buhmann , J. M. ( 2002 ) Stability-Based Model Selection . \n\t', '\n\t\t Advances in Neural Information Processing Systems 15 . \n\t', '\n\t\t Law , M. H. , Figueiredo , M. , & Jain , A. K. ( 2002 ) Feature Selection in Mixture-Based Clustering . \n\t', '\n\t\t Advances in Neural Information Processing Systems 15 . \n\t', '\n\t\t Leacock , C. , Chodorow , M. , & Miller A. G. ( 1998 ) Using Corpus Statistics and WordNet Relations for Sense Identification . \n\t', '\n\t\t Computational Linguistics , 24:1 , 147\x96 165 . \n\t', '\n\t\t Levine , E. , & Domany , E. ( 2001 ) Resampling Method for Unsupervised Estimation of Cluster Validity . \n\t', '\n\t\t Neural Computation , Vol. 13 , 2573\x962593 . \n\t', '\n\t\t Mitra , P. , Murthy , A. C. , & Pal , K. S. ( 2002 ) Unsupervised Feature Selection Using Feature Similarity . \n\t', '\n\t\t IEEE Transactions on Pattern Analysis and Machine Intelligence , 24:4 , 301\x96312 . \n\t', '\n\t\t Modha , D. S. , & Spangler , W. S. ( 2003 ) Feature Weighting in k-Means Clustering . \n\t', '\n\t\t Machine Learning , 52:3 , 217\x96237 . \n\t', '\n\t\t Pantel , P. & Lin , D. K. ( 2002 ) Discovering Word Senses from Text . \n\t', '\n\t\t Proc . \n\t', '\n\t\t of ACM SIGKDD Conf . \n\t', '\n\t\t on Knowledge Discovery and Data Mining(pp . \n\t', '\n\t\t 613-619 ) . \n\t', '\n\t\t Pedersen , T. , & Bruce , R. ( 1997 ) Distinguishing Word Senses in Untagged Text . \n\t', '\n\t\t Proceedings of the 2nd Conference on Empirical Methods in Natural Language Processing(pp . \n\t', '\n\t\t 197\x96207 ) . \n\t', '\n\t\t Pudil , P. , Novovicova , J. , & Kittler , J. ( 1994 ) Floating Search Methods in Feature Selection . \n\t', '\n\t\t Pattern Recognigion Letters , Vol. 15 , 1119-1125 . \n\t', '\n\t\t Rissanen , J. ( 1978 ) Modeling by Shortest Data Description . \n\t', '\n\t\t Automatica , Vol. 14 , 465\x96471 . \n\t', '\n\t\t Sch¨utze , H. ( 1998 ) Automatic Word Sense Discrimination . \n\t', '\n\t\t Computational Linguistics , 24:1 , 97\x96123 . \n\t', '\n\t\t Talavera , L. ( 1999 ) Feature Selection as a Preprocessing Step for Hierarchical Clustering . \n\t', '\n\t\t Proc . \n\t', '\n\t\t of the 16th Int. Conf . \n\t', '\n\t\t on Machine Learning(pp . \n\t', '\n\t\t 389\x96397 ) . \n\t', '\n\t\t Widdows , D. ( 2003 ) Unsupervised methods for developing taxonomies by combining syntactic and statistical information . \n\t', '\n\t\t Proc . \n\t', '\n\t\t of the Human Language Technology / Conference of the North American Chapter of the Association for Computational Linguistics(pp . \n\t', '\n\t\t 276\x96283 ) . \n\t', ""\n\t\t A Kernel PCA Method for Superior Word Sense Disambiguation Dekai WU ' Weifeng SU Marine CARPUAT dekai@cs.ust.hk weifeng@cs.ust.hk marine@cs.ust.hk Human Language Technology Center HKUST Department of Computer Science University of Science and Technology Clear Water Bay , Hong Kong Abstract We introduce a new method for disambiguating word senses that exploits a nonlinear Kernel Principal Component Analysis ( KPCA ) technique to achieve accuracy superior to the best published individual models . \n\t"", '\n\t\t We present empirical results demonstrating significantly better accuracy compared to the state-of-the-art achieved by either naive Bayes or maximum entropy models , on Senseval-2 data . \n\t', '\n\t\t We also contrast against another type of kernel method , the support vector machine ( SVM ) model , and show that our KPCA-based model outperforms the SVM-based model . \n\t', '\n\t\t It is hoped that these highly encouraging first results on KPCA for natural language processing tasks will inspire further development of these directions . \n\t', '\n\t\t 1 Introduction Achieving higher precision in supervised word sense disambiguation ( WSD ) tasks without resorting to ad hoc voting or similar ensemble techniques has become somewhat daunting in recent years , given the challenging benchmarks set by naive Bayes models ( e.g. , \n\t\t']",Positive
['\n\t\t A good foundation for comparative studies has been established by the Senseval data and evaluations ; of particular relevance here are the lexical sample tasks from Senseval-1 \n\t\t'],Positive
"['\n\t\t We therefore chose this problem to introduce an efficient and accurate new word sense disambiguation approach that exploits a nonlinear Kernel PCA technique to make predictions implicitly based on generalizations over feature combinations . \n\t', ""\n\t\t The ' The author would like to thank the Hong Kong Re- search Grants Council ( RGC ) for supporting this research in part through grants RGC6083/99E , RGC6256/00E , and DAG03/04.EG09 . \n\t"", '\n\t\t technique is applicable whenever vector representations of a disambiguation task can be generated ; thus many properties of our technique can be expected to be highly attractive from the standpoint of natural language processing in general . \n\t', '\n\t\t In the following sections , we first analyze the potential of nonlinear principal components with respect to the task of disambiguating word senses . \n\t', '\n\t\t Based on this , we describe a full model for WSD built on KPCA . \n\t', '\n\t\t We then discuss experimental results confirming that this model outperforms state- of-the-art published models for Senseval-related lexical sample tasks as represented by ( 1 ) naive Bayes models , as well as ( 2 ) maximum entropy models . \n\t', '\n\t\t We then consider whether other kernel methods\x97in particular , the popular SVM model\x97 are equally competitive , and discover experimentally that KPCA achieves higher accuracy than the SVM model . \n\t', '\n\t\t 2 Nonlinear principal components and WSD The Kernel Principal Component Analysis technique , or KPCA , is a nonlinear kernel method for extraction of nonlinear principal components from vector sets in which , conceptually , the n- dimensional input vectors are nonlinearly mapped from their original space Rn to a high-dimensional feature space F where linear PCA is performed , yielding a transform by which the input vectors can be mapped nonlinearly to a new set of vectors ( Sch¨olkopf et al. , 1998 ) . \n\t', '\n\t\t A major advantage of KPCA is that , unlike other common analysis techniques , as with other kernel methods it inherently takes combinations of predictive features into account when optimizing dimensionality reduction . \n\t', '\n\t\t For natural language problems in general , of course , it is widely recognized that significant accuracy gains can often be achieved by generalizing over relevant feature combinations ( e.g. , \n\t\t']",Positive
"['\n\t\t Another advantage of KPCA for the WSD task is that the dimensionality of the input data is generally very Table 1 : Two of the Senseval-2 sense classes for the target word \x93art\x94 , from WordNet 1.7 \n\t\t']",Positive
"['\n\t\t Class Sense 1 the creation of beautiful or significant things 2 a superior skill large , a condition where kernel methods excel . \n\t', '\n\t\t Nonlinear principal components \n\t\t']",Positive
"['\n\t\t Suppose we are given a training set of M pairs ( xt , ct ) where the observed vectors xt E Rn in an n- dimensional input space X represent the context of the target word being disambiguated , and the correct class ct represents the sense of the word , for t = 1 , .. , M . \n\t', '\n\t\t Suppose ` b is a nonlinear mapping from the input space Rn to the feature space F . \n\t', '\n\t\t Without loss of generality we assume the M vectors are centered vectors in the feature space , i.e. , ~Mt=1 ` b ( xt ) = 0 ; uncentered vectors can easily be converted to centered vectors ( Sch¨olkopf et al. , 1998 ) . \n\t', '\n\t\t We wish to diagonalize the covariance matrix in F : ` b ( xj ) ` bT ( xj ) ( 1 ) To do this requires solving the equation Av = Cv for eigenvalues A > 0 and eigenvectors v E F. Because ( ` b ( xj ) - v)`b ( xj ) ( 2 ) we can derive the following two useful results . \n\t', '\n\t\t First , A ( ` b(xt) - v ) = ` b ( xt ) - Cv ( 3 ) for t = 1 , .. , M. Second , there exist ai for i = 1 , ... , M such that v= ~M ai`b ( xi ) ( 4 ) i=1 Combining ( 1 ) , ( 3 ) , and ( 4 ) , we obtain MA ~M ai ( ` b(xt) - ` b(xi)) i=1 ` b ( xj ) ) ( ` b(xj) - ` b(xi ) ) for t = 1 , .. , M . \n\t', '\n\t\t Let K\x88 be the M x M matrix such that \x88Kij = ` b ( xi ) - ` b ( xj ) ( 5 ) and let \x88A1 > \x88A2 > ... > \x88AM denote the eigenvalues of K\x88 and \x88a1 , ... , \x88aM denote the corresponding complete set of normalized eigenvectors , such that \x88At ( \x88at - \x88at ) = 1 when \x88At > 0 . \n\t', '\n\t\t Then the lth nonlinear principal component of any test vector xt is defined as yi = ~M \x88ali ( ` b(xi) - ` b(xt)) ( 6 ) i=1 where \x88ali is the lth element of \x88al . \n\t', '\n\t\t To illustrate the potential of nonlinear principal components for WSD , consider a simplified disambiguation example for the ambiguous target word \x93art\x94 , with the two senses shown in Table 1 . \n\t', '\n\t\t Assume a training corpus of the eight sentences as shown in Table 2 , adapted from Senseval-2 English lexical sample corpus . \n\t', '\n\t\t For each sentence , we show the feature set associated with that occurrence of \x93art\x94 and the correct sense class . \n\t', '\n\t\t These eight occurrences of \x93art\x94 can be transformed to a binary vector representation containing one dimension for each feature , as shown in Table 3 . \n\t', '\n\t\t Extracting nonlinear principal components for the vectors in this simple corpus results in nonlinear generalization , reflecting an implicit consideration of combinations of features . \n\t', '\n\t\t Table 3 shows the first three dimensions of the principal component vectors obtained by transforming each of the eight training vectors xt into ( a ) principal component vectors zt using the linear transform obtained via PCA , and ( b ) nonlinear principal component vectors yt using the nonlinear transform obtained via KPCA as described below . \n\t', '\n\t\t Similarly , for the test vector x9 , Table 4 shows the first three dimensions of the principal component vectors obtained by transforming it into ( a ) a principal component vector z9 using the linear PCA transform obtained from training , and ( b ) a nonlinear principal component vector y9 using the nonlinear KPCA transform obtained obtained from training . \n\t', '\n\t\t The vector similarities in the KPCA-transformed space can be quite different from those in the PCAtransformed space . \n\t', '\n\t\t This causes the KPCA-based model to be able to make the correct class prediction , whereas the PCA-based model makes the 1 C = M M E j=1 Cv = M M E j=1 ~M i=1 ai(`b ( xt ) - M E j=1 Table 2 : A tiny corpus for the target word \x93art\x94 , adapted from the Senseval-2 English lexical sample corpus \n\t\t']",Positive
"['\n\t\t The training and testing examples can be represented as a set of binary vectors : each row shows the correct class c for an observed vector x of five dimensions . \n\t', '\n\t\t TRAINING design/N media/N the/DT entertainment/N world/N Class x1 He studies art in London . \n\t', '\n\t\t 1 x2 Punch\x92s weekly guide to the world of the arts , entertainment , media and more . \n\t', '\n\t\t 1 1 1 1 x3 All such studies have in- fluenced every form of art , design , and entertainment in some way . \n\t', '\n\t\t 1 1 1 x4 Among the techni- 1 2 cal arts cultivated in some continental schools that began to affect England soon after the Norman Conquest were those of measurement and calculation . \n\t', '\n\t\t x5 The Art of Love . \n\t', '\n\t\t 1 2 x6 Indeed , the art of doc- 1 2 toring does contribute to better health results and discourages unwarranted malpractice litigation . \n\t', '\n\t\t x7 Countless books and 1 2 classes teach the art of asserting oneself . \n\t', '\n\t\t x8 Pop art is an example . \n\t', '\n\t\t 1 TESTING x9 In the world of de- 1 1 1 1 sign arts particularly , this led to appointments made for political rather than academic reasons . \n\t', '\n\t\t wrong class prediction . \n\t', '\n\t\t What permits KPCA to apply stronger generalization biases is its implicit consideration of combinations of feature information in the data distribution from the high-dimensional training vectors . \n\t', '\n\t\t In this simplified illustrative example , there are just five input dimensions ; the effect is stronger in more realistic high dimensional vector spaces . \n\t', '\n\t\t Since the KPCA transform is computed from unsupervised training vector data , and extracts generalizations that are subsequently utilized during supervised classification , it is quite possible to combine large amounts of unsupervised data with reasonable smaller amounts of supervised data . \n\t', '\n\t\t It can be instructive to attempt to interpret this example graphically , as follows , even though the interpretation in three dimensions is severely limiting . \n\t', '\n\t\t Figure 1(a) depicts the eight original observed training vectors xt in the first three of the five dimensions ; note that among these eight vectors , there happen to be only four unique points when restricting our view to these three dimensions . \n\t', '\n\t\t Ordinary linear PCA can be straightforwardly seen as projecting the original points onto the principal axis , Table 3 : The original observed training vectors ( showing only the first three dimensions ) and their first three principal components as transformed via PCA and KPCA . \n\t', '\n\t\t Observed vectors PCA-transformed vectors KPCA-transformed vectors Class t 1 2 3 1 2 3 1 2 3 ct ( xt,xt,xt ) ( zt,zt,zt ) ( yt,yt,yt ) 1 ( 0 , 0 , 0 ) ( -1.961 , 0.2829 , 0.2014 ) ( 0.2801 , -1.005 , -0.06861 ) 1 2 ( 0 , 1 , 1 ) ( 1.675 , -1.132 , 0.1049 ) ( 1.149 , 0.02934 , 0.322 ) 1 3 ( 1 , 0 , 0 ) ( -0.367 , 1.697 , -0.2391 ) ( 0.8209 , 0.7722 , -0.2015 ) 1 4 ( 0 , 0 , 1 ) ( -1.675 , -1.132 , -0.1049 ) ( -1.774 , -0.1216 , 0.03258 ) 2 5 ( 0 , 0 , 1 ) ( -1.675 , -1.132 , -0.1049 ) ( -1.774 , -0.1216 , 0.03258 ) 2 6 ( 0 , 0 , 1 ) ( -1.675 , -1.132 , -0.1049 ) ( -1.774 , -0.1216 , 0.03258 ) 2 7 ( 0 , 0 , 1 ) ( -1.675 , -1.132 , -0.1049 ) ( -1.774 , -0.1216 , 0.03258 ) 2 8 ( 0 , 0 , 0 ) ( -1.961 , 0.2829 , 0.2014 ) ( 0.2801 , -1.005 , -0.06861 ) 1 Table 4 : Testing vector ( showing only the first three dimensions ) and its first three principal components as transformed via the trained PCA and KPCA parameters . \n\t', '\n\t\t The PCA-based and KPCA-based sense class predictions disagree . \n\t', '\n\t\t Observed vectors PCA-transformed vectors KPCA-transformed vec- tors Predicted Class Correct Class t 1 2 3 1 2 3 1 2 3 \x88ct ct ( xt,xt,xt ) ( zt,zt,zt ) ( yt,yt,yt ) 9 ( 1 , 0,1 ) ( -0.3671 , -0.5658 , -0.2392 ) 2 1 9 ( 1 , 0 , 1 ) ( 4e-06 , 8e-07 , 1.111 e- 18 ) 1 1 as can be seen for the case of the first principal axis in Figure 1(b) . \n\t', '\n\t\t Note that in this space , the sense 2 instances are surrounded by sense 1 instances . \n\t', '\n\t\t We can traverse each of the projections onto the principal axis in linear order , simply by visiting each of the first principal components z1t along the principle axis in order of their values , i.e. , such that zi - z18- z14- z15- z16- z17- z12- z13- z19 It is significantly more difficult to visualize the nonlinear principal components case , however . \n\t', '\n\t\t Note that in general , there may not exist any principal axis in X , since an inverse mapping from F may not exist . \n\t', '\n\t\t If we attempt to follow the same procedure to traverse each of the projections onto the first principal axis as in the case of linear PCA , by considering each of the first principal components y1t in order of their value , i.e. , such that y14 - y15 - y16 - y17 - y19 - y11 - y18 - y13 - y12 then we must arbitrarily select a \x93quasi-projection\x94 direction for each y1t since there is no actual principal axis toward which to project . \n\t', '\n\t\t This results in a \x93quasi-axis\x94 roughly as shown in Figure 1(c) which , though not precisely accurate , provides some idea as to how the nonlinear generalization capability allows the data points to be grouped by principal components reflecting nonlinear patterns in the data distribution , in ways that linear PCA cannot do . \n\t', '\n\t\t Note that in this space , the sense 1 instances are already better separated from sense 2 data points . \n\t', '\n\t\t Moreover , unlike linear PCA , there may be up to M of the \x93quasi-axes\x94 , which may number far more than five . \n\t', '\n\t\t Such effects can become pronounced in the high dimensional spaces are actually used for real word sense disambiguation tasks . \n\t', '\n\t\t 3 A KPCA-based WSD model To extract nonlinear principal components efficiently , note that in both Equations ( 5 ) and ( 6 ) the explicit form of 4b ( xi ) is required only in the form of ( 4b ( xi ) \x95 4b ( xj ) ) , i.e. , the dot product of vectors in F . \n\t', '\n\t\t This means that we can calculate the nonlinear principal components by substituting a kernel function k(xi , xj ) for ( 4b( xi ) \x95 4b(xj ) ) in Equations ( 5 ) and ( 6 ) without knowing the mapping 4b explicitly ; instead , the mapping 4b is implicitly defined by the kernel function . \n\t', '\n\t\t It is always possible to construct a mapping into a space where k acts as a dot product so long as k is a continuous kernel of a positive integral operator ( Sch¨olkopf et al. , 1998 ) . \n\t', '\n\t\t class 2 ( correct sense class=1 ) : test example with predicted sense class 1 ( correct sense class=1 ) Figure 1 : Original vectors , PCA projections , and KPCA \x93quasi-projections\x94 ( see text ) . \n\t', '\n\t\t Table 5 : Experimental results showing that the KPCA-based model performs significantly better than naive Bayes and maximum entropy models . \n\t', '\n\t\t Significance intervals are computed via bootstrap resampling . \n\t', '\n\t\t WSD Model Accuracy Sig . \n\t', '\n\t\t Int. naive Bayes 63.3 % +/-0.91 % maximum entropy 63.8 % +/-0.79 % KPCA-based model 65.8 % +/-0.79 % Thus we train the KPCA model using the following algorithm : 1 . \n\t', '\n\t\t Compute an M x M matrix K\x88 such that \x88Kij = k(xi , xj ) ( 7 ) 2 . \n\t', '\n\t\t Compute the eigenvalues and eigenvectors of matrix K\x88 and normalize the eigenvectors . \n\t', '\n\t\t Let \x88A1 > \x88A2 > ... > \x88AM denote the eigenvalues and \x88~1 , ... , \x88~M denote the corresponding complete set of normalized eigenvectors . \n\t', '\n\t\t To obtain the sense predictions for test instances , we need only transform the corresponding vectors using the trained KPCA model and classify the resultant vectors using nearest neighbors . \n\t', '\n\t\t For a given test instance vector x , its lth nonlinear principal component is ylt = ~M \x88~lik(xi , xt ) ( 8 ) i=1 where \x88~li is the ith element of \x88~l . \n\t', '\n\t\t For our disambiguation experiments we employ a polynomial kernel function of the form k(xi , xj ) = ( xi \x95 xj )d , although other kernel functions such as gaussians could be used as well . \n\t', '\n\t\t Note that the degenerate case of d = 1 yields the dot product kernel k(xi,xj) = ( xi\x95xj ) which covers linear PCA as a special case , which may explain why KPCA always outperforms PCA . \n\t', '\n\t\t 4 Experiments 4.1 KPCA versus naive Bayes and maximum entropy models We established two baseline models to represent the state-of-the-art for individual WSD models : ( 1 ) naive Bayes , and ( 2 ) maximum entropy models . \n\t', '\n\t\t The naive Bayes model was found to be the most accurate classifier in a comparative study using a ( c ) the/DT the/DT media/N 2 4 , 5 , 6 , 7 9 1 , 8 3 design/N ( a ) the/DT media/N 2 first principal axis 4 , 5 , 6 , 7 9 1 , 8 3 design/N ( b ) first principal ` quasi-axis 2 media/N 4 , 5 , 6 , 7 9 1 , 8 3 : training example with sense class 1 ^ : training example with sense class 2 : test example with unknown sense class : test example with predicted sense design/N subset of Senseval-2 English lexical sample data by \n\t\t']",Positive
"['\n\t\t However , the maximum entropy \n\t\t']",Positive
"['\n\t\t To control for data variation , we built and tuned models of both kinds . \n\t', '\n\t\t Note that our objective in these experiments is to understand the performance and characteristics of KPCA relative to other individual methods . \n\t', '\n\t\t It is not our objective here to compare against voting or other ensemble methods which , though known to be useful in practice ( e.g. , \n\t\t']",Positive
"['\n\t\t To compare as evenly as possible , we employed features approximating those of the \x93feature- enhanced naive Bayes model\x94 of \n\t\t']",Positive
['\n\t\t The models in the comparative study by \n\t\t'],Positive
"['\n\t\t We also verified the maximum entropy results against several different implementations , using various smoothing criteria , to ensure that the comparison was even . \n\t', '\n\t\t Evaluation was done on the Senseval 2 English lexical sample task . \n\t', '\n\t\t It includes 73 target words , among which nouns , adjectives , adverbs and verbs . \n\t', '\n\t\t For each word , training and test instances tagged with WordNet senses are provided . \n\t', '\n\t\t There are an average of 7.8 senses per target word type . \n\t', '\n\t\t On average 109 training instances per target word are available . \n\t', '\n\t\t Note that we used the set of sense classes from Senseval\x92s \x94fine-grained\x94 rather than \x94coarse-grained\x94 classification task . \n\t', '\n\t\t The KPCA-based model achieves the highest accuracy , as shown in Table 5 , followed by the maximum entropy model , with na¨íve Bayes doing the poorest . \n\t', '\n\t\t Bear in mind that all of these models are significantly more accurate than any of the other reported models on Senseval . \n\t', '\n\t\t \x93Accuracy\x94 here refers to both precision and recall since disambiguation of all target words in the test set is attempted . \n\t', '\n\t\t Results are statistically significant at the 0.10 level , using bootstrap resampling \n\t\t']",Positive
"['\n\t\t WSD Model Accuracy Sig . \n\t', '\n\t\t Int. SVM-based model 65.2 % +/-1.00 % KPCA-based model 65.8 % +/-0.79 % many variations of the experiments . \n\t', '\n\t\t 4.2 KPCA versus SVM models Support vector machines ( e.g. , \n\t\t']",Positive
"['\n\t\t Given that SVM and KPCA are both kernel methods , we are frequently asked whether SVM-based WSD could achieve similar results . \n\t', '\n\t\t To explore this question , we trained and tuned an SVM model , providing the same rich set of features and also varying the feature representations to optimize for SVM biases . \n\t', '\n\t\t As shown in Table 6 , the highest-achieving SVM model is also able to obtain higher accuracies than the naive Bayes and maximum entropy models . \n\t', '\n\t\t However , in all our experiments the KPCA-based model consistently outperforms the SVM model ( though the margin falls within the statistical significance interval as computed by bootstrap resampling for this single experiment ) . \n\t', '\n\t\t The difference in KPCA and SVM performance is not surprising given that , aside from the use of kernels , the two models share little structural resemblance . \n\t', '\n\t\t 4.3 Running times Training and testing times for the various model implementations are given in Table 7 , as reported by the Unix time command . \n\t', '\n\t\t Implementations of all models are in C++ , but the level of optimization is not controlled . \n\t', '\n\t\t For example , no attempt was made to reduce the training time for naive Bayes , or to reduce the testing time for the KPCA-based model . \n\t', '\n\t\t Nevertheless , we can note that in the operating range of the Senseval lexical sample task , the running times of the KPCA-based model are roughly within the same order of magnitude as for naive Bayes or maximum entropy . \n\t', '\n\t\t On the other hand , training is much faster than the alternative kernel method based on SVMs . \n\t', '\n\t\t However , the KPCAbased model\x92s times could be expected to suffer in situations where significantly larger amounts of Table 7 : Comparison of training and testing times for the different WSD model implementations . \n\t', '\n\t\t WSD Model Training time [ CPU sec ] Testing time [ CPU sec ] naive Bayes 103.41 16.84 maximum entropy 104.62 59.02 SVM-based model 5024.34 16.21 KPCA-based model 216.50 128.51 training data are available . \n\t', '\n\t\t 5 Conclusion This work represents , to the best of our knowledge , the first application of Kernel PCA to a true natural language processing task . \n\t', '\n\t\t We have shown that a KPCA-based model can significantly outperform state-of-the-art results from both naive Bayes as well as maximum entropy models , for supervised word sense disambiguation . \n\t', '\n\t\t The fact that our KPCA-based model outperforms the SVMbased model indicates that kernel methods other than SVMs deserve more attention . \n\t', '\n\t\t Given the theoretical advantages of KPCA , it is our hope that this work will encourage broader recognition , and further exploration , of the potential of KPCA modeling within NLP research . \n\t', '\n\t\t Given the positive results , we plan next to combine large amounts of unsupervised data with reasonable smaller amounts of supervised data such as the Senseval lexical sample . \n\t', '\n\t\t Earlier we mentioned that one of the promising advantages of KPCA is that it computes the transform purely from unsupervised training vector data . \n\t', '\n\t\t We can thus make use of the vast amounts of cheap unannotated data to augment the model presented in this paper . \n\t', '\n\t\t References Clara Cabezas , Philip Resnik , and Jessica Stevens . \n\t', '\n\t\t Supervised sense tagging using support vector machines . \n\t', '\n\t\t In Proceedings of Senseval-2 , Second International Workshop on Evaluating Word Sense Disambiguation Systems , pages 59\x9662 , Toulouse , France , July 2001 . \n\t', '\n\t\t SIGLEX , Association for Computational Linguistics . \n\t', '\n\t\t Martin Chodorow , Claudia Leacock , and George A. Miller . \n\t', '\n\t\t A topical/local classifier for word sense identification . \n\t', '\n\t\t Computers and the Humanities , 34(1-2):115\x96120 , 1999 . \n\t', '\n\t\t Special issue on SENSEVAL . \n\t', '\n\t\t Hoa Trang Dang and Martha Palmer . \n\t', '\n\t\t Combining contextual features for word sense disambiguation . \n\t', '\n\t\t In Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense Disambiguation : Recent Successes and Future Directions , pages 88\x96 94 , Philadelphia , July 2002 . \n\t', '\n\t\t SIGLEX , Association for Computational Linguistics . \n\t', '\n\t\t Konstantinos I. Diamantaras and Sun Yuan Kung . \n\t', '\n\t\t Principal Component Neural Networks . \n\t', '\n\t\t Wiley , New York , 1996 . \n\t', '\n\t\t Bradley Efron and Robert J. Tibshirani . \n\t', '\n\t\t An Introduction to the Bootstrap . \n\t', '\n\t\t Chapman and Hall , 1993 . \n\t', '\n\t\t Hideki Isozaki and Hideto Kazawa . \n\t', '\n\t\t Efficient support vector classifiers for named entity recognition . \n\t', '\n\t\t In Proceedings of COLING-2002 , pages 390\x96396 , Taipei , 2002 . \n\t', '\n\t\t E.T. Jaynes . \n\t', '\n\t\t Where do we Stand on Maximum Entropy ? \n\t', '\n\t\t MIT Press , Cambridge MA , 1978 . \n\t', '\n\t\t Thorsten Joachims . \n\t', '\n\t\t Text categorization with support vector machines : Learning with many relevant features . \n\t', '\n\t\t In Proceedings of ECML-98 , 10th European Conference on Machine Learning , pages 137\x96142 , 1998 . \n\t', '\n\t\t Adam Kilgarriff and Joseph Rosenzweig . \n\t', '\n\t\t Framework and results for English Senseval . \n\t', '\n\t\t Computers and the Humanities , 34(1):15\x9648 , 1999 . \n\t', '\n\t\t Special issue on SENSEVAL . \n\t', '\n\t\t Adam Kilgarriff . \n\t', '\n\t\t English lexical sample task description . \n\t', '\n\t\t In Proceedings of Senseval-2 , Second International Workshop on Evaluating Word Sense Disambiguation Systems , pages 17\x9620 , Toulouse , France , July 2001 . \n\t', '\n\t\t SIGLEX , Association for Computational Linguistics . \n\t', '\n\t\t Dan Klein and Christopher D. Manning . \n\t', '\n\t\t Conditional structure versus conditional estimation in NLP models . \n\t', '\n\t\t In Proceedings of EMNLP2002 , Conference on Empirical Methods in Natural Language Processing , pages 9\x9616 , Philadelphia , July 2002 . \n\t', '\n\t\t SIGDAT , Association for Computational Linguistics . \n\t', '\n\t\t Taku Kudo and Yuji Matsumoto . \n\t', '\n\t\t Fast methods for kernel-based text analysis . \n\t', '\n\t\t In Proceedings of the 41set Annual Meeting of the Asoociation for Computational Linguistics , pages 24\x9631 , 2003 . \n\t', '\n\t\t James Mayfield , Paul McNamee , and Christine Piatko . \n\t', '\n\t\t Named entity recognition using hundreds of thousands of features . \n\t', '\n\t\t In Walter Daelemans and Miles Osborne , editors , Proceedings of CoNLL2003 , pages 184\x96187 , Edmonton , Canada , 2003 . \n\t', '\n\t\t Raymond J. Mooney . \n\t', '\n\t\t Comparative experiments on disambiguating word senses : An illustration of the role of bias in machine learning . \n\t', '\n\t\t In Proceedings of the Conference on Empirical Methods in Natural Language Processing , Philadelphia , May 1996 . \n\t', '\n\t\t SIGDAT , Association for Computational Linguistics . \n\t', '\n\t\t Ted Pedersen . \n\t', '\n\t\t Machine learning with lexical features : The Duluth approach to SENSEVAL-2 . \n\t', '\n\t\t In Proceedings of Senseval-2 , Second International Workshop on Evaluating Word Sense Disambiguation Systems , pages 139\x96142 , Toulouse , France , July 2001 . \n\t', '\n\t\t SIGLEX , Association for Computational Linguistics . \n\t', '\n\t\t Bernhard Sch¨olkopf , Alexander Smola , and KlausRober M¨uller . \n\t', '\n\t\t Nonlinear component analysis as a kernel eigenvalue problem . \n\t', '\n\t\t Neural Computation , 10(5) , 1998 . \n\t', '\n\t\t Hiroya Takamura and Yuji Matsumoto . \n\t', '\n\t\t Feature space restructuring for SVMs with application to text categorization . \n\t', '\n\t\t In Proceedings of EMNLP2001 , Conference on Empirical Methods in Natural Language Processing , pages 51\x9657 , 2001 . \n\t', '\n\t\t Vladimir N. Vapnik . \n\t', '\n\t\t The Nature of Statistical Learning Theory . \n\t', '\n\t\t Springer-Verlag , New York , 1995 . \n\t', '\n\t\t David Yarowsky and Radu Florian . \n\t', '\n\t\t Evaluating sense disambiguation across diverse parameter spaces . \n\t', '\n\t\t Natural Language Engineering , 8(4):293\x96310 , 2002 . \n\t', '\n\t\t David Yarowsky , Silviu Cucerzan , Radu Florian , Charles Schafer , and Richard Wicentowski . \n\t', '\n\t\t The Johns Hopkins SENSEVAL2 system descriptions . \n\t', '\n\t\t In Proceedings of Senseval-2 , Second International Workshop on Evaluating Word Sense Disambiguation Systems , pages 163\x96166 , Toulouse , France , July 2001 . \n\t', '\n\t\t SIGLEX , Association for Computational Linguistics . \n\t', '\n\t\t Using linguistic principles to recover empty categories Richard CAMPBELL Microsoft Research One Microsoft Way Redmond , WA 98052 USA richcamp@microsoft.com Abstract This paper describes an algorithm for detecting empty nodes in the Penn Treebank \n\t\t']",Positive
"['\n\t\t Unlike previous approaches to this task , the current method is not corpus-based , but rather makes use of the principles of early Government-Binding theory \n\t\t']",Positive
['\n\t\t Using the evaluation metric proposed by \n\t\t'],Positive
"['\n\t\t Some problems with this evaluation metric are noted and an alternative is proposed along with the results . \n\t', '\n\t\t The paper considers the reasons a principle- based approach to this problem should outperform corpus-based approaches , and speculates on the possibility of a hybrid approach . \n\t', '\n\t\t 1 Introduction Many recent approaches to parsing ( e.g. Charniak , 2000 ) have focused on labeled bracketing of the input string , ignoring aspects of structure that are not reflected in the string , such as phonetically null elements and long-distance dependencies , many of which provide important semantic information such as predicate-argument structure . \n\t', '\n\t\t In the Penn Treebank \n\t\t']",Positive
"['\n\t\t Empty categories are coindexed with their antecedents in the same sentence . \n\t', '\n\t\t In addition , if a node has a particular grammatical function ( such as subject ) or semantic role ( such as location ) , it has a function tag indicating that role ; empty categories may also have function tags . \n\t', '\n\t\t Thus in the sentence below , who is coindexed with the empty category *T* in the embedded S ; the function tag SBJ indicates that this empty category is the subject of that S : [ WHNP-1 who ] NP want [ S [ NP-SBJ-1*T* ] to VP ] Empty categories , with coindexation and function tags , allow a transparent reconstruction of predicate-argument structure not available from a simple bracketed string . \n\t', '\n\t\t In addition to bracketing the input string , a fully adequate syntactic analyzer should also locate empty categories in the parse tree , identify their antecedents , if any , and assign them appropriate function tags . \n\t', '\n\t\t State-of-the-art statistical parsers ( e.g. Charniak , 2000 ) typically provide a labeled bracketing only ; i.e. , a parse tree without empty categories . \n\t', '\n\t\t This paper describes an algorithm for inserting empty categories in such impoverished trees , coindexing them with their antecedents , and assigning them function tags . \n\t', '\n\t\t This is the first approach to include function tag assignment as part of the more general task of empty category recovery . \n\t', '\n\t\t Previous approaches to the problem ( Collins , 1997 ; Johnson , 2002 ; Dienes and Dubey , 2003a,b ; Higgins , 2003 ) have all been learning-based ; the primary difference between the present algorithm and earlier ones is that it is not learned , but explicitly incorporates principles of Government- Binding theory \n\t\t']",Positive
"['\n\t\t The absence of rule- based approaches up until now is not motivated by the failure of such approaches in this domain ; on the contrary , no one seems to have tried a rule- based approach to this problem . \n\t', '\n\t\t Instead it appears that there is an understandable predisposition against rule-based approaches , given the fact that data-driven , especially machine-learning , approaches have worked so much better in many other domains.1 Empty categories however seem different , in that , for the most part , their location and existence is determined , not by observable data , but by explicitly constructed linguistic principles , which 1Both Collins ( 1997 : 19 ) and Higgins ( 2003 : 100 ) are explicit about this predisposition . \n\t', '\n\t\t were consciously used in the annotation ; i.e. , unlike overt words and phrases , which correspond to actual strings in the data , empty categories are in the data only because linguists doing the annotation put them there . \n\t', '\n\t\t This paper therefore explores a rule-based approach to empty category recovery , with two purposes in mind : first , to explore the limits of such an approach ; and second , to establish a more realistic baseline for future ( possibly data-driven or hybrid ) approaches . \n\t', '\n\t\t Although it does not seem likely that any application trying to glean semantic information from a parse tree would care about the exact string position of an empty category , the algorithm described here does try to insert empty categories in the correct position in the string . \n\t', '\n\t\t The main reason for this is to facilitate comparison with previous approaches to the problem , which evaluate accuracy by including such information . \n\t', '\n\t\t In Section 5 , however , a revised evaluation metric is proposed that does not depend on string position per se . \n\t', '\n\t\t Before proceeding , a note on terminology is in order . \n\t', '\n\t\t I use the term detection ( of empty categories ) for the insertion of a labeled empty category into the tree ( and/or string ) , and the term resolution for the coindexation of the empty category with its antecedent(s) , if any . \n\t', '\n\t\t The term recovery refers to the complete package : detection , resolution , and assignment of function tags to empty categories . \n\t', '\n\t\t 2 Empty nodes in the Penn Treebank The major types of empty category in the Penn Treebank ( PTB ) are shown in Table 1 , along with their distribution in section 24 of the Wall Street Journal portion of the PTB . \n\t', '\n\t\t Empty category type Count Description NP * 1044 NP trace or PRO NP *T* 265 Trace of WHNP *U* 227 Empty unit 0 178 Empty complementizer ADVP *T* 97 Trace of WHADVP S *T* 76 Trace of topicalized quoted S WHNP 0 43 Null WHNP SBAR 41 Trace of topicalized non-quoted S WHADVP 0 25 Null WHADVP others 95 Total : 2091 Table 1 : Common empty categories and their distribution in section 24 of the PTB A detailed description of the categories and their uses in the treebank is provided in Chapter 4 of the annotation guidelines \n\t\t']",Positive
['\n\t\t Following \n\t\t'],Positive
"['\n\t\t This compound category is labeled SBAR in Table 1 . \n\t', '\n\t\t The PTB annotation in general , but especially the annotation of empty categories , follows a modified version of Government-Binding ( GB ) theory \n\t\t']",Positive
"['\n\t\t In GB , the existence and location of empty categories is determined by the interaction of linguistic principles . \n\t', '\n\t\t In addition , the type of a given empty category is determined by its syntactic context , with the result that the various types of empty category are in complementary distribution . \n\t', '\n\t\t For example , the GB categories NP- trace and PRO ( which are conflated to a single category in the PTB ) occur only in argument positions in which an overt NP could not occur , namely as the object of a passive verb or as the subject of certain kinds of infinitive . \n\t', '\n\t\t 3 Previous work Previous approaches to this task have all been learning-based . \n\t', '\n\t\t Collins\x92 ( 1997 ) Model 3 integrates the detection and resolution of WH-traces in relative clauses into a lexicalized PCFG . \n\t', '\n\t\t Collins\x92 results are not directly comparable to the works cited below , since he does not provide a separate evaluation of the empty category detection and resolution task . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t As in the present approach , Johnson inserts empty nodes as a post-process on an existing tree . \n\t', '\n\t\t He proposes an evaluation metric ( discussed further below ) , and presents results for both detection and detection plus resolution , given two different kinds of input : perfect trees ( with empty nodes removed ) and parser output . \n\t', '\n\t\t Dienes and Dubey ( 2003a,b ) , on the other hand , integrate their empty node resolution algorithm into their own PCFG parser . \n\t', '\n\t\t They first locate empty nodes in the string , taking a POS-tagged string as input , and outputting a POS-tagged string with labeled empty nodes inserted . \n\t', '\n\t\t The PCFG parser is then trained , using the enhanced strings as input , without inserting any additional empty nodes . \n\t', '\n\t\t Antecedent resolution is handled by a separate post-process . \n\t', '\n\t\t Using Johnson\x92s ( 2002 ) evaluation metric , Dienes and Dubey present results on the detection task alone ( i.e. , inserting empty categories into the POS-tagged string ) , as well as on the combined detection and resolution tasks in combination with their parser.2 \n\t\t']",Positive
"['\n\t\t Higgins\x92 method , like Johnson\x92s ( 2002 ) and the present one , involves post-processing of trees . \n\t', '\n\t\t Higgins\x92 results are not directly comparable to the other works cited , since he assumes all WH-phrases as given , even those that are themselves empty . \n\t', '\n\t\t 4 The recovery algorithm 4.1 The algorithm The proposed algorithm for recovering empty categories is shown in Figure 1 ; the algorithm walks the tree from top to bottom , at each node X deterministically inserting an empty category of a given type ( usually as a daughter of X ) if the syntactic context for that type is met by X . \n\t', '\n\t\t It makes four separate passes over the tree , on each pass applying a different set of rules . \n\t', '\n\t\t 1 for each tree , iterate over nodes from top down 2 for each node X 3 try to insert NP* in X 4 try to insert 0 in X 5 try to insert WHNP 0 or WHADVP 0 in X 6 try to insert *U* in X 7 try to insert a VP ellipsis site in X 8 try to insert S*T* or SBAR in X 9 try to insert trace of topicalized XP in X 10 try to insert trace of extraposition in X 11 for each node X 12 try to insert WH-trace in X 13 for each node X 14 try to insert NP-SBJ * in finite clause X 15 for each node X 16 if X = NP* , try to find antecedent for X Figure 1 : Empty category recovery algorithm The rules called by this algorithm that try to insert empty categories of a particular type specify the syntactic context in which that type of empty category can occur , and if the context exists , specify where to insert the empty category . \n\t', '\n\t\t For example , the category NP* , which conflates the GB categories NP-trace and PRO , occurs typically3 2 It is unclear whether Dienes and Dubey\x92s evaluation of empty category detection is based on actual tags provided by the annotation ( perfect input ) , or on the output of a POS-tagger . \n\t', '\n\t\t 3 NP* is used in roles that go beyond the GB notions of NP-trace and PRO , including e.g. the subject of as the object of a passive verb or as the subject of an infinitive . \n\t', '\n\t\t The rule which tries to insert this category and assign it a function tag is called in line 3 of Figure 1 and given in pseudo-code in Figure 2 . \n\t', '\n\t\t Some additional rules are given in the Appendix . \n\t', '\n\t\t if X is a passive VP & X has no complement S if there is a postmodifying dangling PP Y then insert NP* before all postmodifiers of Y else insert NP* before all postmodifiers of X else if X is a non-finite S and X has no subject then insert NP-SBJ* after all premodifiers of X Figure 2 : Rule to insert NP* This rule , which accounts for about half the empty category tokens in the PTB , makes no use of lexical information such as valency of the verb , etc. . \n\t', '\n\t\t This is potentially a problem , since in GB the infinitives that can have NP-trace or PRO as subjects ( raising and control infinitives ) are distinguished from those that can have overt NPs or WH-trace as subjects ( exceptional-Casemarked , or ECM , infinitives ) , and the distinction relies on the class of the governing verb . \n\t', '\n\t\t Nevertheless , the rules that insert empty nodes do not have access to a lexicon , and very little lexical information is encoded in the rules : reference is made in the rules to individual function words such as complementizers , auxiliaries , and the infinitival marker to , but never to lexical properties of content words such as valency or the raising/ECM distinction . \n\t', '\n\t\t In fact , the only reference to content words at all is in the rule which tries to insert null WH-phrases , called in line 5 of Figure 1 : when this rule has found a relative clause in which it needs to insert a null WH-phrase , it checks if the head of the NP the relative clause modifies is reason(s) , way(s) , time(s) , day(s) , or place(s) ; if it is , then it inserts WHADVP with the appropriate function tag , rather than WHNP . \n\t', '\n\t\t The rule shown in Figure 2 depends for its successful application on the system\x92s being able to identify passives , non-finite sentences , heads of phrases ( to identify pre- and post-modifiers ) , and functional information such as subject ; similar information is accessed by the other rules used in the algorithm . \n\t', '\n\t\t Simple functions to identify passives , etc. are therefore called by the implemented versions of these rules . \n\t', '\n\t\t Functional information , such as subject , can be gleaned from the function tags in the treebank annotation ; the rules make frequent use of a variety of function tags as they occur on various nodes . \n\t', '\n\t\t The output of imperatives ; see below . \n\t', '\n\t\t Charniak\x92s parser \n\t\t']",Positive
"['\n\t\t Presumably , the accuracy of the algorithm on parser output would be enhanced by accurate prior assignment of the tags to all relevant nodes , as in \n\t\t']",Positive
"['\n\t\t Each empty category insertion rule , in addition to inserting an empty node in the tree , also may assign a function tag to the empty node . \n\t', '\n\t\t This is illustrated in Figure 2 , where the final line inserts NP* with the function tag SBJ in the case where it is the subject of an infinitive clause . \n\t', '\n\t\t The rule that inserts WH-trace ( called in line 12 in Figure 1 ) takes a WHXP needing a trace as input , and walks the tree until an appropriate insertion site is found ( see Appendix for a fuller description ) . \n\t', '\n\t\t Since this rule requires a WHXP as input , and that WHXP may itself be an empty category ( inserted by an earlier rule ) , it is handled in a separate pass through the tree . \n\t', '\n\t\t A separate rule inserts NP* as the subject in sentences which have no overt subject , and which have not had a subject inserted by any of the other rules . \n\t', '\n\t\t Most commonly , these are imperative sentences , but calling this rule in a separate pass through the tree , as in Figure 1 , ensures that any subject position missed by the other rules is filled . \n\t', '\n\t\t Finally , a separate rule tries to find an antecedent for NP* under certain conditions . \n\t', '\n\t\t The antecedent of NP* may be an empty node inserted by rules in any of the first three passes through the tree , even the subject of an imperative ; therefore this rule is applied in a separate pass through the tree . \n\t', '\n\t\t This rule is also fairly simple , assigning the local subject as antecedent for a non-subject NP* , while for an NP* in the subject position of a nonfinite S it searches up the tree , given certain locality conditions , for another NP subject . \n\t', '\n\t\t All the rules that insert empty categories are fairly simple , and derive straighforwardly from standard GB theory and from the annotation guidelines . \n\t', '\n\t\t The most complex rule is the rule that inserts WH-trace when it finds a WHXP daughter of SBAR ; most are about as simple as the rule shown in Figure 2 , some more so . \n\t', '\n\t\t Representative examples are given in the Appendix . \n\t', '\n\t\t 4.2 Development method After implementing the algorithm , it was run over sections 1 , 3 , and 11 of the WSJ portion of the PTB , followed by manual inspection of the trees to perform error analysis , with revisions made as necessary to correct errors . \n\t', '\n\t\t Initially sections 22 and 24 were used for development testing . \n\t', '\n\t\t However , it was found that these two sections differ from each other substantially with respect to the annotation of antecedents of NP* ( which is described somewhat vaguely in the annotation guidelines ) , so all of sections 2-21 were used as a development test corpus . \n\t', '\n\t\t Section 23 was used only for the final evaluation , reported in Section 5 below . \n\t', '\n\t\t 5 Evaluation Following \n\t\t']",Positive
"['\n\t\t Each is discussed in turn below . \n\t', '\n\t\t 5.1 Perfect input The system was run on PTB trees stripped of all empty categories . \n\t', '\n\t\t To facilitate comparison to previous approaches , we used Johnson\x92s label and string position evaluation metric , according to which an empty node is identified by its label plus its string position , and evaluated the detection task alone . \n\t', '\n\t\t We then evaluated detection and resolution combined , identifying each empty category as before , plus the label and string position of its antecedent , if any , again following Johnson\x92s work . \n\t', '\n\t\t The results are shown in Table 2 . \n\t', '\n\t\t Precision here and throughout is the percentage of empty nodes proposed by the system that are in the gold standard ( section 23 of the PTB ) , recall is the percentage of empty nodes in the gold standard that are proposed by the system , and F1 is balanced f-measure ; i.e. , 2PR/(P+R) . \n\t', '\n\t\t Task Prec . \n\t', '\n\t\t Rec . \n\t', '\n\t\t F1 Detection only 94.9 91.1 93.0 Detection + resolution 90.1 86.6 88.4 Table 2 : Detection and resolution of empty categories given perfect input ( label + string position method ) , expressed as percentage These results compare favorably to previously reported results , exceeding them mainly by achieving higher recall . \n\t', '\n\t\t \n\t\t']",Positive
['\n\t\t In contrast to \n\t\t'],Positive
"['\n\t\t For Dienes and Dubey , the further task of finding antecedents for empty categories is integrated with their own PCFG parser , so they report no numbers directly relevant to the task of detection and resolution given perfect input . \n\t', '\n\t\t 5.2 Parser output The system was also run using as input the output of Charniak\x92s parser \n\t\t']",Positive
"['\n\t\t The results , again using the label and string position method , are given in Table 3 . \n\t', '\n\t\t Task Prec . \n\t', '\n\t\t Rec . \n\t', '\n\t\t F1 Detection only 85.2 81.7 83.4 Detection + resolution 78.3 75.1 76.7 Table 3 : Detection and resolution of empty categories on parser output ( label + string position method ) , expressed as percentage Again the results exceed those previously reported . \n\t', '\n\t\t \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t 5.3 Perfect input with no function tags The lower results on parser output obviously reflect errors introduced by the parser , but may also be due to the parser not outputting function tags on any nodes . \n\t', '\n\t\t As mentioned in Section 4 , it is believed that the results of the current method on parser output would improve if that output were reliably assigned function tags , perhaps along the lines of \n\t\t']",Positive
"['\n\t\t Testing this hypothesis directly is beyond the scope of the present work , but a simple experiment can give some idea of the extent to which the current algorithm relies on function tags in the input . \n\t', '\n\t\t The system was run on PTB trees with all nodes stripped of function tags ; the results are given in Table 4 . \n\t', '\n\t\t Task Prec . \n\t', '\n\t\t Rec . \n\t', '\n\t\t F1 Detection only 94.1 89.5 91.7 Detection + resolution 89.5 85.2 87.3 Table 4 : Detection and resolution of empty categories on PTB input without function tags ( label + string position method ) , expressed as percentage While not as good as the results on perfect input with function tags , these results are much better than the results on parser output . \n\t', '\n\t\t This suggests that function tag assignment should improve the results shown on parser output , but that the greater part of the difference between the results on perfect input and on parser output is due to errors introduced by the parser . \n\t', '\n\t\t 5.4 Refining the evaluation The results reported in the previous subsections are quite good , and demonstrate that the current approach outperforms previously reported approaches on the detection and resolution of empty categories . \n\t', '\n\t\t In this subsection some refinements to the evaluation method are considered . \n\t', '\n\t\t The label and string position method is useful if one sees the task as inserting empty nodes into a string , and thus is quite useful for evaluating systems that detect empty categories without parse trees , as in \n\t\t']",Positive
"['\n\t\t However , if the task is to insert empty nodes into a tree , then the method leads both to false positives and to false negatives . \n\t', '\n\t\t Suppose for example that the sentence When do you expect to finish ? \n\t', '\n\t\t has the bracketing shown below , where \x911\x92 and \x912\x92 indicate two possible locations in the tree for the trace of the WHADVP : When do you [ VP expect to [ VP finish 1 ] 2 ] Suppose position 1 is correct ; i.e. it represents the position of the trace in the gold standard . \n\t', '\n\t\t Since 1 and 2 correspond to the same string position , if a system inserts the trace in position 2 , the string position evaluation method will count it as correct . \n\t', '\n\t\t This is a serious problem with the string-based method of evaluation , if one assumes , as seems reasonable , that the purpose of inserting empty categories into trees is to be able to recover semantic information such as predicate-argument structure and modification relations . \n\t', '\n\t\t In the above example , it is clearly semantically relevant whether the system proposes that when modifies expect instead of finish . \n\t', '\n\t\t Conversely , suppose the sentence Who ( besides me ) cares ? \n\t', '\n\t\t has the bracketing shown : Who [ S 1 ( besides me ) 2 [ VP cares ] ] Again suppose that position 1 represents the placement of the WHNP trace in the gold standard . \n\t', '\n\t\t If a system places the trace in position 2 instead , the string position method will count it as an error , since 1 and 2 have different string positions . \n\t', '\n\t\t However it is not at all clear what it means to say that one of those two positions is correct and the other not , since there is no semantic , grammatical , or textual indicator of its exact position . \n\t', '\n\t\t If the task is to be able to recover semantic information using traces , then it does not matter in this case whether the system inserts the trace to the left or to the right of the parenthetical . \n\t', '\n\t\t Given that both false positives and false negatives are possible , I propose that future evaluations of this task should identify empty categories by their label and by their parent category , instead of , or perhaps in addition to , doing so by label and string position . \n\t', '\n\t\t Since the parent of an empty node is always an overt node4 , the parent could be identified by its label and string position ( left and right edges ) . \n\t', '\n\t\t Resolution is evaluated by a natural extension , by identifying the antecedent ( which could itself be an empty category ) according to its label and its parent\x92s label and string position . \n\t', '\n\t\t This would serve to identify an empty category by its position in the tree , rather than in the string , and would avoid the false positives and false negatives described above . \n\t', '\n\t\t In addition to an evaluation based on tree position rather than string position , I propose to evaluate the entire recovery task , i.e. , including function tag assignment , not just detection and resolution . \n\t', '\n\t\t The revised evaluation is still not perfect : when inserting an NP* or NP*T* into a double-object construction , it clearly matters semantically whether it is the first or second object , though both positions have the same parent .5 Ideally , we would evaluate based on a richer set of grammatical relations than are annotated in the PTB , or perhaps based on thematic roles . \n\t', '\n\t\t However , it is difficult to see how to accomplish this without additional annotation . \n\t', '\n\t\t It is probable that constructions of this sort are relatively rare in the PTB in any case , so for now the proposed evaluation method , however imperfect , will suffice . \n\t', '\n\t\t The result of this revised evaluation , given perfect input , is presented in Table 5 . \n\t', '\n\t\t The first two rows are comparable to the string-based results in Table 2 ; the last row , showing the results of the full recovery task ( i.e. , including antecedents and function tags ) , is not much lower , suggesting that labeling empty categories with function tags does not pose any serious difficulties . \n\t', '\n\t\t 4 The only exception is the 0 complementizer and S*T* daughters of the SBAR category in Table 1 ; but since the entire SBAR is treated as a single empty node for evaluation purposes , this does not pose a problem . \n\t', '\n\t\t 5 I am indebted to two ACL reviewers for calling this to my attention . \n\t', '\n\t\t Task Prec . \n\t', '\n\t\t Rec . \n\t', '\n\t\t F1 Detection only 95.6 91.9 93.7 Detection + resolution 90.8 87.3 89.0 Recovery ( det.+res.+func. tags ) 89.8 86.3 88.0 Table 5 : Detection , resolution and recovery of empty categories given perfect input ( label + parent method ) , expressed as percentage Three similar evaluations were also run , using parser output as input to the algorithm ; the results are given in Table 6 . \n\t', '\n\t\t Task Prec . \n\t', '\n\t\t Rec . \n\t', '\n\t\t F1 Detection only 78.4 75.2 76.7 Detection + resolution 72.3 69.3 70.8 Recovery ( det.+res.+func. tags ) 69.7 66.8 68.2 Table 6 : Detection , resolution and recovery of empty categories on parser output ( label + parent method ) , expressed as percentage The results here are less impressive , no doubt reflecting errors introduced by the parser in the labeling and bracketing of the parent category , a problem which does not affect a string-based evaluation . \n\t', '\n\t\t However it does not seem reasonable to have an effective evaluation of empty node insertion in parser output that does not depend to some extent on the correctness of the parse . \n\t', '\n\t\t The fact that our proposed evaluation metric depends more heavily on the accuracy of the input structure may be an unavoidable consequence of using a tree-based evaluation . \n\t', '\n\t\t 6 Discussion The empty category recovery algorithm reported on here outperforms previously published approaches on the detection and resolution tasks ; it also does well on the task of function tag assignment to empty categories , which has not been considered in other work . \n\t', '\n\t\t As suggested in the introduction , the reason a rule-based approach works so well in this domain may be that empty categories are not naturally in the text , but are only inserted by the annotator , who is consciously following explicit linguistic principles , in this case , the principles of early GB theory . \n\t', '\n\t\t As a result , the recovery of empty categories is , for the most part , more amenable to a rule-based approach than to a learning approach . \n\t', '\n\t\t It makes little sense to learn , for example , that NP* occurs as the object of a passive verb or as the subject of certain infinitives in the PTB , if that information is already explicit in the annotation guidelines . \n\t', '\n\t\t This is not to say that learning approaches have nothing to contribute to this task . \n\t', '\n\t\t Information about individual lexical items , such as valency , the raising/ECM distinction , or subject vs. object control , which is presumably most robustly acquired from large amounts of data , would probably help in the task of detecting certain empty categories . \n\t', '\n\t\t Consider for example an input structure V [ S to VP ] . \n\t', '\n\t\t GB principles , which are enforced in the annotation guidelines , dictate that an empty category must be inserted as the subject of the infinitival S ; but exactly which empty category , NP* or NP*T* , depends on properties of the governing verb , including whether it is a raising or control verb , such as seem or try , or an ECM verb , such as believe . \n\t', '\n\t\t In the present algorithm , the rule that inserts NP* applies first , without access to lexical information of any kind , so NP* is inserted , instead of NP*T* , regardless of the value of V . \n\t', '\n\t\t This leads to some errors which might be corrected given learned lexical information . \n\t', '\n\t\t Such errors are fewer than might have been expected , however : the present system achieved 97.7 % precision and 97.3 % recall ( F1 = 97.5 % ) on the isolated task of detecting NP* , even without lexical knowledge ( see Table 7 ) . \n\t', '\n\t\t A combined learning and rule-based algorithm might stand to make a bigger gain in the task of deciding whether NP* in subject position has an antecedent or not , and if it does , whether the antecedent is a subject or not . \n\t', '\n\t\t The annotation guidelines and the theory that underlies it are less explicit on the principles underlying this task than they are on the other subtasks . \n\t', '\n\t\t As a result , the accuracy of the current system drops considerably when this task is taken into account , from 97.5 % F1 to 86.9 % ( see Table 7 ) . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t Empty category type Detection only ( F1 ) Detection + resolution ( F1 ) NP* 97.5 86.9 NP*T* 96.2 96.0 *U* 98.6 - 0 98.5 - ADVP*T* 79.9 79.9 S*T* 92.7 92.7 WHNP 0 92.4 - SBAR 84.4 84.4 WHADVP 0 73.3 - Table 7 : F1 for detection and resolution of empty categories by type , using perfect input ( label + parent method ) , expressed as percentage 7 Conclusion In this paper I have presented an algorithm for the recovery of empty categories in PTB-style trees that otherwise lack them . \n\t', '\n\t\t Unlike previous approaches , the current algorithm is rule-based rather than learning-based , which I have argued is appropriate for this task , given the highly theoretical nature of empty categories in the PTB . \n\t', '\n\t\t Moreover , the algorithm has no access to lexical information such as valency or verb class . \n\t', '\n\t\t Using the string-based evaluation metric proposed by \n\t\t']",Positive
"['\n\t\t In addition , we have performed additional evaluation using a tree-based metric , and including an evaluation of function tag assignment as well . \n\t', '\n\t\t 8 Acknowledgements I would like to thank Simon Corston-Oliver , Mark Johnson , and Hisami Suzuki for their helpful input . \n\t', '\n\t\t References Bies , A. , M. Ferguson , K. Katz and R. MacIntyre . \n\t', '\n\t\t 1995. Bracketing Guidelines for Treebank II style Penn Treebank Project . \n\t', '\n\t\t Linguistic Data Consortium . \n\t', '\n\t\t Blaheta , D. and E. Charniak . \n\t', '\n\t\t 2000 . \n\t', '\n\t\t Assigning Function Tags to Parsed Text . \n\t', '\n\t\t In Proceedings of the North American Chapter of the Association for Computational Linguistics , pages 234-240 . \n\t', '\n\t\t Charniak , E. 2000 . \n\t', '\n\t\t A maximum-entropy-inspired parser . \n\t', '\n\t\t In In Proceedings of the North American Chapter of the Association for Computational Linguistics , pages 132-139 . \n\t', '\n\t\t Chomsky , N. 1981 . \n\t', '\n\t\t Lectures on Government and Binding . \n\t', '\n\t\t Foris Publications , Dordrecht . \n\t', '\n\t\t Collins , M. 1997 . \n\t', '\n\t\t Three Generative , Lexicalised Models for Statistical Parsing . \n\t', '\n\t\t In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics , pages 16-23 . \n\t', '\n\t\t Dienes , P. and A. Dubey . \n\t', '\n\t\t 2003a . \n\t', '\n\t\t Deep Syntactic Processing by Combining Shallow Methods . \n\t', '\n\t\t In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics , pages 431-438 . \n\t', '\n\t\t Dienes , P. and A. Dubey . \n\t', '\n\t\t 2003b . \n\t', '\n\t\t Antecedent Recovery : Experiments with a Trace Tagger . \n\t', '\n\t\t In Proceedings of the Conference on Empirical Methods in Natural Language Processing , pages 33-40 . \n\t', '\n\t\t Higgins , D. 2003 . \n\t', '\n\t\t A machine-learning approach to the identification of WH gaps . \n\t', '\n\t\t In Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics , pages 99-102 . \n\t', '\n\t\t Johnson , M. 2002 . \n\t', '\n\t\t A simple pattern-matching algorithm for recovering empty nodes and their antecedents . \n\t', '\n\t\t In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , pages 136-143 . \n\t', '\n\t\t Marcus , M. , B. Santorini and M.A.Marcinkiewicz . \n\t', '\n\t\t 1993. Building a large annotated corpus of English : The Penn Treebank . \n\t', '\n\t\t Computational Linguistics , 19(2):313-330 . \n\t', '\n\t\t Appendix : Sample rules To insert 0 Comp : if X=SBAR & !Comp(X) & !WHXP daughter(X) & 3 S daughter Y of X & !(parent(X)=NP & sister(X)=NP) then insert 0 to left of Y To insert WHNP/WHADVP : if X=SBAR & parent(X)=NP & sister(X)=NP & !Comp(X) & ! \n\t', '\n\t\t WHXP daughter(X) & 3 S daughter Y of X if head(parent(X)) in { reason(s) way(s) time(s) day(s) place(s) } then insert WHADVP to left of Y else insert WHNP to left of Y To insert *U* : insert *U* / $ CD+ _ To insert WH-trace : if X=SBAR & 3 S daughter Y of X & 3 WHXP daughter W of X then find trace(W) in Y To find trace(W) in X : insert trace : ( for W = WHXP , insert XP*T* ) if X has conjuncts then find trace(W) in each conjunct of X else if X has a PP daughter Y with no object & W=WHNP then insert *T* to right of P else if X=S and !subject(X) & W=WHNP then insert *T* as last pre-mod of X else if X contains a VP Y then find trace(W) in Y else if X contains ADJP or clausal complement Y & W=WHNP then find trace(W) in Y else if W=WHNP & 3 infinival rel . \n\t', '\n\t\t clause R , R=sister(W) & X=VP & X has an object NP & subject(R) is an empty node E then insert *T* as last pre-mod of R then delete E else if W=WHNP then insert *T* as first post-mod of X else insert *T* as last post-mod of X assign function tag : if W = WHNP & *T* a pre-mod of S then assign \x91SBJ\x92 to *T* if W = WHADVP & W is not empty if W = \x91why\x92 then assign \x91PRP\x92 to *T* if W = \x91when\x92 then assign \x91TMP\x92 to *T* if W = \x91where\x92 then assign \x91LOC\x92 to *T* if W = \x91how\x92 then assign \x91MNR\x92 to *T* else if W = WHADVP & parent(parent(W)) =NP if head(sister(parent(W))) = \x91reason(s)\x92 then assign \x91PRP\x92 to *T* if head(sister(parent(W)))=\x91time(s)\x92 or \x91day(s)\x92 then assign \x91TMP\x92 to *T* if head(sister(parent(W))) = \x91place(s)\x92 then assign \x91LOC\x92 to *T* if head(sister(parent(W))) = \x91way(s)\x92 then assign \x91MNR\x92 to *T* \n\t', '\n\t\t Statistical Machine Translation by Parsing I. Dan Melamed Computer Science Department New York University New York , NY , U.S.A. 10003-6806 lastname @cs.nyu.edu Abstract In an ordinary syntactic parser , the input is a string , and the grammar ranges over strings . \n\t', '\n\t\t This paper explores generalizations of ordinary parsing algorithms that allow the input to consist of string tuples and/or the grammar to range over string tuples . \n\t', '\n\t\t Such algorithms can infer the synchronous structures hidden in parallel texts . \n\t', '\n\t\t It turns out that these generalized parsers can do most of the work required to train and apply a syntax-aware statistical machine translation system . \n\t', '\n\t\t 1 Introduction A parser is an algorithm for inferring the structure of its input , guided by a grammar that dictates what structures are possible or probable . \n\t', '\n\t\t In an ordinary parser , the input is a string , and the grammar ranges over strings . \n\t', '\n\t\t This paper explores generalizations of ordinary parsing algorithms that allow the input to consist of string tuples and/or the grammar to range over string tuples . \n\t', '\n\t\t Such inference algorithms can perform various kinds of analysis on parallel texts , also known as multitexts . \n\t', '\n\t\t Figure 1 shows some of the ways in which ordinary parsing can be generalized . \n\t', '\n\t\t A synchronous parser is an algorithm that can infer the syntactic structure of each component text in a multitext and simultaneously infer the correspondence relation between these structures . \n\t', ""\n\t\t ' When a parser\x92s input can have fewer dimensions than the parser\x92s grammar , we call it a translator . \n\t"", '\n\t\t When a parser\x92s grammar can have fewer dimensions than the parser\x92s input , we call it a synchronizer . \n\t', '\n\t\t The corresponding processes are called translation and synchronization . \n\t', '\n\t\t To our knowledge , synchronization has never been explored as a class of algorithms . \n\t', '\n\t\t Neither has the relationship between parsing and word alignment . \n\t', ""\n\t\t The relationship between translation and ordinary parsing was noted a long time ' A suitable set of ordinary parsers can also infer the syntac- tic structure of each component , but cannot infer the correspondence relation between these structures . \n\t"", '\n\t\t Figure 1 : Generalizations of ordinary parsing . \n\t', '\n\t\t ago ( Aho & Ullman , 1969 ) , but here we articulate it in more detail : ordinary parsing is a special case of synchronous parsing , which is a special case of translation . \n\t', '\n\t\t This paper offers an informal guided tour of the generalized parsing algorithms in Figure 1 . \n\t', '\n\t\t It culminates with a recipe for using these algorithms to train and apply a syntax-aware statistical machine translation ( SMT ) system . \n\t', '\n\t\t 2 Multitext Grammars and Multitrees The algorithms in this paper can be adapted for any synchronous grammar formalism . \n\t', '\n\t\t The vehicle for the present guided tour shall be multitext grammar ( MTG ) , which is a generalization of context-free grammar to the synchronous case \n\t\t']",Positive
['\n\t\t We shall limit our attention to MTGs in Generalized Chomsky Normal Form ( GCNF ) \n\t\t'],Positive
['\n\t\t This normal form allows simpler algorithm descriptions than the normal forms used by \n\t\t'],Negative
"['\n\t\t In GCNF , every production is either a terminal production or a nonterminal production . \n\t', '\n\t\t A nonterminal production might look like this : 1 2 3 ... \n\t', '\n\t\t I = dimensionality of input ordinary parsing 3 2 1 generalized parsing ( any D ; any I ) synchronization ( I >= D ) synchronous translation ( D >= I ) word alignment ordinary parsing ro=P=1 ) parsing ( D=I ) D(2) A B E ( 1 ) There are nonterminals on the left-hand side ( LHS ) and in parentheses on the right-hand side ( RHS ) . \n\t', '\n\t\t Each row of the production describes rewriting in a different component text of a multitext . \n\t', '\n\t\t In each row , a role template describes the relative order and contiguity of the RHS nonterminals . \n\t', '\n\t\t E.g. , in the top row , [ 1,2 ] indicates that the first nonterminal ( A ) precedes the second ( B ) . \n\t', '\n\t\t In the bottom row , [ 1,2 , 1 ] indicates that the first nonterminal both precedes and follows the second , i.e. D is discontinuous . \n\t', '\n\t\t Discontinuous nonterminals are annotated with the number of their contiguous segments , as in . \n\t', '\n\t\t The ( \x93join\x94 ) operator rearranges the non- terminals in each component according to their role template . \n\t', '\n\t\t The nonterminals on the RHS are written in columns called links . \n\t', '\n\t\t Links express translational equivalence . \n\t', '\n\t\t Some nonterminals might have no translation in some components , indicated by ( ) , as in the 2nd row . \n\t', '\n\t\t Terminal productions have exactly one \x93active\x94 component , in which there is exactly one terminal on the RHS . \n\t', '\n\t\t The other components are inactive . \n\t', '\n\t\t E.g. , ( 2 ) The semantics of are the usual semantics of rewriting systems , i.e. , that the expression on the LHS can be rewritten as the expression on the RHS . \n\t', '\n\t\t However , all the nonterminals in the same link must be rewritten simultaneously . \n\t', '\n\t\t In this manner , MTGs generate tuples of parse trees that are isomorphic up to reordering of sibling nodes and deletion . \n\t', '\n\t\t Figure 2 shows two representations of a tree that might be generated by an MTG in GCNF for the imperative sentence pair Wash the dishes / Pasudu moy . \n\t', '\n\t\t The tree exhibits both deletion and inversion in translation . \n\t', '\n\t\t We shall refer to such multidimensional trees as multitrees . \n\t', '\n\t\t The different classes of generalized parsing algorithms in this paper differ only in their grammars and in their logics . \n\t', '\n\t\t They are all compatible with the same parsing semirings and search strategies . \n\t', '\n\t\t Therefore , we shall describe these algorithms in terms of their underlying logics and grammars , abstracting away the semirings and search strategies , in order to elucidate how the different classes of algorithms are related to each other . \n\t', '\n\t\t Logical descriptions of inference algorithms involve inference rules : means that can be inferred from and . \n\t', '\n\t\t An item that appears in an inference rule stands for the proposition that the item is in the parse chart . \n\t', '\n\t\t A production rule that appears in an inference rule stands for the proposition that the production is in the grammar . \n\t', '\n\t\t Such specifications are nondeter- Figure 2 : Above : A tree generated by a 2-MTG in English and ( transliterated ) Russian . \n\t', '\n\t\t Every internal node is annotated with the linear order of its children , in every component where there are two children . \n\t', '\n\t\t Below : A graphical representation of the same tree . \n\t', '\n\t\t Rectangles are 2D constituents . \n\t', '\n\t\t ministic : they do not indicate the order in which a parser should attempt inferences . \n\t', '\n\t\t A deterministic parsing strategy can always be chosen later , to suit the application . \n\t', '\n\t\t We presume that readers are familiar with declarative descriptions of inference algorithms , as well as with semiring parsing \n\t\t']",Positive
"['\n\t\t 3 A Synchronous CKY Parser Figure 3 shows Logic C. Parser C is any parser based on Logic C. As in Melamed (2003)\x92s Parser A , Parser C\x92s items consist of a -dimensional label vector and a -dimensional d-span vector.2 The items contain d-spans , rather than ordinary spans , because 2Superscripts and subscripts indicate the range of dimensions of a vector . \n\t', '\n\t\t E.g. , is a vector spanning dimensions 1 through . \n\t', '\n\t\t See \n\t\t']",Positive
"['\n\t\t Wash ^the moy dishes Pasudu S NP N V WASH D DISH Wash the dishes PAS Pasudu MIT V moy S NP N Parser C needs to know all the boundaries of each item , not just the outermost boundaries . \n\t', '\n\t\t Some ( but not all ) dimensions of an item can be inactive , denoted , and have an empty d-span ( ) . \n\t', '\n\t\t The input to Parser C is a tuple of parallel texts , with lengths . \n\t', '\n\t\t The notation indicates that the Goal item must span the input from the left of the first word to the right of the last word in each component . \n\t', '\n\t\t Thus , the Goal item must be contiguous in all dimensions . \n\t', '\n\t\t Parser C begins with an empty chart . \n\t', '\n\t\t The only inferences that can fire in this state are those with no antecedent items ( though they can have antecedent production rules ) . \n\t', '\n\t\t In Logic C , is the value that the grammar assigns to the terminal production . \n\t', '\n\t\t The range of this value depends on the semiring used . \n\t', '\n\t\t A Scan inference can fire for the th word in component for every terminal pro- duction in the grammar where appears in the th component . \n\t', '\n\t\t Each Scan consequent has exactly one active d-span , and that d-span always has the form because such items always span one word , so the distance between the item\x92s boundaries is always one . \n\t', '\n\t\t The Compose inference in Logic C is the same as in Melamed\x92s Parser A , using slightly different notation : In Logic C , the function represents the value that the grammar assigns to the nonterminal production . \n\t', '\n\t\t Parser C can compose two items if their labels appear on the RHS of a production rule in the grammar , and if the contiguity and relative order of their intervals is consistent with the role templates of that production rule . \n\t', '\n\t\t Figure 3 : Logic C ( \x93C\x94 for CKY ) These constraints are enforced by the d-span operators and . \n\t', '\n\t\t Parser C is conceptually simpler than the synchronous parsers of \n\t\t']",Positive
"['\n\t\t The inference rules of Logic C are the multidimensional generalizations of inference rules with the same names in ordinary CKY parsers . \n\t', '\n\t\t For example , given a suitable grammar and the input ( imperative ) sentence pair Wash the dishes / Pasudu moy , Parser C might make the 9 inferences in Figure 4 to infer the multitree in Figure 2 . \n\t', '\n\t\t Note that there is one inference per internal node of the multitree . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t Depending on the chosen semiring , a parsing logic can compute the single most probable derivation and/or its probability , the most probable derivations and/or their total probability , all possible derivations and/or their total probability , the number of possible derivations , etc. . \n\t', '\n\t\t All the parsing semirings catalogued by Goodman apply the same way to synchronous parsing , and to all the other classes of algorithms discussed in this paper . \n\t', '\n\t\t The class of synchronous parsers includes some algorithms for word alignment . \n\t', '\n\t\t A translation lexicon ( weighted or not ) can be viewed as a degenerate MTG ( not in GCNF ) where every production has a link of terminals on the RHS . \n\t', '\n\t\t Under such an MTG , the logic of word alignment is the one in Melamed (2003)\x92s Parser A , but without Compose inferences . \n\t', '\n\t\t The only other difference is that , instead of a single item , the Goal of word alignment is any set of items that covers all dimensions of the input . \n\t', '\n\t\t This logic can be used with the expectation semiring \n\t\t']",Positive
"['\n\t\t An important application of Parser C is parameter estimation for probabilistic MTGs ( PMTGs ) . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t If so , then there is a straightforward generalization for PMTGs . \n\t', '\n\t\t Parameter estimation is beyond the scope of this paper , however . \n\t', '\n\t\t The next section assumes that we have an MTG , probabilistic or not , as required by the semiring . \n\t', '\n\t\t 4 Translation A -MTG can guide a synchronous parser to infer the hidden structure of a -component multi- text . \n\t', '\n\t\t Now suppose that we have a -MTG and an input multitext with only components , Inference Rules Scan component d , : Compose : Item Form : Goal : . \n\t', '\n\t\t Figure 4 : Possible sequence of inferences of Parser C on input Wash the dishes / Pasudu moy . \n\t', '\n\t\t When some of the component texts are missing , we can ask the parser to infer a -dimensional multitree that includes the missing components . \n\t', '\n\t\t The resulting multitree will cover the input components/dimensions among its dimensions . \n\t', '\n\t\t It will also express the output compo- nents/dimensions , along with their syntactic structures . \n\t', '\n\t\t Figure 5 : Logic CT ( \x93T\x94 for Translation ) Figure 5 shows Logic CT , which is a generalization of Logic C. Translator CT is any parser based on Logic CT . \n\t', '\n\t\t The items of Translator CT have a -dimensional label vector , as usual . \n\t', '\n\t\t However , their d-span vectors are only -dimensional , because it is not necessary to constrain absolute word positions in the output dimensions . \n\t', '\n\t\t Instead , we need only constrain the cardinality of the output nonterminals , which is accomplished by the role templates in the term . \n\t', '\n\t\t Translator CT scans only the input components . \n\t', '\n\t\t Terminal productions with active output components are simply loaded from the grammar , and their LHSs are added to the chart without d-span information . \n\t', '\n\t\t Composition proceeds as before , except that there are no constraints on the role templates in the output dimensions \x96 the role templates in are free variables . \n\t', '\n\t\t In summary , Logic CT differs from Logic C as follows : Items store no position information ( d-spans ) for the output components . \n\t', '\n\t\t For the output components , the Scan inferences are replaced by Load inferences , which are not constrained by the input . \n\t', '\n\t\t The Compose inference does not constrain the d-spans of the output components . \n\t', '\n\t\t ( Though it still constrains their cardinality . \n\t', '\n\t\t ) Compose : Item Form : Goal : Inference Rules Scan component : Load component , : We have constructed a translator from a synchronous parser merely by relaxing some constraints on the output dimensions . \n\t', '\n\t\t Logic C is just Logic CT for the special case where . \n\t', '\n\t\t The relationship between the two classes of algorithms is easier to see from their declarative logics than it would be from their procedural pseudocode or equations . \n\t', '\n\t\t Like Parser C , Translator CT can Compose items that have no dimensions in common . \n\t', '\n\t\t If one of the items is active only in the input dimension(s) , and the other only in the output dimension(s) , then the inference is , de facto , a translation . \n\t', '\n\t\t The possible translations are determined by consulting the grammar . \n\t', '\n\t\t Thus , in addition to its usual function of evaluating syntactic structures , the grammar simultaneously functions as a translation model . \n\t', '\n\t\t Logic CT can be coupled with any parsing semiring . \n\t', '\n\t\t For example , under a boolean semiring , this logic will succeed on an -dimensional input if and only if it can infer a -dimensional multitree whose root is the goal item . \n\t', '\n\t\t Such a tree would contain a -dimensional translation of the input . \n\t', '\n\t\t Thus , under a boolean semiring , Translator CT can determine whether a translation of the input exists . \n\t', '\n\t\t Under an inside-probability semiring , Translator CT can compute the total probability of all multitrees containing the input and its translations in the output components . \n\t', '\n\t\t All these derivation trees , along with their probabilities , can be efficiently represented as a packed parse forest , rooted at the goal item . \n\t', '\n\t\t Unfortunately , finding the most probable output string still requires summing probabilities over an exponential number of trees . \n\t', '\n\t\t This problem was shown to be NP-hard in the one-dimensional case ( Sima\x92an , 1996 ) . \n\t', '\n\t\t We have no reason to believe that it is any easier when each internal node of the tree . \n\t', '\n\t\t The intended ordering of the terminals in each output dimension can be assembled from these templates by a linear-time linearization post-process that traverses the finished multitree in postorder . \n\t', '\n\t\t To the best of our knowledge , Logic CT is the first published translation logic to be compatible with all of the semirings catalogued by \n\t\t']",Positive
"['\n\t\t It is also the first to simultaneously accommodate multiple input components and multiple output components . \n\t', '\n\t\t When a source document is available in multiple languages , a translator can benefit from the disambiguating information in each . \n\t', '\n\t\t Translator CT can take advantage of such information without making the strong independence assumptions of Och & \n\t\t']",Positive
"['\n\t\t When output is desired in multiple languages , Translator CT offers all the putative benefits of the interlingual approach to MT , including greater efficiency and greater consistency across output components . \n\t', '\n\t\t Indeed , the language of multitrees can be viewed as an interlingua . \n\t', '\n\t\t 5 Synchronization We have explored inference of -dimensional multi- trees under a -dimensional grammar , where . \n\t', '\n\t\t Now we generalize along the other axis of Figure 1(a) . \n\t', '\n\t\t Multitext synchronization is most often used to infer -dimensional multitrees without the benefit of an -dimensional grammar . \n\t', '\n\t\t One application is inducing a parser in one language from a parser in another ( L¨u et al. , 2002 ) . \n\t', '\n\t\t The application that is most relevant to this paper is bootstrapping an -dimensional grammar . \n\t', '\n\t\t In theory , it is possible to induce a PMTG from multitext in an unsupervised manner . \n\t', '\n\t\t A more reliable way is to start from a corpus of multitrees \x97 a multitreebank.3 We are not aware of any multitreebanks at this time . \n\t', '\n\t\t The most straightforward way to create one is to parse some multitext using a synchronous parser , such as Parser C . \n\t', '\n\t\t However , if the goal is to bootstrap an -PMTG , then there is no -PMTG that can evaluate the terms in the parser\x92s logic . \n\t', '\n\t\t Our solution is to orchestrate lower-dimensional knowledge sources to evaluate the terms . \n\t', '\n\t\t Then , we can use the same parsing logic to synchronize multitext into a multitreebank . \n\t', '\n\t\t To illustrate , we describe a relatively simple synchronizer , using the Viterbi-derivation semiring.4 Under this semiring , a synchronizer computes the single most probable multitree for a given multitext . \n\t', '\n\t\t 3In contrast , a parallel treebank might contain no information about translational equivalence . \n\t', '\n\t\t 4The inside-probability semiring would be required for maximum-likelihood synchronization . \n\t', '\n\t\t . \n\t', '\n\t\t The Viterbi-derivation semiring would be the most often used with Translator CT in practice . \n\t', '\n\t\t Given a -PMTG , Translator CT can use this semiring to find the single most probable -dimensional multitree that covers the -dimensional input . \n\t', '\n\t\t The multitree inferred by the translator will have the words of both the input and the output components in its leaves . \n\t', '\n\t\t For example , given a suitable grammar and the input Pasudu moy , Translator CT could infer the multitree in Figure 2 . \n\t', '\n\t\t The set of inferences would be exactly the same as those listed in Figure 4 , except that the items would have no d-spans in the English component . \n\t', '\n\t\t In practice , we usually want the output as a string tuple , rather than as a multitree . \n\t', '\n\t\t Under the various derivation semirings \n\t\t']",Positive
"['\n\t\t Only one synchronous dependency structure ( dashed arrows ) is compatible with the monolingual structure ( solid arrows ) and word alignment ( shaded cells ) . \n\t', '\n\t\t If we have no suitable PMTG , then we can use other criteria to search for trees that have high probability . \n\t', '\n\t\t We shall consider the common synchronization scenario where a lexicalized monolingual grammar is available for at least one component.5 Also , given a tokenized set of -tuples of parallel sentences , it is always possible to estimate a word-to-word translation model ( e.g. , Och & Ney , 2003).6 A word-to-word translation model and a lexicalized monolingual grammar are sufficient to drive a synchronizer . \n\t', '\n\t\t For example , in Figure 6 a monolingual grammar has allowed only one dependency structure on the English side , and a word-to-word translation model has allowed only one word alignment . \n\t', '\n\t\t The syntactic structures of all dimensions of a multitree are isomorphic up to reordering of sibling nodes and deletion . \n\t', '\n\t\t So , given a fixed correspondence between the tree leaves ( i.e. words ) across components , choosing the optimal structure for one component is tantamount to choosing the optimal synchronous structure for all components . \n\t', '\n\t\t 7 Ignoring the nonterminal labels , only one dependency structure is compatible with these constraints \x96 the one indicated by dashed arrows . \n\t', '\n\t\t Bootstrapping a PMTG from a lower-dimensional PMTG and a word-to-word translation model is similar in spirit to the way that regular grammars can help to estimate CFGs ( Lari & Young , 1990 ) , and the way that simple translation models can help to bootstrap more sophisticated ones \n\t\t']",Positive
"['\n\t\t 5 Such a grammar can be induced from a treebank , for example . \n\t', '\n\t\t We are currently aware of treebanks for English , Spanish , German , Chinese , Czech , Arabic , and Korean . \n\t', '\n\t\t 6Although most of the literature discusses word translation models between only two languages , it is possible to combine several 2D models into a higher-dimensional model ( Mann & Yarowsky , 2001 ) . \n\t', '\n\t\t 7Except where the unstructured components have words that are linked to nothing . \n\t', '\n\t\t We need only redefine the terms in a way that does not rely on an -PMTG . \n\t', '\n\t\t Without loss of generality , we shall assume a -PMTG that ranges over the first components , where . \n\t', '\n\t\t We shall then refer to the structured components and the unstructured components . \n\t', '\n\t\t We begin with . \n\t', '\n\t\t For the structured compo- nents , we retain the grammar- based definition : and continues by making independence assumptions . \n\t', '\n\t\t The first assumption is that the structured components of the production\x92s RHS are conditionally independent of the unstructured components of its LHS : ( 1 ) The above probability can be looked up in the -PMTG . \n\t', '\n\t\t Second , since we have no useful non- terminals in the unstructured components , we let ( 2 ) if and otherwise . \n\t', '\n\t\t Third , we assume that the word-to-word translation probabilities are independent of anything else : ( 7 ) 8 We have ignored lexical heads so far , but we need them for this synchronizer . \n\t', '\n\t\t 9The procedure is analogous when the heir is the first non- terminal link on the RHS , rather than the second . \n\t', '\n\t\t ( 3 ) ( 4 ) , s where the latter probability can be looked up in our -PMTG . \n\t', '\n\t\t For the unstructured components , there are no useful nonterminal labels . \n\t', '\n\t\t Therefore , we assume that the unstructured components use only one ( dummy ) nonterminal label , so that if and undefined oth- erwise for . \n\t', '\n\t\t Our treatment of nonterminal productions begins by applying the chain rule9 These probabilities can be obtained from our wordto-word translation model , which would typically be estimated under exactly such an independence assumption . \n\t', '\n\t\t Finally , we assume that the output role templates are independent of each other and uniformly distributed , up to some maximum cardinality . \n\t', '\n\t\t Let be the number of unique role templates of cardinality or less . \n\t', '\n\t\t Then Under Assumptions 5\x968 , ( 5 ) if and 0 otherwise . \n\t', '\n\t\t We can use these definitions of the grammar terms in the inference rules of Logic C to synchronize multitexts into multitreebanks . \n\t', '\n\t\t More sophisticated synchronization methods are certainly possible . \n\t', '\n\t\t For example , we could project a part-of-speech tagger ( Yarowsky & Ngai , 2001 ) to improve our estimates in Equation 6 . \n\t', '\n\t\t Yet , despite their relative simplicity , the above methods for estimating production rule probabilities use all of the available information in a consistent manner , without double-counting . \n\t', '\n\t\t This kind of synchronizer stands in contrast to more ad-hoc approaches ( e.g. , Matsumoto , 1993 ; Meyers , 1996 ; Wu , 1998 ; Hwa et al. , 2002 ) . \n\t', '\n\t\t Some of these previous works fix the word alignments first , and then infer compatible parse structures . \n\t', '\n\t\t Others do the opposite . \n\t', '\n\t\t Information about syntactic structure can be inferred more accurately given information about translational equivalence , and vice versa . \n\t', '\n\t\t Commitment to either kind of information without consideration of the other increases the potential for compounded errors . \n\t', '\n\t\t 6 Multitree-based Statistical MT Multitree-based statistical machine translation ( MTSMT ) is an architecture for SMT that revolves around multitrees . \n\t', '\n\t\t Figure 7 shows how to build and use a rudimentary MTSMT system , starting from some multitext and one or more monolingual tree- banks . \n\t', '\n\t\t The recipe follows : T1 . \n\t', '\n\t\t Induce a word-to-word translation model . \n\t', '\n\t\t T2 . \n\t', '\n\t\t Induce PCFGs from the relative frequencies of productions in the monolingual treebanks . \n\t', '\n\t\t T3 . \n\t', '\n\t\t Synchronize some multitext , e.g. using the approximations in Section 5 . \n\t', '\n\t\t T4 . \n\t', '\n\t\t Induce an initial PMTG from the relative frequencies of productions in the multitreebank . \n\t', '\n\t\t T5 . \n\t', '\n\t\t Re-estimate the PMTG parameters , using a synchronous parser with the expectation semiring . \n\t', '\n\t\t A1 . \n\t', '\n\t\t Use the PMTG to infer the most probable multitree covering new input text . \n\t', '\n\t\t A2 . \n\t', '\n\t\t Linearize the output dimensions of the multi- tree . \n\t', '\n\t\t Steps T2 , T4 and A2 are trivial . \n\t', '\n\t\t Steps T1 , T3 , T5 , and A1 are instances of the generalized parsers described in this paper . \n\t', '\n\t\t Figure 7 is only an architecture . \n\t', '\n\t\t Computational complexity and generalization error stand in the way of its practical implementation . \n\t', '\n\t\t Nevertheless , it is satisfying to note that all the non-trivial algo- rithms in Figure 7 are special cases of Translator CT . \n\t', '\n\t\t It is therefore possible to implement an MTSMT system using just one inference algorithm , param- eterized by a grammar , a semiring , and a search strategy . \n\t', '\n\t\t An advantage of building an MT system in this manner is that improvements invented for ordi- nary parsing algorithms can often be applied to all the main components of the system . \n\t', '\n\t\t For example , \n\t\t']",Positive
"['\n\t\t The same opti- mization can be applied to the inference algorithms in this paper . \n\t', '\n\t\t With proper software design , such op- timizations need never be implemented more than once . \n\t', '\n\t\t For simplicity , the algorithms in this paper are based on CKY logic . \n\t', '\n\t\t However , the architecture in Figure 7 can also be implemented using general- izations of more sophisticated parsing logics , such as those inherent in Earley or Head-Driven parsers . \n\t', '\n\t\t 7 Conclusion This paper has presented generalizations of ordinary parsing that emerge when the grammar and/or the input can be multidimensional . \n\t', '\n\t\t Along the way , it has elucidated the relationships between ordinary parsers and other classes of algorithms , some previously known and some not . \n\t', '\n\t\t It turns out that , given some multitext and a monolingual treebank , a rudimentary multitree-based statistical machine translation system can be built and applied using only generalized parsers and some trivial glue . \n\t', '\n\t\t There are three research benefits of using generalized parsers to build MT systems . \n\t', '\n\t\t First , we can ( 8 ) PCFG(s) T2 relative frequency computation A1 translation input multitext A2 linearization multitree output multitext monolingual treebank(s) training multitext T1 word alignment T5 word^to^word translation model estimation via multitreebank synchronous parameter parsing T4 synchronization relative frequency computation T3 PMTG Figure 7 : Data-flow diagram for a rudimentary MTSMT system based on generalizations of parsing . \n\t', '\n\t\t take advantage of past and future research on making parsers more accurate and more efficient . \n\t', '\n\t\t Therefore , second , we can concentrate our efforts on better models , without worrying about MT-specific search algorithms . \n\t', '\n\t\t Third , more generally and most importantly , this approach encourages MT research to be less specialized and more transparently related to the rest of computational linguistics . \n\t', '\n\t\t Acknowledgments Thanks to Joseph Turian , Wei Wang , Ben Wellington , and the anonymous reviewers for valuable feedback . \n\t', '\n\t\t This research was supported by an NSF CAREER Award , the DARPA TIDES program , and an equipment gift from Sun Microsystems . \n\t', '\n\t\t References A. Aho & J. \n\t\t', '\n\t\t H. Alshawi , S. Bangalore , & S. \n\t\t', '\n\t\t P. F. Brown , S. A. Della Pietra , V. J. Della Pietra , & R. L. \n\t\t', '\n\t\t J. \n\t\t', '\n\t\t R. Hwa , P. Resnik , A. Weinberg , & O. \n\t\t', '\n\t\t J. \n\t\t', '\n\t\t K. Lari & S. \n\t\t', '\n\t\t Y. L¨u , S. Li , T. Zhao , & M. \n\t\t', '\n\t\t G. S. Mann & D. \n\t\t', '\n\t\t Y. \n\t\t', '\n\t\t I. D. \n\t\t', '\n\t\t I. D. Melamed , G. Satta , & B. \n\t\t', '\n\t\t A. Meyers , R. Yangarber , & R. \n\t\t', '\n\t\t F. Och & H. \n\t\t', '\n\t\t F. Och & H. \n\t\t', '\n\t\t K. Sima\x92an ( 1996 ) \x93Computational Complexity of Probabilistic Disambiguation by means of Tree-Grammars,\x94 Proceedings of COLING . \n\t', '\n\t\t D. \n\t\t', '\n\t\t D. \n\t\t', '\n\t\t D. Wu & H. \n\t\t', '\n\t\t K. Yamada & K. \n\t\t', '\n\t\t D. Yarowsky & G. \n\t\t', '\n\t\t Generalized Multitext Grammars I. Dan Melamed Computer Science Department New York University 715 Broadway , 7th Floor New York , NY , 10003 , USA lastname @cs.nyu.edu Giorgio Satta Dept. of Information Eng\x92g University of Padua via Gradenigo 6/A I-35131 Padova , Italy lastname @dei.unipd.it Benjamin Wellington Computer Science Department New York University 715 Broadway , 7th Floor New York , NY , 10003 , USA lastname @cs.nyu.edu Abstract Generalized Multitext Grammar ( GMTG ) is a synchronous grammar formalism that is weakly equivalent to Linear Context-Free Rewriting Systems ( LCFRS ) , but retains much of the notational and intuitive simplicity of Context-Free Grammar ( CFG ) . \n\t', '\n\t\t GMTG allows both synchronous and independent rewriting . \n\t', '\n\t\t Such flexibility facilitates more perspicuous modeling of parallel text than what is possible with other synchronous formalisms . \n\t', '\n\t\t This paper investigates the generative capacity of GMTG , proves that each component grammar of a GMTG retains its generative power , and proposes a generalization of Chomsky Normal Form , which is necessary for synchronous CKY-style parsing . \n\t', '\n\t\t 1 Introduction Synchronous grammars have been proposed for the formal description of parallel texts representing translations of the same document . \n\t', '\n\t\t As shown by \n\t\t']",Positive
"['\n\t\t Since linguistic expressions can vanish in translation , a good model must be able to express independent ( in addition to synchronous ) rewriting . \n\t', '\n\t\t Inversion Transduction Grammar ( ITG ) \n\t\t']",Negative
['\n\t\t Synchronous Tree Adjoining Grammar ( STAG ) \n\t\t'],Negative
"['\n\t\t Generalized Multitext Grammar ( GMTG ) offers a way to synchronize Mildly Context-Sensitive Grammar ( MCSG ) , while satisfying both of the above criteria . \n\t', '\n\t\t The move to MCSG is motivated by our desire to more perspicuously account for certain syntactic phenomena that cannot be easily captured by context-free grammars , such as clitic climbing , extraposition , and other types of long- distance movement \n\t\t']",Positive
"['\n\t\t On the other hand , MCSG still observes some restrictions that make the set of languages it generates less ex- pensive to analyze than the languages generated by ( properly ) context-sensitive formalisms . \n\t', '\n\t\t More technically , our proposal starts from Multitext Grammar ( MTG ) , a formalism for synchronizing context-free grammars recently proposed by \n\t\t']",Positive
"['\n\t\t In MTG , synchronous rewriting is implemented by means of an indexing relation that is maintained over occurrences of nonterminals in a sentential form , using essentially the same machinery as SDTS . \n\t', '\n\t\t Unlike SDTS , MTG can extend the dimensionality of the translation relation beyond two , and it can implement independent rewriting by means of partial deletion of syntactic structures . \n\t', '\n\t\t Our proposal generalizes MTG by moving from component grammars that generate context- free languages to component grammars whose generative power is equivalent to Linear Context-Free Rewriting Systems ( LCFRS ) , a formalism for describing a class of MCSGs . \n\t', '\n\t\t The generalization is achieved by allowing context-free productions to rewrite tuples of strings , rather than single strings . \n\t', '\n\t\t Thus , we retain the intuitive top-down definition of synchronous derivation original in SDTS and MTG but not found in LCFRS , while extending the generative power to linear context-free rewriting languages . \n\t', '\n\t\t In this respect , GMTG has also been inspired by the class of Local Unordered Scattered Context Grammars \n\t\t']",Positive
['\n\t\t A syntactically very different synchronous formalism involving LCFRS has been presented by \n\t\t'],Positive
"['\n\t\t This paper begins with an informal description of GMTG . \n\t', '\n\t\t It continues with an investigation of this formalism\x92s generative capacity . \n\t', '\n\t\t Next , we prove that in GMTG each component grammar retains its generative power , a requirement for synchronous formalisms that \n\t\t']",Positive
"['\n\t\t 2 Informal Description and Comparisons GMTG is a generalization of MTG , which is itself a generalization of CFG to the synchronous case . \n\t', '\n\t\t Here we present MTG in a new notation that shows the relation to CFG more clearly . \n\t', '\n\t\t For example , the following MTG productions can generate the multi- text [ ( Ifed the cat ) , ( ya kota kormil)]:1 ( S ) ( S ) PN VP PN VP ( 1 ) PN PN I ya ( 2 ) VP VP V NP NP V ( 3 ) V V fed kormil ( 4 ) NP NP D N N ( 5 ) D the ( 6 ) N N cat kota ( 7 ) Each production in this example has two components , the first modeling English and the second ( transliterated ) Russian . \n\t', '\n\t\t Nonterminals with the same index must be rewritten together ( synchronous rewriting ) . \n\t', '\n\t\t One strength of MTG , and thus also GMTG , is shown in Productions ( 5 ) and ( 6 ) . \n\t', '\n\t\t There is a determiner in English , but not in Russian , so Production ( 5 ) does not have the nonterminal D in the Russian component and ( 6 ) applies only to the English component ( independent rewriting ) . \n\t', '\n\t\t Formalisms that do not allow independent rewriting require a corresponding to appear in the second component on the right-hand side ( RHS ) of Production ( 5 ) , and this would eventually generate the empty string . \n\t', '\n\t\t This approach has the disadvantage that it introduces spurious ambiguity about the position of the \x93empty\x94 nonterminal with respect to the other nonterminals in its component . \n\t', '\n\t\t Spurious ambiguity leads to wasted effort during parsing . \n\t', '\n\t\t GMTG\x92s implementation of independent rewriting through the empty tuple ( ) serves a very different function from the empty string . \n\t', '\n\t\t Consider the following GMTG : ( 8 ) ( 9 ) ( 10 ) ( 11 ) Production ( 8 ) asserts that symbol vanishes in translation . \n\t', '\n\t\t Its application removes both of the non- terminals on the left-hand side ( LHS ) , pre-empting any other production . \n\t', ""\n\t\t In contrast , Production ( 9 ) ' We write production components both side by side and one above another to save space , but each component is always in parentheses . \n\t"", '\n\t\t explicitly relaxes the synchronization constraint , so that the two components can be rewritten independently . \n\t', '\n\t\t The other six productions make assertions about only one component and are agnostic about the other component . \n\t', '\n\t\t Incidentally , generating the same language with only fully synchronized productions would raise the number of required productions to 11 , so independent rewriting also helps to reduce grammar size . \n\t', '\n\t\t Independent rewriting is also useful for modeling paraphrasing . \n\t', '\n\t\t Take , for example , [ ( Tim got a pink slip ) , ( Tim got laid off ) ] . \n\t', '\n\t\t While the two sentences have the same meaning , the objects of their verb phrases are structured very differently . \n\t', '\n\t\t GMTG can express their relationships as follows : S S NP VP NP VP ( 12 ) VP VP V NP V PP ( 13 ) NP PP DT A N VB R ( 14 ) NP NP Tim Tim ( 15 ) V V got got ( 16 ) DT a ( 17 ) A pink ( 18 ) N slip ( 19 ) VB laid ( 20 ) R off^ ( 21 ) As described by \n\t\t']",Positive
"['\n\t\t GMTG removes this restriction . \n\t', '\n\t\t Take , for example , the sentence pair [ ( The doctor treats his teeth ) , ( El m´edico le examino los dientes ) ] \n\t\t']",Positive
"['\n\t\t The Spanish clitic le and the NP los dientes should both be paired with the English NP his teeth , giving rise to a discontinuous constituent in the Spanish component . \n\t', '\n\t\t A GMTG fragment for the sentence is shown below : S S NP VP NP VP VP VP V NP NP V NP NP NP The doctor El m´edico V V treats examino NP NP NP his teeth le los dientes Note the discontinuity between le and los dientes . \n\t', '\n\t\t Such discontinuities are marked by commas on both the LHS and the RHS of the relevant component . \n\t', '\n\t\t GMTG\x92s flexibility allows it to deal with many complex syntactic phenomena . \n\t', '\n\t\t For example , \n\t\t']",Positive
"['\n\t\t They examine the English/German sentence fragment [ ( ... that the detective has promised the client to indict the suspect of the crime ) , ( ... daß des Verbrechens der Detektiv den Verd¨achtigen dem Klienten zu ¨uberf¨uhren versprochen hat ) ] . \n\t', '\n\t\t The verbs versprochen and ¨uberf¨uhren both have two noun phrases as arguments . \n\t', '\n\t\t In German , these noun phrases can appear to the left of the verbs in any order . \n\t', '\n\t\t The following is a GMTG fragment for the above sentence pair2 : S S S S S S The discontinuities allow the noun arguments of versprochen to be placed in any order with the noun arguments of ¨uberf¨uhren . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t 3 Formal Definitions Let be a finite set of nonterminal symbols and let be the set of integers.3 We define .4 Elements of will be called indexed nonterminal symbols . \n\t', '\n\t\t In what follows we also consider a finite set of terminal symbols , disjoint from , and work with strings in , where . \n\t', '\n\t\t For , we define , i.e. the set of indexes that ap- pear in . \n\t', '\n\t\t An indexed tuple vector , or ITV , is a vector of tuples of strings over , having the form where , and for , . \n\t', '\n\t\t We write , , to denote the -th component of and to denote the arity of such a tuple , which is . \n\t', '\n\t\t When , is the empty tuple , written . \n\t', '\n\t\t This should not be confused with , that is the tuple of arity one containing the empty string . \n\t', '\n\t\t A link is an ITV where 2These are only a small subset of the necessary productions . \n\t', '\n\t\t The subscripts on the nonterminals indicate what terminals they will eventually yield ; the terminal productions have been left out to save space . \n\t', '\n\t\t 3Any other infinite set of indexes would suit too . \n\t', '\n\t\t 4The parentheses around indexes distinguish them from other uses of superscripts in formal language theory . \n\t', '\n\t\t However , we shall omit the parentheses when the context is unambiguous . \n\t', '\n\t\t each consists of one indexed nonterminal and all of these nonterminals are coindexed . \n\t', '\n\t\t As we shall see , the notion of a link generalizes the notion of nonterminal in context-free grammars : each production rewrites a single link . \n\t', '\n\t\t Definition 1 Let be some integer con- stant . \n\t', '\n\t\t A generalized multitext grammar with dimensions ( -GMTG for short ) is a tuple where , arefinite , disjoint sets of nonterminal and terminal symbols , respectively , is the start symbol and is a finite set of productions . \n\t', '\n\t\t Each production has the form , where is a -dimensional link and is a - dimensional ITV such that for . \n\t', '\n\t\t If contains , then We omit symbol from -GMTG whenever it is not relevant . \n\t', '\n\t\t To simplify notation , we write productions as , with each , . \n\t', '\n\t\t I.e. we omit the unique index appearing on the LHS of . \n\t', '\n\t\t Each is called a production component . \n\t', '\n\t\t The production component is called the inactive production component . \n\t', '\n\t\t All other production components are called active and we set . \n\t', '\n\t\t Inactive production components are used to relax synchronous rewriting on some dimen- sions , that is to implement rewriting on com- ponents . \n\t', '\n\t\t When , rewriting is licensed on one component , independently of all the others . \n\t', '\n\t\t Two grammar parameters play an important role in this paper . \n\t', '\n\t\t Let and . \n\t', '\n\t\t Definition 2 The rank of a production is the number of links on its RHS : . \n\t', '\n\t\t The rank of a GMTG is For example , the rank of Production ( 23 ) is two and its fan-out is four . \n\t', '\n\t\t In GMTG , the derives relation is defined over ITVs . \n\t', '\n\t\t GMTG derivation proceeds by synchronous application of all the active components in some production . \n\t', '\n\t\t The indexed nonterminals to be rewritten simultaneously must all have the same index , and all nonterminals indexed with in the ITV must be rewritten simultaneously . \n\t', '\n\t\t Some additional notation will help us to define rewriting precisely . \n\t', '\n\t\t A reindexing is a one-to-one function on , and is extended to by letting for Definition 3 The fan-out of , and are , respectively , , and . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t and for . \n\t', '\n\t\t We also extend to strings in analogously . \n\t', '\n\t\t We say that are independent if. . \n\t', '\n\t\t Definition 4 Let be a -GMTG and let with and . \n\t', '\n\t\t Let and be two ITVs with and . \n\t', '\n\t\t Assume that is some concatenation of all and that is some concatenation of all , holds whenever there exists an index such that the following two conditions are satisfied : ( i ) for each we have such that , and each is obtained from by replacing each with ; ( ii ) for each we have and . \n\t', '\n\t\t We generalize the relation to and in the usual way , to represent derivations . \n\t', '\n\t\t We can now introduce the notion of generated language ( or generated relation ) . \n\t', '\n\t\t A start link of a -GMTG is a -dimensional link where at least one component is , the start symbol , and the rest of the components are . \n\t', '\n\t\t Thus , there are start links . \n\t', '\n\t\t The language generated by a -GMTG is a start link or^ . \n\t', '\n\t\t Each ITV in is called a multitext . \n\t', '\n\t\t For every -GMTG , can be partitioned into subsets , each containing multitexts derived from a different start link . \n\t', '\n\t\t These subsets are disjoint , since every non- empty tuple of a start link is eventually rewritten as a string , either empty or not.5 A start production is a production whose LHS is a start link . \n\t', '\n\t\t A GMTG writer can choose the combinations of components in which the grammar can generate , by including start productions with the desired combinations of active components . \n\t', '\n\t\t If a grammar contains no start productions with a certain combination of active components , then the corresponding subset of will be empty . \n\t', '\n\t\t Allowing a single GMTG to generate multitexts with 5 We are assuming that there are no useless nonterminals . \n\t', '\n\t\t some empty tuples corresponds to modeling relations of different dimensionalities . \n\t', '\n\t\t This capability enables a synchronous grammar to govern lower- dimensional sublanguages/translations . \n\t', '\n\t\t For example , an English/Italian GMTG can include Production ( 9 ) , an English CFG , and an Italian CFG . \n\t', '\n\t\t A single GMTG can then govern both translingual and monolingual information in applications . \n\t', '\n\t\t Furthermore , this capability simplifies the normalization procedure described in Section 6 . \n\t', '\n\t\t Otherwise , this procedure would require exceptions to be made when eliminating epsilons from start productions . \n\t', '\n\t\t 4 Generative Capacity In this section we compare the generative capacity of GMTG with that of mildly context-sensitive grammars . \n\t', '\n\t\t We focus on LCFRS , using the notational variant introduced by \n\t\t']",Positive
"['\n\t\t Throughout this section , strings and vectors of the form will be identified . \n\t', '\n\t\t For lack of space , some proofs are only sketched , or entirely omitted when relatively intuitive : \n\t\t']",Positive
"['\n\t\t Let be some terminal alphabet . \n\t', '\n\t\t A function has rank if it is defined on , for integers ( 24 ) where represents some grouping into strings of all and only the variables appearing in the left-hand side , possibly with some additional terminal symbols . \n\t', '\n\t\t ( Symbols , and are overloaded below . \n\t', '\n\t\t ) , and is a finite set of productions of the form , where , , and where is a linear regular function having rank and fan-out , defined on . \n\t', '\n\t\t For every and , we write if ( i ) and ; or else ,, and let be some reindexing such that strings and are independent . \n\t', '\n\t\t The derives relation with , . \n\t', '\n\t\t Also , has fan-out if its range is a subset of . \n\t', '\n\t\t Let , , , and , be string-valued variables . \n\t', '\n\t\t Function is linear regular if it is defined by an equation of the form Definition 5 A Linear Context-Free Rewriting System ( LCFRS ) is a quadruple where , and are as in GMTGs , every is associated with an integer with Let . \n\t', '\n\t\t Also let and let be the number of occurrences of appearing in . \n\t', '\n\t\t We define an alphabet .For each and with , and , we define a string over as fol- lows . \n\t', '\n\t\t Let , each . \n\t', '\n\t\t Then , where in case ; and in case , where is the index of and the indicated occurrence of is the -th occurrence of such symbol appearing from left to right in string . \n\t', '\n\t\t Next , for every possible , , and as above , we add to a production where ( each above satisfies ) . \n\t', '\n\t\t Note that is a function with rank and fan-out .Thus we have and . \n\t', '\n\t\t Without loss of generality , we assume that contains only one production with appearing on the left-hand side , having the form . \n\t', '\n\t\t To complete the construction of , we then add a last production where . \n\t', '\n\t\t We claim that , for each , and as above iff . \n\t', '\n\t\t The lemma follows from this claim . \n\t', '\n\t\t The proof of the next lemma is relatively intuitive and therefore omitted . \n\t', '\n\t\t Lemma 2 For any -GMTG , there exists a properly synchronous -GMTG such that , and . \n\t', '\n\t\t The language generated by is defined as .Let , .The rank of and are , respectively , and . \n\t', '\n\t\t The fan-out of and are , respectively , and . \n\t', '\n\t\t The proof of the following theorem is relatively intuitive and therefore omitted . \n\t', '\n\t\t ( ii ) , for every , and . \n\t', '\n\t\t Next , we show that the generative capacity of GMTG does not exceed that of LCFRS . \n\t', '\n\t\t In order to compare string tuples with bare strings , we introduce two special functions ranging over multi- texts . \n\t', '\n\t\t Assume two fresh symbols .For a multitext we write , where if and otherwise , . \n\t', '\n\t\t For a multitext with no empty tuple , we write . \n\t', '\n\t\t We extend both functions to sets of multitexts in the obvious way : and . \n\t', '\n\t\t In a -GMTG , a production with active components , , is said to be -active . \n\t', '\n\t\t A -GMTG whose start productions are all -active is called properly synchronous . \n\t', '\n\t\t Lemma 1 For any properly synchronous -GMTG , there exists some LCFRS with and such that . \n\t', '\n\t\t Outline of the proof . \n\t', '\n\t\t We set , where , is the set of all indexes appearing in the productions of , and is constructed as follows . \n\t', '\n\t\t Let with , , , and .Assume that can rewrite the right- hand side of , that is , Then there must be at least one index such that for each , contains exactly occurrences of . \n\t', '\n\t\t Combining Lemmas 1 and 2 , we have Theorem 2 For any -GMTG , there exists some LCFRS with and such that . \n\t', '\n\t\t Theorem 1 For any LCFRS , there exists some 1-GMTG with and such that . \n\t', '\n\t\t 5 Weak Language Preservation Property GMTGs have the weak language preservation property , which is one of the defining requirements of synchronous rewriting systems \n\t\t']",Positive
"['\n\t\t Informally stated , the generative capacity of the class of all component grammars of a GMTG exactly corresponds to the class of all projected languages . \n\t', '\n\t\t In other words , the interaction among different grammar components in the rewriting process of GMTG does not increase the generative power beyond the above mentioned class . \n\t', '\n\t\t The next result states this property more formally . \n\t', '\n\t\t Let be a -GMTG with production set . \n\t', '\n\t\t For , the -th component gram- mar of , written , is the 1-GMTG with productions projected language of is , because component grammars interact with each other in the rewriting process of . \n\t', '\n\t\t To give a simple example , consider the 2- GMTG with productions , and . \n\t', '\n\t\t Then , and thus . \n\t', '\n\t\t On the other hand , . \n\t', '\n\t\t Let LCFRS be the class of all lan- guages generated by LCFRSs . \n\t', '\n\t\t Also let and be the classes of languages and , respectively , for every , ev- ery -GMTG and every with Proof . \n\t', '\n\t\t The cases directly follow from Theo- rem 1 . \n\t', '\n\t\t Let be some -GMTG and let be an integer such that . \n\t', '\n\t\t It is not difficult to see that . \n\t', '\n\t\t Hence can be generated by some LCFRS , by Theorem 2 . \n\t', '\n\t\t We now define a LCFRS such that . \n\t', '\n\t\t Assume is a properly synchronous -GMTG generating ( Lemma 2 ) . \n\t', '\n\t\t Let , where and are constructed from almost as in the proof of Lemma 1 . \n\t', '\n\t\t The only difference is in the definition of strings and the production rewriting , specified as follows ( we use the same notation as in the proof of Lemma 1 ). , where for each : ( i ) if and ; ( ii ) if and ; ( iii ) if , with , as in the original proof . \n\t', '\n\t\t Finally , the production rewriting has the form , where . \n\t', '\n\t\t To conclude the proof , note that and can differ only with respect to string . \n\t', '\n\t\t The theorem then follows from the fact that LCFRS is closed under intersection with regular languages \n\t\t']",Positive
"['\n\t\t 6 Generalized Chomsky Normal Form Certain kinds of text analysis require a grammar in a convenient normal form . \n\t', '\n\t\t The prototypical example for CFG is Chomsky Normal Form ( CNF ) , which is required for CKY-style parsing . \n\t', '\n\t\t A -GMTG is in Generalized Chomsky Normal Form ( GCNF ) if it has no useless links or useless terminals , and every production is in one of two forms : ( i ) A nonterminal production has rank = 2 and no terminals or \x92s on the RHS . \n\t', '\n\t\t ( ii ) A terminal production has exactly one component of the form , where and . \n\t', '\n\t\t The other components are inactive . \n\t', '\n\t\t The algorithm to convert a GMTG to GCNF has the following steps : ( 1 ) add a new start-symbol ( 2 ) isolate terminals , ( 3 ) binarize productions , ( 4 ) remove \x92s , ( 5 ) eliminate useless links and terminals , and ( 6 ) eliminate unit productions . \n\t', '\n\t\t The steps are generalizations of those presented by \n\t\t']",Positive
"['\n\t\t The ordering of these steps is important , as some steps can restore conditions that others eliminate . \n\t', '\n\t\t Traditionally , the terminal isolation and binarization steps came last , but the alternative order reduces the number of productions that can be created during -elimination . \n\t', '\n\t\t Steps ( 1 ) , ( 2 ) , ( 5 ) and ( 6 ) are the same for CFG and GMTG , except that the notion of nonterminal in CFG is replaced with links in GMTG . \n\t', '\n\t\t Some complications arise , however , in the generalization of steps ( 3 ) and ( 4 ) . \n\t', '\n\t\t 6.1 Step 3 : Binarize The third step of converting to GCNF is binarization of the productions , making the rank of the grammar two . \n\t', '\n\t\t For and , we write D-GMTG to represent the class of all -GMTGs with rank and fan-out . \n\t', '\n\t\t A CFG can always be binarized into another CFG : two adjacent nonterminals are replaced with a single nonterminal that yields them . \n\t', '\n\t\t In con- trast , it can be impossible to binarize a -GMTG ~into an equivalent -GMTG . \n\t', '\n\t\t From results pre- sented by \n\t\t']",Positive
"['\n\t\t . \n\t', '\n\t\t In general . \n\t', '\n\t\t ( S ) ( S ) NPat Vwent Phome Aearly PdamoyNPatArano Vpashol Pat went home early damoy Pat rano pashol Figure 1 : A production that requires an increased fan-out to binarize , and its 2D illustration . \n\t', '\n\t\t for every fan-out and rank , there are some index orderings that can be generated by ~-GMTG but not -GMTG . \n\t', '\n\t\t The distin- guishing characteristic of such index orderings is apparent in Figure 1 , which shows a production in a grammar with fan-out two , and a graph that illustrates which nonterminals are coindexed . \n\t', '\n\t\t No two nonterminals are adjacent in both components , so replacing any two nonterminals with a single non- terminal causes a discontinuity . \n\t', '\n\t\t Increasing the fan- out of the grammar allows a single nonterminal to rewrite as non-adjacent nonterminals in the same string . \n\t', '\n\t\t Increasing the fan-out can be necessary even for binarizing a 1-GMTG production such as : To binarize , we nondeterministically split each nonterminal production of rank into two nonterminal productions and of rank , but possibly with higher fan-out . \n\t', '\n\t\t Since this algorithm replaces with two productions that have rank , recursively applying the algorithm to productions of rank greater than two will reduce the rank of the grammar to two . \n\t', '\n\t\t The algorithm follows : ( i ) Nondeterministically chose links to be removed from and replaced with a single link to make , where . \n\t', '\n\t\t We call these links the m-links . \n\t', '\n\t\t ( ii ) Create a new ITV . \n\t', '\n\t\t Two nonterminals are neighbors if they are adjacent in the same string in a production RHS . \n\t', '\n\t\t For each set of m- link neighbors in component in , place that set of neighbors into the \x92th component of in the order in which they appeared in , so that each set of neighbors becomes a different string , for ( iii ) Create a new unique nonterminal , say , and replace each set of neighbors in production with , to create . \n\t', '\n\t\t The production is For example , binarization of the productions for the English/Russian multitext [ ( Pat went home early ) , ( damoy Pat rano pashol)]6 in Figure 1 requires that we increase the fan-out of the language to three . \n\t', '\n\t\t The binarized productions are as follows : Pdamoy V^pashol 6.2 Step 4 : Eliminate \x92s Grammars in GCNF cannot have \x92s in their productions . \n\t', '\n\t\t Thus , GCNF is a more restrictive normal form than those used by \n\t\t']",Positive
['\n\t\t The absence of \x92s simplifies parsers for GMTG \n\t\t'],Positive
"['\n\t\t Given a GMTG with in some productions , we give the construction of a weakly equivalent grammar without any \x92s . \n\t', '\n\t\t First , determine all nullable links and associated strings in . \n\t', '\n\t\t A link is nullable if , where is an ITV where at least one is . \n\t', '\n\t\t We say the link is nullable and the string at address in is nullable . \n\t', '\n\t\t For each nullable link , we create versions of the link , where is the number of nullable strings of that link . \n\t', '\n\t\t There is one version for each of the possible combinations of the nullable strings being present or absent . \n\t', '\n\t\t The version of the link with all strings present is its original version . \n\t', '\n\t\t Each non-original version of the link ( except in the case of start links ) gets a unique subscript , which is applied to all the nonterminals in the link , so that each link is unique in the grammar . \n\t', '\n\t\t We construct a new grammar whose set of productions is determined as follows : for each production , we identify the nullable links on the RHS and replace them with each combination of the non-original versions found earlier . \n\t', '\n\t\t If a string is left empty during this process , that string is removed from the RHS and the fan-out of the production component is reduced by one . \n\t', '\n\t\t The link on the LHS is replaced with its appropriate matching non-original link . \n\t', '\n\t\t There is one exception to the replacements . \n\t', '\n\t\t If a production consists of all nullable strings , do not include this case . \n\t', '\n\t\t Lastly , we remove all strings on the RHS of productions that have \x92s , and reduce the fan-out of the productions accordingly . \n\t', '\n\t\t Once 6The Russian is topicalized but grammatically correct . \n\t', '\n\t\t S,S NVPA PNAV ( 25 ) . \n\t', '\n\t\t S S VP VP VP V VV NPatVP ( 26 ) VP NPat VP V Aearly ( 27 ) V Arano V VwentPhome ( 28 ) again , we replace the LHS link with the appropriate version . \n\t', '\n\t\t Consider the example grammar : ( 29 ) ( 30 ) ( 31 ) ( 32 ) We first identify which links are nullable . \n\t', '\n\t\t In this case and are nullable so we create a new version of both links : and . \n\t', '\n\t\t We then alter the productions . \n\t', '\n\t\t Production ( 31 ) gets replaced by ( 40 ) . \n\t', '\n\t\t A new production based on ( 30 ) is Production ( 38 ) . \n\t', '\n\t\t Lastly , Production ( 29 ) has two nullable strings on the RHS , so it gets altered to add three new productions , ( 34 ) , ( 35 ) and ( 36 ) . \n\t', '\n\t\t The altered set of productions are the following : ( 33 ) ( 34 ) ( 35 ) ( 36 ) ( 37 ) ( 38 ) ( 39 ) ( 40 ) \n\t\t']",Positive
"['\n\t\t 7 Conclusions Generalized Multitext Grammar is a convenient and intuitive model of parallel text . \n\t', '\n\t\t In this paper , we have presented some formal properties of GMTG , including proofs that the generative capacity of GMTG is comparable to ordinary LCFRS , and that GMTG has the weak language preservation property . \n\t', '\n\t\t We also proposed a synchronous generalization of Chomsky Normal Form , laying the foundation for synchronous CKY parsing under GMTG . \n\t', '\n\t\t In future work , we shall explore the empirical properties of GMTG , by inducing stochastic GMTGs from real multitexts . \n\t', '\n\t\t Acknowledgments Thanks to Owen Rambow and the anonymous reviewers for valuable feedback . \n\t', '\n\t\t This research was supported by an NSF CAREER Award , the DARPA TIDES program , the Italian MIUR under project PRIN No. 2003091149 005 , and an equipment gift from Sun Microsystems . \n\t', '\n\t\t References A. Aho and J. Ullman . \n\t', '\n\t\t 1969. Syntax directed translations and the pushdown assembler . \n\t', '\n\t\t Journal of Computer and System Sciences , 3:37\x9656 , February . \n\t', '\n\t\t T. Becker , A. Joshi , and O. Rambow . \n\t', '\n\t\t 1991 . \n\t', '\n\t\t Long-distance scrambling and tree adjoining grammars . \n\t', '\n\t\t In Proceedings of the 5th Meeting of the European Chapter of the Association for Computational Linguistics ( EACL ) , Berlin , Germany . \n\t', '\n\t\t E. Bertsch and M. J. Nederhof . \n\t', '\n\t\t 2001. On the complexity of some extensions of RCG parsing . \n\t', '\n\t\t In Proceedings of the 7th International Workshop on Parsing Technologies ( IWPT ) , pages 66\x9677 , Beijing , China . \n\t', '\n\t\t M. Dras and T. Bleam . \n\t', '\n\t\t 2000. How problematic are clitics for S-TAG translation ? \n\t', '\n\t\t In Proceedings of the 5th International Workshop on Tree Adjoining Grammars and Related For- malisms ( TAG+5 ) , Paris , France . \n\t', '\n\t\t J. Hopcroft , R. Motwani , and J. Ullman . \n\t', '\n\t\t 2001. Introduction to Automota Theory , Languages and Computation . \n\t', '\n\t\t Addison- Wesley , USA . \n\t', '\n\t\t I. Dan Melamed , G. Satta , and B. Wellington . \n\t', '\n\t\t 2004. Generalized multitext grammars . \n\t', '\n\t\t Technical Report 04-003 , NYU Proteus Project . \n\t', '\n\t\t http://nlp.cs.nyu.edu/pubs/ . \n\t', '\n\t\t I. Dan Melamed . \n\t', '\n\t\t 2003. Multitext grammars and synchronous parsers . \n\t', '\n\t\t In Proceedings ofthe Human Language Technology Conference and the North American Association for Computational Linguistics ( HLT-NAACL ) , pages 158\x96165 , Edmonton , Canada . \n\t', '\n\t\t I. Dan Melamed . \n\t', '\n\t\t 2004. Statistical machine translation by parsing . \n\t', '\n\t\t In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ( ACL ) , Barcelona , Spain . \n\t', '\n\t\t O. Rambow and G. Satta . \n\t', '\n\t\t 1996. Synchronous models of language . \n\t', '\n\t\t In Proceedings ofthe 34th Annual Meeting ofthe Association for Computational Linguistics ( ACL ) , Santa Cruz , USA . \n\t', '\n\t\t O. Rambow and G. Satta . \n\t', '\n\t\t 1999. Independent parallelism in finite copying parallel rewriting systems . \n\t', '\n\t\t Theoretical Computer Science , 223:87\x96120 , July . \n\t', '\n\t\t O. Rambow . \n\t', '\n\t\t 1995. Formal and Computational Aspects ofNatural Language Syntax . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , University of Pennsylvania , Philadelphia , PA . \n\t', '\n\t\t S. Shieber . \n\t', '\n\t\t 1994. Restricting the weak-generative capactiy of synchronous tree-adjoining grammars . \n\t', '\n\t\t Computational Intelligence , 10(4):371\x96386 . \n\t', '\n\t\t D. J. Weir . \n\t', '\n\t\t 1988 . \n\t', '\n\t\t Characterizing Mildly Context-Sensitive Grammar Formalisms . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , Department of Computer and Information Science , University of Pennsylvania . \n\t', '\n\t\t D. Wu . \n\t', '\n\t\t 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora . \n\t', '\n\t\t Computational Linguistics , 23(3):377\x96404 , September . \n\t', '\n\t\t D. H. . \n\t', '\n\t\t Younger . \n\t', '\n\t\t 1967. Recognition and parsing of context-free languages in time . \n\t', '\n\t\t Information and Control , 10(2):189\x96 208 , February . \n\t', '\n\t\t Identifying Agreement and Disagreement in Conversational Speech : Use of Bayesian Networks to Model Pragmatic Dependencies Michel Galley , Kathleen McKeown , Julia Hirschberg , and Elizabeth Shriberg SRI International Speech Technology and Research Laboratory 333 Ravenswood Avenue Menlo Park , CA 94025 , USA ees@speech.sri.com Columbia University Computer Science Department 1214 Amsterdam Avenue New York , NY 10027 , USA galley,kathy,julia @cs.columbia.edu Abstract We describe a statistical approach for modeling agreements and disagreements in conversational interaction . \n\t', '\n\t\t Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical , durational , and structural features that look both forward and backward in the discourse . \n\t', '\n\t\t We then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic influences of previous agreement or disagreement on the current utterance . \n\t', '\n\t\t Our approach achieves 86.9 % accuracy , a 4.9 % increase over previous work . \n\t', '\n\t\t 1 Introduction One of the main features of meetings is the occurrence of agreement and disagreement among participants . \n\t', '\n\t\t Often meetings include long stretches of controversial discussion before some consensus decision is reached . \n\t', '\n\t\t Our ultimate goal is automated summarization of multi-participant meetings and we hypothesize that the ability to automatically identify agreement and disagreement between participants will help us in the summarization task . \n\t', '\n\t\t For example , a summary might resemble minutes of meetings with major decisions reached ( consensus ) along with highlighted points of the pros and cons for each decision . \n\t', '\n\t\t In this paper , we present a method to automatically classify utterances as agreement , disagreement , or neither . \n\t', '\n\t\t Previous work in automatic identification of agreement/disagreement \n\t\t']",Positive
"['\n\t\t We build on their approach and show that we can get an improvement in accuracy when contextual information is taken into account . \n\t', '\n\t\t Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical , durational and structural features that look both forward and backward in the discourse . \n\t', '\n\t\t This allows us to acquire , and subsequently process , knowledge about who speaks to whom . \n\t', '\n\t\t We hypothesize that pragmatic features that center around previous agreement between speakers in the dialog will influence the determination of agreement/disagreement . \n\t', '\n\t\t For example , if a speaker disagrees with another person once in the conversation , is he more likely to disagree with him again ? \n\t', '\n\t\t We model context using Bayesian networks that allows capturing of these pragmatic dependencies . \n\t', '\n\t\t Our accuracy for classifying agreements and disagreements is 86.9 % , which is a 4.9 % improvement over \n\t\t']",Negative
"['\n\t\t In the following sections , we begin by describing the annotated corpus that we used for our experiments . \n\t', '\n\t\t We then turn to our work on identifying adjacency pairs . \n\t', '\n\t\t In the section on identification of agreement/disagreement , we describe the contextual features that we model and the implementation of the classifier . \n\t', '\n\t\t We close with a discussion of future work . \n\t', '\n\t\t 2 Corpus The ICSI Meeting corpus \n\t\t']",Positive
"['\n\t\t These are naturally occurring , regular weekly meetings of various ICSI research teams . \n\t', '\n\t\t Meetings in general run just under an hour each ; they have an average of 6.5 participants . \n\t', '\n\t\t These meetings have been labeled with adjacency pairs ( AP ) , which provide information about speaker interaction . \n\t', '\n\t\t They reflect the structure of conversations as paired utterances such as question- answer and offer-acceptance , and their labeling is used in our work to determine who are the addressees in agreements and disagreements . \n\t', '\n\t\t The annotation of the corpus with adjacency pairs is described in \n\t\t']",Positive
"['\n\t\t Seven of those meetings were segmented into spurts , defined as periods of speech that have no pauses greater than .5 second , and each spurt was labeled with one of the four categories : agreement , disagreement , backchannel , and other.1 We used spurt segmentation as our unit of analysis instead of sentence segmentation , because our ultimate goal is to build a system that can be fully automated , and in that respect , spurt segmentation is easy to obtain . \n\t', '\n\t\t Backchannels ( e.g. \x93uhhuh\x94 and \x93okay\x94 ) were treated as a separate category , since they are generally used by listeners to indicate they are following along , while not necessarily indicating agreement . \n\t', '\n\t\t The proportion of classes is the following : 11.9 % are agreements , 6.8 % are disagreements , 23.2 % are backchannels , and 58.1 % are others . \n\t', '\n\t\t Inter-labeler reliability estimated on 500 spurts with 2 labelers was considered quite acceptable , since the kappa coefficient was .63 \n\t\t']",Positive
['\n\t\t 3 Adjacency Pairs 3.1 Overview Adjacency pairs ( AP ) are considered fundamental units of conversational organization \n\t\t'],Positive
"['\n\t\t Their identification is central to our problem , since we need to know the identity of addressees in agreements and disagreements , and adjacency pairs provide a means of acquiring this knowledge . \n\t', '\n\t\t An adjacency pair is said to consist of two parts ( later referred to as A and B ) that are ordered , adjacent , and produced by different speakers . \n\t', '\n\t\t The first part makes the second one immediately relevant , as a question does with an answer , or an offer does with an acceptance . \n\t', '\n\t\t Extensive work in conversational analysis uses a less restrictive definition of adjacency pair that does not impose any actual adjacency requirement ; this requirement is problematic in many respects \n\t\t']",Positive
"['\n\t\t Even when APs are not directly adjacent , the same constraints between pairs and mechanisms for selecting the next speaker remain in place ( e.g. the case of embedded question and answer pairs ) . \n\t', '\n\t\t This relaxation on a strict adjacency requirement is particularly important in interactions of multiple speakers since other speakers have more opportunities to insert utterances between the two elements of the AP construction ( e.g. interrupted , abandoned or ignored utterances ; backchannels ; APs with multiple second elements , e.g. a question followed by answers of multiple speakers).2 Information provided by adjacency pairs can be used to identify the target of an agreeing or disagreeing utterance . \n\t', '\n\t\t We define the problem of AP 1Part of these annotated meetings were provided by the authors of \n\t\t']",Positive
"['\n\t\t 2The percentage of APs labeled in our data that have noncontiguous parts is about 21 % . \n\t', '\n\t\t identification as follows : given the second element ( B ) of an adjacency pair , determine who is the speaker of the first element ( A ) . \n\t', '\n\t\t A quite effective baseline algorithm is to select as speaker of utterance A the most recent speaker before the occurrence of utterance B . \n\t', '\n\t\t This strategy selects the right speaker in 79.8 % of the cases in the 50 meetings that were annotated with adjacency pairs . \n\t', '\n\t\t The next subsection describes the machine learning framework used to significantly outperform this already quite effective baseline algorithm . \n\t', '\n\t\t 3.2 Maximum Entropy Ranking We view the problem as an instance of statistical ranking , a general machine learning paradigm used for example in statistical parsing \n\t\t']",Positive
['\n\t\t We use maximum entropy modeling \n\t\t'],Positive
"['\n\t\t is represented here by only one variable for notational ease , but it possibly represents several lexical , durational , structural , and acoustic observations . \n\t', '\n\t\t Given feature functions and model parameters , the prob- ability of the maximum entropy model is defined as : The only role of the denominator is to ensure that is a proper probability distribution . \n\t', '\n\t\t It is defined as : To find the most probable speaker of part A , we use the following decision rule : Note that we have also attempted to model the problem as a binary classification problem where 3 The approach is generally called re-ranking in cases where candidates are assigned an initial rank beforehand . \n\t', '\n\t\t each speaker is either classified as speaker A or not , but we abandoned that approach , since it gives much worse performance . \n\t', '\n\t\t This finding is consistent with previous work \n\t\t']",Positive
"['\n\t\t 3.3 Features We will now describe the features used to train the maximum entropy model mentioned previously . \n\t', '\n\t\t To rank all speakers ( aside from the B speaker ) and to determine how likely each one is to be the A speaker of the adjacency pair involving speaker B , we use four categories of features : structural , durational , lexical , and dialog act ( DA ) information . \n\t', '\n\t\t For the remainder of this section , we will interchangeably use A to designate either the potential A speaker or the most recent utterance4 of that speaker , assuming the distinction is generally unambiguous . \n\t', '\n\t\t We use B to designate either the B speaker or the current spurt for which we need to identify a corresponding A part . \n\t', '\n\t\t The feature sets are listed in Table 1 . \n\t', '\n\t\t Structural features encode some helpful information regarding ordering and overlap of spurts . \n\t', '\n\t\t Note that with only the first feature listed in the table , the maximum entropy ranker matches exactly the performance of the baseline algorithm ( 79.8 % accuracy ) . \n\t', '\n\t\t Regarding lexical features , we used a count- based feature selection algorithm to remove many first-word and last-word features that occur infrequently and that are typically uninformative for the task at hand . \n\t', '\n\t\t Remaining features essentially contained function words , in particular sentence-initial indicators of questions ( \x93where\x94 , \x93when\x94 , and so on ) . \n\t', '\n\t\t Note that all features in Table 1 are \x93backward- looking\x94 , in the sense that they result from an analysis of context preceding B . \n\t', '\n\t\t For many of them , we built equivalent \x93forward-looking\x94 features that pertain to the closest utterance of the potential speaker A that follows part B . \n\t', '\n\t\t The motivation for extracting these features is that speaker A is generally expected to react if he or she is addressed , and thus , to take the floor soon after B is produced . \n\t', '\n\t\t 3.4 Results We used the labeled adjacency pairs of 50 meetings and selected 80 % of the pairs for training . \n\t', '\n\t\t To train the maximum entropy ranking model , we used the generalized iterative scaling algorithm \n\t\t']",Positive
"['\n\t\t Speaker ranking features Feature sets Accuracy Baseline 79.80 % Structural 83.97 % Durational 84.71 % Lexical 75.43 % Structural and durational 87.88 % All 89.38 % All ( only backward looking ) 86.99 % All ( Gaussian smoothing , FS ) 90.20 % Table 2 . \n\t', '\n\t\t Speaker ranking accuracy Table 2 summarizes the accuracy of our statistical ranker on the test data with different feature sets : the performance is 89.39 % when using all feature sets , and reaches 90.2 % after applying Gaussian smoothing and using incremental feature selection as described in \n\t\t']",Positive
"['\n\t\t We also wanted to determine if information about 6http://www.isi.edu/\x98ravichan/YASMET.html Structural features : number of speakers taking the floor between A and B number of spurts between A and B number of spurts of speaker B between A and B do A and B overlap ? \n\t', '\n\t\t Durational features : duration of A if A and B do not overlap : time separating A and B if they do overlap : duration of overlap seconds of overlap with any other speaker speech rate in A Lexical features : number of words in A number of content words in A ratio of words of A ( respectively B ) that are also in B ( respectively A ) ratio of content words of A ( respectively B ) that are also in B ( respectively A ) number of -grams present both in A and B ( we built 3 features for ranging from 2 to 4 ) first and last word of A number of instances at any position of A of each cue word listed in \n\t\t']",Positive
"['\n\t\t dialog acts ( DA ) helps the ranking task . \n\t', '\n\t\t If we hypothesize that only a limited set of paired DAs ( e.g. offer-accept , question-answer , and apology- downplay ) can be realized as adjacency pairs , then knowing the DA category of the B part and of all potential A parts should help in finding the most meaningful dialog act tag among all potential A parts ; for example , the question-accept pair is admittedly more likely to correspond to an AP than e.g. backchannel-accept . \n\t', '\n\t\t We used the DA annotation that we also had available , and used the DA tag sequence of part A and B as a feature .7 When we add the DA feature set , the accuracy reaches 91.34 % , which is only slightly better than our 90.20 % accuracy , which indicates that lexical , durational , and structural features capture most of the informativeness provided by DAs . \n\t', '\n\t\t This improved accuracy with DA information should of course not be considered as the actual accuracy of our system , since DA information is difficult to acquire automatically \n\t\t']",Positive
"['\n\t\t 4 Agreements and Disagreements 4.1 Overview This section focusses on the use of contextual information , in particular the influence of previous agreements and disagreements and detected adjacency pairs , to improve the classification of agreements and disagreements . \n\t', '\n\t\t We first define the classification problem , then describe non-contextual features , provide some empirical evidence justifying our choice of contextual features , and finally evaluate the classifier . \n\t', '\n\t\t 4.2 Agreement/Disagreement Classification We need to first introduce some notational conventions and define the classification problem with the agreement/disagreement tagset . \n\t', '\n\t\t In our classification problem , each spurt among the spurts of a meeting must be assigned a tag AGREE DISAGREE BACKCHANNEL OTHER . \n\t', '\n\t\t To specify the speaker of the spurt ( e.g. speaker B ) , the notation will sometimes be augmented to incorporate speaker information , as with , and to designate the addressee of B ( e.g. listener A ) , we will use the notation . \n\t', '\n\t\t For example , AGREE simply means that B agrees with A in the spurt of index . \n\t', '\n\t\t This notation makes it obvious that we do not necessarily assume that agreements and disagreements are reflexive 7The annotation of DA is particularly fine-grained with a choice of many optional tags that can be associated with each DA . \n\t', '\n\t\t To deal with this problem , we used various scaled-down versions of the original tagset . \n\t', '\n\t\t relations . \n\t', '\n\t\t We define : as the tag of the most recent spurt before that is produced by Y and addresses X . \n\t', '\n\t\t This definition will help our multi-party analyses of agreement and disagreement behaviors . \n\t', '\n\t\t 4.3 Local Features Many of the local features described in this subsection are similar in spirit to the ones used in the previous work of \n\t\t']",Positive
"['\n\t\t We did not use acoustic features , since the main purpose of the current work is to explore the use of contextual information . \n\t', '\n\t\t Table 3 lists the features that were found most helpful at identifying agreements and disagreements . \n\t', '\n\t\t Regarding lexical features , we selected a list of lexical items we believed are instrumental in the expression of agreements and disagreements : agreement markers , e.g. \x93yes\x94 and \x93right\x94 , as listed in \n\t\t']",Positive
"['\n\t\t We incorporated a set of durational features that were described in the literature as good predictors of agreements : utterance length distinguishes agreement from disagreement , the latter tending to be longer since the speaker elaborates more on the reasons and circumstances of her disagreement than for an agreement \n\t\t']",Positive
"['\n\t\t Duration is also a good predictor of backchannels , since they tend to be quite short . \n\t', '\n\t\t Finally , a fair amount of silence and filled pauses is sometimes an indicator of disagreement , since it is a dispreferred response in most social contexts and can be associated with hesitation \n\t\t']",Positive
"['\n\t\t 4.4 Contextual Features : An Empirical Study We first performed several empirical analyses in order to determine to what extent contextual information helps in discriminating between agreement and disagreement . \n\t', '\n\t\t By integrating the interpretation of the pragmatic function of an utterance into a wider context , we aim to detect cases of mismatch between a correct pragmatic interpretation and the surface form of the utterance , e.g. the case of weak or \x93empty\x94 agreement , which has some properties of downright agreement ( lexical items of positive polarity ) , but which is commonly considered to be a disagreement \n\t\t']",Positive
"['\n\t\t While the actual classification problem incorporates four classes , the BACKCHANNEL class is ig- Table 3 . \n\t', '\n\t\t Local features for agreement and disagreement classification nored here to make the empirical study easier to interpret . \n\t', '\n\t\t We assume in that study that accurate AP labeling is available , but for the purpose of building and testing a classifier , we use only automatically extracted adjacency pair information . \n\t', '\n\t\t We tested the validity of four pragmatic assumptions : 1. previous tag dependency : a tag is influ- enced by its predecessor 2. same-interactants previous tag dependency : a tag is influenced by , the most recent tag of the same speaker addressing the same listener ; for example , it might be reasonable to assume that if speaker B disagrees with A , B is likely to disagree with A in his or her next speech addressing A. 3. reflexivity : a tag is influenced by ; the assumption is that is influenced by the polarity ( agreement or disagreement ) of what A said last to B. 4. transitivity : assuming there is a speaker for which exists , then a tag is influ- enced by and ;an example of such an influence is a case where speaker first agrees with , then speaker disagrees with , from which one could possi bly conclude that is actually in disagreement with . \n\t', '\n\t\t Table 4 presents the results of our empirical evaluation of the first three assumptions . \n\t', '\n\t\t For comparison , the distribution of classes is the following : 18.8 % are agreements , 10.6 % disagreements , and 70.6 % other . \n\t', '\n\t\t The dependencies empirically evaluated in the two last columns are non-local ; they create dependencies between spurts separated by an arbitrarily long time span . \n\t', '\n\t\t Such long range dependencies are often undesirable , since the influence of one spurt on the other is often weak or too difficult to capture with our model . \n\t', '\n\t\t Hence , we made a Markov assumption by limiting context to an arbitrarily chosen value . \n\t', '\n\t\t In this analysis subsection and for all classification results presented thereafter , we used a value of . \n\t', '\n\t\t The table yields some interesting results , showing quite significant variations in class distribution when it is conditioned on various types of contextual information . \n\t', '\n\t\t We can see for example , that the proportion of agreements and disagreements ( re- spectively 18.8 % and 10.6 % ) changes to 13.9 % and 20.9 % respectively when we restrict the counts to spurts that are preceded by a DISAGREE . \n\t', '\n\t\t Similarly , that distribution changes to 21.3 % and 7.3 % when the previous tag is an AGREE . \n\t', '\n\t\t The variable is even more noticeable between probabilities and . \n\t', '\n\t\t In 26.1 % of the cases where a given speaker B disagrees with A , he or she will continue to disagree in the next exchange involving the same speaker and the same listener . \n\t', '\n\t\t Similarly with the same probability distribution , a tendency to agree is confirmed in 25 % of the cases . \n\t', '\n\t\t The results in the last column are quite different from the two preceding ones . \n\t', '\n\t\t While agreements in response to agreements ( AGREE AGREE ) are slightly less probable than agreements without conditioning on any previous tag ( AGREE ) , the probability of an agreement produced in response to a disagreement is quite high ( with 23.4 % ) , even higher than the proportion of agreements in the entire data ( 18.8 % ) . \n\t', '\n\t\t This last result would arguably be quite different with more quarrelsome meeting participants . \n\t', '\n\t\t Table 5 represents results concerning the fourth pragmatic assumption . \n\t', '\n\t\t While none of the results characterize any strong conditioning of by and , we can nevertheless notice some interesting phenomena . \n\t', '\n\t\t For example , there is a tendency for agreements to be transitive , i.e. if X agrees with A and B agrees with X within a limited segment of speech , then agreement between B and A is con- Structural features : is the previous/next spurt of the same speaker ? \n\t', '\n\t\t is the previous/next spurt involving the same B speaker ? \n\t', '\n\t\t Durational features : duration of the spurt seconds of overlap with any other speaker seconds of silence during the spurt speech rate in the spurt Lexical features : number of words in the spurt number of content words in the spurt perplexity of the spurt with respect to four language models , one for each class first and last word of the spurt number of instances of adjectives with positive polarity \n\t\t']",Positive
"['\n\t\t The only slightly surprising result appears in the last column of the table , from which we cannot conclude that disagreement with a disagreement is equivalent to agreement . \n\t', '\n\t\t This might be explained by the fact that these sequences of agreement and disagreement do not necessarily concern the same propositional content . \n\t', '\n\t\t The probability distributions presented here are admittedly dependent on the meeting genre and particularly speaker personalities . \n\t', '\n\t\t Nonetheless , we believe this model can as well be used to capture salient interactional patterns specific to meetings with different social dynamics . \n\t', '\n\t\t We will next discuss our choice of a statistical model to classify sequence data that can deal with non-local label dependencies , such as the ones tested in our empirical study . \n\t', '\n\t\t 4.5 Sequence Classification with Maximum Entropy Models Extensive research has targeted the problem of labeling sequence information to solve a variety of problems in natural language processing . \n\t', '\n\t\t Hidden Markov models ( HMM ) are widely used and considerably well understood models for sequence labeling . \n\t', '\n\t\t Their drawback is that , as most generative models , they are generally computed to maximize the joint likelihood of the training data . \n\t', '\n\t\t In order to define a probability distribution over the sequences of observation and labels , it is necessary to enumerate all possible sequences of observations . \n\t', '\n\t\t Such enumeration is generally prohibitive when the model incorporates many interacting features and long-range dependencies ( the reader can find a discussion of the problem in \n\t\t']",Positive
"['\n\t\t Conditional models address these concerns . \n\t', '\n\t\t Conditional Markov models ( CMM ) \n\t\t']",Positive
"['\n\t\t In a left-to-right CMM as shown in Figure 1(a) , the probability of a sequence of L tags is decomposed as : is the vector of observations and each is the index of a spurt . \n\t', '\n\t\t The probability distribution associated with each state of the Markov chain only depends on the preceding tag and the local observation . \n\t', '\n\t\t However , in order to incorporate more than one label dependency and , in particular , to take into account the four pragmatic Figure 1. ( a ) Left-to-right CMM . \n\t', '\n\t\t ( b ) More complex Bayesian network . \n\t', '\n\t\t Assuming for example that and , there is then a direct dependency be- tween and , and the probability model becomes . \n\t', '\n\t\t This is a simplifying example ; in practice , each label is dependent on a fixed number of other labels . \n\t', '\n\t\t contextual dependencies discussed in the previous subsection , we must augment the structure of our model to obtain a more general one . \n\t', '\n\t\t Such a model is shown in Figure 1(b) , a Bayesian network model that is well-understood and that has precisely defined semantics . \n\t', '\n\t\t To this Bayesian network representation , we apply maximum entropy modeling to define a probability distribution at each node ( ) dependent on the observation variable and the five contextual tags used in the four pragmatic dependencies . \n\t', '\n\t\t 8 For notational simplicity , the contextual tags representing these pragmatic dependencies are represented here as a vector ( ,, and so on ) . \n\t', '\n\t\t Given feature functions Again , the only role of the denominator is to ensure that sums to 1 , and need not be computed when searching for the most probable tags . \n\t', '\n\t\t Note that in our case , the structure of the Bayesian network is known and need not be inferred , since AP identification is performed before the actual agreement and disagreement classification . \n\t', '\n\t\t Since tag sequences are known during training , the inference of a model for sequence labels is no more difficult than inferring a model in a non-sequential case . \n\t', '\n\t\t We compute the most probable sequence by performing a left-to-right decoding using a beam search . \n\t', '\n\t\t The algorithm is exactly the same as the one described in \n\t\t']",Positive
"['\n\t\t We used a large beam of size =100 , which is not computationally prohibitive , since the tagset contains only four ele- 8The transitivity dependency is conditioned on two tags , while all others on only one . \n\t', '\n\t\t These five contextual tags are defaulted to OTHER when dependency spans exceed the threshold of . \n\t', '\n\t\t c1h c2h c1h c2 c ( a ) ( b ) d1 d2 d1 , d2 d ( both local and contextual , like previous tag features ) and model parameters , the probability of the model is defined as : AGREE AGREE .213 .250 .175 OTHER AGREE .713 .643 .737 DISAGREE AGREE .073 .107 .088 AGREE OTHER .187 .115 .177 OTHER OTHER .714 .784 .710 DISAGREE OTHER .098 .100 .113 AGREE DISAGREE .139 .087 .234 OTHER DISAGREE .651 .652 .638 DISAGREE DISAGREE .209 .261 .128 Table 4 . \n\t', '\n\t\t Contextual dependencies ( previous tag , same-interactants previous tag , and reflexivity ) , where and AGREE AGREE AGREE DISAGREE DISAGREE DISAGREE DISAGREE AGREE AGREE OTHER DISAGREE .225 .658 .117 .147 .677 .177 .131 .683 .186 .152 .668 .180 Table 5 . \n\t', '\n\t\t Contextual dependencies ( transitivity ) ments . \n\t', '\n\t\t Note however that this algorithm can lead to search errors . \n\t', '\n\t\t An alternative would be to use a variant of the Viterbi algorithm , which was successfully used in \n\t\t']",Positive
"['\n\t\t 4.6 Results We had 8135 spurts available for training and testing , and performed two sets of experiments to evaluate the performance of our system . \n\t', '\n\t\t The tools used to perform the training are the same as those described in section 3.4 . \n\t', '\n\t\t In the first set of experiments , we reproduced the experimental setting of \n\t\t']",Positive
"['\n\t\t Performance is reported in Table 6 . \n\t', '\n\t\t In the second set of experiments , we aimed at reducing the expected variance of our experimental results and performed N-fold cross-validation in a four-way classification task , at each step retaining the hand-labeled data of a meeting for testing and the rest of the data for training . \n\t', '\n\t\t Table 7 summarizes the performance of our classifier with the different feature sets in this classification task , distinguishing the case where the four label-dependency pragmatic features are available during decoding from the case where they are not . \n\t', '\n\t\t First , the analysis of our results shows that with our three local feature sets only , we obtain substantially better results than \n\t\t']",Negative
['\n\t\t This Feature sets Accuracy \n\t\t'],Positive
"['\n\t\t No label dep . \n\t', '\n\t\t Lexical 83.54 % 82.62 % Structural , durational 62.10 % 58.86 % All 84.07 % 83.11 % Table 7. 4-way classification accuracy might be due to some additional features the latter work didn\x92t exploit ( e.g. structural features and adjective polarity ) , and to the fact that the learning algorithm used in our experiments might be more accurate than decision trees in the given task . \n\t', '\n\t\t Second , the table corroborates the findings of \n\t\t']",Positive
"['\n\t\t Finally , we observe that by incorporating label-dependency features representing pragmatic influences , we further improve the performance ( about 1 % in Table 7 ) . \n\t', '\n\t\t This seems to indicate that modeling label dependencies in our classification problem is useful . \n\t', '\n\t\t 5 Conclusion We have shown how identification of adjacency pairs can help in designing features representing pragmatic dependencies between agreement and disagreement labels . \n\t', '\n\t\t These features are shown to be informative and to help the classification task , yielding a substantial improvement ( 1.3 % to reach a 86.9 % accuracy in three-way classification ) . \n\t', '\n\t\t We also believe that the present work may be useful in other computational pragmatic research focusing on multi-party dialogs , such as dialog act ( DA ) classification . \n\t', '\n\t\t Most previous work in that area is limited to interaction between two speakers ( e.g. . \n\t', '\n\t\t Switchboard , \n\t\t']",Negative
"['\n\t\t When more than two speakers are involved , the question of who is the addressee of an utterance is crucial , since it generally determines what DAs are relevant after the addressee\x92s last utterance . \n\t', '\n\t\t So , knowledge about adjacency pairs is likely to help DA classification . \n\t', '\n\t\t In future work , we plan to extend our inference process to treat speaker ranking ( i.e. AP identification ) and agreement/disagreement classification as a single , joint inference problem . \n\t', '\n\t\t Contextual information about agreements and disagreements can also provide useful cues regarding who is the addressee of a given utterance . \n\t', '\n\t\t We also plan to incorporate acoustic features to increase the robustness of our procedure in the case where only speech recognition output is available . \n\t', '\n\t\t Acknowledgments We are grateful to Mari Ostendorf and Dustin Hillard for providing us with their agreement and disagreement labeled data . \n\t', '\n\t\t This material is based on research supported by the National Science Foundation under Grant No . \n\t', '\n\t\t IIS-012196 . \n\t', '\n\t\t Any opinions , findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation . \n\t', '\n\t\t References A. Berger , S. Della Pietra , and V Della Pietra . \n\t', '\n\t\t 1996. A maximum entropy approach to natural language processing . \n\t', '\n\t\t Computational Linguistics , 22(1):39\x9672 . \n\t', '\n\t\t J. Cohen . \n\t', '\n\t\t 1960. A coefficient of agreement for nominal scales . \n\t', '\n\t\t Educational and Psychological measurements , 20:37\x9646 . \n\t', '\n\t\t S. Cohen . \n\t', '\n\t\t 2002. A computerized scale for monitoring levels of agreement during a conversation . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 26th Penn Linguistics Colloquium . \n\t', '\n\t\t M. Collins . \n\t', '\n\t\t 2000. Discriminative reranking for nat- ural language parsing . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 17th Interna- tional Conf . \n\t', '\n\t\t on Machine Learning , pages 175\x96 182. J. N. Darroch and D. Ratcliff . \n\t', '\n\t\t 1972. Generalized iterative scaling for log-linear models . \n\t', '\n\t\t Annals of Mathematical Statistics , 43:1470\x961480 . \n\t', '\n\t\t R. Dhillon , S. Bhagat , H. Carvey , and E. Shriberg . \n\t', '\n\t\t 2004. Meeting recorder project : Dialog act labeling guide . \n\t', '\n\t\t Technical Report TR-04-002 , ICSI . \n\t', '\n\t\t V. Hatzivassiloglou and K. McKeown . \n\t', '\n\t\t 1997. Predicting the semantic orientation of adjectives . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ofACL . \n\t', '\n\t\t D. Hillard , M. Ostendorf , and E Shriberg . \n\t', '\n\t\t 2003. Detection of agreement vs. disagreement in meetings : training with unlabeled data . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of HL T/NAACL . \n\t', '\n\t\t J. Hirschberg and D. Litman . \n\t', '\n\t\t 1994. Empirical studies on the disambiguation of cue phrases . \n\t', '\n\t\t Computational Linguistics , 19(3):501\x96530 . \n\t', '\n\t\t A. Janin , D. Baron , J. Edwards , D. Ellis , D. Gelbart , N. Morgan , B. Peskin , T. Pfau , E. Shriberg , A. Stolcke , and C. Wooters . \n\t', '\n\t\t 2003. The ICSI meeting corpus . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of ICASSP-03 , Hong Kong . \n\t', '\n\t\t D. Klein and C. D. Manning . \n\t', '\n\t\t 2002. Conditional structure versus conditional estimation in NLP models . \n\t', '\n\t\t Technical report . \n\t', '\n\t\t S. Levinson . \n\t', '\n\t\t 1983. Pragmatics . \n\t', '\n\t\t Cambridge University Press . \n\t', '\n\t\t A. McCallum , D. Freitag , and F. Pereira . \n\t', '\n\t\t 2000. Maximum entropy markov models for information extraction and segmentation . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of ICML . \n\t', '\n\t\t A. Pomerantz . \n\t', '\n\t\t 1984. Agreeing and disagreeing with assessments : some features of preferred/dispreferred turn shapes . \n\t', '\n\t\t In J.M. Atkinson and J.C. Heritage , editors , Structures of Social Action , pages 57\x96101 . \n\t', '\n\t\t A. Ratnaparkhi . \n\t', '\n\t\t 1996. A maximum entropy partof-speech tagger . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ofEMNLP . \n\t', '\n\t\t D. Ravichandran , E. Hovy , and F. J. Och . \n\t', '\n\t\t 2003. Statistical QA - classifier vs re-ranker : What\x92s the difference ? \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the ACL Workshop on Multilingual Summarization and Question Answering . \n\t', '\n\t\t E. A. Schegloff and H Sacks . \n\t', '\n\t\t 1973. Opening up closings . \n\t', '\n\t\t Semiotica , 7-4:289\x96327 . \n\t', '\n\t\t E. Shriberg , R. Dhillon , S. Bhagat , J. Ang , and H. Carvey . \n\t', '\n\t\t 2004. The ICSI meeting recorder dialog act ( MRDA ) corpus . \n\t', '\n\t\t In SIGdial Workshop on Discourse and Dialogue , pages 97\x96100 . \n\t', '\n\t\t A. Stolcke , K. Ries , N. Coccaro , E. Shriberg , R. Bates , D. Jurafsky , P. Taylor , R. Martin , C. Van Ess-Dykema , and M. Meteer . \n\t', '\n\t\t 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech . \n\t', '\n\t\t Computational Linguistics , 26(3):339\x96373 . \n\t', '\n\t\t Using Conditional Random Fields to Predict Pitch Accents in Conversational Speech Michelle L. Gregory Linguistics Department University at Buffalo Buffalo , NY 14260 mgregory@buffalo.edu Yasemin Altun Department of Computer Science Brown University Providence , RI 02912 altun@cs.brown.edu Abstract The detection of prosodic characteristics is an important aspect of both speech synthesis and speech recognition . \n\t', '\n\t\t Correct placement of pitch accents aids in more natural sounding speech , while automatic detection of accents can contribute to better word- level recognition and better textual understanding . \n\t', '\n\t\t In this paper we investigate probabilistic , contextual , and phonological factors that influence pitch accent placement in natural , conversational speech in a sequence labeling setting . \n\t', '\n\t\t We introduce Conditional Random Fields ( CRFs ) to pitch accent prediction task in order to incorporate these factors efficiently in a sequence model . \n\t', '\n\t\t We demonstrate the usefulness and the incremental effect of these factors in a sequence model by performing experiments on hand labeled data from the Switchboard Corpus . \n\t', '\n\t\t Our model outperforms the baseline and previous models of pitch accent prediction on the Switchboard Corpus . \n\t', '\n\t\t 1 Introduction The suprasegmental features of speech relay critical information in conversation . \n\t', '\n\t\t Yet , one of the major roadblocks to natural sounding speech synthesis has been the identification and implementation of prosodic characteristics . \n\t', '\n\t\t The difficulty with this task lies in the fact that prosodic cues are never absolute ; they are relative to individual speakers , gender , dialect , discourse context , local context , phonological environment , and many other factors . \n\t', '\n\t\t This is especially true of pitch accent , the acoustic cues that make one word more prominent than others in an utterance . \n\t', '\n\t\t For example , a word with a fundamental frequency ( f0 ) of 120 Hz would likely be quite prominent in a male speaker , but not for a typical female speaker . \n\t', '\n\t\t Likewise , the accent on the utterance \x94Jon\x92s leaving.\x94 is critical in determining whether it is the answer to the question \x94Who is leaving?\x94 ( \x94JON\x92s leaving.\x94 ) or \x94What is Jon doing?\x94 ( \x94Jon\x92s LEAVING.\x94 ) . \n\t', '\n\t\t Accurate pitch accent prediction lies in the successful combination of as many of the con- textual variables as possible . \n\t', '\n\t\t Syntactic information such as part of speech has proven to be a successful predictor of accentuation \n\t\t']",Positive
"['\n\t\t In general , function words are not accented , while content words are . \n\t', '\n\t\t Various measures of a word\x92s informativeness , such as the information content ( IC ) of a word \n\t\t']",Positive
"['\n\t\t However , in open topic conversational speech , accent is very unpredictable . \n\t', '\n\t\t Part of speech and the informativeness of a word do not capture all aspects of accentuation , as we see in this example taken from Switchboard , where a function word gets accented ( accented words are in uppercase ) : I , I have STRONG OBJECTIONS to THAT . \n\t', '\n\t\t Accent is also influenced by aspects of rhythm and timing . \n\t', '\n\t\t The length of words , in both number of phones and normalized duration , affect its likelihood of being accented . \n\t', '\n\t\t Additionally , whether the immediately surrounding words bear pitch accent also affect the likelihood of accentuation . \n\t', '\n\t\t In other words , a word that might typically be accented may be unaccented because the surrounding words also bear pitch accent . \n\t', '\n\t\t Phrase boundaries seem to play a role in accentuation as well . \n\t', '\n\t\t The first word of intonational phrases ( IP ) is less likely to be accented while the last word of an IP tends be accented . \n\t', '\n\t\t In short , accented words within the same IP are not independent of each other . \n\t', '\n\t\t Previous work on pitch accent prediction , however , neglected the dependency between labels . \n\t', '\n\t\t Different machine learning techniques , such as decision trees \n\t\t']",Negative
['\n\t\t One exception to this line of research is the use of Hidden Markov Models ( HMM ) for pitch accent prediction \n\t\t'],Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t Until recently , HMMs were the predominant formalism to model label sequences . \n\t', '\n\t\t However , they have two major shortcomings . \n\t', '\n\t\t They are trained non-discriminatively using maximum likelihood estimation to model the joint probability of the observation and label sequences . \n\t', '\n\t\t Also , they require questionable independence assumptions to achieve efficient inference and learning . \n\t', '\n\t\t Therefore , variables used in Hidden Markov models of pitch accent prediction have been very limited , e.g. part of speech and frequency \n\t\t']",Negative
"['\n\t\t Discriminative learning methods , such as Maximum Entropy Markov Models \n\t\t']",Positive
"['\n\t\t Among these methods , CRFs is the most common technique used in NLP and has been successfully applied to Part-of-Speech Tagging \n\t\t']",Positive
"['\n\t\t The goal of this study is to better identify which words in a string of text will bear pitch accent . \n\t', '\n\t\t Our contribution is two-fold : employing new predictors and utilizing a discriminative model . \n\t', '\n\t\t We combine the advantages of probabilistic , syntactic , and phonological predictors with the advantages of modeling pitch accent in a sequence labeling setting using CRFs \n\t\t']",Positive
"['\n\t\t The rest of the paper is organized as follows : In Section 2 , we introduce CRFs . \n\t', '\n\t\t Then , we describe our corpus and the variables in Section 3 and Section 4 . \n\t', '\n\t\t We present the experimental setup and report results in Section 5 . \n\t', '\n\t\t Finally , we discuss our results ( Section 6 ) and conclude ( Section 7 ) . \n\t', '\n\t\t 2 Conditional Random Fields CRFs can be considered as a generalization of logistic regression to label sequences . \n\t', '\n\t\t They define a conditional probability distribution of a label sequence y given an observation sequence x . \n\t', '\n\t\t In this paper , x = ( x 1 , x2 , . \n\t', '\n\t\t .. , xn ) denotes a sentence of length n and y = ( y1 , y2 , . \n\t', '\n\t\t .. , yn ) denotes the label sequence corresponding to x . \n\t', '\n\t\t In pitch accent prediction , xt is a word and yt is a binary label denoting whether xt is accented or not . \n\t', '\n\t\t CRFs specify a linear discriminative function F parameterized by A over a feature representation of the observation and label sequence IF ( x , y ) . \n\t', '\n\t\t The model is assumed to be stationary , thus the feature representation can be partitioned with respect to positions t in the sequence and linearly combined with respect to the importance of each feature Ok , denoted by Ak . \n\t', '\n\t\t Then the discriminative function can be stated as in Equation 1 : F ( x , y ; A ) = E ~A , IFt ( x , y ) ) ( 1 ) t Then , the conditional probability is given by p(ylx ; A ) = Z(x1 A ) F(x , y ; A ) ( 2 ) where Z ( x , A ) = ~y¯ F ( x , ¯y ; A ) is a normaliza- tion constant which is computed by summing over all possible label sequences y¯ of the observation sequence x . \n\t', '\n\t\t We extract two types of features from a sequence pair : 1 . \n\t', '\n\t\t Current label and information about the observation sequence , such as part-of-speech tag of a word that is within a window centered at the word currently labeled , e.g. . \n\t', '\n\t\t Is the current word pitch accented and the part-of-speech tag of the previous word=Noun ? \n\t', '\n\t\t Current label and the neighbors of that label , i.e. features that capture the inter-label dependencies , e.g. . \n\t', '\n\t\t Is the current wordpitch accented and the previous word not accented ? \n\t', '\n\t\t Since CRFs condition on the observation sequence , they can efficiently employ feature representations that incorporate overlapping features , i.e. multiple interacting features or long-range dependencies of the observations , as opposed to HMMs which generate observation sequences . \n\t', '\n\t\t In this paper , we limit ourselves to 1-order Markov model features to encode inter-label dependencies . \n\t', '\n\t\t The information used to encode the observation-label dependencies is explained in detail in Section 4 . \n\t', '\n\t\t In CRFs , the objective function is the log-loss of the model with A parameters with respect to a training set D . \n\t', '\n\t\t This function is defined as the negative sum of the conditional probabilities of each training label sequence yi , given the observation sequence xi , where D - { ( xi , yi ) : i = 1 , ... , m } . \n\t', '\n\t\t CRFs are known to overfit , especially with noisy data if not regularized . \n\t', '\n\t\t To overcome this problem , we penalize the objective function by adding a Gaussian prior ( a term proportional to the squared norm 11A112 ) as suggested in \n\t\t']",Positive
"['\n\t\t Then the loss function is given as : logp(yi1xi ; A ) + 1c11A112 F(xi , yi ; A ) + log Z(xi , A ) + 1c11A112 ( 3 ) where c is a constant . \n\t', '\n\t\t \n\t\t']",Negative
"['\n\t\t However , gradient-based methods have often found to be more efficient for minimizing Equation 3 \n\t\t']",Positive
"['\n\t\t In this paper , we use the conjugate gradient descent method to optimize the above objective function . \n\t', '\n\t\t The gradients are computed as in Equation 4 : VAL = Em E Ep [ IFt ( xi , y ) ] \x97 IFt ( xi , yi ) i t + cA ( 4 ) where the expectation is with respect to all possible label sequences of the observation sequence xi and can be computed using the forward backward algorithm . \n\t', '\n\t\t Given an observation sequence x , the best label sequence is given by : F(x , y ; \x88A ) ( 5 ) where A\x88 is the parameter vector that minimizes L(A ; D ) . \n\t', '\n\t\t The best label sequence can be identified by performing the Viterbi algorithm . \n\t', '\n\t\t 3 Corpus The data for this study were taken from the Switchboard Corpus \n\t\t']",Positive
"['\n\t\t Participants were both male and female and represented all major dialects of American English . \n\t', '\n\t\t We used a portion of this corpus that was phonetically hand- transcribed \n\t\t']",Positive
"['\n\t\t Fragments contained seven words on average . \n\t', '\n\t\t Additionally , each word was coded for probabilistic and contextual information , such as word frequency , conditional probabilities , the rate of speech , and the canonical pronunciation \n\t\t']",Positive
"['\n\t\t The dataset used in all analysis in this study consists of only the first hour of the database , comprised of 1,824 utterances with 13,190 words . \n\t', '\n\t\t These utterances were hand coded for pitch accent and intonational phrase brakes . \n\t', '\n\t\t 3.1 Pitch Accent Coding The utterances were hand labeled for accents and boundaries according to the Tilt Intonational Model \n\t\t']",Positive
"['\n\t\t This model is characterized by a series of intonational events : accents and boundaries . \n\t', '\n\t\t Labelers were instructed to use duration , amplitude , pausing information , and changes in f0 to identify events . \n\t', '\n\t\t In general , labelers followed the basic conventions of EToBI for coding \n\t\t']",Positive
"['\n\t\t However , the Tilt coding scheme was simplified . \n\t', '\n\t\t Accents were coded as either major or minor ( and some rare level accents ) and breaks were either rising or falling . \n\t', '\n\t\t Agreement for the Tilt coding was reported at 86 % . \n\t', '\n\t\t The CU coding also used a simplified EToBI coding scheme , with accent types conflated and only major breaks coded . \n\t', '\n\t\t Accent and break coding pair-wise agreement was between 85- 95 % between coders , with a kappa r. of 71%-74 % where r. is the difference between expected agreement and actual agreement . \n\t', '\n\t\t 4 Variables The label we were predicting was a binary distinction of accented or not . \n\t', '\n\t\t The variables we used for prediction fall into three main categories : syntactic , probabilistic variables , which include word frequency and collocation measures , and phonological variables , which capture aspects of rhythm and timing that affect accentuation . \n\t', ""\n\t\t 4.1 Syntactic variables The only syntactic category we used was a four- way classification for hand-generated part of speech ( POS ) : Function , Noun , Verb , Other , where Other includes all adjectives and adverbs ' . \n\t"", '\n\t\t Table 1 gives the percentage of accented and unaccented items by POS . \n\t', ""\n\t\t ' We also tested a categorization of 14 distinct part of speech classes , but the results did not improve , so we only report on the four-way classification . \n\t"", '\n\t\t L(A ; D ) = \x97 Em i = \x97 Em i y\x88 = arg max Y Accented Unaccented Function 21 % 79 % Verb 59 % 41 % Noun 30 % 70 % Other 49 % 51 % Table 1 : Percentage of accented and unaccented items by POS . \n\t', '\n\t\t Variable Definition Example Unigram log p(wz) and , I Bigram log p(wz l wz_ 1 ) roughing it Rev Bigram log p(wzlwz+1) rid of Joint log p(wz_1 , wz ) and I Rev Joint log p(wz , wz+1 ) and I Table 2 : Definition of probabilistic variables . \n\t', '\n\t\t 4.2 Probabilistic variables Following a line of research that incorporates the information content of a word as well as collocation measures \n\t\t']",Positive
"['\n\t\t The probabilistic variables we used were the unigram frequency , the predictability of a word given the preceding word ( bigram ) , the predictability of a word given the following word ( reverse bigram ) , the joint probability of a word with the preceding ( joint ) , and the joint probability of a word with the following word ( reverse joint ) . \n\t', '\n\t\t Table 2 provides the definition for these , as well as high probability examples from the corpus ( the emphasized word being the current target ) . \n\t', '\n\t\t Note all probabilistic variables were in log scale . \n\t', '\n\t\t The values for these probabilities were obtained using the entire 2.4 million words of SWBD2 . \n\t', '\n\t\t Table 3 presents the Spearman\x92s rank correlation coefficient between the probabilistic measures and accent \n\t\t']",Positive
"['\n\t\t These values indicate the strong correlation of accents to the probabilistic variables . \n\t', '\n\t\t As the probability increases , the chance of an accent decreases . \n\t', '\n\t\t Note that all values are significant at the p < .001 level . \n\t', '\n\t\t We also created a combined part of speech and unigram frequency variable in order to have a variable that corresponds to the variable used in ( Pan 2Our current implementation of CRF only takes categorical variables , thus for the experiments , all probabilistic variables were binned into 5 equal categories . \n\t', '\n\t\t We also tried more bins and produced similar results , so we only report on the 5-binned categories . \n\t', '\n\t\t We computed correlations between pitch accent and the original 5 variables as well as the binned variables and they are very similar . \n\t', '\n\t\t Variables Spearman\x92s p Unigram -.451 Bigram -.309 Reverse Bigram -.383 Joint -.207 Reverse joint -.265 Table 3 : Spearman\x92s correlation values for the probabilistic measures . \n\t', '\n\t\t and McKeown , 1999 ) . \n\t', '\n\t\t 4.3 Phonological variables The last category of predictors , phonological variables , concern aspects of rhythm and timing of an utterance . \n\t', '\n\t\t We have two main sources for these variables : those that can be computed solely from a string of text ( textual ) , and those that require some sort of acoustic information ( acoustic ) . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t While Sun was concerned with predicting accented syllables , some of the same variables apply to word level targets as well . \n\t', '\n\t\t For our textual phonological features , we included the number of syllables in a word and the number of phones ( both in citation form as well as transcribed form ) . \n\t', '\n\t\t Instead of position in a sentence , we used the position of the word in an utterance since the fragments do not necessarily correspond to sentences in the database we used . \n\t', '\n\t\t We also made use of the utterance length . \n\t', '\n\t\t Below is the list of our textual features : \x95 Number of canonical syllables \x95 Number of canonical phones \x95 Number of transcribed phones \x95 The length of the utterance in number of words \x95 The position of the word in the utterance The main purpose of this study is to better predict which words in a string of text receive accent . \n\t', '\n\t\t So far , all of our predictors are ones easily computed from a string of text . \n\t', '\n\t\t However , we have included a few variables that affect the likelihood of a word being accented that require some acoustic data . \n\t', '\n\t\t To the best of our knowledge , these features have not been used in acoustic models of pitch accent prediction . \n\t', '\n\t\t These features include the duration of the word , speech rate , and following intonational phrase boundaries . \n\t', '\n\t\t Given the nature of the SWBD corpus , there are many disfluencies . \n\t', '\n\t\t Thus , we also Feature X2 Sig canonical syllables 1636 p < .001 canonical phones 2430 p < .001 transcribed phones 2741 p < .001 utt length 80 p < .005 utt position 295 p < .001 duration 3073 p < .001 speech rate 101 p < .001 following pause 27 p < .001 foll filled pause 328 p < .001 foll IP boundary 1047 p < .001 Table 4 : Significance of phonological features on pitch accent prediction . \n\t', '\n\t\t included following pauses and filled pauses as predictors . \n\t', '\n\t\t Below is the list of our acoustic features : \x95 Log of duration in milliseconds normalized by number of canonical phones binned into 5 equal categories . \n\t', '\n\t\t \x95 Log Speech Rate ; calculated on strings of speech bounded on either side by pauses of 300 ms or greater and binned into 5 equal categories . \n\t', '\n\t\t \x95 Following pause ; a binary distinction of whether a word is followed by a period of silence or not . \n\t', '\n\t\t \x95 Following filled pause ; a binary distinction of whether a word was followed by a filled pause ( uh , um ) or not . \n\t', '\n\t\t \x95 Following IP boundary Table 4 indicates that each of these features significantly affect the presence of pitch accent . \n\t', '\n\t\t While certainly all of these variables are not independent of on another , using CRFs , one can incorporate all of these variables into the pitch accent prediction model with the advantage of making use of the dependencies among the labels . \n\t', '\n\t\t 4.4 Surrounding Information \n\t\t']",Positive
"['\n\t\t We also experimented with the effects of the surrounding values by varying the window size of the observation-label feature extraction described in Section 2 . \n\t', '\n\t\t When the window size is 1 , only values of the word that is labelled are incorporated in the model . \n\t', '\n\t\t When the window size is 3 , the values of the previous and the following words as well as the current word are incorporated in the model . \n\t', '\n\t\t Window size 5 captures the values of the current word , the two previous words and the two following words . \n\t', '\n\t\t 5 Experiments and Results All experiments were run using 10 fold cross- validation . \n\t', '\n\t\t We used Viterbi decoding to find the most likely sequence and report the performance in terms of label accuracy . \n\t', '\n\t\t We ran all experiments with varying window sizes ( w E { 1 , 3 , 5 } ) . \n\t', '\n\t\t The baseline which simply assigns the most common label , unaccented , achieves 60.53 f 1.50 % . \n\t', '\n\t\t Previous research has demonstrated that part of speech and frequency , or a combination of these two , are very reliable predictors of pitch accent . \n\t', '\n\t\t Thus , to test the worthiness of using a CRF model , the first experiment we ran was a comparison of an HMM to a CRF using just the combination of part of speech and unigram . \n\t', '\n\t\t The HMM score ( referred as HMM:POS , Unigram in Table 5 ) was 68.62 f 1.78 , while the CRF model ( referred as CRF:POS , Unigram in Table 5 ) performed significantly better at 72.56 f 1.86 . \n\t', '\n\t\t Note that \n\t\t']",Positive
"['\n\t\t The difference is due to the different corpora used in each case . \n\t', '\n\t\t While they also used spontaneous speech , it was a limited domain in the sense that it was speech from discharge orders from doctors at one medical facility . \n\t', '\n\t\t The SWDB corpus is open domain conversational speech . \n\t', '\n\t\t In order to capture some aspects of the IC and collocational strength of a word , in the second experiment we ran part of speech plus all of the probabilistic variables ( referred as CRF:POS , Prob in Table 5 ) . \n\t', '\n\t\t The model accuracy was 73.94 % , thus improved over the model using POS and unigram values by 1.38 % . \n\t', '\n\t\t In the third experiment we wanted to know if TTS applications that made use of purely textual input could be aided by the addition of timing and rhythm variables that can be gleaned from a text string . \n\t', '\n\t\t Thus , we included the textual features described in Section 4.3 in addition to the probabilistic and syntactic features ( referred as CRF:POS , Prob , Txt in Table 5 ) . \n\t', '\n\t\t The accuracy was improved by 1.73 % . \n\t', '\n\t\t For the final experiment , we added the acoustic variable , resulting in the use of all the variables described in Section 4 ( referred as CRF:All in Table 5 ) . \n\t', '\n\t\t We get about 0.5 % increase in accuracy , 76.1 % with a window of size w = 1 . \n\t', '\n\t\t Using larger windows resulted in minor increases in the performance of the model , as summarized in Table 5 . \n\t', '\n\t\t Our best accuracy was 76.36 % using all features in a w = 5 window size . \n\t', '\n\t\t Model:Variables w= 1 w=3 w=5 Baseline 60.53 HMM : POS,Unigram 68.62 CRF : POS , Unigram 72.56 CRF : POS , Prob 73.94 74.19 74.51 CRF : POS , Prob , Txt 75.67 75.74 75.89 CRF : All 76.1 76.23 76.36 Table 5 : Test accuracy of pitch accent prediction on SWDB using various variables and window sizes . \n\t', '\n\t\t 6 Discussion Pitch accent prediction is a difficult task , in that , the number of different speakers , topics , utterance fragments and disfluent production of the SWBD corpus only increase this difficulty . \n\t', '\n\t\t The fact that 21 % of the function words are accented indicates that models of pitch accent that mostly rely on part of speech and unigram frequency would not fair well with this corpus . \n\t', '\n\t\t We have presented a model of pitch accent that captures some of the other factors that influence accentuation . \n\t', '\n\t\t In addition to adding more probabilistic variables and phonological factors , we have used a sequence model that captures the interdependence of accents within a phrase . \n\t', '\n\t\t Given the distinct natures of corpora used , it is difficult to compare these results with earlier models . \n\t', '\n\t\t However , in experiment 1 ( HMM : POS , Unigram vs CRF : POS , Unigram ) we have shown that a CRF model achieves a better performance than an HMM model using the same features . \n\t', '\n\t\t However , the real strength of CRFs comes from their ability to incorporate different sources of information efficiently , as is demonstrated in our experiments . \n\t', '\n\t\t We did not test directly the probabilistic measures ( or collocation measures ) that have been used before for this task , namely information content ( IC ) \n\t\t']",Positive
"['\n\t\t However , the measures we have used encompass similar information . \n\t', '\n\t\t For example , IC is only the additive inverse of our unigram measure : IC(w) = \x97 log p(w) ( 6 ) Rather than using mutual information as a measure of collocational strength , we used unigram , bigram and joint probabilities . \n\t', '\n\t\t A model that includes both joint probability and the unigram probabilities of wz and wz_i is comparable to one that includes mutual information . \n\t', '\n\t\t Just as the likelihood of a word being accented is influenced by a following silence or IP boundary , the collocational strength of the target word with the following word ( captured by reverse bigram and reverse joint ) is also a factor . \n\t', '\n\t\t With the use of POS , unigram , and all bigram and joint probabilities , we have shown that ( a ) CRFs outperform HMMs , and ( b ) our probabilistic variables increase accuracy from a model that include POS + unigram ( 73.94 % compared to 72.56 % ) . \n\t', '\n\t\t For tasks in which pitch accent is predicted solely based on a string of text , without the addition of acoustic data , we have shown that adding aspects of rhythm and timing aids in the identification of accent targets . \n\t', '\n\t\t We used the number of words in an utterance , where in the utterance a word falls , how long in both number of syllables and number of phones all affect accentuation . \n\t', '\n\t\t The addition of these variables improved the model by nearly 2 % . \n\t', '\n\t\t These results suggest that Accent prediction models that only make use of textual information could be improved with the addition of these variables . \n\t', '\n\t\t While not trying to provide a complete model of accentuation from acoustic information , in this study we tested a few acoustic variables that have not yet been tested . \n\t', '\n\t\t The nature of the SWBD corpus allowed us to investigate the role of disfluencies and widely variable durations and speech rate on accentuation . \n\t', '\n\t\t Especially speech rate , duration and surrounding silence are good predictors of pitch accent . \n\t', '\n\t\t The addition of these predictors only slightly improved the model ( about .5 % ) . \n\t', '\n\t\t Acoustic features are very sensitive to individual speakers . \n\t', '\n\t\t In the corpus , there are many different speakers of varying ages and dialects . \n\t', '\n\t\t These variables might become more useful if one controls for individual speaker differences . \n\t', '\n\t\t To really test the usefulness of these variables , one would have to combine them with acoustic features that have been demonstrated to be good predictors of pitch accent \n\t\t']",Positive
"['\n\t\t 7 Conclusion We used CRFs with new measures of collocational strength and new phonological factors that capture aspects of rhythm and timing to model pitch accent prediction . \n\t', '\n\t\t CRFs have the theoretical advantage of incorporating all these factors in a principled and efficient way . \n\t', '\n\t\t We demonstrated that CRFs outperform HMMs also experimentally . \n\t', '\n\t\t We also demonstrated the usefulness of some new probabilistic variables and phonological variables . \n\t', '\n\t\t Our results mainly have implications for the textual prediction of accents in TTS applications , but might also be useful in automatic speech recognition tasks such as automatic transcription of multi-speaker meetings . \n\t', '\n\t\t In the near future we would like to incorporate reliable acoustic information , controlling for individual speaker difference and also apply different discriminative sequence labeling techniques to pitch accent prediction task . \n\t', '\n\t\t 8 Acknowledgements This work was partially funded by CAREER award #IIS 9733067 IGERT . \n\t', '\n\t\t We would also like to thank Mark Johnson for the idea of this project , Dan Jurafsky , Alan Bell , Cynthia Girand , and Jason Brenier for their helpful comments and help with the database . \n\t', '\n\t\t References Y. Altun , T. Hofmann , and M. Johnson . \n\t', '\n\t\t 2003a . \n\t', '\n\t\t Discriminative learning for label sequences via boosting . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of Advances in Neural Information Processing Systems . \n\t', '\n\t\t Y. Altun , I. Tsochantaridis , and T. Hofmann. 2003b . \n\t', '\n\t\t Hidden markov support vector machines . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of 20th International Conference on Machine Learning . \n\t', '\n\t\t M. Collins . \n\t', '\n\t\t 2002. Discriminative training methods for Hidden Markov Models : Theory and experiments with perceptron algorithms . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of Empirical Methods of Natural Language Processing . \n\t', '\n\t\t A. Conkie , G. Riccardi , and R. Rose . \n\t', '\n\t\t 1999. Prosody recognition from speech utterances using acoustic and linguistic based models of prosodic events . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ofEUROSPEECH\x9299 . \n\t', '\n\t\t W. J. Conover . \n\t', '\n\t\t 1980. Practical Nonparametric Statistics . \n\t', '\n\t\t Wiley , New York , 2nd edition . \n\t', '\n\t\t E. Fosler-Lussier and N. Morgan . \n\t', '\n\t\t 1999. Effects of speaking rate and word frequency on conversational pronunci ations . \n\t', '\n\t\t In Speech Communication . \n\t', '\n\t\t J. Godfrey , E. Holliman , and J. McDaniel . \n\t', '\n\t\t 1992. SWITCHBOARD : Telephone speech corpus for research and develo pment . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the International Conference on Acoustics , Speech , and Signal Processing . \n\t', '\n\t\t S. Greenberg , D. Ellis , and J. Hollenback . \n\t', '\n\t\t 1996. Insights into spoken language gleaned from phonetic transcripti on of the Switchboard corpus . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of International Conference on Spoken Language Processsing . \n\t', '\n\t\t J. Hirschberg . \n\t', '\n\t\t 1993. Pitch accent in context : Predicting intonational prominence from text . \n\t', '\n\t\t ArtificialIntelligence , 63(1-2):305\x96340 . \n\t', '\n\t\t M. Johnson , S. Geman , S. Canon , Z. Chi , and S. Riezler . \n\t', '\n\t\t 1999. Estimators for stochastic unification-based grammars . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ofACL\x9299 Association for Computational Linguistics . \n\t', '\n\t\t J. Lafferty , A. McCallum , and F. Pereira . \n\t', '\n\t\t 2001. Conditional random fields : Probabilistic models for segmenting and labeling sequence data . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of 18th International Conference on Machine Learning . \n\t', '\n\t\t A. McCallum , D. Freitag , and F. Pereira . \n\t', '\n\t\t 2000. Maximum Entropy Markov Models for Information Extraction and Segmentation . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of 17th International Conference on Machine Learning . \n\t', '\n\t\t A. McCallum . \n\t', '\n\t\t 2003. Efficiently inducing features of Conditional Random Fields . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of Uncertainty in Articifical Intelligence . \n\t', '\n\t\t T. Minka . \n\t', '\n\t\t 2001. Algorithms for maximum- likelihood logistic regression . \n\t', '\n\t\t Technical report , CMU , Department of Statistics , TR 758 . \n\t', '\n\t\t S. Pan and J. Hirschberg . \n\t', '\n\t\t 2001. Modeling local context for pitch accent prediction . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of ACL\x9201 , Association for Computational Linguistics . \n\t', '\n\t\t S. Pan and K. McKeown . \n\t', '\n\t\t 1999. Word informativeness and automatic pitch accent modeling . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the Joint SIGDAT Conference on EMNLP and VLC . \n\t', '\n\t\t V. Punyakanok and D. Roth . \n\t', '\n\t\t 2000. The use of classifiers in sequential inference . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of Advances in Neural Information Processing Systems . \n\t', '\n\t\t F. Sha and F. Pereira . \n\t', '\n\t\t 2003. Shallow parsing with conditional random fields . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of Human Language Technology . \n\t', '\n\t\t Xuejing Sun . \n\t', '\n\t\t 2002. Pitch accent prediction using ensemble machine learning . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the International Conference on Spoken Language Processing . \n\t', '\n\t\t B. Taskar , C. Guestrin , and D. Koller . \n\t', '\n\t\t 2004. Max- margin markov networks . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of Advances in Neural Information Processing Systems . \n\t', '\n\t\t P. Taylor . \n\t', '\n\t\t 2000. Analysis and synthesis of intonation using the Tilt model . \n\t', '\n\t\t Journal of the Acoustical Society ofAmerica . \n\t', '\n\t\t C. W. Wightman , A. K. Syrdal , G. Stemmer , A. Conkie , and M. Beutnagel . \n\t', '\n\t\t 2000. Perceptually Based Automatic Prosody Labeling and Prosodically Enriched Unit Selection Improve Concatenative Text-To-Speech Synthesis . \n\t', '\n\t\t volume 2 , pages 71\x9674 . \n\t', '\n\t\t Acquiring the Meaning of Discourse Markers Ben Hutchinson School of Informatics University of Edinburgh B.Hutchinson@sms.ed.ac.uk Abstract This paper applies machine learning techniques to acquiring aspects of the meaning of discourse markers . \n\t', '\n\t\t Three subtasks of acquiring the meaning of a discourse marker are considered : learning its polarity , veridicality , and type ( i.e. causal , temporal or additive ) . \n\t', '\n\t\t Accuracy of over 90 % is achieved for all three tasks , well above the baselines . \n\t', '\n\t\t 1 Introduction This paper is concerned with automatically acquiring the meaning of discourse markers . \n\t', '\n\t\t By considering the distributions of individual tokens of discourse markers , we classify discourse markers along three dimensions upon which there is substantial agreement in the literature : polarity , veridicality and type . \n\t', '\n\t\t This approach of classifying linguistic types by the distribution of linguistic tokens makes this research similar in spirit to that of \n\t\t']",Positive
"['\n\t\t Discourse markers signal relations between discourse units . \n\t', '\n\t\t As such , discourse markers play an important role in the parsing of natural language discourse \n\t\t']",Positive
"['\n\t\t In addition , generating natural language discourse requires the appropriate selection and placement of discourse markers \n\t\t']",Positive
"['\n\t\t It follows that a detailed account of the semantics and pragmatics of discourse markers would be a useful resource for natural language processing . \n\t', '\n\t\t Rather than looking at the finer subtleties in meaning of particular discourse markers ( e.g. \n\t\t']",Positive
"['\n\t\t This breadth of coverage is of particular importance for discourse parsing , where a wide range of linguistic realisations must be catered for . \n\t', '\n\t\t This work can be seen as orthogonal to that of Di \n\t\t']",Positive
"['\n\t\t Unfortunately , the manual classification of large numbers of discourse markers has proven to be a difficult task , and no complete classification yet exists . \n\t', '\n\t\t For example , \n\t\t']",Positive
"['\n\t\t A general method of automatically classifying discourse markers would therefore be of great utility , both for English and for languages with fewer manually created resources . \n\t', '\n\t\t This paper constitutes a step in that direction . \n\t', '\n\t\t It attempts to classify discourse markers whose classes are already known , and this allows the classifier to be evaluated empirically . \n\t', '\n\t\t The proposed task of learning automatically the meaning of discourse markers raises several questions which we hope to answer : Q1 . \n\t', '\n\t\t Difficulty How hard is it to acquire the meaning of discourse markers ? \n\t', '\n\t\t Are some aspects of meaning harder to acquire than others ? \n\t', '\n\t\t Q2 . \n\t', '\n\t\t Choice of features What features are useful for acquiring the meaning of discourse markers ? \n\t', '\n\t\t Does the optimal choice of features depend on the aspect of meaning being learnt ? \n\t', '\n\t\t Q3 . \n\t', '\n\t\t Classifiers Which machine learning algorithms work best for this task ? \n\t', '\n\t\t Can the right choice of empirical features make the classification problems linearly separable ? \n\t', '\n\t\t Q4 . \n\t', '\n\t\t Evidence Can corpus evidence be found for the existing classifications of discourse markers ? \n\t', '\n\t\t Is there empirical evidence for a separate class of TEMPORAL markers ? \n\t', '\n\t\t We proceed by first introducing the classes of discourse markers that we use in our experiments . \n\t', '\n\t\t Section 3 discusses the database of discourse markers used as our corpus . \n\t', '\n\t\t In Section 4 we describe our experiments , including choice of features . \n\t', '\n\t\t The results are presented in Section 5 . \n\t', '\n\t\t Finally , we conclude and discuss future work in Section 6 . \n\t', '\n\t\t 2 Discourse markers Discourse markers are lexical items ( possibly multi- word ) that signal relations between propositions , events or speech acts . \n\t', '\n\t\t Examples of discourse markers are given in Tables 1 , 2 and 3 . \n\t', '\n\t\t In this paper we will focus on a subclass of discourse markers known as structural connectives . \n\t', '\n\t\t These markers , even though they may be multiword expressions , function syntactically as if they were coordinating or subordinating conjunctions \n\t\t']",Positive
"['\n\t\t The literature contains many different classifications of discourse markers , drawing upon a wide range of evidence including textual cohesion \n\t\t']",Positive
"['\n\t\t Nevertheless there is also considerable agreement . \n\t', '\n\t\t Three dimensions of classification that recur , albeit under a variety of names , are polarity , veridicality and type . \n\t', '\n\t\t We now discuss each of these in turn . \n\t', '\n\t\t 2.1 Polarity Many discourse markers signal a concession , a contrast or the denial of an expectation . \n\t', '\n\t\t These markers have been described as having the feature polarity=NEG-POL . \n\t', '\n\t\t An example is given in ( 1 ) . \n\t', '\n\t\t ( 1 ) Suzy\x92s part-time , but she does more work than the rest of us put together . \n\t', '\n\t\t ( Taken from Knott ( 1996 , p. 185 ) ) This sentence is true if and only if Suzy both is part- time and does more work than the rest of them put together . \n\t', '\n\t\t In addition , it has the additional effect of signalling that the fact Suzy does more work is surprising \x97 it denies an expectation . \n\t', '\n\t\t A similar effect can be obtained by using the connective and and adding more context , as in ( 2 ) ( 2 ) Suzy\x92s efficiency is astounding . \n\t', '\n\t\t She\x92s part-time , and she does more work than the rest of us put together . \n\t', '\n\t\t The difference is that although it is possible for and to co-occur with a negative polarity discourse relation , it need not . \n\t', '\n\t\t Discourse markers like and are said to have the feature polarity=POS-POL . \n\t', '\n\t\t 1 On 1An alternative view is that discourse markers like and are underspecified with respect to polarity \n\t\t']",Positive
"['\n\t\t In this the other hand , a NEG-POL discourse marker like but always co-occurs with a negative polarity discourse relation . \n\t', '\n\t\t The gold standard classes of POS-POL and NEGPOL discourse markers used in the learning experiments are shown in Table 1 . \n\t', '\n\t\t The gold standards for all three experiments were compiled by consulting a range of previous classifications \n\t\t']",Positive
"['\n\t\t 2 POS-POL NEG-POL after , and , as , as soon as , although , because , before , considering but , even if , that , ever since , for , given that , even though , if , in case , in order that , in that , even when , insofar as , now , now that , on only if , only the grounds that , once , seeing when , or , or as , since , so , so that , the in- else , though , stant , the moment , then , to the unless , until , extent that , when , whenever whereas , yet Table 1 : Discourse markers used in the polarity experiment 2.2 Veridicality A discourse relation is veridical if it implies the truth of both its arguments \n\t\t']",Positive
"['\n\t\t For example , in ( 3 ) it is not necessarily true either that David can stay up or that he promises , or will promise , to be quiet . \n\t', '\n\t\t For this reason we will say if has the feature veridicality=NON-VERIDICAL . \n\t', '\n\t\t ( 3 ) David can stay up if he promises to be quiet . \n\t', '\n\t\t The disjunctive discourse marker or is also NON- VERIDICAL , because it does not imply that both of its arguments are true . \n\t', '\n\t\t On the other hand , and does imply this , and so has the feature veridicality=VERIDICAL . \n\t', '\n\t\t The VERIDICAL and NON-VERIDICAL discourse markers used in the learning experiments are shown in Table 2 . \n\t', '\n\t\t Note that the polarity and veridicality are independent , for example even if is both NEGPOL and NON-VERIDICAL . \n\t', '\n\t\t 2.3 Type Discourse markers like because signal a CAUSAL relation , for example in ( 4 ) . \n\t', '\n\t\t account , discourse markers have positive polarity only if they can never be paraphrased using a discourse marker with negative polarity . \n\t', '\n\t\t Interpreted in these terms , our experiment aims to distinguish negative polarity discourse markers from all others . \n\t', '\n\t\t 2An effort was made to exclude discourse markers whose classification could be contentious , as well as ones which showed ambiguity across classes . \n\t', '\n\t\t Some level ofjudgement was therefore exercised by the author . \n\t', '\n\t\t VERIDICAL NON- VERIDICAL after , although , and , as , as soon assuming as , because , but , considering that , even if , that , even though , even when , if , if ever , if ever since , for , given that , in or- only , in case , der that , in that , insofar as , now , on condition now that , on the grounds that , that , on the once , only when , seeing as , assumption since , so , so that , the instant , that , only if , the moment , then , though , to or , or else , the extent that , until , when , supposing whenever , whereas , while , yet that , unless Table 2 : Discourse markers used in the veridicality experiment ( 4 ) The tension in the boardroom rose sharply because the chairman arrived . \n\t', '\n\t\t As a result , because has the feature type=CAUSAL . \n\t', '\n\t\t Other discourse markers that express a temporal relation , such as after , have the feature type=TEMPORAL . \n\t', '\n\t\t Just as a POS-POL discourse marker can occur with a negative polarity discourse relation , the context can also supply a causal relation even when a TEMPORAL discourse marker is used , as in ( 5 ) . \n\t', '\n\t\t ( 5 ) The tension in the boardroom rose sharply after the chairman arrived . \n\t', '\n\t\t If the relation a discourse marker signals is neither CAUSAL or TEMPORAL it has the feature type=ADDITIVE . \n\t', '\n\t\t The need for a distinct class of TEMPORAL discourse relations is disputed in the literature . \n\t', '\n\t\t On the one hand , it has been suggested that TEMPORAL relations are a subclass of ADDITIVE ones on the grounds that the temporal reference inherent in the marking of tense and aspect \x93more or less\x94 fixes the temporal ordering of events \n\t\t']",Positive
['\n\t\t This contrasts with arguments that resolving discourse relations and temporal order occur as distinct but inter-related processes \n\t\t'],Positive
"['\n\t\t On the other hand , several of the discourse markers we count as TEMPORAL , such as as soon as , might be described as CAUSAL \n\t\t']",Positive
"['\n\t\t One of the results of the experiments described below is that corpus evidence suggests ADDITIVE , TEMPORAL and CAUSAL discourse markers have distinct distributions . \n\t', '\n\t\t The ADDITIVE , TEMPORAL and CAUSAL discourse markers used in the learning experiments are shown in Table 3 . \n\t', '\n\t\t These features are independent of the previous ones , for example even though is CAUSAL , VERIDICAL and NEG-POL . \n\t', '\n\t\t ADDITIVE TEMPORAL CAUSAL and , but , after , as although , because , whereas soon as , even though , for , given that , if , if ever , in case , on condition that , on before , ever since , the assumption that , now , now that , once , until , on the grounds that , provided that , providing that , so , so that , supposing that , though , unless when , whenever Table 3 : Discourse markers used in the type experiment 3 Corpus The data for the experiments comes from a database of sentences collected automatically from the British National Corpus and the world wide web \n\t\t']",Positive
"['\n\t\t The database contains example sentences for each of 140 discourse structural connectives . \n\t', '\n\t\t Many discourse markers have surface forms with other usages , e.g. before in the phrase before noon . \n\t', '\n\t\t The following procedure was therefore used to select sentences for inclusion in the database . \n\t', '\n\t\t First , sentences containing a string matching the surface form of a structural connective were extracted . \n\t', '\n\t\t These sentences were then parsed using a statistical parser \n\t\t']",Positive
"['\n\t\t Potential structural connectives were then classified on the basis of their syntactic context , in particular their proximity to S nodes . \n\t', '\n\t\t Figure 1 shows example syntactic contexts which were used to identify discourse markers . \n\t', '\n\t\t ( S ... ) ( CC and ) ( S ... ) ( SBAR ( IN after ) ( S ... ) ) ( PP ( IN after ) ( S ... ) ) ( PP ( VBN given ) ( SBAR ( IN that ) ( S ... ) ) ) ( NP ( DT the ) ( NN moment ) ( SBAR ... ) ) ( ADVP ( RB as ) ( RB long ) ( SBAR ( IN as ) ( S ... ) ) ) ( PP ( IN in ) ( SBAR ( IN that ) ( S ... ) ) ) Figure 1 : Identifying structural connectives It is because structural connectives are easy to identify in this manner that the experiments use only this subclass of discourse markers . \n\t', '\n\t\t Due to both parser errors , and the fact that the syntactic heuristics are not foolproof , the database contains noise . \n\t', '\n\t\t Manual analysis of a sample of 500 sentences revealed about 12 % of sentences do not contain the discourse marker they are supposed to . \n\t', '\n\t\t Of the discourse markers used in the experiments , their frequencies in the database ranged from 270 for the instant to 331,701 for and . \n\t', '\n\t\t The mean number of instances was 32,770 , while the median was 4,948 . \n\t', '\n\t\t 4 Experiments This section presents three machine learning experiments into automatically classifying discourse markers according to their polarity , veridicality and type . \n\t', '\n\t\t We begin in Section 4.1 by describing the features we extract for each discourse marker token . \n\t', '\n\t\t Then in Section 4.2 we describe the different classifiers we use . \n\t', '\n\t\t The results are presented in Section 4.3 . \n\t', '\n\t\t 4.1 Features used We only used structural connectives in the experiments . \n\t', '\n\t\t This meant that the clauses linked syntactically were also related at the discourse level \n\t\t']",Positive
"['\n\t\t Two types of features were extracted from the conjoined clauses . \n\t', '\n\t\t Firstly , we used lexical co-occurrences with words of various parts of speech . \n\t', '\n\t\t Secondly , we used a range of linguistically motivated syntactic , semantic , and discourse features . \n\t', '\n\t\t 4.1.1 Lexical co-occurrences Lexical co-occurrences have previously been shown to be useful for discourse level learning tasks \n\t\t']",Positive
"['\n\t\t For each discourse marker , the words occurring in their superordinate ( main ) and subordinate clauses were recorded,3 along with their parts of speech . \n\t', '\n\t\t We manually clustered the Penn Treebank parts of speech together to obtain coarser grained syntactic categories , as shown in Table 4 . \n\t', '\n\t\t We then lemmatised each word and excluded all lemmas with a frequency of less than 1000 per million in the BNC. . \n\t', '\n\t\t Finally , words were attached a prefix of either SUB or SUPER according to whether they occurred in the sub- or superordinate clause linked by the marker . \n\t', '\n\t\t This distinguished , for example , between occurrences of then in the antecedent ( subordinate ) and consequent ( main ) clauses linked by if. . \n\t', '\n\t\t We also recorded the presence of other discourse markers in the two clauses , as these had previously 3For coordinating conjunctions , the left clause was taken to be superordinate/main clause , the right , the subordinate clause . \n\t', '\n\t\t New label Penn Treebank labels vb vb vbd vbg vbn vbp vbz nn nn nns nnp jj jj jjrjjs rb rb rbr rbs aux aux auxg md prp prp prp $ in in Table 4 : Clustering of POS labels been found to be useful on a related classification task \n\t\t']",Positive
['\n\t\t The discourse markers used for this are based on the list of 350 markers given by \n\t\t'],Positive
"['\n\t\t Due to the sparser nature of discourse markers , compared to verbs for example , no frequency cutoffs were used . \n\t', '\n\t\t 4.1.2 Linguistically motivated features These included a range of one and two dimensional features representing more abstract linguistic information , and were extracted through automatic analysis of the parse trees . \n\t', '\n\t\t One dimensional features Two one dimensional features recorded the location of discourse markers . \n\t', '\n\t\t POSITION indicated whether a discourse marker occurred between the clauses it linked , or before both of them . \n\t', '\n\t\t It thus relates to information structuring . \n\t', '\n\t\t EMBEDDING indicated the level of embedding , in number of clauses , of the discourse marker beneath the sentence\x92s highest level clause . \n\t', '\n\t\t We were interested to see if some types of discourse relations are more often deeply embedded . \n\t', '\n\t\t The remaining features recorded the presence of linguistic features that are localised to a particular clause . \n\t', '\n\t\t Like the lexical co-occurrence features , these were indexed by the clause they occurred in : either SUPER or SUB . \n\t', '\n\t\t We expected negation to correlate with negative polarity discourse markers , and approximated negation using four features . \n\t', '\n\t\t NEG-SUBJ and NEGVERB indicated the presence of subject negation ( e.g. nothing ) or verbal negation ( e.g. n\x92t ) . \n\t', '\n\t\t We also recorded the occurrence of a set of negative polarity items ( NPI ) , such as any and ever . \n\t', '\n\t\t The features NPI-AND-NEG and NPI-WO-NEG indicated whether an NPI occurred in a clause with or without verbal or subject negation . \n\t', '\n\t\t Eventualities can be placed or ordered in time us- ing not just discourse markers but also temporal expressions . \n\t', '\n\t\t The feature TEMPEX recorded the number of temporal expressions in each clause , as returned by a temporal expression tagger \n\t\t']",Positive
"['\n\t\t If the main verb was an inflection of to be or to do we recorded this using the features BE and DO . \n\t', '\n\t\t Our motivation was to capture any correlation of these verbs with states and events respectively . \n\t', '\n\t\t If the final verb was a modal auxiliary , this ellipsis was evidence of strong cohesion in the text \n\t\t']",Positive
"['\n\t\t We recorded this with the feature VP-ELLIPSIS . \n\t', '\n\t\t Pronouns also indicate cohesion , and have been shown to correlate with subjectivity \n\t\t']",Positive
"['\n\t\t A class of features PRONOUNS represented pronouns , with denoting either 1 st person , 2nd person , or 3rd person animate , inanimate or plural . \n\t', '\n\t\t The syntactic structure of each clause was captured using two features , one finer grained and one coarser grained . \n\t', '\n\t\t STRUCTURAL-SKELETON identified the major constituents under the S or VP nodes , e.g. a simple double object construction gives \x93NP VB NP NP\x94 . \n\t', '\n\t\t ARGS identified whether the clause contained an ( overt ) object , an ( overt ) subject , or both , or neither . \n\t', '\n\t\t The overall size of a clause was represented using four features . \n\t', '\n\t\t WORDS , NPS and PPS recorded the numbers of words , NPs and PPs in a clause ( not counting embedded clauses ) . \n\t', '\n\t\t The feature CLAUSES counted the number of clauses embedded beneath a clause . \n\t', '\n\t\t Two dimensional features These features all recorded combinations of linguistic features across the two clauses linked by the discourse marker . \n\t', '\n\t\t For example the MOOD feature would take the value DECL,IMP for the sentence John is coming , but don\x92t tell anyone ! \n\t', '\n\t\t These features were all determined automatically by analysing the auxiliary verbs and the main verbs\x92 POS tags . \n\t', '\n\t\t The features and the possible values for each clause were as follows : MODALITY : one of FUTURE , ABILITY or NULL ; MOOD : one of DECL , IMP or INTERR ; PERFECT : either YES or NO ; PROGRESSIVE : either YES or NO ; TENSE : either PAST or PRESENT . \n\t', '\n\t\t 4.2 Classifier architectures Two different classifiers , based on local and global methods of comparison , were used in the experiments . \n\t', '\n\t\t The first , 1 Nearest Neighbour ( 1NN ) , is an instance based classifier which assigns each marker to the same class as that of the marker nearest to it . \n\t', '\n\t\t For this , three different distance metrics were explored . \n\t', '\n\t\t The first metric was the Euclidean distance function , shown in ( 6 ) , applied to probability distributions . \n\t', '\n\t\t ( 6 ) The second , , is a smoothed variant of the information theoretic Kullback-Leibner divergence ( Lee , 2001 , with ) . \n\t', '\n\t\t Its definition is given in ( 7 ) . \n\t', '\n\t\t ( 7 ) The third metric , , is a -test weighted adap- tion of the Jaccard coefficient \n\t\t']",Positive
"['\n\t\t In it basic form , the Jaccard coefficient is essentially a measure of how much two distributions overlap . \n\t', '\n\t\t The -test variant weights co-occurrences by the strength of their collocation , using the following function : This is then used define the weighted version of the Jaccard coefficient , as shown in ( 8 ) . \n\t', '\n\t\t The words associated with distributions and are indicated by and , respectively . \n\t', '\n\t\t ( 8 ) and had previously been found to be the best metrics for other tasks involving lexi cal similarity . \n\t', '\n\t\t is included to indicate what can be achieved using a somewhat naive metric . \n\t', '\n\t\t The second classifier used , Naive Bayes , takes the overall distribution of each class into account . \n\t', '\n\t\t It essentially defines a decision boundary in the form of a curved hyperplane . \n\t', '\n\t\t The Weka implementation \n\t\t']",Positive
"['\n\t\t 4.3 Results We began by comparing the performance of the 1NN classifier using the various lexical co- occurrence features against the gold standards . \n\t', '\n\t\t The results using all lexical co-occurrences are shown Task Baseline All POS Best Best subset single POS polarity 67.4 74.4 72.1 74.4 76.7 ( rb ) 83.7 ( rb ) 76.7 ( rb ) 83.7 veridicality 73.5 81.6 85.7 75.5 83.7 ( nn ) 91.8 ( vb ) 87.8 ( vb ) 91.8 type 58.1 74.2 64.5 81.8 74.2 ( in ) 74.2 ( rb ) 77.4 ( jj ) 87.8 Using and either rb or DMs+rb . \n\t', '\n\t\t Using both and vb , and and vb+in . \n\t', '\n\t\t Using and vb+aux+in Table 5 : Results using the 1NN classifier on lexical co-occurrences Feature Positively correlated discourse marker co-occurrences POS-POL though , but , although , assuming that NEG-POL otherwise , still , in truth , still , after that , in this way , granted that , in contrast , by then , in the event VERIDICAL obviously , now , even , indeed , once more , considering that , even after , once more , atfirst sight NON-VERIDICAL or , no doubt , in turn , then , by all means , before then ADDITIVE also , in addition , still , only , at the same time , clearly , naturally , now , of course TEMPORAL back , once more , like , and , once more , which was why , CAUSAL again , altogether , back , finally , also , thereby , at once , while , clearly , Table 6 : Most informative discourse marker co-occurrences in the super- ( ) and subordinate ( ) clauses in Table 5 . \n\t', '\n\t\t The baseline was obtained by assigning discourse markers to the largest class , i.e. with the most types . \n\t', '\n\t\t The best results obtained using just a single POS class are also shown . \n\t', '\n\t\t The results across the different metrics suggest that adverbs and verbs are the best single predictors of polarity and veridicality , respectively . \n\t', '\n\t\t We next applied the 1NN classifier to co- occurrences with discourse markers . \n\t', '\n\t\t The results are shown in Table 7 . \n\t', '\n\t\t The results show that for each task 1NN with the weighted Jaccard coefficient performs at least as well as the other three classifiers . \n\t', '\n\t\t 1NN with metric : Naive Task Bayes polarity 74.4 81.4 81.4 81.4 veridicality 83.7 79.6 83.7 73.5 type 74.2 80.1 80.1 58.1 Table 7 : Results using co-occurrences with DMs We also compared using the following combinations of different parts of speech : vb + aux , vb + in , vb + rb , nn + prp , vb + nn + prp , vb + aux + rb , vb + aux + in , vb + aux + nn + prp , nn + prp + in , DMs + rb , DMs + vb and DMs + rb + vb . \n\t', '\n\t\t The best results obtained using all combinations tried are shown in the last column of Table 5 . \n\t', '\n\t\t For DMs + rb , DMs + vb and DMs + rb + vb we also tried weighting the co- occurrences so that the sums of the co-occurrences with each of verbs , adverbs and discourse markers were equal . \n\t', '\n\t\t However this did not lead to any better results . \n\t', '\n\t\t One property that distinguishes from the other metrics is that it weights features the strength of their collocation . \n\t', '\n\t\t We were therefore interested to see which co-occurrences were most informative . \n\t', '\n\t\t Using Weka\x92s feature selection utility , we ranked discourse marker co-occurrences by their information gain when predicting polarity , veridicality and type . \n\t', '\n\t\t The most informative co-occurrences are listed in Table 6 . \n\t', '\n\t\t For example , if also occurs in the subordinate clause then the discourse marker is more likely to be ADDITIVE . \n\t', '\n\t\t The 1NN and Naive Bayes classifiers were then applied to co-occurrences with just the DMs that were most informative for each task . \n\t', '\n\t\t The results , shown in Table 8 , indicate that the performance of 1NN drops when we restrict ourselves to this subset . \n\t', '\n\t\t 4 However Naive Bayes outperforms all previous 1NN classifiers . \n\t', '\n\t\t Base- 1NN with : Naive Task line Bayes polarity 67.4 72.1 69.8 90.7 veridicality 73.5 85.7 77.6 91.8 type 58.1 67.7 58.1 93.5 Table 8 : Results using most informative DMs 4The metric is omitted because it essentially already has its own method of factoring in informativity . \n\t', '\n\t\t Feature Positively correlated features POS-POL No significantly informative predictors correlated positively NEG-POL NEG-VERBAL , NEG-SUBJ , ARGS=NONE , MODALITY= ABILITY,ABILITY VERIDICAL VERB=BE , WORDS , WORDS , MODALITY= NULL,NULL NON-VERID TEMPEX , PRONOUN , PRONOUN ADDITIVE WORDS , WORDS , CLAUSES , MODALITY= ABILITY,FUTURE , MODALITY= ABILITY,ABILITY , NPS , MODALITY= FUTURE,FUTURE , MOOD= DECLARATIVE,DECLARATIVE TEMPORAL EMBEDDING=7 , PRONOUN , MOOD= INTERROGATIVE,DECLARATIVE CAUSAL NEG-SUBJ , NEG-VERBAL , NPI-WO-NEG , NPI-AND-NEG , MODALITY= NULL,FUTURE Table 9 : The most informative linguistically motivated predictors for each class . \n\t', '\n\t\t The indices and indicate that a one dimensional feature belongs to the superordinate or subordinate clause , respectively . \n\t', '\n\t\t Weka\x92s feature selection utility was also applied to all the linguistically motivated features described in Section 4.1.2 . \n\t', '\n\t\t The most informative features are shown in Table 9 . \n\t', '\n\t\t Naive Bayes was then applied using both all the linguistically motivated features , and just the most informative ones . \n\t', '\n\t\t The results are shown in Table 10 . \n\t', '\n\t\t Task Baseline All Most features informative polarity veridicality type 67.4 73.5 58.1 74.4 77.6 64.5 72.1 79.6 77.4 Table 10 : Naive Bayes and linguistic features 5 Discussion The results demonstrate that discourse markers can be classified along three different dimensions with an accuracy of over 90 % . \n\t', '\n\t\t The best classifiers used a global algorithm ( Naive Bayes ) , with co- occurrences with a subset of discourse markers as features . \n\t', '\n\t\t The success of Naive Bayes shows that with the right choice of features the classification task is highly separable . \n\t', '\n\t\t The high degree of accuracy attained on the type task suggests that there is empirical evidence for a distinct class of TEMPORAL markers . \n\t', '\n\t\t The results also provide empirical evidence for the correlation between certain linguistic features and types of discourse relation . \n\t', '\n\t\t Here we restrict ourselves to making just five observations . \n\t', '\n\t\t Firstly , verbs and adverbs are the most informative parts of speech when classifying discourse markers . \n\t', '\n\t\t This is presumably because of their close relation to the main predicate of the clause . \n\t', '\n\t\t Secondly , Table 6 shows that the discourse marker DM in the structure X , but/though/although Y DM Z is more likely to be signalling a positive polarity discourse relation between Y and Z than a negative polarity one . \n\t', '\n\t\t This suggests that a negative polarity discourse relation is less likely to be embedded directly beneath another negative polarity discourse relation . \n\t', '\n\t\t Thirdly , negation correlates with the main clause of NEG-POL discourse markers , and it also correlates with subordinate clause of CAUSAL ones . \n\t', '\n\t\t Fourthly , NON-VERIDICAL correlates with second person pronouns , suggesting that a writer/speaker is less likely to make assertions about the reader/listener than about other entities . \n\t', '\n\t\t Lastly , the best results with knowledge poor features , i.e. lexical co-occurrences , were better than those with linguistically sophisticated ones . \n\t', '\n\t\t It may be that the sophisticated features are predictive of only certain subclasses of the classes we used , e.g. hypotheticals , or signallers of contrast . \n\t', '\n\t\t 6 Conclusions and future work We have proposed corpus-based techniques for classifying discourse markers along three dimensions : polarity , veridicality and type . \n\t', '\n\t\t For these tasks we were able to classify with accuracy rates of 90.7 % , 91.8 % and 93.5 % respectively . \n\t', '\n\t\t These equate to error reduction rates of 71.5 % , 69.1 % and 84.5 % from the baseline error rates . \n\t', '\n\t\t In addition , we determined which features were most informative for the different classification tasks . \n\t', '\n\t\t In future work we aim to extend our work in two directions . \n\t', '\n\t\t Firstly , we will consider finer-grained classification tasks , such as learning whether a causal discourse marker introduces a cause or a consequence , e.g. distinguishing because from so . \n\t', '\n\t\t Secondly , we would like to see how far our results can be extended to include adverbial discourse markers , such as instead or for example , by using just features of the clauses they occur in . \n\t', '\n\t\t Acknowledgements I would like to thank Mirella Lapata , Alex Lascarides , Bonnie Webber , and the three anonymous reviewers for their comments on drafts of this paper . \n\t', '\n\t\t This research was supported by EPSRC Grant GR/R40036/01 and a University of Sydney Travelling Scholarship . \n\t', '\n\t\t References Nicholas Asher and Alex Lascarides . \n\t', '\n\t\t 2003. Logics of Conversation . \n\t', '\n\t\t Cambridge University Press . \n\t', '\n\t\t Timothy Baldwin and Francis Bond . \n\t', '\n\t\t 2003. Learning the countability of English nouns from corpus data . \n\t', '\n\t\t In Proceedings ofACL 2003 , pages 463\x96470 . \n\t', '\n\t\t Yves Bestgen , Liesbeth Degand , and Wilbert Spooren . \n\t', '\n\t\t 2003. On the use of automatic techniques to determine the semantics of connectives in large newspaper corpora : An exploratory study . \n\t', '\n\t\t In Proceedings of the MAD\x9203 workshop on Multidisciplinary Approaches to Discourse , October . \n\t', '\n\t\t Eugene Charniak . \n\t', '\n\t\t 2000. A maximum-entropy-inspired parser . \n\t', '\n\t\t In Proceedings of the First Conference of the North American Chapter of the Association for Computational Linguistics ( NAACL-2000 ) , Seattle , Washington , USA . \n\t', '\n\t\t James R. Curran and M. Moens . \n\t', '\n\t\t 2002. Improvements in automatic thesaurus extraction . \n\t', '\n\t\t In Proceedings of the Workshop on Unsupervised Lexical Acquisition , pages 59\x9667 , Philadelphia , PA , USA . \n\t', '\n\t\t Barbara Di Eugenio , Johanna D. Moore , and Massimo Paolucci . \n\t', '\n\t\t 1997. Learning features that predict cue usage . \n\t', '\n\t\t In Proceedings of the 35th Conference of the Association for Computational Linguistics ( ACL97 ) , Madrid , Spain , July . \n\t', '\n\t\t Katherine Forbes , Eleni Miltsakaki , Rashmi Prasad , Anoop Sarkar , Aravind Joshi , and Bonnie Webber . \n\t', '\n\t\t 2001. D-LTAG system\x97discourse parsing with a lexicalised tree adjoining grammar . \n\t', '\n\t\t In Proceedings ofthe ESSLI 2001 Workshop on Information Structure , Discourse Structure , and Discourse Semantics , Helsinki , Finland . \n\t', '\n\t\t Brigitte Grote and Manfred Stede . \n\t', '\n\t\t 1998. Discourse marker choice in sentence planning . \n\t', '\n\t\t In Eduard Hovy , editor , Proceedings of the Ninth International Workshop on Natural Language Generation , pages 128\x96 137 . \n\t', '\n\t\t Association for Computational Linguistics , New Brunswick , New Jersey . \n\t', '\n\t\t M. Halliday and R. Hasan . \n\t', '\n\t\t 1976. Cohesion in English . \n\t', '\n\t\t Longman . \n\t', '\n\t\t Ben Hutchinson . \n\t', '\n\t\t 2003. Automatic classification of discourse markers by their co-occurrences . \n\t', '\n\t\t In Proceedings of the ESSLLI2003 workshop on Discourse Particles : Meaning and Implementation , Vienna , Austria . \n\t', '\n\t\t Ben Hutchinson . \n\t', '\n\t\t 2004. Mining the web for discourse markers . \n\t', '\n\t\t In Proceedings of the Fourth International Conference on Language Resources and Evaluation ( LREC 2004 ) , Lisbon , Portugal . \n\t', '\n\t\t Alistair Knott and Robert Dale . \n\t', '\n\t\t 1994. Using linguistic phenomena to motivate a set of coherence relations . \n\t', '\n\t\t Discourse Processes , 18(1):35\x9662 . \n\t', '\n\t\t Alistair Knott . \n\t', '\n\t\t 1996. A data-driven methodology for motivating a set of coherence relations . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , University of Edinburgh . \n\t', '\n\t\t Mirella Lapata and Alex Lascarides . \n\t', '\n\t\t 2004. Inferring sentence-internal temporal relations . \n\t', '\n\t\t In In Proceedings of the Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics Annual Meeting , Boston , MA . \n\t', '\n\t\t Alex Lascarides and Nicholas Asher . \n\t', '\n\t\t 1993 . \n\t', '\n\t\t Temporal interpretation , discourse relations and common sense entailment . \n\t', '\n\t\t Linguistics and Philosophy , 16(5):437\x96 493 . \n\t', '\n\t\t Lillian Lee . \n\t', '\n\t\t 2001. On the effectiveness of the skew divergence for statistical language analysis . \n\t', '\n\t\t Artificial Intelligence and Statistics , pages 65\x9672 . \n\t', '\n\t\t Max M Louwerse . \n\t', '\n\t\t 2001. An analytic and cognitive parameterization of coherence relations . \n\t', '\n\t\t Cognitive Linguistics , 12(3):291\x96315 . \n\t', '\n\t\t Inderjeet Mani and George Wilson . \n\t', '\n\t\t 2000 . \n\t', '\n\t\t Robust temporal processing of news . \n\t', '\n\t\t In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics ( ACL 2000 ) , pages 69\x9676 , New Brunswick , New Jersey . \n\t', '\n\t\t Daniel Marcu and Abdessamad Echihabi . \n\t', '\n\t\t 2002. An unsupervised approach to recognizing discourse relations . \n\t', '\n\t\t In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics ( ACL2002 ) , Philadelphia , PA . \n\t', '\n\t\t Daniel Marcu . \n\t', '\n\t\t 2000. The Theory and Practice of Dis- course Parsing and Summarization . \n\t', '\n\t\t The MIT Press . \n\t', '\n\t\t Jim Martin . \n\t', '\n\t\t 1992. English Text : System and Structure . \n\t', '\n\t\t Benjamin , Amsterdam . \n\t', '\n\t\t M. Moser and J. Moore . \n\t', '\n\t\t 1995. Using discourse analysis and automatic text generation to study discourse cue usage . \n\t', '\n\t\t In Proceedings of the AAAI 1995 Spring Symposium on Empirical Methods in Discourse Interpretation and Generation , pages 92\x9698 . \n\t', '\n\t\t Jon Oberlander and Alistair Knott . \n\t', '\n\t\t 1995. Issues in cue phrase implicature . \n\t', '\n\t\t In Proceedings of the AAAI Spring Symposium on Empirical Methods in Discourse Interpretation and Generation . \n\t', '\n\t\t Ted J. M. Sanders , W. P. M. Spooren , and L. G. M. Noordman . \n\t', '\n\t\t 1992. Towards a taxonomy of coherence relations . \n\t', '\n\t\t Discourse Processes , 15:1\x9635 . \n\t', '\n\t\t Suzanne Stevenson and Paola Merlo . \n\t', '\n\t\t 1999. Automatic verb classification using distributions of grammatical features . \n\t', '\n\t\t In Proceedings of the 9th Conference of the European Chapter of the ACL , pages 45\x9652 , Bergen , Norway . \n\t', '\n\t\t Bonnie Webber , Matthew Stone , Aravind Joshi , and Alistair Knott . \n\t', '\n\t\t 2003. Anaphora and discourse structure . \n\t', '\n\t\t Computational Linguistics , 29(4):545\x96588 . \n\t', '\n\t\t Ian H. Witten and Eibe Frank . \n\t', '\n\t\t 2000. Data Mining : Practical machine learning tools with Java implementations . \n\t', '\n\t\t Morgan Kaufmann , San Francisco . \n\t', '\n\t\t FLSA : Extending Latent Semantic Analysis with features for dialogue act classification Riccardo Serafin CEFRIEL Via Fucini 2 20133 Milano , Italy Riccardo.Serafin@students.cefriel.it Barbara Di Eugenio Computer Science University of Illinois Chicago , IL 60607 USA bdieugen@cs.uic.edu Abstract We discuss Feature Latent Semantic Analysis ( FLSA ) , an extension to Latent Semantic Analysis ( LSA ) . \n\t', '\n\t\t LSA is a statistical method that is ordinarily trained on words only ; FLSA adds to LSA the richness of the many other linguistic features that a corpus may be labeled with . \n\t', '\n\t\t We applied FLSA to dialogue act classification with excellent results . \n\t', '\n\t\t We report results on three corpora : CallHome Spanish , MapTask , and our own corpus of tutoring dialogues . \n\t', '\n\t\t 1 Introduction In this paper , we propose Feature Latent Semantic Analysis ( FLSA ) as an extension to Latent Semantic Analysis ( LSA ) . \n\t', '\n\t\t LSA can be thought as representing the meaning of a word as a kind of average of the meanings of all the passages in which it appears , and the meaning of a passage as a kind of average of the meaning of all the words it contains \n\t\t']",Positive
"['\n\t\t It builds a semantic space where words and passages are represented as vectors . \n\t', '\n\t\t LSA is based on Single Value Decomposition ( SVD ) , a mathematical technique that causes the semantic space to be arranged so as to reflect the major associative patterns in the data . \n\t', '\n\t\t LSA has been successfully applied to many tasks , such as assessing the quality of student essays \n\t\t']",Positive
"['\n\t\t A common criticism of LSA is that it uses only words and ignores anything else , e.g. syntactic information : to LSA , man bites dog is identical to dog bites man . \n\t', '\n\t\t We suggest that an LSA semantic space can be built from the co-occurrence of arbitrary textual features , not just words . \n\t', '\n\t\t We are calling LSA augmented with features FLSA , for Feature LSA . \n\t', '\n\t\t Relevant prior work on LSA only includes Structured Latent Semantic Analysis \n\t\t']",Negative
"['\n\t\t We will show that for our task , dialogue act classification , syntactic features do not help , but most dialogue related features do . \n\t', '\n\t\t Surprisingly , one dialogue related feature that does not help is the dialogue act history . \n\t', '\n\t\t We applied LSA / FLSA to dialogue act classification . \n\t', '\n\t\t Dialogue systems need to perform dialogue act classification , in order to understand the role the user\x92s utterance plays in the dialogue ( e.g. , a question for information or a request to perform an action ) . \n\t', '\n\t\t In recent years , a variety of empirical techniques have been used to train the dialogue act classifier \n\t\t']",Positive
"['\n\t\t A second contribution of our work is to show that FLSA is successful at dialogue act classification , reaching comparable or better results than other published methods . \n\t', '\n\t\t With respect to a baseline of choosing the most frequent dialogue act ( DA ) , LSA reduces error rates between 33 % and 52 % , and FLSA reduces error rates between 60 % and 78 % . \n\t', '\n\t\t LSA is an attractive method for this task because it is straightforward to train and use . \n\t', '\n\t\t More importantly , although it is a statistical theory , it has been shown to mimic many aspects of human competence / performance \n\t\t']",Positive
"['\n\t\t Thus , it appears to capture important components of meaning . \n\t', '\n\t\t Our results suggest that LSA / FLSA do so also as concerns DA classification . \n\t', '\n\t\t On Map- Task , our FLSA classifier agrees with human coders to a satisfactory degree , and makes most of the same mistakes . \n\t', '\n\t\t 2 Feature Latent Semantic Analysis We will start by discussing LSA . \n\t', '\n\t\t The input to LSA is a Word-Document matrix W with a row for each word , and a column for each document ( for us , a document is a unit , e.g. an utterance , tagged with a DA ) . \n\t', '\n\t\t Cell c(i , j ) contains the frequency with which wordi appears in documentj.1 Clearly , this w x d matrix W will be very sparse . \n\t', '\n\t\t Next , LSA applies 1Word frequencies are normally weighted according to specific functions , but we used raw frequencies because we wanted to assess our extensions to LSA independently from any bias introduced by the specific weighting technique . \n\t', '\n\t\t to W Singular Value Decomposition ( SVD ) , to decompose it into the product of three other matrices , W = T050DT0 , so that T0 and D0 have orthonormal columns and 50 is diagonal . \n\t', '\n\t\t SVD then provides a simple strategy for optimal approximate fit using smaller matrices . \n\t', '\n\t\t If the singular values in 50 are ordered by size , the first k largest may be kept and the remaining smaller ones set to zero . \n\t', '\n\t\t The product of the resulting matrices is a matrix W\x88 of rank k which is approximately equal to W ; it is the matrix of rank k with the best possible least-squares-fit to W . \n\t', '\n\t\t The number of dimensions k retained by LSA is an empirical question . \n\t', '\n\t\t However , crucially k is much smaller than the dimension of the original space . \n\t', '\n\t\t The results we will report later are for the best k we experimented with . \n\t', '\n\t\t Figure 1 shows a hypothetical dialogue annotated with MapTask style DAs . \n\t', '\n\t\t Table 1 shows the Word- Document matrix W that LSA starts with \x96 note that as usual stop words such as a , the , you have been eliminated . \n\t', '\n\t\t 2 Table 2 shows the approximate representation of W in a much smaller space . \n\t', '\n\t\t To choose the best tag for a document in the test set , we first compute its vector representation in the semantic space LSA computed , then we compare the vector representing the new document with the vector of each document in the training set . \n\t', '\n\t\t The tag of the document which has the highest similarity with our test vector is assigned to the new document \x96 it is customary to use the cosine between the two vectors as a measure of similarity . \n\t', '\n\t\t In our case , the new document is a unit ( utterance ) to be tagged with a DA , and we assign to it the DA of the document in the training set to which the new document is most similar . \n\t', '\n\t\t Feature LSA . \n\t', '\n\t\t In general , in FLSA we add extra features to LSA by adding a new \x93word\x94 for each value that the feature of interest can take ( in some cases , e.g. when adding POS tags , we extend the matrix in a different way \x97 see Sec . \n\t', '\n\t\t 4 ) . \n\t', '\n\t\t The only assumption is that there are one or more non word related features associated with each document that can take a finite number of values . \n\t', '\n\t\t In the Word\x96 Document matrix , the word index is increased to include a new place holder for each possible value the feature may take . \n\t', '\n\t\t When creating the matrix , a count of one is placed in the rows related to the new indexes if a particular feature applies to the document under analysis . \n\t', '\n\t\t For instance , if we wish to include the speaker identity as a new feature for the dialogue 2We use a very short list of stop words ( < 50 ) , as our experiments revealed that for dialogue act annotation LSA is sensitive to the most common words too . \n\t', '\n\t\t This is why to is included in Table 1. in Figure 1 , the initial Word\x96Document matrix will be modified as in Table 3 ( its first 14 rows are as in Table 1 ) . \n\t', '\n\t\t This process is easily extended if more than one non-word feature is desired per document , if more than one feature value applies to a single document or if a single feature appears more than once in a document \n\t\t']",Positive
"['\n\t\t 3 Corpora We report experiments on three corpora , Spanish CallHome , MapTask , and DIAG-NLP . \n\t', '\n\t\t The Spanish CallHome corpus \n\t\t']",Positive
"['\n\t\t The Spanish CallHome corpus is annotated at three levels : DAs , dialogue games and dialogue activities . \n\t', '\n\t\t The DA annotation augments a basic tag such as statement along several dimensions , such as whether the statement describes a psychological state of the speaker . \n\t', '\n\t\t This results in 232 different DA tags , many with very low frequencies . \n\t', '\n\t\t In this sort of situations , tag categories are often collapsed when running experiments so as to get meaningful frequencies \n\t\t']",Positive
"['\n\t\t In CallHome37 , we collapsed different types of statements and backchannels , obtaining 37 different tags . \n\t', '\n\t\t CallHome37 maintains some subcategorizations , e.g. whether a question is yes/no or rhetorical . \n\t', '\n\t\t In Call- Home 10 , we further collapse these categories . \n\t', '\n\t\t CallHome10 is reduced to 8 DAs proper ( e.g. , statement , question , answer ) plus the two tags \x91 \x91 %\x92 \x92 for abandoned sentences and \x91 \x91x\x92 \x92 for noise . \n\t', '\n\t\t CallHome Spanish is further annotated for dialogue games and activities . \n\t', '\n\t\t Dialogue game annotation is based on the MapTask notion of a dialogue game , a set of utterances starting with an initiation and encompassing all utterances up until the purpose of the game has been fulfilled ( e.g. , the requested information has been transferred ) or abandoned \n\t\t']",Positive
"['\n\t\t Moves are the components of games , they correspond to a single or more DAs , and each is tagged as Initiative , Response or Feedback . \n\t', '\n\t\t Each game is also given a label , such as Info(rmation) or Direct(ive) . \n\t', '\n\t\t Finally , activities pertain to the main goal of a certain discourse stretch , such as gossip or argue . \n\t', '\n\t\t The HCRC MapTask corpus is a collection of dialogues regarding a \x93Map Task\x94 experiment . \n\t', '\n\t\t Two participants sit opposite one another and each of them receives a map , but the two maps differ . \n\t', '\n\t\t The instruction giver (G)\x92s map has a route indicated while instruction follower (F)\x92s map does not in- ( Doc 1 ) G : Do you see the lake with the black swan ? \n\t', '\n\t\t Query\x96yn ( Doc 2 ) F : Yes , I do Reply\x96y ( Doc 3 ) G : Ok , Ready ( Doc 4 ) G : draw a line straight to it Instruct ( Doc 5 ) F : straight to the lake ? \n\t', '\n\t\t Check ( Doc 6 ) G : yes , that\x92s right Reply\x96y ( Doc 7 ) F : Ok , I\x92ll do it Acknowledge Figure 1 : A hypothetical dialogue annotated with MapTask tags ( Doc 1 ) ( Doc 2 ) ( Doc 3 ) ( Doc 4 ) ( Doc 5 ) ( Doc 6 ) ( Doc 7 ) do 1 1 0 0 0 0 1 see 1 0 0 0 0 0 0 lake 1 0 0 0 1 0 0 black 1 0 0 0 0 0 0 swan 1 0 0 0 0 0 0 yes 0 1 0 0 0 1 0 ok 0 0 1 0 0 0 1 draw 0 0 0 1 0 0 0 line 0 0 0 1 0 0 0 straight 0 0 0 1 1 0 0 to 0 0 0 1 1 0 0 it 0 0 0 1 0 0 1 that 0 0 0 0 0 1 0 right 0 0 0 0 0 1 0 Table 1 : The 14-dimensional word-document matrix W clude the drawing of the route . \n\t', '\n\t\t The task is for G to give directions to F , so that , at the end , F is able to reproduce G\x92s route on her map . \n\t', '\n\t\t The MapTask corpus is composed of 128 dialogues , for a total of 1,835 unique words and 27,084 DAs . \n\t', '\n\t\t It has been tagged at various levels , from POS to disfluencies , from syntax to DAs . \n\t', '\n\t\t The MapTask coding scheme uses 13 DAs ( called moves ) , that include : Instruct ( a request that the partner carry out an action ) , Explain ( one of the partners states some information that was not explicitly elicited by the other ) , Queryyn/-w , Acknowledge , Reply -y/-n/-w and others . \n\t', '\n\t\t The MapTask corpus is also tagged for games as defined above , but differently from CallHome , 6 DAs are identified as potential initiators of games ( of course not every initiator DA initiates a game ) . \n\t', '\n\t\t Finally , transactions provide the subdialogue structure of a dialogue ; each is built of several dialogue games and corresponds to one step of the task . \n\t', '\n\t\t DIAG-NLP is a corpus of computer mediated tutoring dialogues between a tutor and a student who is diagnosing a fault in a mechanical system with a tutoring system built with the DIAG authoring tool \n\t\t']",Positive
"['\n\t\t The student\x92s input is via menu , the tutor is in a different room and answers via a text window . \n\t', '\n\t\t The DIAG-NLP corpus comprises 23 \x92dialogues\x92 for a total of 607 unique words and 660 DAs ( it is thus much smaller than the other two ) . \n\t', '\n\t\t It has been annotated for a variety of features , including four DAs3 \n\t\t']",Positive
"['\n\t\t Other features encode domain objects and their properties , and Consult Type , the type of student query . \n\t', '\n\t\t 4 Results Table 4 reports the results we obtained for each corpus and method ( to train and evaluate each method , we used 5-fold cross-validation ) . \n\t', '\n\t\t We include the baseline , computed as picking the most frequent DA 3 They should be more appropriately termed tutor moves . \n\t', '\n\t\t ( Doc 1 ) ( Doc 2 ) ( Doc 3 ) ( Doc 4 ) ( Doc 5 ) ( Doc 6 ) ( Doc 7 ) Dim . \n\t', '\n\t\t 1 1.3076 0.4717 0.1529 1.6668 1.1737 0.1193 0.9101 Dim . \n\t', '\n\t\t 2 1.5991 0.6797 0.0958 -1.3697 -0.4771 0.2844 0.4205 Table 2 : The reduced 2-dimensional matrix W\x88 ( Doc 1 ) ( Doc 2 ) ( Doc 3 ) ( Doc 4 ) ( Doc 5 ) ( Doc 6 ) ( Doc 7 ) do 1 1 0 0 0 0 1 ... ... ... ... ... ... ... ... right 0 0 0 0 0 1 0 <Giver> 1 0 1 1 0 1 0 <Follower> 0 1 0 0 1 0 1 Table 3 : Word-document matrix W augmented with speaker identity in each corpus;4 the accuracy for LSA ; the best accuracy for FLSA , and with what combination of features it was obtained ; the best published result , taken from \n\t\t']",Positive
"['\n\t\t Finally , for both LSA and FLSA , Table 4 includes , in parenthesis , the dimension k of the reduced semantic space . \n\t', '\n\t\t For each LSA method and corpus , we experimented with values of k between 25 and 350 . \n\t', '\n\t\t The values of k that give us the best resuls for each method were thus selected empirically . \n\t', '\n\t\t In all cases , we can see that LSA performs much better than baseline . \n\t', '\n\t\t Moreover , we can see that FLSA further improves performance , dramatically in the case of MapTask . \n\t', '\n\t\t FLSA reduces error rates between 60 % and 78 % , for all corpora other than DIAG-NLP ( all differences in performance between LSA and FLSA are significant , other than for DIAG-NLP ) . \n\t', '\n\t\t DIAG-NLP may be too small a corpus to train FLSA ; or Consult Type may not be effective , but it was the only feature appropriate for FLSA ( Sec . \n\t', '\n\t\t 5 discusses how we chose appropriate features ) . \n\t', '\n\t\t Another extension to LSA we developed , Clustered LSA , did give an improvement in performance for DIAG ( 79.24 % ) \x97 please see \n\t\t']",Positive
"['\n\t\t As regards comparable approaches , the performance of FLSA is as good or better . \n\t', '\n\t\t For Spanish CallHome , \n\t\t']",Positive
"['\n\t\t However , \n\t\t']",Positive
"['\n\t\t On MapTask , \n\t\t']",Negative
"['\n\t\t We achieve much better performance on MapTask with a number of our FLSA models . \n\t', '\n\t\t As regards results on DA classification for other corpora , the best performances obtained are up to 75 % for task-oriented dialogues such as Verbmobil \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t These are unrestricted English telephone conversations between two strangers that discuss a general interest topic . \n\t', '\n\t\t The DA classification task appears more difficult for corpora such as Switchboard and CallHome Spanish , that cannot benefit from the regularities imposed on the dialogue by a specific task . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t Table 5 reports a breakdown of the experimental results obtained with FLSA for the three tasks for which it was successful ( Table 5 does not include k , which is always 25 for CallHome37 and Call- Home 10 , and varies between 25 and 75 for Map- Task ) . \n\t', '\n\t\t For each corpus , under the line we find results that are significantly better than those obtained with LSA . \n\t', '\n\t\t For MapTask , the first 4 results that are 4The baselines for CallHome37 and CallHome10 are the same because in both statement is the most frequent DA . \n\t', '\n\t\t 5An inquiry to clarify this issue went unanswered . \n\t', '\n\t\t Corpus Baseline LSA FLSA Features Best known result CallHome37 42.68 % 65.36 % ( k = 50 ) 74.87 % ( k = 25 ) Game + Initiative 76.20 % CallHome10 42.68 % 68.91 % ( k = 25 ) 78.88 % ( k = 25 ) Game + Initiative 76.20 % MapTask 20.69 % 42.77 % ( k = 75 ) 73.91 % ( k = 25 ) Game + Speaker 62.10 % DIAG-NLP 43.64 % 75.73 % ( k = 50 ) 74.81 % ( k = 50 ) Consult Type n.a. . \n\t', '\n\t\t Table 4 : Accuracy for LSA and FLSA Corpus accuracy Features CallHome37 62.58 % Previous DA CallHome37 71.08 % Initiative CallHome37 72.69 % Game CallHome37 74.87 % Game+Initiative CallHome10 68.32 % Previous DA CallHome10 73.97 % Initiative CallHome10 76.52 % Game CallHome10 78.88 % Game+Initiative MapTask 41.84 % SRule MapTask 43.28 % POS MapTask 43.59 % Duration MapTask 46.91 % Speaker MapTask 47.09 % Previous DA MapTask 66.00 % Game MapTask 69.37 % Game+Prev . \n\t', '\n\t\t DA MapTask 73.25 % Game+Speaker+Prev . \n\t', '\n\t\t DA MapTask 73.91 % Game+Speaker Table 5 : FLSA Accuracy better than LSA ( from POS to Previous DA ) are still pretty low ; there is a difference of 19 % in performance for FLSA when Previous DA is added and when Game is added . \n\t', '\n\t\t Analysis . \n\t', '\n\t\t A few general conclusions can be drawn from Table 5 , as they apply in all three cases . \n\t', '\n\t\t First , using the previous DA does not help , either at all ( CallHome37 and CallHome10 ) , or very little ( MapTask ) . \n\t', '\n\t\t Increasing the length of the dialogue history does not improve performance . \n\t', '\n\t\t In other experiments , we increased the length up to n = 4 : we found that the higher n , the worse the performance . \n\t', '\n\t\t As we will see in Section 5 , introducing any new feature results in a larger and sparser initial matrix , which makes the task harder for FLSA ; to be effective , the amount of information provided by the new feature must be sufficient to overcome this handicap . \n\t', '\n\t\t It is clear that , the longer the dialogue history , the sparser the initial matrix becomes , which explains why performance decreases . \n\t', '\n\t\t However , this does not explain why using even only the previous DA does not help . \n\t', '\n\t\t This implies that the previous DA does not provide a lot of information , as in fact is shown numerically in Section 5 . \n\t', '\n\t\t This is surprising because the DA history is usually considered an important determinant of the current DA ( but \n\t\t']",Positive
"['\n\t\t Second , the notion of Game appears to be really powerful , as it vastly improves performance on two very different corpora such as CallHome and MapTask.6 We will come back to discussing the usage of Game in a real dialogue system in Section 6 . \n\t', '\n\t\t Third , the syntactic features we had access to do not seem to improve performance ( they were available only for MapTask ) . \n\t', '\n\t\t In MapTask SRule indicates the main structure of the utterance , such as Declarative or Wh-question . \n\t', '\n\t\t It is not surprising that SRule did not help , since it is well known that syntactic form is not predictive of DAs , especially those of indirect speech act flavor \n\t\t']",Positive
"['\n\t\t POS tags don\x92t help LSA either , as has already been observed by \n\t\t']",Positive
"['\n\t\t The likely reason is that it is necessary to add a different \x92word\x92 for each distinct pair word-POS , e.g. , route becomes split as route- NN and route-VB . \n\t', '\n\t\t This makes the Word-Document matrix much sparser : for MapTask , the number of rows increases from 1,835 for plain LSA to 2,324 for FLSA . \n\t', '\n\t\t These negative results on adding syntactic information to LSA may just reinforce one of the claims of the LSA proponents , that structural information is irrelevant for determining meaning \n\t\t']",Positive
"['\n\t\t Alternatively , syntactic information may need to be added to LSA in different ways . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t The results are better than with plain LSA . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t 6Using Game in MapTask does not introduce circularity , even if a game is identified by its initiating DA . \n\t', '\n\t\t We checked the matching rates for initiating and non initiating DAs with the FLSA model which employs Game + Speaker : they are 78.12 % and 71.67 % respectively . \n\t', '\n\t\t Hence , even if Game makes initiating moves easier to classify , it is highly beneficial for the classification of non initiating moves as well . \n\t', '\n\t\t 5 How to select features for FLSA An important issue is how to select features for FLSA . \n\t', '\n\t\t One possible answer is to exhaustively train every FLSA model that corresponds to one possible feature combination . \n\t', '\n\t\t The problem is that training LSA models is in general time consuming . \n\t', '\n\t\t For example , training each FLSA model on CallHome37 takes about 35 minutes of CPU time , and on Map- Task 17 minutes , on computers with one Pentium 1.7Ghz processor and 1Gb of memory . \n\t', '\n\t\t Thus , it would be better to focus only on the most promising models , especially when the number of features is high , because of the exponential number of combinations . \n\t', '\n\t\t In this work , we trained FLSA on each individual feature . \n\t', '\n\t\t Then , we trained FLSA on each feature combinations that we expected to be effective , either because of the good performances of each individual feature , or because they include features that are deemed predictive of DAs , such as the previous DA(s) , even if they did not perform well individually . \n\t', '\n\t\t After we ran our experiments , we performed a post hoc analysis based on the notion of Information Gain ( IG ) from decision tree learning \n\t\t']",Positive
"['\n\t\t One approach to choosing the next feature to add to the tree at each iteration is to pick the one with the highest IG . \n\t', '\n\t\t Suppose the data set S is classified using n categories v1 ... vn , each with probability pi . \n\t', '\n\t\t S\x92s entropy H can be seen as an indicator of how uncertain the outcome of the classification is , and is given by : H(S) = \x97 ~n pilog2(pi) ( 1 ) i=1 If feature F divides S into k subsets S1 ... \n\t', '\n\t\t Sk , then IG is the expected reduction in entropy caused by partitioning the data according to the values of F : llSill H(Si) ( 2 ) In our case , we first computed the entropy of the corpora with respect to the classification induced by the DA tags ( see Table 6 , which also includes the LSA accuracy for convenience ) . \n\t', '\n\t\t Then , we computed the IG of the features or feature combinations we used in the FLSA experiments . \n\t', '\n\t\t Table 7 reports the IG for most of the features from Table 5 ; it is ordered by FLSA performance . \n\t', '\n\t\t On the whole , IG appears to be a reasonably accurate predictor of performance . \n\t', '\n\t\t When a feature or feature combination has a high IG , e.g. over 1 , there Corpus Entropy LSA CallHome37 3.004 65.36 % CallHome10 2.51 68.91 % MapTask 3.38 42.77 % Table 7 : Information gain for FLSA is also a high performance improvement . \n\t', '\n\t\t Occasionally , if the IG is small this does not hold . \n\t', '\n\t\t For example , using the previous DA reduces the entropy by 0.21 for CallHome37 , but performance actually decreases . \n\t', '\n\t\t Most likely , the amount of new information introduced is rather low and it is overcome by having a larger and sparser initial matrix , which makes the task harder for FLSA . \n\t', '\n\t\t Also , when performance improves it does not necessarily increase linearly with IG ( see e.g. . \n\t', '\n\t\t Game + Speaker + Previous DA and Game + Speaker for MapTask ) . \n\t', '\n\t\t Nevertheless , IG can be effectively used to weed out unpromising features , or to rank feature combinations so that the most promising FLSA models can be trained first . \n\t', '\n\t\t 6 Discussion and future work In this paper , we have presented a novel extension to LSA , that we have called Feature LSA . \n\t', '\n\t\t Our work is the first to show that FLSA is more effective than LSA , at least for the specific task we worked on , DA classification . \n\t', '\n\t\t In parallel , we have shown that FLSA can be effectively used to train a DA classifier . \n\t', '\n\t\t We have reached performances comparable to or better than published results on DA classification , and we have used an easily trainable method . \n\t', '\n\t\t FLSA also highlights the effectiveness of other dialogue related features , such as Game , to classify DAs . \n\t', '\n\t\t The drawback of features such as Game is that IG(S , A ) = H(S) \x97 ~k i=1 Table 6 : Entropy measures Corpus Features IG FLSA CallHome37 Previous DA 0.21 62.58 % CallHome37 Initiative 0.69 71.08 % CallHome37 Game 0.59 72.69 % CallHome37 Game+Initiative 1.09 74.87 % CallHome1 0 Previous DA 0.13 68.32 % CallHome1 0 Initiative 0.53 73.97 % CallHome1 0 Game 0.53 76.52 % CallHome1 0 Game+Initiative 1.01 78.88 % MapTask Duration 0.54 43.59 % MapTask Speaker 0.31 46.91 % MapTask Prev. DA 0.58 47.09 % MapTask Game 1.21 66.00 % MapTask Game+Speaker+Prev . \n\t', '\n\t\t DA 2.04 73.25 % MapTask Game+Speaker 1.62 73.91 % Corpus FLSA CallHome37 0.676 CallHome10 0.721 MapTask 0.740 Table 8 : r. measures of agreement a dialogue system may not have them at its disposal when doing DA classification in real time . \n\t', '\n\t\t However , this problem may be circumvented . \n\t', '\n\t\t The number of different games is in general rather low ( 8 in CallHome Spanish , 6 in MapTask ) , and the game label is constant across DAs belonging to the same game . \n\t', '\n\t\t Each DA can be classified by augmenting it with each possible game label , and by choosing the most accurate match among those returned by each of these classification attempts . \n\t', '\n\t\t Further , if the system can reliably recognize the end of a game , the method just described needs to be used only for the first DA of each game . \n\t', '\n\t\t Then , the game label that gives the best result becomes the game label used for the next few DAs , until the end of the current game is detected . \n\t', '\n\t\t Another reason why we advocate FLSA over other approaches is that it appears to be close to human performance for DA classification , in the same way that LSA approximates well many aspects of human competence / performance \n\t\t']",Positive
"['\n\t\t To support this claim , first , we used the r. coefficient \n\t\t']",Positive
['\n\t\t A general rule of thumb on how to interpret the values of r. \n\t\t'],Positive
"['\n\t\t As a whole , Table 8 shows that FLSA achieves a satisfying level of agreement with human coders . \n\t', '\n\t\t To put Table 8 in perspective , note that expert human coders achieved r. = 0.83 on DA classification for MapTask , but also had available the speech source \n\t\t']",Positive
['\n\t\t We also compared the confusion matrix from \n\t\t'],Positive
"['\n\t\t For humans , the largest sources of confusion are between : check and queryyn ; instruct and clarify ; and acknowledge , reply -y and ready . \n\t', '\n\t\t Likewise , our FLSA method makes the most mistakes when distinguishing between instruct and clarify ; and acknowledge , reply-y , and ready . \n\t', '\n\t\t Instead it performs better than humans on distinguishing check and query -yn . \n\t', '\n\t\t Thus , most of the sources of confusion for humans are the same as for FLSA . \n\t', '\n\t\t Future work includes further investigating how to select promising feature combinations , e.g. by using logical regression . \n\t', '\n\t\t We are also exploring whether FLSA can be used as the basis for semi-automatic annotation of dialogue acts , to be incorporated into MUP , an annotation tool we have developed \n\t\t']",Positive
"['\n\t\t The problem is that large corpora are necessary to train methods based on LSA . \n\t', '\n\t\t This would seem to defeat the purpose of using FLSA as the basis for semi-automatic dialogue annotation , since , to train FLSA in a new domain , we would need a large hand annotated corpus to start with . \n\t', '\n\t\t Co-training \n\t\t']",Positive
"['\n\t\t In co-training , two different classifiers are initially trained on a small set of annotated data , by using different features . \n\t', '\n\t\t Afterwards , each classifier is allowed to label some unlabelled data , and picks its most confidently predicted positive and negative examples ; this data is added to the annotated data . \n\t', '\n\t\t The process repeats until the desired perfomance is achieved . \n\t', '\n\t\t In our scenario , we will experiment with training two different FLSA models , or one FLSA model and a different classifier , such as a naive Bayes classifier , on a small portion of annotated data that includes features like DAs , Game , etc. . \n\t', '\n\t\t We will then proceed as described on the unlabelled data . \n\t', '\n\t\t Finally , we have started applying FLSA to a different problem , that of judging the coherence of texts . \n\t', '\n\t\t Whereas LSA has been already successfully applied to this task \n\t\t']",Positive
"['\n\t\t Acknowledgments This work is supported by grant N00014-00-1-0640 from the Office of Naval Research , and in part , by award 0133123 from the National Science Foundation . \n\t', '\n\t\t Thanks to Michael Glass for initially suggesting extending LSA with features and to HCRC ( University of Edinburgh ) for sharing their annotated MapTask corpus . \n\t', '\n\t\t The work was performed while the first author was at the University of Illinois in Chicago . \n\t', '\n\t\t References Avrim Blum and Tom Mitchell . \n\t', '\n\t\t 1998. Combining labeled and unlabeled data with co-training . \n\t', '\n\t\t In COLT98 , Proceedings of the Conference on Computational Learning Theory . \n\t', '\n\t\t Jean Carletta , Amy Isard , Stephen Isard , Jacqueline C. Kowtko , Gwyneth Doherty-Sneddon , and Anne H. Anderson . \n\t', '\n\t\t 1997. The reliability of a dialogue structure coding scheme . \n\t', '\n\t\t Computational Lingustics , 23(1):13\x9631 . \n\t', '\n\t\t Jean Carletta . \n\t', '\n\t\t 1996. Assessing agreement on classification tasks : the Kappa statistic . \n\t', '\n\t\t Computational Linguistics , 22(2):249\x96254 . \n\t', '\n\t\t Peter W. Foltz , Walter Kintsch , and Thomas K. Landauer . \n\t', '\n\t\t 1998. The measurement of textual coherence with Latent Semantic Analysis . \n\t', '\n\t\t Discourse Processes , 25:285\x96308 . \n\t', '\n\t\t Peter W. Foltz , Darrell Laham , and Thomas K. Landauer . \n\t', '\n\t\t 1999. The intelligent essay assessor : Applications to educational technology . \n\t', '\n\t\t Interactive Multimedia Electronic Journal of Computer- Enhanced Learning , 1(2) . \n\t', '\n\t\t Michael Glass and Barbara Di Eugenio . \n\t', '\n\t\t 2002. MUP : The UIC standoff markup tool . \n\t', '\n\t\t In The Third SigDIAL Workshop on Discourse and Dialogue , Philadelphia , PA , July . \n\t', '\n\t\t Michael Glass , Heena Raval , Barbara Di Eugenio , and Maarika Traat . \n\t', '\n\t\t 2002. The DIAG-NLP dialogues : coding manual . \n\t', '\n\t\t Technical Report UICCS 02-03 , University of Illinois - Chicago . \n\t', '\n\t\t Dharmendra Kanejiya , Arun Kumar , and Surendra Prasad . \n\t', '\n\t\t 2003. Automatic Evaluation of Students\x92 Answers using Syntactically Enhanced LSA . \n\t', '\n\t\t In HLT-NAACL Workshop on Building Educational Applications using Natural Language Processing , pages 53\x9660 , Edmonton , Canada . \n\t', '\n\t\t Walter Kintsch . \n\t', '\n\t\t 2001. Predication . \n\t', '\n\t\t Cognitive Science , 25:173\x96202 . \n\t', '\n\t\t Klaus Krippendorff . \n\t', '\n\t\t 1980. Content Analysis : an Introduction to its Methodology . \n\t', '\n\t\t Sage Publications , Beverly Hills , CA . \n\t', '\n\t\t T. Lager and N. Zinovjeva . \n\t', '\n\t\t 1999. Training a dialogue act tagger with the p-TBL system . \n\t', '\n\t\t In The Third Swedish Symposium on Multimodal Communication , Link¨oping University Natural Language Processing Laboratory ( NLPLAB ) . \n\t', '\n\t\t Thomas K. Landauer and S.T. Dumais . \n\t', '\n\t\t 1997. A solution to Plato\x92s problem : The latent semantic analysis theory of acquisition , induction , and representation of knowledge . \n\t', '\n\t\t Psychological Review , 104:211\x96240 . \n\t', '\n\t\t Lori Levin , Ann Thym´e-Gobbel , Alon Lavie , Klaus Ries , and Klaus Zechner . \n\t', '\n\t\t 1998. A discourse coding scheme for conversational Spanish . \n\t', '\n\t\t In Proceedings ICSLP . \n\t', '\n\t\t J. Ross Quinlan . \n\t', '\n\t\t 1993. C4.5 : Programs for Machine Learning . \n\t', '\n\t\t Morgan Kaufmann. Klaus Ries . \n\t', '\n\t\t 1999. HMM and Neural Network Based Speech Act Detection . \n\t', '\n\t\t In Proceedings of ICASSP 99 , Phoenix , Arizona , March . \n\t', '\n\t\t Ken Samuel , Sandra Carberry , and K. VijayShanker . \n\t', '\n\t\t 1998. Dialogue act tagging with transformation-based learning . \n\t', '\n\t\t In ACL/COLING 98 , Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics ( joint with the 17th International Conference on Computational Linguistics ) , pages 1150\x961156 . \n\t', '\n\t\t John R. Searle . \n\t', '\n\t\t 1975. Indirect Speech Acts . \n\t', '\n\t\t In P. Cole and J.L. Morgan , editors , Syntax and Semantics 3 . \n\t', '\n\t\t Speech Acts . \n\t', '\n\t\t Academic Press . \n\t', '\n\t\t Reprinted in Pragmatics . \n\t', '\n\t\t A Reader , Steven Davis editor , Oxford University Press , 1991 . \n\t', '\n\t\t Riccardo Serafin . \n\t', '\n\t\t 2003. Feature Latent Semantic Analysis for dialogue act interpretation . \n\t', '\n\t\t Master\x92s thesis , University of Illinois - Chicago . \n\t', '\n\t\t A. Stolcke , K. Ries , N. Coccaro , E. Shriberg , R. Bates , D. Jurafsky , P. Taylor , R. Martin , C. Van Ess-Dykema , and M. Meteer . \n\t', '\n\t\t 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech . \n\t', '\n\t\t Computational Linguistics , 26(3):339\x96373 . \n\t', '\n\t\t Douglas M. Towne . \n\t', '\n\t\t 1997. Approximate reasoning techniques for intelligent diagnostic instruction . \n\t', '\n\t\t International Journal ofArtificial Intelligence in Education . \n\t', '\n\t\t Peter Wiemer-Hastings . \n\t', '\n\t\t 2001. Rules for syntax , vectors for semantics . \n\t', '\n\t\t In CogSci01 , Proceedings of the Twenty-Third Annual Meeting of the Cognitive Science Society , Edinburgh , Scotland . \n\t', '\n\t\t Determining the Specificity of Terms using Compositional and Con- textual Information Pum-Mo Ryu Department of Electronic Engineering and Computer Science KAIST Pum-Mo.Ryu@kaist.ac.kr Abstract This paper introduces new specificity determining methods for terms using compositional and contextual information . \n\t', '\n\t\t Specificity of terms is the quantity of domain specific information that is contained in the terms . \n\t', '\n\t\t The methods are modeled as information theory like measures . \n\t', '\n\t\t As the methods don\x92t use domain specific information , they can be applied to other domains without extra processes . \n\t', '\n\t\t Experiments showed very promising result with the precision of 82.0 % when the methods were applied to the terms in MeSH thesaurus . \n\t', '\n\t\t 1. Introduction Terminology management concerns primarily with terms , i.e. , the words that are assigned to concepts used in domain-related texts . \n\t', '\n\t\t A term is a meaningful unit that represents a specific concept within a domain \n\t\t']",Positive
"['\n\t\t Specificity of a term represents the quantity of domain specific information contained in the term . \n\t', '\n\t\t If a term has large quantity of domain specific information , specificity value of the term is large ; otherwise specificity value of the term is small . \n\t', '\n\t\t Specificity of term X is quantified to positive real number as equation ( 1 ) . \n\t', '\n\t\t Spec(X)^ R+ ( 1 ) Specificity of terms is an important necessary condition in term hierarchy , i.e. , if X1 is one of ancestors of X2 , then Spec(X1) is less than Spec(X2) . \n\t', '\n\t\t Specificity can be applied in automatic construction and evaluation of term hierarchy . \n\t', '\n\t\t When domain specific concepts are represented as terms , the terms are classified into two categories based on composition of unit words . \n\t', '\n\t\t In the first category , new terms are created by adding modifiers to existing terms . \n\t', '\n\t\t For example \x93insulin-dependent diabetes mellitus\x94 was created by adding modifier \x93insulin-dependent\x94 to its hypernym \x93diabetes mellitus\x94 as in Table 1 . \n\t', '\n\t\t In English , the specific level terms are very commonly compounds of the generic level term and some modifier \n\t\t']",Positive
"['\n\t\t In this case , compositional information is important to get their meaning . \n\t', '\n\t\t In the second category , new terms are created independently to existing terms . \n\t', '\n\t\t For example , \x93wolfram syndrome\x94 is semantically related to its ancestor terms as in Table 1 . \n\t', '\n\t\t But it shares no common words with its ancestor terms . \n\t', '\n\t\t In this case , contextual information is used to discriminate the features of the terms . \n\t', '\n\t\t Node Number Terms C18.452.297 diabetes mellitus C18.452.297.267 insulin-dependent diabetes mellitus C18.452.297.267.960 wolfram syndrome Table 1 . \n\t', '\n\t\t Subtree of MeSH1 tree . \n\t', '\n\t\t Node numbers represent hierarchical structure of terms Contextual information has been mainly used to represent the characteristics of terms . \n\t', '\n\t\t ( Caraballo , 1999A ) \n\t\t']",Positive
"['\n\t\t ( Caraballo , 1999B ) also used contextual information to determine the specificity of nouns . \n\t', '\n\t\t Contrary , compositional information of terms has not been commonly discussed . \n\t', '\n\t\t 1 MeSH is available at http://www.nlm.nih.gov/mesh . \n\t', '\n\t\t MeSH 2003 was used in this research . \n\t', '\n\t\t We propose new specificity measuring methods based on both compositional and contextual information . \n\t', '\n\t\t The methods are formulated as information theory like measures . \n\t', ""\n\t\t Because the methods do n't use domain specific information , they are easily adapted to terms of other domains . \n\t"", '\n\t\t This paper consists as follow : compositional and contextual information is discussed in section 2 , information theory like measures are described in section 3 , experiment and evaluation is discussed in section 4 , finally conclusions are drawn in section 5. 2 . \n\t', '\n\t\t Information for Term Specificity In this section , we describe compositional information and contextual information . \n\t', '\n\t\t 2.1 . \n\t', '\n\t\t Compositional Information By compositionality , the meaning of whole term can be strictly predicted from the meaning of the individual words \n\t\t']",Positive
"['\n\t\t Many terms are created by appending modifiers to existing terms . \n\t', '\n\t\t In this mechanism , features of modifiers are added to features of existing terms to make new concepts . \n\t', '\n\t\t Word frequency and tf.idf value are used to quantify features of unit words . \n\t', '\n\t\t Internal modifier-head structure of terms is used to measure specificity incrementally . \n\t', '\n\t\t We assume that terms composed of low frequency words have large quantity of domain information . \n\t', '\n\t\t Because low frequency words appear only in limited number of terms , the words can clearly discriminate the terms to other terms . \n\t', '\n\t\t tf.idf , multiplied value of term frequency ( tf ) and inverse document frequency ( idf ) , is widely used term weighting scheme in information retrieval \n\t\t']",Positive
"['\n\t\t Words with high term frequency and low document frequency get large tf.idf value . \n\t', '\n\t\t Because a document usually discusses one topic , and words of large tf.idf values are good index terms for the document , the words are considered to have topic specific information . \n\t', '\n\t\t Therefore , if a term includes words of large tf.idf value , the term is assumed to have topic or domain specific information . \n\t', '\n\t\t If the modifier-head structure of a term is known , the specificity of the term is calculated incrementally starting from head noun . \n\t', '\n\t\t In this manner , specificity value of a term is always larger than that of the base ( head ) term . \n\t', '\n\t\t This result answers to the assumption that more specific term has larger specificity value . \n\t', '\n\t\t However , it is very difficult to analyze modifier-head structure of compound noun . \n\t', '\n\t\t We use simple nesting relations between terms to analyze structure of terms . \n\t', '\n\t\t A term X is nested to term Y , when X is substring of Y \n\t\t']",Positive
"['\n\t\t For example two terms , \x93diabetes mellitus\x94 and \x93insulin dependent diabetes mellitus\x94 , are all disease names , and the former is nested in the latter . \n\t', '\n\t\t In this case , \x93diabetes mellitus\x94 is base term and \x93insulin dependent\x94 is modifier of \x93insulin dependent diabetes mellitus\x94 by definition 1 . \n\t', '\n\t\t If multiple terms are nested in a term , the longest term is selected as head term . \n\t', '\n\t\t Specificity of Y is measured as equation ( 2 ) . \n\t', '\n\t\t Spec(Y) = Spec(X)+^^ Spec(W1)+^^ Spec(W2)(2) where Spec(X) , Spec(W1) , and Spec(W2) are specificity values of X , W1 , W2 respectively . \n\t', '\n\t\t ^ and ^ , real numbers between 0 and 1 , are weighting schemes for specificity of modifiers . \n\t', '\n\t\t They are obtained experimentally . \n\t', '\n\t\t 2.2. Contextual Information There are some problems that are hard to address using compositional information alone . \n\t', '\n\t\t Firstly , although features of \x93wolfram syndrome\x94 share many common features with features of \x93insulin- dependent diabetes mellitus\x94 in semantic level , they don\x92t share any common words in lexical level . \n\t', '\n\t\t In this case , it is unreasonable to compare two specificity values measured based on compositional information alone . \n\t', '\n\t\t Secondly , when several words are combined to a term , there are additional semantic components that are not predicted by unit words . \n\t', '\n\t\t For example , \x93wolfram syndrome\x94 is a kind of \x93diabetes mellitus\x94 . \n\t', '\n\t\t We can not predict \x93diabetes mellitus\x94 from two separate words \x93wolfram\x94 and \x93syndrome\x94 . \n\t', '\n\t\t Finally , modifier-head structure of some terms is ambiguous . \n\t', '\n\t\t For instance , \x93vampire slayer\x94 might be a slayer who is vampire or a slayer of vampires . \n\t', '\n\t\t Therefore contextual is used to complement these problems . \n\t', '\n\t\t Contextual information is distribution of surrounding words of target terms . \n\t', '\n\t\t For example , the distribution of co-occurrence words of the terms , the distribution of predicates which have the terms as arguments , and the distribution of modifiers of the terms are contextual information . \n\t', '\n\t\t General terms usually tend to be modified by other words . \n\t', '\n\t\t Contrary , domain specific terms don\x92t tend to be modified by other words , because they have sufficient information in themselves ( Caraballo , 1999B ) . \n\t', '\n\t\t Under this assumption , we use probabilistic distribution of modifiers as contextual information . \n\t', '\n\t\t Because domain specific terms , unlike general words , are rarely modified in corpus , it is important to collect statistically sufficient modifiers from given corpus . \n\t', '\n\t\t Therefore accurate text processing , such as syntactic parsing , is needed to extract modifiers . \n\t', '\n\t\t As Caraballo\x92s work was for general words , they extracted only rightmost prenominals as context information . \n\t', '\n\t\t We use Conexor functional dependency parser \n\t\t']",Positive
"['\n\t\t Among many dependency functions defined in Conexor parser , \x93attr\x94 and \x93mod\x94 functions are used to extract modifiers from analyzed structures . \n\t', '\n\t\t If a term or modifiers of the term do not occur in corpus , specificity of the term can not be measured using contextual information 3. Specificity Measuring Methods In this section , we describe information theory like methods using compositional and contextual information . \n\t', '\n\t\t Here , we call information theory like methods , because some probability values used in these methods are not real probability , rather they are relative weight of terms or words . \n\t', '\n\t\t Because information theory is well known formalism describing information , we adopt the mechanism to measure information quantity of terms . \n\t', '\n\t\t In information theory , when a message with low probability occurs on channel output , the amount of surprise is large , and the length of bits to represent this message becomes long . \n\t', '\n\t\t Therefore the large quantity of information is gained by this message \n\t\t']",Positive
"['\n\t\t If we consider the terms in a corpus as messages of a channel output , the information quantity of the terms can be measured using various statistics acquired from the corpus . \n\t', '\n\t\t A set of terms is defined as equation ( 3 ) for further explanation . \n\t', '\n\t\t T={tk|1^k^n} ( 3 ) where tk is a term and n is total number of terms . \n\t', '\n\t\t In next step , a discrete random variable X is defined as equation ( 4 ) . \n\t', '\n\t\t X={xk|1^k^n} p(xk) = Prob(X = xk ) where xk is an event of a term tk occurs in corpus , p(xk) is the probability of event xk . \n\t', '\n\t\t The information quantity , I(xk) , gained after observing the event xk , is defined by the logarithmic function . \n\t', '\n\t\t Finally I(xk) is used as specificity value of tk as equation ( 5 ) . \n\t', '\n\t\t Spec(tk) ^ I(xk) = ^log p(xk) ( 5 ) In equation ( 5 ) , we can measure specificity of tk , by estimating p(xk) . \n\t', '\n\t\t We describe three estimating methods of p(xk) in following sections . \n\t', '\n\t\t 3.1 . \n\t', '\n\t\t Compositional Information based Method ( Method 1 ) In this section , we describe a method using compositional information introduced in section 2.1 . \n\t', '\n\t\t This method is divided into two steps : In the first step , specificity values of all words are measured independently . \n\t', '\n\t\t In the second step , the specificity values of words are summed up . \n\t', '\n\t\t For detail description , we assume that a term tk consists of one or more words as equation ( 6 ) . \n\t', '\n\t\t tk = w1w2 ... wm ( 6 ) where wi is i-th word in tk . \n\t', '\n\t\t In next step , a discrete random variable Y is defined as equation ( 7 ) . \n\t', '\n\t\t Y={yi |1^ i^ p(yi) = Prob(Y = yi ) where yi is an event of a word wi occurs in term tk , p(yi) is the probability of event yi . \n\t', '\n\t\t Information quantity , I(xk) , in equation ( 5 ) is redefined as equation ( 8 ) based on previous assumption . \n\t', '\n\t\t m I(xk) = ^^ p(yi) log p(yi ) ( 8 ) i=1 where I(xk) is average information quantity of all words in tk . \n\t', '\n\t\t Two information sources , word frequency , tf.idf are used to estimate p(yi) . \n\t', '\n\t\t In this ( 4 ) ( 7 ) mechanism , p(yi) for informative words should be smaller than that of non informative words . \n\t', '\n\t\t When word frequency is used to quantify features of words , p(yi) in equation ( 8 ) is estimated as equation ( 9 ) . \n\t', '\n\t\t where freq(w) is frequency of word w in corpus , PMLE(wi) is maximum likelihood estimation of P(wi) , and j is index of all words in corpus . \n\t', '\n\t\t In this equation , as low frequency words are informative , P(yi) for the words becomes small . \n\t', '\n\t\t When tf.idf is used to quantify features of words , p(yi) in equation ( 8 ) is estimated as equation ( 10 ) . \n\t', '\n\t\t p(yi) ^ pMLE ( wO =1^ tf ^ idf ( wi ) ( 10 ) ^ tf^idf ( ) j where tf idf(w) is tf.idf value of word w . \n\t', '\n\t\t In this equation , as words of large tf. idf values are informative , p(yi) of the words becomes small . \n\t', '\n\t\t 3.2. Contextual Information based Method ( Method 2 ) In this section , we describe a method using contextual information introduced in section 2.2 . \n\t', '\n\t\t Entropy of probabilistic distribution of modifiers for a term is defined as equation ( 11 ) . \n\t', '\n\t\t Hmod ( tk ) = ^^ p(modi , tk ) log p(modi , tk ) ( 11 ) i where p(modi tk ) is the probability of modi modifies tk and is estimated as equation ( 12 ) . \n\t', '\n\t\t pMLE(modi , tk ) = freq(modi,tk) ( 12 ) ^ freq(modj,tk) j where freq(modi,tk) is number of frequencies that modi modifies tk in corpus , j is index of all modifiers of tk in corpus . \n\t', '\n\t\t The entropy calculated by equation ( 11 ) is the average information quantity of all ( modi , tk ) pairs . \n\t', '\n\t\t Specific terms have low entropy , because their modifier distributions are simple . \n\t', '\n\t\t Therefore inversed entropy is assigned to I(xk) in equation ( 5 ) to make specific terms get large quantity of information as equation ( 13 ) . \n\t', '\n\t\t I (xk)^max(Hmod ( ti ) ) ^ Hmod Qk ) 1 ^ ^ i n where the first term of approximation is the maximum value among modifier entropies of all terms . \n\t', '\n\t\t 3.3. Hybrid Method ( Method 3 ) In this section , we describe a hybrid method to overcome shortcomings of previous two methods . \n\t', '\n\t\t This method measures term specificity as equation ( 14 ) . \n\t', '\n\t\t I ( xk ) ^ 1 1 + 1 A ICmp ( xk /1 ) ) l ^^)( ICtx ( xk ) ) where ICmp(xk) and ICtx(xk) are normalized I(xk) values between 0 and 1 , which are measured by compositional and contextual information based methods respectively . \n\t', '\n\t\t ^(0^ ^ ^ 1 ) is weight of two values . \n\t', '\n\t\t If ^ = 0.5 , the equation is harmonic mean of two values . \n\t', '\n\t\t Therefore I(xk) becomes large when two values are equally large . \n\t', '\n\t\t 4. Experiment and Evaluation In this section , we describe the experiments and evaluate proposed methods . \n\t', '\n\t\t For convenience , we simply call compositional information based method , contextual information based method , hybrid method as method 1 , method 2 , method 3 respectively . \n\t', '\n\t\t 4.1. Evaluation A sub-tree of MeSH thesaurus is selected for experiment . \n\t', '\n\t\t \x93metabolic diseases(C18.452)\x94 node is root of the subtree , and the subtree consists of 436 disease names which are target terms of specificity measuring . \n\t', '\n\t\t A set of journal abstracts was extracted from MEDLINE2 database using the disease names as quires . \n\t', '\n\t\t Therefore , all the abstracts are related to some of the disease names . \n\t', '\n\t\t The set consists of about 170,000 abstracts ( 20,000,000 words ) . \n\t', '\n\t\t The abstracts are analyzed using Conexor parser , and various statistics are extracted : 1 ) frequency , tf.idf of the disease names , 2 ) distribution of modifiers of the disease names , 3 ) frequency , tf.idf of unit words of the disease names . \n\t', '\n\t\t The system was evaluated by two criteria , coverage and precision . \n\t', '\n\t\t Coverage is the fraction wj 2 MEDLINE is a database of biomedical articles serviced by National Library of Medicine , USA . \n\t', '\n\t\t ( http://www.nlm.nih.gov ) = p(yi) ^ pMLE ( wi ) .freq(wi ) j ^ freq(wj) ( 9 ) ( 13 ) ( 14 ) Methods Precision Coverage Type I Type II Total Human subjects(Average) 96.6 86.4 87.4 Term frequency 100.0 53.5 60.6 89.5 Term tf· idf 52.6 59.2 58.2 89.5 Compositional Word Freq . \n\t', '\n\t\t 0.37 72.5 69.0 100.0 Information Method ( Method 1 ) Word Freq.+Structure ( ^=^=0.2 ) 100.0 72.8 75.5 100.0 Word tf· idf 44.2 75.3 72.2 100.0 Word tf· idf +Structure ( ^=^=0.2 ) 100.0 76.6 78.9 100.0 Contextual Information Method ( Method 2 ) ( mod cnt>1 ) 90.0 66.4 70.0 70.2 Hybrid Method ( Method 3 ) ( tf· idf + Struct , ^=0.8 ) 95.0 79.6 82.0 70.2 Table 2 . \n\t', '\n\t\t Experimental results ( % ) of the terms which have specificity values by given measuring method as equation ( 15 ) . \n\t', '\n\t\t c # of terms with specificity ( 15 ) # of all terms Method 2 gets relatively lower coverage than method 1 , because method 2 can measure specificity when both the terms and their modifiers appear in corpus . \n\t', '\n\t\t Contrary , method 1 can measure specificity of the terms , when parts of unit words appear in corpus . \n\t', '\n\t\t Precision is the fraction of relations with correct specificity values as equation ( 16 ) . \n\t', '\n\t\t p # of R(p,c) # of all R(p , c ) where R(p,c) is a parent-child relation in MeSH thesaurus , and this relation is valid only when specificity of two terms are measured by given method . \n\t', '\n\t\t If child term c has larger specificity value than that of parent term p , then the relation is said to have correct specificity values . \n\t', '\n\t\t We divided parent-child relations into two types . \n\t', '\n\t\t Relations where parent term is nested in child term are categorized as type I . \n\t', '\n\t\t Other relations are categorized as type II . \n\t', '\n\t\t There are 43 relations in type I and 393 relations in type II . \n\t', '\n\t\t The relations in type I always have correct specificity values provided structural information method described section 2.1 is applied . \n\t', '\n\t\t We tested prior experiment for 10 human subjects to find out the upper bound of precision . \n\t', '\n\t\t The subjects are all medical doctors of internal medicine , which is closely related division to \x93metabolic diseases\x94 . \n\t', '\n\t\t They were asked to identify parent-child relation of given two terms . \n\t', '\n\t\t The average precisions of type I and type II were 96.6 % and 86.4 % respectively . \n\t', '\n\t\t We set these val ues as upper bound of precision for suggested methods . \n\t', '\n\t\t Specificity values of terms were measured with method 1 , method 2 , and method 3 as Table 2 . \n\t', '\n\t\t In method 1 , word frequency based method , word tf.idf based method , and structure information added methods were separately experimented . \n\t', '\n\t\t Two additional methods , based on term frequency and term tf.idf , were experimented to compare compositionality based method and whole term based method . \n\t', '\n\t\t Two methods which showed the best performance in method 1 and method 2 were combined into method 3 . \n\t', '\n\t\t Word frequency and tf.idf based method showed better performance than term based methods . \n\t', '\n\t\t This result indicates that the information of terms is divided into unit words rather than into whole terms . \n\t', '\n\t\t This result also illustrate basic assumption of this paper that specific concepts are created by adding information to existing concepts , and new concepts are expressed as new terms by adding modifiers to existing terms . \n\t', '\n\t\t Word tf.idf based method showed better precision than word frequency based method . \n\t', '\n\t\t This result illustrate that tf.idf of words is more informative than frequency of words . \n\t', '\n\t\t Method 2 showed the best performance , precision 70.0 % and coverage 70.2 % , when we counted modifiers which modify the target terms two or more times . \n\t', '\n\t\t However , method 2 showed worse performance than word tf.idf and structure based method . \n\t', '\n\t\t It is assumed that sufficient contextual information for terms was not collected from corpus , because domain specific terms are rarely modified by other words . \n\t', '\n\t\t Method 3 , hybrid method of method 1 ( tf.idf of words , structure information ) and method 2 , showed the best precision of 82.0 % of all , because the two methods interacted complementary . \n\t', '\n\t\t with correct specificity ( 16 ) The coverage of this method was 70.2 % which equals to the coverage of method 2 , because the specificity value is measured only when the specificity of method 2 is valid . \n\t', '\n\t\t In hybrid method , the weight value ^ = 0.8 indicates that compositional information is more informatives than contextual information when measuring the specificity of domain-specific terms . \n\t', '\n\t\t The precision of 82.0 % is good performance compared to upper bound of 87.4 % . \n\t', '\n\t\t 4.2. Error Analysis One reason of the errors is that the names of some internal nodes in MeSH thesaurus are category names rather disease names . \n\t', ""\n\t\t For example , as \x93acid-base imbalance (C18.452.076)\x94 is name of disease category , it does n't occur as frequently as other real disease names . \n\t"", '\n\t\t Other predictable reason is that we didn\x92t consider various surface forms of same term . \n\t', '\n\t\t For example , although \x93NIDDM\x94 is acronym of \x93non insulin dependent diabetes mellitus\x94 , the system counted two terms independently . \n\t', '\n\t\t Therefore the extracted statistics can\x92t properly reflect semantic level information . \n\t', '\n\t\t If we analyze morphological structure of terms , some errors can be reduced by internal structure method described in section 2.1 . \n\t', '\n\t\t For example , \x93nephrocalcinosis\x94 have modifier-head structure in morpheme level ; \x93nephro\x94 is modifier and \x93calcinosis\x94 is head . \n\t', '\n\t\t Because word formation rules are heavily dependent on the domain specific morphemes , additional information is needed to apply this approach to other domains . \n\t', '\n\t\t 5. Conclusions This paper proposed specificity measuring methods for terms based on information theory like measures using compositional and contextual information of terms . \n\t', '\n\t\t The methods are experimented on the terms in MeSH thesaurus . \n\t', '\n\t\t Hybrid method showed the best precision of 82.0 % , because two methods complemented each other . \n\t', ""\n\t\t As the proposed methods do n't use domain dependent information , the methods easily can be adapted to other domains . \n\t"", '\n\t\t In the future , the system will be modified to handle various term formations such as abbreviated form . \n\t', '\n\t\t Morphological structure analysis of words is also needed to use the morpheme level information . \n\t', '\n\t\t Finally we will apply the proposed methods to terms of other domains and terms in general domains such as WordNet . \n\t', '\n\t\t Acknowledgements This work was supported in part by Ministry of Science & Technology of Korean government and Korea Science & Engineering Foundation . \n\t', '\n\t\t References Caraballo , S. A. 1999A . \n\t', '\n\t\t Automatic construction of a hypernym-labeled noun hierarchy from text Corpora . \n\t', '\n\t\t In the proceedings of ACL Caraballo , S. A. and Charniak , E. 1999B . \n\t', '\n\t\t Determining the Specificity of Nouns from Text . \n\t', '\n\t\t In the proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora Conexor . \n\t', '\n\t\t 2004. Conexor Functional Dependency Grammar Parser . \n\t', '\n\t\t http://www.conexor.com Frantzi , K. , Anahiadou , S. and Mima , H. 2000 . \n\t', '\n\t\t Automatic recognition of multi-word terms : the Cvalue/NC-value method . \n\t', '\n\t\t Journal of Digital Libraries , vol. 3 , num . \n\t', '\n\t\t 2 Grefenstette , G. 1994 . \n\t', '\n\t\t Explorations in Automatic The- saurus Discovery . \n\t', '\n\t\t Kluwer Academic Publishers Haykin , S. 1994 . \n\t', '\n\t\t Neural Network . \n\t', '\n\t\t IEEE Press , pp. 444 Hearst , M. A. 1992 . \n\t', '\n\t\t Automatic Acquisition of Hyponyms from Large Text Corpora . \n\t', '\n\t\t In proceedings of ACL Manning , C. D. and Schutze , H. 1999 . \n\t', '\n\t\t Foundations of Statistical Natural Language Processing . \n\t', '\n\t\t The MIT Presss Pereira , F. , Tishby , N. , and Lee , L. 1993 . \n\t', '\n\t\t Distributational clustering of English words . \n\t', '\n\t\t In the proceedings of ACL Sanderson , M. 1999 . \n\t', '\n\t\t Deriving concept hierarchies from text . \n\t', '\n\t\t In the Proceedings of the 22th Annual ACM S1GIR Conference on Research and Development in Information Retrieval Wright , S. E. , Budin , G .. 1997 . \n\t', '\n\t\t Handbook of Term Management : vol. 1 . \n\t', '\n\t\t John Benjamins publishing company William Croft . \n\t', '\n\t\t 2004. Typology and Universals . \n\t', '\n\t\t 2nd ed . \n\t', '\n\t\t Cambridge Textbooks in Linguistics , Cambridge Univ . \n\t', '\n\t\t Press \n\t', '\n\t\t Minimizing the Length of Non-Mixed Initiative Dialogs R. Bryce Inouye Department of Computer Science Duke University Durham , NC 27708 rbi@cs.duke.edu Abstract Dialog participants in a non-mixed initiative dialogs , in which one participant asks questions exclusively and the other participant responds to those questions exclusively , can select actions that minimize the expected length of the dialog . \n\t', '\n\t\t The choice of question that minimizes the expected number of questions to be asked can be computed in polynomial time in some cases . \n\t', '\n\t\t The polynomial-time solutions to special cases of the problem suggest a number of strategies for selecting dialog actions in the intractable general case . \n\t', '\n\t\t In a simulation involving 1000 dialog scenarios , an approximate solution using the most probable rule set/least probable question resulted in expected dialog length of 3.60 questions per dialog , as compared to 2.80 for the optimal case , and 5.05 for a randomly chosen strategy . \n\t', '\n\t\t 1 Introduction Making optimal choices in unconstrained natural language dialogs may be impossible . \n\t', '\n\t\t The difficulty of defining consistent , meaningful criteria for which behavior can be optimized and the infinite number of possible actions that may be taken at any point in an unconstrained dialog present generally insurmountable obstacles to optimization . \n\t', '\n\t\t Computing the optimal dialog action may be intractable even in a simple , highly constrained model of dialog with narrowly defined measures of success . \n\t', '\n\t\t This paper presents an analysis of the optimal behavior of a participant in non-mixed initiative dialogs , a restricted but important class of dialogs . \n\t', '\n\t\t 2 Non-mixed initiative dialogs In recent years , dialog researchers have focused much attention on the study of mixed-initiative behaviors in natural language dialogs . \n\t', '\n\t\t In general , mixed initiative refers to the idea that control over the content and direction of a dialog may pass from one participant to another . \n\t', '\n\t\t 1 \n\t\t']",Positive
['\n\t\t Our work adopts a definition similar to \n\t\t'],Positive
"['\n\t\t This paper considers non-mixed-initiative dialogs , which we shall take to mean dialogs with the following characteristics : 1 . \n\t', '\n\t\t The dialog has two participants , the leader and the follower , who are working cooperatively to achieve some mutually desired dialog goal . \n\t', '\n\t\t 2. The leader may request information from the follower , or may inform the follower that the dialog has succeeded or failed to achieve the dialog goal . \n\t', ""\n\t\t ' There is no generally accepted consensus as to how ini- tiative should be defined . \n\t"", '\n\t\t 3. The follower may only inform the leader of a fact in direct response to a request for information from the leader , or inform the leader that it cannot fulfill a particular request . \n\t', '\n\t\t The model assumes the leader knows sets of questions .. . \n\t', '\n\t\t such that if all questions in any one set are answered successfully by the follower , the dia- log goal will be satisfied . \n\t', '\n\t\t The sets will be re- ferred to hereafter as rule sets . \n\t', '\n\t\t The leader\x92s task is to find a rule set whose constituent questions can all be successfully answered . \n\t', '\n\t\t The method is to choose a sequence of questions which will lead to its dis- covery . \n\t', '\n\t\t For example , in a dialog in a customer service setting in which the leader attempts to locate the follower\x92s account in a database , the leader might request the follower\x92s name and account number , or might request the name and telephone number . \n\t', '\n\t\t The corresponding rule sets for such a dialog would be and . \n\t', '\n\t\t One complicating factor in the leader\x92s task is that a question in one rule set may occur in several other rule sets so that choosing to ask can have ramifications for several sets . \n\t', '\n\t\t We assume that for every question the leader knows an associated probability that the fol- lower has the knowledge necessary to answer.2 These probabilities enable us to compute an expected length for a dialog , measured by the number of questions asked by the leader . \n\t', '\n\t\t Our goal in selecting a sequence of questions will be to minimize the expected length of the dialog . \n\t', '\n\t\t The probabilities may be estimated by aggregating the results from all interactions , or a more sophisticated individualized model might be maintained for each participant . \n\t', '\n\t\t Some examples of how these probabilities might be estimated can be 2In addition to modeling the follower\x92s knowledge , these probabilities can also model aspects of the dialog system\x92s performance , such as the recognition rate of an automatic speech recognizer . \n\t', '\n\t\t found in \n\t\t']",Positive
"['\n\t\t Our model of dialog derives from rule-based theories of dialog structure , such as \n\t\t']",Positive
"['\n\t\t In particular , this form of the problem models exactly the \x93missing axiom theory\x94 of Smith and Hipp ( 1994 ; 1995 ) which proposes that dialog is aimed at proving the top-level goal in a theorem-proving tree and \x93missing axioms\x94 in the proof provide motivation for interactions with the dialog partner . \n\t', '\n\t\t The rule sets are sets of missing axioms that are sufficient to complete the proof of the top-level goal . \n\t', '\n\t\t Our format is quite general and can model other dialog systems as well . \n\t', '\n\t\t For example , a dialog system that is organized as a decision tree with a question at the root , with additional questions at successor branches , can be modeled by our format . \n\t', '\n\t\t As an example , suppose we have top- level goal and these rules to prove it : ( AND ) implies ( OR ) implies . \n\t', '\n\t\t The corresponding rule sets are . \n\t', '\n\t\t If all of the questions in either or are satisfied , will be proven . \n\t', '\n\t\t If we have values for the probabilities , and , we can design an optimum ordering of the questions to minimize the expected length of dialogs . \n\t', '\n\t\t Thus if is much smaller than , we would ask before asking . \n\t', '\n\t\t The reader might try to decide when should be asked before any other questions in order to minimize the expected length of dialogs . \n\t', '\n\t\t The rest of the paper examines how the leader can select the questions which minimize the overall expected length of the dialog , as measured by the number of questions asked . \n\t', '\n\t\t Each question- response pair is considered to contribute equally to the length . \n\t', '\n\t\t Sections 3 , 4 , and 5 describe polynomial-time algorithms for finding the optimum order of questions in three special instances of the question ordering optimization problem . \n\t', '\n\t\t Section 6 gives a polynomial-time method to approximate optimum behavior in the general case of rule sets which may have many common questions . \n\t', '\n\t\t = = 3 Case : One rule set Many dialog tasks can be modeled with a single rule set . \n\t', '\n\t\t For example , a leader might ask the follower to supply values for each field in a form . \n\t', '\n\t\t Here the optimum strategy is to ask the questions first that have the least probability of being successfully answered . \n\t', '\n\t\t Theorem 1 . \n\t', '\n\t\t Given a rule set , asking the questions in the order of their probability of success ( least first ) results in the minimum expected dialog length ; that is , for where is the probability that the follower will answer question success- fully . \n\t', '\n\t\t A formal proof is available in a longer version of this paper . \n\t', '\n\t\t Informally , we have two cases ; the first assumes that all questions are answered successfully , leading to a dialog length of , since questions will be asked and then answered . \n\t', '\n\t\t The second case assumes that some will not be answered successfully . \n\t', '\n\t\t The expected length increases as the probabilities of success of the questions asked increases . \n\t', '\n\t\t However , the expected length does not depend on the probability of success for the last question asked , since no questions follow it regardless of the outcome . \n\t', '\n\t\t Therefore , the question with the greatest probability of success appears at the end of the optimal ordering . \n\t', '\n\t\t Similarly , we can show that given the last question in the ordering , the expected length does not depend upon the probability of the second to last question in the ordering , and so on until all questions have been placed in the proper position . \n\t', '\n\t\t The optimal ordering is in order of increasing probability of success . \n\t', '\n\t\t 4 Case : Two independent rule sets We now consider a dialog scenario in which the leader has two rule sets for completing the dialog task . \n\t', '\n\t\t Definition 4.1 . \n\t', '\n\t\t Two rule sets and are independent if. . \n\t', '\n\t\t If is non-empty , then the members of are said to be common to and . \n\t', '\n\t\t A question is unique to rule set if andfor all In a dialog scenario in which the leader has multiple , mutually independent rule sets for accomplishing the dialog goal , the result of asking a question contained in one rule set has no effect on the success or failure of the other rule sets known by the leader . \n\t', '\n\t\t Also , it can be shown that if the leader makes optimal decisions at each turn in the dialog , once the leader begins asking questions belonging to one rule set , it should continue to ask questions from the same rule set until the rule set either succeeds or fails . \n\t', '\n\t\t The problem of selecting the question that minimizes the expected dialog length becomes the problem of selecting which rule set should be used first by the leader . \n\t', '\n\t\t Once the rule set has been selected , Theorem 1 shows how to select a question from the selected rule set that minimizes . \n\t', '\n\t\t By expected dialog length , we mean the usual definition of expectation Thus , to calculate the expected length of a dialog , we must be able to enumerate all of the possible outcomes of that dialog , along with the probability of that outcome occurring , and the length associated with that outcome . \n\t', '\n\t\t Before we show how the leader should decide which rule set it should use first , we introduce some notation . \n\t', '\n\t\t The expected length in case of failure for an ordering of the questions of a rule set is the expected length of the dialog that would result if were the only rule set available to the leader , the leader asked questions in the order given by , and one of the questions in failed . \n\t', '\n\t\t The expected length in case of failure is The factor is a scaling factor that ac- counts for the fact that we are counting only cases in which the dialog fails . \n\t', '\n\t\t We will let represent the minimum expected length in case of failure for rule set , obtained by ordering the questions of by increasing probability of success , as per Theorem 1 . \n\t', '\n\t\t The probability of success of a rule set is . \n\t', '\n\t\t The definition , of probability of success of a rule set assumes that the probabilities of success for individual questions are mutually independent . \n\t', '\n\t\t Theorem 2 . \n\t', '\n\t\t Let be the set of mutu- ally independent rule sets available to the leader for accomplishing the dialog goal . \n\t', '\n\t\t For a rule set in , let be the probability of success of , be the number of questions in , and be the minimum expected length in case offailure . \n\t', '\n\t\t To minimize the expected length of the dialog , the leader should select the question with the least probability of success from the rule set with the least value of . \n\t', '\n\t\t Proof : If the leader uses questions from first , the expected dialog length is The first term , , is the probability of success for times the length of . \n\t', '\n\t\t The second term , , is the probability that will and will succeed times the length of that dialog . \n\t', '\n\t\t The third term , , is the probability that both and fail times the associated length . \n\t', '\n\t\t We can multiply out and rearrange terms to get If the leader uses questions from first , is Comparing and , and eliminating any common terms , we find that is the correct ordering if Thus , if the above inequality holds , then , and the leader should ask questions from first . \n\t', '\n\t\t Otherwise , , and the leader should ask questions from first . \n\t', '\n\t\t We conjecture that in the general case of mutually independent rule sets , the proper ordering of rule sets is obtained by calculating for each rule set , and sorting the rule sets by those values . \n\t', '\n\t\t Preliminary experimental evidence supports this conjecture , but no formal proof has been derived yet . \n\t', '\n\t\t Note that calculating and for each rule set takes polynomial time , as does sorting the rule sets into their proper order and sorting the questions within each rule set . \n\t', '\n\t\t Thus the solution can be obtained in polynomial time . \n\t', '\n\t\t As an example , consider the rule sets and . \n\t', '\n\t\t Suppose that we assign and . \n\t', '\n\t\t In this case , and are the same for both rule sets . \n\t', '\n\t\t However , and , so evaluating for both rule sets , we discover that asking questions from first results in the minimum expected dialog length . \n\t', '\n\t\t . \n\t', '\n\t\t In this section , we will use to denote the minimum expected length of the dialog ( computed using Theorem 1 ) resulting from the leader using only to accomplish the dialog task . \n\t', '\n\t\t The notation will denote the minimum expected length of the dialog resulting from the leader using only the rule set to accomplish the dialog task . \n\t', '\n\t\t For example , a rule set with and , has and . \n\t', '\n\t\t Theorem 3 . \n\t', '\n\t\t Given rule sets and , such that , if the leader asks questions from until either succeeds or fails before asking any questions unique to , then the ordering of questions of that results in the minimum expected dialog length is given by ordering the questions by increasing , where The proof is in two parts . \n\t', '\n\t\t First we show that the questions unique to should be ordered by 5 Case : Two rule sets , one common question We now examine the simplest case in which the rule sets are not mutually independent : the leader has two rule sets and , and dering if immediately follows in the or- dering . \n\t', '\n\t\t is the expected length for the or- dering with at position . \n\t', '\n\t\t We can show that if then Figure 1 : A general expression for the expected dialog length for the dialog scenario described in section 5 . \n\t', '\n\t\t The questions of are asked in the arbitrary order , where is the question common to and . \n\t', '\n\t\t and are defined in Section 5. increasing probability of success given that the po- sition of is fixed . \n\t', '\n\t\t Then we show that given the correct ordering of unique questions of , should appear in that ordering at the position where falls in the correspond- ing sequence of questions probabilities of success . \n\t', '\n\t\t Space considerations preclude a complete listing of the proof , but an outline follows . \n\t', '\n\t\t Figure 1 shows an expression for the expected dialog length for a dialog in which the leader asks questions from until either succeeds or fails before asking any questions unique to . \n\t', '\n\t\t The expression assumes an arbitrary ordering . \n\t', '\n\t\t Note that if a question occurring before fails , the rest of the dialog has a minimum expected length . \n\t', '\n\t\t If fails , the dialog terminates . \n\t', '\n\t\t If a question occurring after fails , the rest of the dialog has minimum expected length by a process similar to that used in the proof of Theorem 2 . \n\t', '\n\t\t Since the unique questions in are ordered by increasing probability of success , finding the optimal position of the common question in the ordering of the questions of corresponds to the problem of finding where the value of falls in the sorted list of proba- bilities of success of the unique questions of . \n\t', '\n\t\t If the value immediately precedes the value of in the list , then the common question should immediately precede in the optimal ordering of questions of . \n\t', '\n\t\t Theorem 3 provides a method for obtaining the optimal ordering of questions in , given that is selected first by the leader . \n\t', '\n\t\t The leader can use the same method to determine the optimal ordering of the questions of if is selected first . \n\t', '\n\t\t The two optimal orderings give rise to two different expected dialog lengths ; the leader should select the rule set and ordering that leads to the minimal expected dialog length . \n\t', '\n\t\t The calculation can be done in polynomial time . \n\t', '\n\t\t 6 Approximate solutions in the general case Specific instances of the optimization problem can be solved in polynomial time , but the general case has worst-case complexity that is exponential in the number of questions . \n\t', '\n\t\t To approximate the optimal solution , we can use some of the insights gained from the analysis of the special cases to generate methods for selecting a rule set , and selecting a question from the chosen rule set . \n\t', '\n\t\t Theorem 1 says that if there is only one rule set available , then the least probable question should be asked first . \n\t', '\n\t\t We can also observe that if the dialog succeeds , then in general , we would like to minimize the number of rule sets that must be tried before succeeding . \n\t', '\n\t\t Combining these two observations produces a policy of selecting the question with the minimal probability of success from the rule set with the maximal probability of success . \n\t', '\n\t\t . \n\t', '\n\t\t If we fix the position of , we can show that the questions unique to must be ordered by increasing probability of success in the optimal ordering . \n\t', '\n\t\t The proof proceeds by showing that switching the positions of any two unique questions and in an arbitrary ordering of the questions of , where occurs before in the original ordering , the expected length for the new ordering is less than the expected length for the original ordering if and only if. . \n\t', '\n\t\t After showing that the unique questions of must be ordered by increasing probability of suc- cess in the optimal ordering , we must then show how to find the position of in the optimal or- dering . \n\t', '\n\t\t We say that occurs at position in or- Method Avg . \n\t', '\n\t\t length Optimal 2.80 Most prob . \n\t', '\n\t\t rule set/least prob . \n\t', '\n\t\t question 3.60 Most prob . \n\t', '\n\t\t rule set/random question 4.26 Random rule set/most prob . \n\t', '\n\t\t question 4.26 Random rule set/random question 5.05 Table 1 : Average expected dialog length ( measured in number of leader questions ) for the optimal case and several simple approximation methods over 1000 dialog scenarios . \n\t', '\n\t\t Each scenario consisted of 6 rule sets of 2 to 5 questions each , created from a pool of 9 different questions . \n\t', '\n\t\t We tested this policy by generating 1000 dialog scenarios . \n\t', '\n\t\t First , a pool of nine questions with randomly assigned probabilities of success was generated . \n\t', '\n\t\t Six rule sets were created using these nine questions , each containing between two and five questions . \n\t', '\n\t\t The number of questions in each rule set was selected randomly , with each value being equally probable . \n\t', '\n\t\t We then calculated the expected length of the dialog that would result if the leader were to select questions according to the following five schemes : 1 . \n\t', '\n\t\t Optimal 2 . \n\t', '\n\t\t Most probable rule set , least probable question 3 . \n\t', '\n\t\t Random rule set , least probable question 4 . \n\t', '\n\t\t Most probable rule set , random question 5 . \n\t', '\n\t\t Random rule set , random question . \n\t', '\n\t\t The results are summarized in Table 1 . \n\t', '\n\t\t 7 Further Research We intend to discover other special cases for which polynomial time solutions exist , and investigate other methods for approximating the optimal solution . \n\t', '\n\t\t With a larger library of studied special cases , even if polynomial time solutions do not exist for such cases , heuristics designed for use in special cases may provide better performance . \n\t', '\n\t\t Another extension to this research is to extend the information model maintained by the leader to allow the probabilities returned by the model to be non-independent . \n\t', '\n\t\t 8 Conclusions Optimizing the behavior of dialog participants can be a complex task even in restricted and specialized environments . \n\t', '\n\t\t For the case of non-mixed ini- tiative dialogs , selecting dialog actions that minimize the overall expected dialog length is a nontrivial problem , but one which has some solutions in certain instances . \n\t', '\n\t\t A study of the characteristics of the problem can yield insights that lead to the development of methods that allow a dialog participant to perform in a principled way in the face of intractable complexity . \n\t', '\n\t\t Acknowledgments This work was supported by a grant from SAIC , and from the US Defense Advanced Research Projects Agency . \n\t', '\n\t\t References Robin Cohen , Coralee Allaby , Christian Cumbaa , Mark Fitzgerald , Kinson Ho , Bowen Hui , Celine Latulipe , Fletcher Lu , Nancy Moussa , David Poo- ley , Alex Qian , and Saheem Siddiqi . \n\t', '\n\t\t 1998. What is initiative ? \n\t', '\n\t\t User Modeling and User-Adapted Interaction , 8(3-4):171\x96214 . \n\t', '\n\t\t C. Conati , A. Gerntner , and K. Vanlehn . \n\t', '\n\t\t 2002. Using bayesian networks to manage uncertainty in user modeling . \n\t', '\n\t\t User Modeling and User-Adapted Interaction , 12(4):371\x96417 . \n\t', '\n\t\t Barbara Grosz and Sarit Kraus . \n\t', '\n\t\t 1996. Collaborative plans for complex group action . \n\t', '\n\t\t Artificial Intelligence , 86(2):269\x96357 . \n\t', '\n\t\t Curry I. Guinn. 1999 . \n\t', '\n\t\t An analysis of initiative selection in collaborative task-oriented discourse . \n\t', '\n\t\t User Modeling and User-adapted Interaction , 8(3- 4):255\x96314 . \n\t', '\n\t\t K. Lochbaum . \n\t', '\n\t\t 1998. A collaborative planning model of intentional structure . \n\t', '\n\t\t Computational Linguistics , 24(4):525\x96572 . \n\t', '\n\t\t C. R. Perrault and J. F. Allen . \n\t', '\n\t\t 1980. A plan-based analysis of indirect speech acts . \n\t', '\n\t\t Computational Linguistics , 6(3-4):167\x96182 . \n\t', '\n\t\t Ronnie . \n\t', '\n\t\t W. Smith and D. Richard Hipp . \n\t', '\n\t\t 1994. Spoken Natural Language Dialog Systems : A Practical Approach . \n\t', '\n\t\t Oxford UP , New York . \n\t', '\n\t\t Ronnie W. Smith and D. Richard Hipp . \n\t', '\n\t\t 1995. An architecture for voice dialog systems based on prolog- style theorem proving . \n\t', '\n\t\t Computational Linguistics , 21(3):281\x96320 . \n\t', '\n\t\t I. Zukerman and D. Albrecht . \n\t', '\n\t\t 2001 . \n\t', '\n\t\t Predictive statistical models for user modeling . \n\t', '\n\t\t User Modeling and User-Adapted Interaction , 11(1-2):5\x9618 . \n\t', '\n\t\t Searching for Topics in a Large Collection of Texts Martin Holub Ji^r´í Semeck´y Ji^r´í Divi^s Center for Computational Linguistics Charles University , Prague holub|semecky @ufal.mff.cuni.cz jiri.divis@atlas.cz Abstract We describe an original method that automatically finds specific topics in a large collection of texts . \n\t', '\n\t\t Each topic is first identified as a specific cluster of texts and then represented as a virtual concept , which is a weighted mixture of words . \n\t', '\n\t\t Our intention is to employ these virtual concepts in document indexing . \n\t', '\n\t\t In this paper we show some preliminary experimental results and discuss directions of future work . \n\t', '\n\t\t 1 Introduction In the field of information retrieval ( for a detailed survey see e.g. \n\t\t']",Positive
"['\n\t\t Within the framework of the well known vector model , the indexed elements are usually individual words , which leads to high dimensional vectors . \n\t', '\n\t\t However , there are several approaches that try to reduce the high dimensionality of the vectors in order to improve the effectivity of retrieving . \n\t', '\n\t\t The most famous is probably the method called Latent Semantic Indexing ( LSI ) , introduced by \n\t\t']",Positive
"['\n\t\t Other two approaches which inspired us , namely \n\t\t']",Positive
"['\n\t\t Our idea is to establish a system of \x93virtual concepts\x94 , which are linear functions represented by vectors , extracted from automatically discovered \x93concept-formative clusters\x94 of documents . \n\t', '\n\t\t Shortly speaking , concept-formative clusters are semantically coherent and specific sets of documents , which represent specific topics . \n\t', '\n\t\t This idea was originally proposed by \n\t\t']",Positive
"['\n\t\t The paper is organized as follows . \n\t', '\n\t\t In section 2 we formalize the notion of concept-formative clusters and give a heuristic method of finding them . \n\t', '\n\t\t Section 3 first introduces virtual concepts in a formal way and shows an algorithm to construct them . \n\t', '\n\t\t Then , some experiments are shown . \n\t', '\n\t\t In sections 4 we compare our model with another approach and give a brief survey of some open questions . \n\t', '\n\t\t Finally , a short summary is given in section 5 . \n\t', '\n\t\t 2 Concept-formative clusters 2.1 Graph of a text collection Let be a collection of text documents ; is the size of the collection . \n\t', '\n\t\t Now suppose that we have a function , which gives a degree of document similarity for each pair of documents . \n\t', '\n\t\t Then we represent the collection as a graph . \n\t', '\n\t\t Definition : A labeled graph is called graph of collection if where and each edge is labeled by number , called weight of ; is a given document similarity threshold ( i.e. a threshold weight of edge ) . \n\t', '\n\t\t Now we introduce some terminology and neces- sary notation . \n\t', '\n\t\t Let be a graph of col- lection . \n\t', '\n\t\t Each subset is called a cut of ; stands for the complement . \n\t', '\n\t\t If are disjoint cuts then . \n\t', '\n\t\t Both functions are positive . \n\t', '\n\t\t Thus , the specificity of cut can be formalized by the following formula \x97 the greater this value , the more specific the cut ; and are positive parameters , which are used for balancing the two factors . \n\t', '\n\t\t The extensity of cut is defined as a positive function where is a threshold size of cut . \n\t', '\n\t\t is a set of edges within cut ; Definition : The total quality of cut is a pos- itive real function composed of all factors mentioned above and is defined as is called weight of cut ; is a set of edges between cuts and ; is called weight of the connection between cuts and ; is the expected weight of edge in graph ; is the expected weight of cut ; is the expected weight of the connection between cut X and the rest of the collection ; each cut naturally splits the collection into three disjoint subsets where and . \n\t', '\n\t\t 2.2 Quality of cuts Now we formalize the property of \x93being concept- -formative\x94 by a positive real function called quality of cut . \n\t', '\n\t\t A high value of quality means that a cut must be specific and extensive . \n\t', '\n\t\t A cut is called specific if ( i ) the weight is relatively high and ( ii ) the connection between and the rest of the collection is relatively small . \n\t', '\n\t\t The first property is called compactness of cut , and is defined as , while the other is called exhaustivity of cut , which is defined as where the three lambdas are parameters whose purpose is balancing the three factors . \n\t', '\n\t\t To be concept-formative , a cut ( i ) must have a sufficiently high quality and ( ii ) must be locally optimal . \n\t', '\n\t\t 2.3 Local optimization of cuts A cut is called locally optimal regarding quality function if each cut which is only a small modification of the original does not have greater quality , i.e. . \n\t', '\n\t\t Now we describe a local search procedure whose purpose is to optimize any input cut ; if is not locally optimal , the output of the Local Search procedure is a locally optimal cut which results from the original as its local modification . \n\t', '\n\t\t First we need the following definition : Definition : Potential of document with re- spect to cut is a real function :defined as The Local Search procedure is described in Fig . \n\t', '\n\t\t 1. Note that 1 . \n\t', '\n\t\t Local Search gradually generates a sequence of cuts so that Figure 1 : The Local Search Algorithm ( i ) for , and ( ii ) cut always arises from by adding or taking away one document into/from it ; 2. since the quality of modified cuts cannot in- crease infinitely , a finite necessarily exists so that is locally optimal and con- sequently the program stops at least after the -th iteration ; 3. each output cut is locally optimal . \n\t', '\n\t\t Now we are ready to precisely define concept- -formative clusters : Definition : A cut is called a concept- -formative cluster if ( ii ) where is the output of the Local Search algorithm . \n\t', '\n\t\t The whole procedure for finding concept- formative clusters consists of two basic stages : first , a set of initial cuts is found within the whole collection , and then each of them is used as a seed for the Local Search algorithm , which locally optimizes the quality function . \n\t', '\n\t\t Note that are crucial parameters , which strongly affect the whole process of searching and consequently also the character of resulting concept-formative clusters . \n\t', '\n\t\t We have optimized their values by a sort of machine learning , using a small manually annotated collection of texts . \n\t', '\n\t\t When optimized -parameters are used , the Local Search procedure tries to simulate the behavior of human annotator who finds topically coherent clusters in a training collection . \n\t', '\n\t\t The task of -optimization leads to a system of linear inequalities , which we solve via linear programming . \n\t', '\n\t\t As there is no scope for this issue here , we cannot go into details . \n\t', '\n\t\t 3 Virtual concepts In this section we first show that concept- -formative clusters can be viewed as fuzzy sets . \n\t', '\n\t\t In this sense , each concept-formative cluster can be characterized by a membership function . \n\t', '\n\t\t Fuzzy clustering allows for some ambiguity in the data , and its main advantage over hard clustering is that it yields much more detailed information on the structure of the data ( cf. \n\t\t']",Positive
"['\n\t\t Then we define virtual concepts as linear functions which estimate degree of membership of documents in concept-formative clusters . \n\t', '\n\t\t Since virtual concepts are weighted mixtures of words represented as vectors , they can also be seen as virtual documents representing specific topics that emerge in the analyzed collection . \n\t', '\n\t\t Definition : Degree of membership of a document in a concept-formative cluster is a function : . \n\t', '\n\t\t For we define where is a constant . \n\t', '\n\t\t For we define . \n\t', '\n\t\t The following holds true for any concept- -formative cluster and any document : iff ; iff . \n\t', '\n\t\t Now we formalize the notion of virtual con- cepts . \n\t', '\n\t\t Let be vector rep- resentations of documents , where Input : the graph of text collection ; an initial cut . \n\t', '\n\t\t Output : locally optimal cut . \n\t', '\n\t\t Algorithm : loop : if then goto loop if then goto loop end ( i ) where is a threshold quality and Figure 2 : The Greedy Regression Algorithm is the number of indexed terms . \n\t', '\n\t\t We look for such a vector so that approximately holds for any . \n\t', '\n\t\t This vector is then called virtual concept corre- sponding to concept -formative cluster . \n\t', '\n\t\t The task of finding virtual concepts can be solved using the Greedy Regression Algorithm ( GRA ) , originally suggested by Semeck´y ( 2003 ) . \n\t', '\n\t\t 3.1 Greedy Regression Algorithm The GRA is directly based on multiple linear regression ( see e.g. \n\t\t']",Positive
"['\n\t\t The GRA works in iterations and gradually increases the number of non-zero elements in the resulting vector , i.e. the number of words with non-zero weight in the resulting mixture . \n\t', '\n\t\t So this number can be explicitly restricted by a parameter . \n\t', '\n\t\t This feature of the GRA has been designed for the sake of generalization , in order to not overfit the input sample . \n\t', '\n\t\t The input of the GRA consists of ( i ) a sample set of document vectors with the corresponding values of , ( ii ) a maximum number of non-zero elements , and ( iii ) an error threshold . \n\t', '\n\t\t The GRA , which is described in Fig . \n\t', '\n\t\t 2 , requires a procedure for solving multiple linear regression ( MLR ) with a limited number of nonzero elements in the resulting vector . \n\t', '\n\t\t Formally , gets on input a set of vectors ; a corresponding set of values to be approximated ; and a set of indexes of the ele- ments which are allowed to be non-zero in the output vector . \n\t', '\n\t\t The output of the MLR is a vector . \n\t', '\n\t\t Implementation and time complexity For solving multiple linear regression we use a public-domain Java package JAMA ( 2004 ) , developed by the MathWorks and NIST . \n\t', '\n\t\t The computation of inverse matrix is based on the LU decomposition , which makes it faster \n\t\t']",Positive
"['\n\t\t As for the asymptotic time complexity of the GRA , it is in complexity of the MLR since the outer loop runs times at maximum and the inner loop always runs nearly times . \n\t', '\n\t\t The MLR substantially consists of matrix multiplica- tions in dimension and a matrix inversion in dimension . \n\t', '\n\t\t Thus the complexity of the MLR is in because . \n\t', '\n\t\t So the total complexity of the GRA is in . \n\t', '\n\t\t To reduce this high computational complexity , we make a term pre-selection using a heuristic method based on linear programming . \n\t', '\n\t\t Then , the GRA does not need to deal with high-dimensional vectors in , but works with vectors in dimen- sion . \n\t', '\n\t\t Although the acceleration is only linear , the required time has been reduced more than ten times , which is practically significant . \n\t', '\n\t\t 3.2 Experiments The experiments reported here were done on a small experimental collection of Output : end ... output concept ; ... quadratic residual error ; ... number of words in the output concept . \n\t', '\n\t\t Algorithm : , while do for each do output of MLR if then , , Input : pairs where ; ... maximal number of words in output concept ; ... quadratic residual error threshold . \n\t', '\n\t\t where each considered must fulfill for any Czech documents . \n\t', '\n\t\t The texts were articles from two different newspapers and one journal . \n\t', '\n\t\t Each document was morphologically analyzed and lemmatized ( Haji^c , 2000 ) and then indexed and represented as a vector . \n\t', '\n\t\t We indexed only lemmas of nouns , adjectives , verbs , adverbs and numerals whose document frequency was greater than and less than . \n\t', '\n\t\t Then the number of indexed terms was . \n\t', '\n\t\t The cosine similarity was used to compute the document similarity ; threshold was . \n\t', '\n\t\t There were edges in the graph of the collection . \n\t', '\n\t\t We had computed a set of concept-formative clusters and then approximated the corresponding membership functions by virtual concepts . \n\t', '\n\t\t The first thing we have observed was that the quadratic residual error systematically and progresivelly decreases in each GRA iteration . \n\t', '\n\t\t Moreover , the words in virtual concepts are obviously intelligible for humans and strongly suggest the topic . \n\t', '\n\t\t An example is given in Table 1. words in the concept the weights Czech lemma literally transl . \n\t', '\n\t\t bosensk´y Bosnian Srb Serb UNPROFOR UNPROFOR OSN UN Sarajevo Sarajevo \x97 \x97 \x97 \x97 \x97 muslimsk´y Muslim ( adj ) odvolat withdraw srbsk´y Serbian gener´al general ( n ) list paper quadratic residual error : Table 1 : Two virtual concepts ( and ) corresponding to cluster #318 . \n\t', '\n\t\t Another example is cluster #19 focused on \x93pension funds\x94 , which was approximated ( ) by the following words ( literally translated ) : pension ( adj ) , pension ( n ) , fund , additional insurance , inheritance , payment , interest ( n ) , dealer , regulation , lawsuit , August ( adj ) , measure ( n ) , approve , increase ( v ) , appreciation , property , trade ( adj ) , attentively , improve , coupon ( adj ) . \n\t', '\n\t\t ( The signs after the words indicate their positive or negative weights in the concept . \n\t', '\n\t\t ) Figure 3 shows the approximation of this cluster by virtual concept . \n\t', '\n\t\t Figure 3 : The approximation of membership function corresponding to cluster #19 by a virtual concept ( the number of words in the concept ) . \n\t', '\n\t\t 4 Discussion 4.1 Related work A similar approach to searching for topics and employing them for document retrieval has been recently suggested by \n\t\t']",Negative
"['\n\t\t They use document clustering , treat each cluster as a topic , and then define topics as probability distributions of words . \n\t', '\n\t\t They use the Kullback-Leibler divergence with some modification as a distance metric to determine the closeness of a document to a cluster . \n\t', '\n\t\t Although our virtual concepts cannot be interpreted as probability distributions , in this point both approaches are quite similar . \n\t', '\n\t\t The substantial difference is in the clustering method used . \n\t', '\n\t\t Xu and Croft have chosen the K-Means algorithm , \x93for its efficiency\x94 . \n\t', '\n\t\t In contrast to this hard clustering algorithm , ( i ) our method is consistently based on empirical analysis of a text collection and does not require an a priori given number of topics ; ( ii ) in order to induce permeable topics , our concept-formative clusters are not disjoint ; ( iii ) the specificity of our clusters is driven by training samples given by human . \n\t', '\n\t\t Xu and Croft suggest that retrieval based on topics may be more robust in comparison with the classic vector technique : Document ranking against a query is based on statistical correlation between query words and words in a document . \n\t', '\n\t\t Since a document is a small sample of text , the statistics in a document are often too sparse to reliably predict how likely the document is relevant to a query . \n\t', '\n\t\t In contrast , we have much more texts for a topic and the statistics are more stable . \n\t', '\n\t\t By excluding clearly unrelated topics , we can avoid retrieving many of the non-relevant documents . \n\t', '\n\t\t 4.2 Future work As our work is still in progress , there are some open questions , which we will concentrate on in the near future . \n\t', '\n\t\t Three main issues are ( i ) evaluation , ( ii ) parameters setting ( which is closely connected to the previous one ) , and ( iii ) an effective implementation of crucial algorithms ( the current implementation is still experimental ) . \n\t', '\n\t\t As for the evaluation , we are building a manually annotated test collection using which we want to test the capability of our model to estimate inter- -document similarity in comparison with the classic vector model and the LSI model . \n\t', '\n\t\t So far , we have been working with a Czech collection for we also test the impact of morphology and some other NLP methods developed for Czech . \n\t', '\n\t\t Next step will be the evaluation on the English TREC collections , which will enable us to rigorously evaluate if our model really helps to improve IR tasks . \n\t', '\n\t\t The evaluation will also give us criteria for parameters setting . \n\t', '\n\t\t We expect that a positive value of will significantly accelerate the computation without loss of quality , but finding the right value must be based on the evaluation . \n\t', '\n\t\t As for the most important parameters of the GRA ( i.e. the size of the sample set and the number of words in concept ) , these should be set so that the resulting concept is a good membership estimator also for documents not included in the sample set . \n\t', '\n\t\t 5 Summary We have designed and implemented a system that automatically discovers specific topics in a text collection . \n\t', '\n\t\t We try to employ it in document indexing . \n\t', '\n\t\t The main directions for our future work are thorough evaluation of the model and optimization of the parameters . \n\t', '\n\t\t Acknowledgments This work has been supported by the Ministry of Education , project Center for Computational Linguistics ( project LN00A063 ) . \n\t', '\n\t\t References Ricardo A. Baeza-Yates and Berthier A. Ribeiro-Neto . \n\t', '\n\t\t 1999. Modern Information Retrieval . \n\t', '\n\t\t ACM Press / Addison-Wesley . \n\t', '\n\t\t Scott C. Deerwester , Susan T. Dumais , Thomas K. Landauer , George W. Furnas , and Richard A. Harshman . \n\t', '\n\t\t 1990 . \n\t', '\n\t\t Indexing by latent semantic analysis . \n\t', '\n\t\t JASIS , 41(6):391\x96407 . \n\t', '\n\t\t Inderjit S. Dhillon and D. S. Modha . \n\t', '\n\t\t 2001. Concept decompositions for large sparse text data using clustering . \n\t', '\n\t\t Machine Learning , 42(1/2):143\x96175 . \n\t', '\n\t\t Jan Haji^c . \n\t', '\n\t\t 2000. Morphological tagging : Data vs. dictionaries . \n\t', '\n\t\t In Proceedings of the 6th ANLP Conference , 1stNAACL Meeting , pages 94\x96101 , Seattle . \n\t', '\n\t\t Martin Holub . \n\t', '\n\t\t 2003. A new approach to conceptual document indexing : Building a hierarchical system of concepts based on document clusters . \n\t', '\n\t\t In M. Aleksy et al . \n\t', '\n\t\t ( eds . \n\t', '\n\t\t ) : ISICT 2003 , Proceedings of the International Symposium on Information and Communication Technologies , pages 311\x96316 . \n\t', '\n\t\t Trinity College Dublin , Ireland . \n\t', '\n\t\t JAMA . \n\t', '\n\t\t 2004. JAMA : A Java Matrix Package . \n\t', '\n\t\t Public- domain , http://math.nist.gov/javanumerics/jama/ . \n\t', '\n\t\t Leonard Kaufman and Peter J. Rousseeuw . \n\t', '\n\t\t 1990. Finding Groups in Data . \n\t', '\n\t\t John Wiley & Sons . \n\t', '\n\t\t W. H. Press , S. A. Teukolsky , W. T. Vetterling , and B. P. Flannery . \n\t', '\n\t\t 1992. Numerical Recipes in C. Second edition , Cambridge University Press , Cambridge . \n\t', '\n\t\t John A. Rice . \n\t', '\n\t\t 1994. Mathematical Statistics and Data Analysis . \n\t', '\n\t\t Second edition , Duxbury Press , California . \n\t', '\n\t\t Ji^r´ í Semeck´y . \n\t', '\n\t\t 2003. Semantic word classes extracted from text clusters . \n\t', '\n\t\t In 12th Annual Conference WDS 2003 , Proceeding of Contributed Papers . \n\t', '\n\t\t MATFYZPRESS , Prague . \n\t', '\n\t\t Kari Torkkola. 2002 . \n\t', '\n\t\t Discriminative features for document classification . \n\t', '\n\t\t In Proceedings of the International Conference on Pattern Recognition , Quebec City , Canada , August 11\x9615 . \n\t', '\n\t\t Jinxi Xu and W. Bruce Croft . \n\t', '\n\t\t 2000. Topic-based language models for distributed retrieval . \n\t', '\n\t\t In W. Bruce Croft ( ed . \n\t', '\n\t\t ) : Advances in Information Retrieval , pages 151\x96172 . \n\t', '\n\t\t Kluwer Academic Publishers . \n\t', '\n\t\t Temporal Context : Applications and Implications for Computational Linguistics Robert A. Liebscher Department of Cognitive Science University of California , San Diego La Jolla , CA 92037 rliebsch@cogsci.ucsd.edu Abstract This paper describes several ongoing projects that are united by the theme of changes in lexical use over time . \n\t', '\n\t\t We show that paying attention to a document\x92s temporal context can lead to improvements in information retrieval and text categorization . \n\t', '\n\t\t We also explore a potential application in document clustering that is based upon different types of lexical changes . \n\t', '\n\t\t 1 Introduction Tasks in computational linguistics ( CL ) normally focus on the content of a document while paying little attention to the context in which it was produced . \n\t', '\n\t\t The work described in this paper considers the importance of temporal context . \n\t', '\n\t\t We show that knowing one small piece of information\x96a document\x92s publication date\x96can be beneficial for a variety of CL tasks , some familiar and some novel . \n\t', '\n\t\t The field of historical linguistics attempts to categorize changes at all levels of language use , typically relying on data that span centuries \n\t\t']",Positive
"['\n\t\t The recent availability of very large textual corpora allows for the examination of changes that take place across shorter time periods . \n\t', '\n\t\t In particular , we focus on lexical change across decades in corpora of academic publications and show that the changes can be fairly dramatic during a relatively short period of time . \n\t', '\n\t\t As a preview , consider Table 1 , which lists the top five unigrams that best distinguished the field of computational linguistics at different points in time , as derived from the ACL proceedings 1 using the odds ratio measure ( see Section 3 ) . \n\t', '\n\t\t One can quickly glean that the field has become increasingly empirical through time . \n\t', '\n\t\t 1979-84 1985-90 1991-96 1997-02 system natural language knowledge database phrase plan structure logical interpret discourse tree word corpus training model data algorithm unification plan Table 1 : ACL\x92s most characteristic terms for four time periods , as measured by the odds ratio With respect to academic publications , the very nature of the enterprise forces the language used within a discipline to change . \n\t', '\n\t\t An author\x92s word choice is shaped by the preceding literature , as she must say something novel while placing her contribution in the context of what has already been said . \n\t', '\n\t\t This begets neologisms , new word senses , and other types of changes . \n\t', '\n\t\t This paper is organized as follows : In Section 2 , we introduce temporal term weighting , a technique that implicitly encodes time into keyword weights to enhance information retrieval . \n\t', '\n\t\t Section 3 describes the technique of temporalfeature modification , which exploits temporal information to improve the text categorization task . \n\t', '\n\t\t Section 4 introduces several types of lexical changes and a potential application in document clustering . \n\t', '\n\t\t 1 The details of each corpus used in this paper can be found in the appendix . \n\t', '\n\t\t 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 Year Figure 1 : Changing frequencies in AI abstracts 2 Time in information retrieval In the task of retrieving relevant documents based upon keyword queries , it is customary to treat each document as a vector of terms with associated \x93weights\x94 . \n\t', '\n\t\t One notion of term weight simply counts the occurrences of each term . \n\t', '\n\t\t Of more utility is the scheme known as term frequency-inverse document frequency ( TF.IDF ) : where is the weight of term k in document d , is the frequency of k in d , N is the total num- ber of documents in the corpus , and is the total number of documents containing k . \n\t', '\n\t\t Very frequent terms ( such as function words ) that occur in many documents are downweighted , while those that are fairly unique have their weights boosted . \n\t', '\n\t\t Many variations of TF.IDF have been suggested \n\t\t']",Positive
"['\n\t\t Our variation , temporal term weighting ( TTW ) , incorporates a term\x92s IDF at different points in time : Under this scheme , the document collection is divided into T time slices , and N and are computed for each slice t . \n\t', '\n\t\t Figure 1 illustrates why such a modification is useful . \n\t', '\n\t\t It depicts the frequency of the terms neural networks and expert system for each year in a collection of Artificial Intelligence-related dissertation abstracts . \n\t', '\n\t\t Both terms follow a fairly linear trend , moving in opposite directions . \n\t', '\n\t\t As was demonstrated for CL in Section 1 , the terms which best characterize AI have also changed through time . \n\t', '\n\t\t Table 2 lists the top five \x93rising\x94 and \x93falling\x94 bigrams in this corpus , along with their least-squares fit to a linear trend . \n\t', '\n\t\t Lexical variants ( such as plurals ) are omitted . \n\t', '\n\t\t Using an atemporal TF.IDF , both rising and falling terms would be assigned weights proportional only to . \n\t', '\n\t\t A novice user issuing a query would be given a temporally random scattering of documents , some of which might be state-of-the- art , others very outdated . \n\t', '\n\t\t But with TTW , the weights are proportional to the collective \x93community interest\x94 in the term at a given point in time . \n\t', '\n\t\t In academic research documents , this yields two benefits . \n\t', '\n\t\t If a term rises from obscurity to popularity over the duration of a corpus , it is not unreasonable to assume that this term originated in one or a few seminal articles . \n\t', '\n\t\t The term is not very frequent across documents when these articles are published , so its weight in the seminal articles will be amplified . \n\t', '\n\t\t Similarly , the term will be downweighted in articles when it has become ubiquitous throughout the literature . \n\t', '\n\t\t For a falling term , its weight in early documents will be dampened , while its later use will be emphasized . \n\t', '\n\t\t If a term is very frequent in a document after it has been relegated to obscurity , this is likely to be an historical review article . \n\t', '\n\t\t Such an article would be a good place to start an investigation for someone who is unfamiliar with the term . \n\t', '\n\t\t Term r neural network 0.9283 fuzzy logic 0.9035 genetic algorithm 0.9624 real world 0.8509 reinforcement learning 0.8447 artificial intelligence -0.9309 expert system -0.9241 knowledge base -0.9144 problem solving -0.9490 knowledge representation -0.9603 Table 2 : Rising and falling AI terms , 1986-1997 neural networks expert system 4 3.5 3 2.5 2 1.5 1 0.5 2.1 Future work We have discovered clear frequency trends over time in several corpora . \n\t', '\n\t\t Given this , TTW seems beneficial for use in information retrieval , but is in an embryonic stage . \n\t', '\n\t\t The next step will be the development and implementation of empirical tests . \n\t', '\n\t\t IR systems typically are evaluated by measures such as precision and recall , but a different test is necessary to compare TTW to an atemporal TF.IDF . \n\t', '\n\t\t One idea we are exploring is to have a system explicitly tag seminal and historical review articles that are centered around a query term , and then compare the results with those generated by bibliometric methods . \n\t', '\n\t\t Few bibliometric analyses have gone beyond examinations of citation networks and the keywords associated with each article . \n\t', '\n\t\t We would consider the entire text . \n\t', '\n\t\t 3 Time in text categorization Text categorization ( TC ) is the problem of assigning documents to one or more pre-defined categories . \n\t', '\n\t\t As Section 2 demonstrated , the terms which best characterize a category can change through time , so intelligent use of temporal context may prove useful in TC . \n\t', '\n\t\t Consider the example of sorting newswire documents into the categories ENTERTAINMENT , BUSINESS , SPORTS , POLITICS , and WEATHER . \n\t', '\n\t\t Suppose we come across the term athens in a training document . \n\t', '\n\t\t We might expect a fairly uniform distribution of this term throughout the five categories ; that is , C athens = 0.20 for each C . \n\t', '\n\t\t However , in the summer of 2004 , we would expect SPORTS athens to be greatly increased relative to the other categories due to the city\x92s hosting of the Olympic games . \n\t', '\n\t\t Documents with \x93temporally perturbed\x94 terms like athens contain potentially valuable information , but this is lost in a statistical analysis based purely on the content of each document , irrespective of its temporal context . \n\t', '\n\t\t This information can be recovered with a technique we call temporal feature modification ( TFM ) . \n\t', '\n\t\t We first outline a formal model of its use . \n\t', '\n\t\t Each term k is assumed to have a generator Gk that produces a \x93true\x94 distribution C k across all categories . \n\t', '\n\t\t External events at time y can per turb k\x92s generator , causing C k to be differ- ent relative to the background C k computed over the entire corpus . \n\t', '\n\t\t If the perturbation is significant , we want to separate the instances of k at time y from all other instances . \n\t', '\n\t\t We thus treat athens and \x93athens+summer2004\x94 as though they were actually different terms , because they came from two different generators . \n\t', '\n\t\t TFM is a two step process that is captured by this pseudocode : VOCABULARY ADDITIONS : for each class C : for each year y : PreModList(C,y,L) = OddsRatio(C,y,L) ModifyList(y) = DecisionRule(PreModList(C,y,L)) for each term k in ModifyList(y) : Add pseudo-term "" k+y "" to Vocab DOCUMENT MODIFICATIONS : for each document : y = year of doc for each term k : if "" k+y "" in Vocab : replace k with "" k+y "" classify modified document PreModList(C,y,L) is a list of the top L lexemes that , by the odds ratio measure 2 , are highly associated with category C in year y . \n\t', '\n\t\t We test the hypothesis that these come from a perturbed generator in year y , as opposed to the atemporal generator Gk , by comparing the odds ratios of term- category pairs in a PreModList in year y with the same pairs across the entire corpus . \n\t', '\n\t\t Terms which pass this test are added to the final ModifyList(y) for year y . \n\t', '\n\t\t For the results that we report , Decision- Rule is a simple ratio test with threshold factor f . \n\t', '\n\t\t Suppose f is 2.0 : if the odds ratio between C and k is twice as great in year y as it is atemporally , the decision rule is \x93passed\x94 . \n\t', '\n\t\t The generator Gkis considered perturbed in year y and k is added to ModifyList(y) . \n\t', '\n\t\t In the training and testing phases , the documents are modified so that a term k is replaced with the pseudo-term \x93k+y\x94 if it passed the ratio test . \n\t', '\n\t\t 3.1 ACM Classifications We tested TFM on corpora representing genres from academic publications to Usenet postings , 2Odds ratio is defined as , wherep is Pr(k|C) , the probability that term k is present given category C , and q is Pr(k|!C) . \n\t', '\n\t\t Corpus Vocab size No . \n\t', '\n\t\t docs No . \n\t', '\n\t\t cats SIGCHI 4542 1910 20 SIGPLAN 6744 3123 22 DAC 6311 2707 20 Table 3 : Corpora characteristics . \n\t', '\n\t\t Terms occurring at least twice are included in the vocabulary . \n\t', '\n\t\t and it improved classification accuracy in every case . \n\t', '\n\t\t The results reported here are for abstracts from the proceedings of several of the Association for Computing Machinery\x92s conferences : SIGCHI , SIGPLAN , and DAC . \n\t', '\n\t\t TFM can benefit the ACM community through retrospective categorization in two ways : ( 1 ) 7.73 % of abstracts ( nearly 6000 ) across the entire ACM corpus that are expected to have category labels do not have them ; ( 2 ) When a group of terms becomes popular enough to induce the formation of a new category , a frequent occurrence in the computing literature , TFM would separate the \x93old\x94 uses from the \x93new\x94 ones . \n\t', '\n\t\t The ACM classifies its documents in a hierarchy of four levels ; we used an aggregating procedure to \x93flatten\x94 these . \n\t', '\n\t\t The characteristics of each corpus are described in Table 3 . \n\t', '\n\t\t The \x93TC minutiae\x94 used in these experiments are : Stoplist , Porter stemming , 90/10 % train/test split , Laplacian smoothing . \n\t', '\n\t\t Parameters such as type of classifier ( Naïve Bayes , KNN , TF.IDF , Probabilistic indexing ) and threshold factor f were varied . \n\t', '\n\t\t 3.2 Results Figure 2 shows the improvement in classification accuracy for different percentages of terms modified , using the best parameter combinations for each corpus , which are noted in Table 4 . \n\t', '\n\t\t A baseline of 0.0 indicates accuracy without any temporal modifications . \n\t', '\n\t\t Despite the relative paucity of data in terms of document length , TFM still performs well on the abstracts . \n\t', '\n\t\t The actual accuracies when no terms are modified are less than stellar , ranging from 30.7 % ( DAC ) to 33.7 % ( SIGPLAN ) when averaged across all conditions , due to the difficulty of the task ( 20-22 categories ; each document can only belong to one ) . \n\t', '\n\t\t Our aim is simply to show improvement . \n\t', '\n\t\t In most cases , the technique performs best when ^0.05 0 5 10 15 20 25 Percent terms modified Figure 2 : Improvement in categorization performance with TFM , using the best parameter combinations for each corpus making relatively few modifications : the left side of Figure 2 shows a rapid performance increase , particularly for SIGCHI , followed by a period of diminishing returns as more terms are modified . \n\t', '\n\t\t After requiring the one-time computation of odds ratios in the training set for each category/year , TFM is very fast and requires negligible extra storage space . \n\t', '\n\t\t 3.3 Future work The \x93bare bones\x94 version of TFM presented here is intended as a proof-of-concept . \n\t', '\n\t\t Many of the parameters and procedures can be set arbitrarily . \n\t', '\n\t\t For initial feature selection , we used odds ratio because it exhibits good performance in TC \n\t\t']",Positive
"['\n\t\t The ratio test is not a very sophisticated way to choose which terms should be modified , and presently only detects the surges in the use of a term , while ignoring the ( admittedly rare ) declines . \n\t', '\n\t\t Using TFM on a Usenet corpus that was more balanced in terms of documents per category and per year , we found that allowing different terms to \x93compete\x94 for modification was more effective than the egalitarian practice of choosing L terms from each category/year . \n\t', '\n\t\t There is no reason to believe that each category/year is equally likely to contribute temporally perturbed terms . \n\t', '\n\t\t Finally , we would like to exploit temporal con- 0.45 SIGCHI 0.4 0.35 0.3 DAC 0.25 0.2 0.15 SIGPLAN 0.1 0.05 Atemporal baseline 0 Corpus Improvement Classifier n-gram size Vocab frequency min . \n\t', '\n\t\t Ratio threshold f SIGCHI 41.0 % TF.IDF Bigram 10 1.0 SIGPLAN 19.4 % KNN Unigram 10 1.0 DAC 23.3 % KNN Unigram 2 1.0 Table 4 : Top parameter combinations for TFM by improvement in classification accuracy . \n\t', '\n\t\t Vocab frequency min . \n\t', '\n\t\t is the minimum number of times a term must appear in the corpus in order to be included . \n\t', '\n\t\t tiguity . \n\t', '\n\t\t The present implementation treats time slices as independent entities , which precludes the possibility of discovering temporal trends in the data . \n\t', '\n\t\t One way to incorporate trends implicitly is to run a smoothing filter across the temporally aligned frequencies . \n\t', '\n\t\t Also , we treat each slice at annual resolution . \n\t', '\n\t\t Initial tests show that aggregating two or more years into one slice improves performance for some corpora , particularly those with temporally sparse data such as DAC . \n\t', '\n\t\t 4 Future work A third part of this research program , presently in the exploratory stage , concerns lexical ( semantic ) change , the broad class of phenomena in which words and phrases are coined or take on new meanings \n\t\t']",Positive
"['\n\t\t Below we describe an application in document clustering and point toward a theoretical framework for lexical change based upon recent advances in network analysis . \n\t', '\n\t\t Consider a scenario in which a user queries a document database for the term artificial intelligence . \n\t', '\n\t\t We would like to create a system that will cluster the returned documents into three categories , corresponding to the types of change the query has undergone . \n\t', '\n\t\t These responses illustrate the three categories , which are not necessarily mutually exclusive : 1 . \n\t', '\n\t\t \x93This term is now more commonly referred to as AI in this collection\x94 , 2 . \n\t', '\n\t\t \x93These documents are about artificial intelligence , though it is now more commonly called machine learning\x94 , 3 . \n\t', '\n\t\t \x93The following documents are about artificial intelligence , though in this collection its use has become tacit\x94 . \n\t', '\n\t\t Figure 3 : Frequencies in the first ( left bar ) and second ( right bar ) halves of an AI discussion forum 4.1 Acronym formation In Section 2 , we introduced the notions of \x93rising\x94 and \x93falling\x94 terms . \n\t', '\n\t\t Figure 3 shows relative frequencies of two common terms and their acronyms in the first and second halves of a corpus of AI discussion board postings collected from 1983-1988 . \n\t', '\n\t\t While the acronyms increased in frequency , the expanded forms decreased or remained the same . \n\t', '\n\t\t A reasonable conjecture is that in this informal register , the acronyms AI and CS largely replaced the expansions . \n\t', '\n\t\t During the same time period , the more formal register of dissertation abstracts did not show this pattern for any acronym/expansion pairs . \n\t', '\n\t\t 4.2 Lexical replacement Terms can be replaced by their acronyms , or by other terms . \n\t', '\n\t\t In Table 1 , database was listed among the top five terms that were most characteristic of the ACL proceedings in 1979- 1984 . \n\t', '\n\t\t Bisecting this time slice and including bi- 3.5 2.5 0.5 1.5 4 3 2 0 1 1 2 3 4 AI artificial intelligence CS computer science grams in the analysis , data base ranks higher than database in 1979-1981 , but drops much lower in 1982-1984 . \n\t', '\n\t\t Within this brief period of time , we see a lexical replacement event taking hold . \n\t', '\n\t\t In the AI dissertation abstracts , artificial intelligence shows the greatest decline , while the conceptually similar terms machine learning and pattern recognition rank sixth and twelfth among the top rising terms . \n\t', '\n\t\t There are social , geographic , and linguistic forces that influence lexical change . \n\t', '\n\t\t One example stood out as having an easily identified cause : political correctness . \n\t', '\n\t\t In a corpus of dissertation abstracts on communication disorders from 1982- 2002 , the term subject showed the greatest relative decrease in frequency , while participant showed the greatest increase . \n\t', '\n\t\t Among the top ten bigrams showing the sharpest declines were three terms that included the word impaired and two that included disabled . \n\t', '\n\t\t 4.3 \x93Tacit\x94 vocabulary Another , more subtle lexical change involves the gradual disappearance of terms due to their increasingly \x93tacit\x94 nature within a particular community of discourse . \n\t', '\n\t\t Their existence becomes so obvious that they need not be mentioned within the community , but would be necessary for an outsider to fully understand the discourse . \n\t', '\n\t\t Take , for example , the terms backpropagation and hidden layer . \n\t', '\n\t\t If a researcher of neural networks uses these terms in an abstract , then neural network does not even warrant printing , because they have come to imply the presence of neural network within this research community . \n\t', '\n\t\t Applied to IR , one might call this \x93retrieval by implication\x94 . \n\t', '\n\t\t Discovering tacit terms is no simple matter , as many of them will not follow simple is-a relationships ( e.g. terrier is a dog ) . \n\t', '\n\t\t The example of the previous paragraph seems to contain a hierarchical relation , but it is difficult to define . \n\t', '\n\t\t We believe that examining the temporal trajectories of closely related networks of terms may be of use here , and is also part of a more general project that we hope to undertake . \n\t', '\n\t\t Our intention is to improve existing models of lexical change using recent advances in network analysis \n\t\t']",Positive
"['\n\t\t References A. Barabasi , H. Jeong , Z. Neda , A. Schubert , and T. Vi csek . \n\t', '\n\t\t 2002. Evolution of the social network of scientific collaborations . \n\t', '\n\t\t Physica A , 311:590\x96614 . \n\t', '\n\t\t L. Bauer . \n\t', '\n\t\t 1994. Watching English Change . \n\t', '\n\t\t Longman Press , London . \n\t', '\n\t\t S. N. Dorogovtsev and J. F. F. Mendes . \n\t', '\n\t\t 2001. Language as an evolving word web. . \n\t', '\n\t\t Proceedings of The Royal Society ofLondon , Series B , 268(1485):2603\x96 2606 . \n\t', '\n\t\t H. H. Hock . \n\t', '\n\t\t 1991. Principles ofHistoricalLingusitics . \n\t', '\n\t\t Mouton de Gruyter , Berlin . \n\t', '\n\t\t R. J. Jeffers and I. Lehiste . \n\t', '\n\t\t 1979. Principles and MethodsforHistorical Lingusitics . \n\t', '\n\t\t The MIT Press , Cambridge , MA . \n\t', '\n\t\t D. Mladenic . \n\t', '\n\t\t 1998. Machine Learning on non- homogeneous , distributed text data . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , University of Ljubljana , Slovenia . \n\t', '\n\t\t A. Singhal . \n\t', '\n\t\t 1997. Term weighting revisited . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , Cornell University . \n\t', '\n\t\t Appendix : Corpora The corpora used in this paper , preceded by the section in which they were introduced : 1 : The annual proceedings of the Association for Computational Linguistics conference ( 1978- 2002 ) . \n\t', '\n\t\t Accessible at http://acl.ldc.upenn.edu/ . \n\t', '\n\t\t 2 : Over 5000 PhD and Masters dissertation abstracts related to Artificial Intelligence , 1986- 1997 . \n\t', '\n\t\t Supplied by University Microfilms Inc. 3.1 : Abstracts from the ACM-IEEE Design Automation Conference ( DAC ; 1964-2002 ) , Special Interest Groups in Human Factors in Computing Systems ( SIGCHI ; 1982-2003 ) and Programming Languages ( SIGPLAN ; 1973-2003 ) . \n\t', '\n\t\t Supplied by the ACM . \n\t', '\n\t\t See also Table 3 . \n\t', '\n\t\t 3.3 : Hand-collected corpus of six dis- cussion groups : misc.consumers , alt.atheism , rec.arts.books , comp . \n\t', '\n\t\t { arch , graphics.algorithms , lang.c } . \n\t', '\n\t\t Each group contains 1000 documents per year from 1993-2002 . \n\t', '\n\t\t Viewable at http://groups.google.com/ . \n\t', '\n\t\t 4.2 : Over 4000 PhD and Masters dissertation abstracts related to communication disorders , 1982-2002 . \n\t', '\n\t\t Supplied by University Microfilms Inc. \n\t', '\n\t\t Automatic Acquisition of English Topic Signatures Based on a Second Language Xinglong Wang Department of Informatics University of Sussex Brighton , BN1 9QH , UK xw20@sussex.ac.uk Abstract We present a novel approach for automatically acquiring English topic signatures . \n\t', '\n\t\t Given a particular concept , or word sense , a topic signature is a set of words that tend to co-occur with it . \n\t', '\n\t\t Topic signatures can be useful in a number of Natural Language Processing ( NLP ) applications , such as Word Sense Disambiguation ( WSD ) and Text Summarisation . \n\t', '\n\t\t Our method takes advantage of the different way in which word senses are lexicalised in English and Chinese , and also exploits the large amount of Chinese text available in corpora and on the Web. . \n\t', '\n\t\t We evaluated the topic signatures on a WSD task , where we trained a second-order vector co- occurrence algorithm on standard WSD datasets , with promising results . \n\t', '\n\t\t 1 Introduction Lexical knowledge is crucial for many NLP tasks . \n\t', '\n\t\t Huge efforts and investments have been made to build repositories with different types of knowledge . \n\t', '\n\t\t Many of them have proved useful , such as WordNet \n\t\t']",Positive
"['\n\t\t However , in some areas , such as WSD , manually created knowledge bases seem never to satisfy the huge requirement by supervised machine learning systems . \n\t', '\n\t\t This is the so-called knowledge acquisition bottleneck . \n\t', '\n\t\t As an alternative , automatic or semi-automatic acquisition methods have been proposed to tackle the bottleneck . \n\t', '\n\t\t For example , \n\t\t']",Positive
"['\n\t\t The Web provides further ways of overcoming the bottleneck . \n\t', '\n\t\t \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t Another type of method , which exploits differences between languages , has shown great promise . \n\t', '\n\t\t For example , some work has been done based on the assumption that mappings of words and meanings are different in different languages . \n\t', '\n\t\t \n\t\t']",Positive
['\n\t\t \n\t\t'],Negative
"['\n\t\t One problem with relying on bilingual corpora for data collection is that bilingual corpora are rare , and aligned bilingual corpora are even rarer . \n\t', '\n\t\t Mining the Web for bilingual text \n\t\t']",Negative
"['\n\t\t Another problem is that if two languages are closely related , data for some words cannot be collected because different senses of polysemous words in one language often translate to the same word in the other . \n\t', '\n\t\t In this paper , we present a novel approach for automatically acquiring topic signatures ( see Ta- ble 1 for an example of topic signatures ) , which also adopts the cross-lingual paradigm . \n\t', '\n\t\t To solve the problem of different senses not being distinguishable mentioned in the previous paragraph , we chose a language very distant to English \x96 Chinese , since the more distant two languages are , the more likely that senses are lexicalised differently \n\t\t']",Positive
"['\n\t\t Because our approach only uses Chinese monolingual text , we also avoid the problem of shortage of aligned bilingual corpora . \n\t', '\n\t\t We build the topic signatures by using Chinese-English and English- Chinese bilingual lexicons and a large amount of Chinese text , which can be collected either from the Web or from Chinese corpora . \n\t', '\n\t\t Since topic signatures are potentially good training data for WSD algorithms , we set up a task to disambiguate 6 words using a WSD algorithm similar to Sch¨utze\x92s ( 1998 ) context-group discrimination . \n\t', '\n\t\t The results show that our topic signatures are useful for WSD . \n\t', '\n\t\t The remainder of the paper is organised as follows . \n\t', '\n\t\t Section 2 describes the process of acquisition of the topic signatures . \n\t', '\n\t\t Section 3 demonstrates the application of this resource on WSD , and presents the results of our experiments . \n\t', '\n\t\t Section 4 discusses factors that could affect the acquisition process and then we conclude in Section 5 . \n\t', '\n\t\t 2 Acquisition of Topic Signatures A topic signature is defined as : TS = { ( t1 , w1 ) , ... , ( ti , wi ) , ... } , where ti is a term highly correlated to a target topic ( or concept ) with association weight wi , which can be omitted . \n\t', '\n\t\t The steps we perform to produce the topic signatures are described below , and illustrated in Figure 1. 1 . \n\t', '\n\t\t Translate an English ambiguous word w to Chinese , using an English-Chinese lexicon . \n\t', '\n\t\t Given the assumption we mentioned , each sense si of w maps to a distinct Chinese word1 . \n\t', '\n\t\t At the end of this step , we have produced a set C , which consists of Chinese words { c1 , c2 , ... , cn } , where ci is the translation corresponding to sense si of w , and n is the number of senses that w has . \n\t', '\n\t\t 2. Query large Chinese corpora or/and a search engine that supports Chinese using each element in C . \n\t', '\n\t\t Then , for each ci in C , we collect the text snippets retrieved and construct a Chinese corpus . \n\t', '\n\t\t 1It is also possible that the English sense maps to a set of Chinese synonyms that realise the same concept . \n\t', '\n\t\t English ambiguous word W Sense 1 of W English-Chinese Sense 2 of W Lexicon Chinese Search Engine Chinese segmentation and POS tagging ; C Chinese- I English Lexicon Chinese translation o Chinese translation o sense 1 ^ sense 2 1 . \n\t', '\n\t\t Chinese document 1 1 . \n\t', '\n\t\t Chinese document 1 2 . \n\t', '\n\t\t Chinese document 2 ... ... 2 . \n\t', '\n\t\t Chinese document 2 ... ... 1 . \n\t', '\n\t\t { English topic signature 1 } 1 . \n\t', '\n\t\t { English topic signature 1 } 2 . \n\t', '\n\t\t { English topic signature 2 } ... ... 2 . \n\t', '\n\t\t { English topic signature 2 } ... ... Figure 1:Process of automatic acquisition of topic signatures . \n\t', '\n\t\t For simplicity , we assume here that w has two senses . \n\t', '\n\t\t 3. Shallow process these Chinese corpora . \n\t', '\n\t\t Text segmentation and POS tagging are done in this step . \n\t', '\n\t\t 4. Either use an electronic Chinese-English lexicon to translate the Chinese corpora word by word to English , or use machine translation software to translate the whole text . \n\t', '\n\t\t In our experiments , we did the former . \n\t', '\n\t\t The complete process is automatic , and unsupervised . \n\t', '\n\t\t At the end of this process , for each sense si of an ambiguous word w , we have a large set of English contexts . \n\t', '\n\t\t Each context is a topic signature , which represents topical information that tends to co-occur with sense si . \n\t', '\n\t\t Note that an element in our topic signatures is not necessarily a single English word . \n\t', '\n\t\t It can be a set of English words which are translations of a Chinese word c . \n\t', '\n\t\t For example , the component of a topic signature , { vesture , clothing , clothes } , is translated from the Chinese word ~~ . \n\t', '\n\t\t Under the assumption that the majority of c\x92s are unambiguous , which we discuss later , we refer to elements in a topic signature as concepts in this paper . \n\t', '\n\t\t Choosing an appropriate English-Chinese dictionary is the first problem we faced . \n\t', '\n\t\t The one we decided to use is the Yahoo ! \n\t', '\n\t\t Student English- Chinese On-line Dictionary2 . \n\t', '\n\t\t As this dictionary is designed for English learners , its sense granularity is far coarser-grained than that of Word- Net . \n\t', '\n\t\t However , researchers argue that the granularity of WordNet is too fine for many applications , and some also proposed new evaluation standards . \n\t', '\n\t\t For example , \n\t\t']",Positive
"['\n\t\t Our approach is in accord with their proposal , since bilingual dictionaries interpret sense distinctions crossing two languages . \n\t', '\n\t\t For efficiency purposes , we extract our topic signatures mainly from the Mandarin portion of the Chinese Gigaword Corpus ( CGC ) , produced by the LDC3 , which contains 1.3GB of newswire text drawn from Xinhua newspaper . \n\t', '\n\t\t Some Chinese translations of English word senses could be sparse , making it impossible to extract sufficient training data simply relying on CGC . \n\t', '\n\t\t In this situation , we can turn to the large amount of Chinese text on the Web. . \n\t', '\n\t\t There are many good search engines and on-line databases supporting the Chinese language . \n\t', '\n\t\t After investigation , we chose People\x92s Daily On-line4 , which is the website for People\x92s Daily , one of the most influential newspaper in mainland China . \n\t', '\n\t\t It maintains a vast database of news stories , available to search by the public . \n\t', '\n\t\t Among other reasons , we chose this website because its articles have similar quality and coverage to those in the CGC , so that we could combine texts from these two resources to get a larger amount of topic signatures . \n\t', '\n\t\t Note that we can always turn to other sources on the Web to retrieve even more data , if needed . \n\t', '\n\t\t For Chinese text segmentation and POS tagging5 we adopted the freely-available software package \x97 ICTCLAS6 . \n\t', '\n\t\t This system includes a word segmenter , a POS tagger and an unknown- word recogniser . \n\t', '\n\t\t The claimed precision of segmentation is 97.58 % , evaluated on a 1.2M word portion of the People\x92s Daily Corpus . \n\t', '\n\t\t To automatically translate the Chinese text back to English , we used the electronic LDC Chinese- English Translation Lexicon Version 3.0 . \n\t', '\n\t\t An alternative was to use machine translation software , which would yield a rather different type of resource , but this is beyond the scope of this paper . \n\t', '\n\t\t Then , we filtered the topic signatures with 3Available at : http://www.ldc.upenn.edu/Catalog/ 4See : http://www.people.com.cn 5POS tagging can be omitted . \n\t', '\n\t\t We did it in our experiments purely for convenience for error analysis in the future . \n\t', '\n\t\t 6See : http://mtgroup.ict.ac.cn/-zhp/ICTCLAS/index.html a stop-word list , to ensure only content words are included in our final results . \n\t', '\n\t\t One might argue that , since many Chinese words are also ambiguous , a Chinese word may have more than one English translation and thus translated concepts in topic signatures would still be ambiguous . \n\t', '\n\t\t This happens for some Chinese words , and will inevitably affect the performance of our system to some extent . \n\t', '\n\t\t A practical solution is to expand the queries with different descriptions associated with each sense of w , normally provided in a bilingual dictionary , when retrieving the Chinese text . \n\t', '\n\t\t To get an idea of the baseline performance , we did not follow this solution in our experiments . \n\t', '\n\t\t Topic signatures for the MfinancialM sense of MinterestM M 1 . \n\t', '\n\t\t rate;2 . \n\t', '\n\t\t bond;3 . \n\t', '\n\t\t payment ; 4. market ; 5 . \n\t', '\n\t\t debt;6 . \n\t', '\n\t\t dollar ; ^ 7. bank;8 . \n\t', '\n\t\t year ; 9. loan ; 10 . \n\t', '\n\t\t income;11.company ; 12. inflation ; 13. reserve ; 14. government ; 15. economy ; 16 . \n\t', '\n\t\t stock;17 . \n\t', '\n\t\t fund;18 . \n\t', '\n\t\t week ; 19. security ; 20. level ; AC 1 . \n\t', '\n\t\t { bank } ; 2 . \n\t', '\n\t\t { loan } ; 3 . \n\t', '\n\t\t { company , firm , corporation } ; 4 . \n\t', '\n\t\t { rate } ; 5 . \n\t', '\n\t\t { deposit } ; 6 . \n\t', '\n\t\t { income , revenue } ; 7 . \n\t', '\n\t\t { fund } ; 8 . \n\t', '\n\t\t { bonus , divident } ; 9 . \n\t', '\n\t\t { investment } ; 10 . \n\t', '\n\t\t {market};^ 11 . \n\t', '\n\t\t { tax , duty } ; 12 . \n\t', '\n\t\t { economy } ; 13 . \n\t', '\n\t\t { debt } ; 14 . \n\t', '\n\t\t { money } ; 15 . \n\t', '\n\t\t { saving } ; 16 . \n\t', '\n\t\t { profit } ; 17 . \n\t', '\n\t\t { bond } ; 18 . \n\t', '\n\t\t { income , earning } ; 19 . \n\t', '\n\t\t { share , stock } ; 20 . \n\t', '\n\t\t { finance , banking } ; ^ Table 1:A sample of our topic signatures . \n\t', '\n\t\t Signature M was extracted from a manually-sense-tagged corpus and A was produced by our algorithm . \n\t', '\n\t\t Words occurring in both A and M are marked in bold . \n\t', '\n\t\t The topic signatures we acquired contain rich topical information . \n\t', '\n\t\t But they do not provide any other types of linguistic knowledge . \n\t', '\n\t\t Since they were created by word to word translation , syntactic analysis of them is not possible . \n\t', '\n\t\t Even the distances between the target ambiguous word and its context words are not reliable because of differences in word order between Chinese and English . \n\t', '\n\t\t Table 1 lists two sets of topic signatures , each containing the 20 most frequent nouns , ranked by occurrence count , that surround instances of the financial sense of interest . \n\t', '\n\t\t One set was extracted from a hand-tagged corpus \n\t\t']",Positive
"['\n\t\t 3 Application on WSD To evaluate the usefulness of the topic signatures acquired , we applied them in a WSD task . \n\t', '\n\t\t We adopted an algorithm similar to Sch¨utze\x92s ( 1998 ) context-group discrimination , which determines a word sense according to the semantic similarity of contexts , computed using a second-order co- occurrence vector model . \n\t', '\n\t\t In this section , we firstly introduce our adaptation of this algorithm , and then describe the disambiguation experiments on 6 words for which a gold standard is available . \n\t', '\n\t\t 3.1 Context-Group Discrimination We chose the so-called context-group discrimination algorithm because it disambiguates instances only relying on topical information , which happens to be what our topic signatures specialise in7 . \n\t', '\n\t\t The original context-group discrimination is a disambiguation algorithm based on clustering . \n\t', '\n\t\t Words , contexts and senses are represented in Word Space , a high-dimensional , real-valued space in which closeness corresponds to semantic similarity . \n\t', '\n\t\t Similarity in Word Space is based on second-order co-occurrence : two tokens ( or contexts ) of the ambiguous word are assigned to the same sense cluster if the words they co-occur with themselves occur with similar words in a training corpus . \n\t', '\n\t\t The number of sense clusters determines sense granularity . \n\t', '\n\t\t In our adaptation of this algorithm , we omitted the clustering step , because our data has already been sense classified according to the senses defined in the English-Chinese dictionary . \n\t', '\n\t\t In other words , our algorithm performs sense classification by using a bilingual lexicon and the level of sense granularity of the lexicon determines the sense distinctions that our system can handle : a finer-grained lexicon would enable our system to identify finer-grained senses . \n\t', '\n\t\t Also , our adaptation represents senses in Concept Space , in contrast to Word Space in the original algorithm . \n\t', '\n\t\t This is because our topic signatures are not realised in the form of words , but concepts . \n\t', '\n\t\t For example , a topic signature may consist of { duty , tariff , customs duty } , which represents a concept of \x93a government tax on imports or exports\x94 . \n\t', '\n\t\t A vector for concept c is derived from all the close neighbours of c , where close neighbours refer to all concepts that co-occur with c in a context window . \n\t', '\n\t\t The size of the window is around 100 7Using our topic signatures as training data , other classification algorithms would also work on this WSD task . \n\t', '\n\t\t words . \n\t', ""\n\t\t The entry for concept c ' in the vector for c records the number of times that c ' occurs close to c in the corpus . \n\t"", '\n\t\t It is this representational vector space that we refer to as Concept Space . \n\t', '\n\t\t In our experiments , we chose concepts that serve as dimensions of Concept Space using a frequency cut-off . \n\t', '\n\t\t We count the number of occurrences of any concepts that co-occur with the ambiguous word within a context window . \n\t', '\n\t\t The 2 , 500 most frequent concepts are chosen as the dimensions of the space . \n\t', '\n\t\t Thus , the Concept Space was formed by collecting a n-by-2 , 500 matrix M , such that element mid records the number of times that concept i and j co-occur in a window , where n is the number of concept vectors that occur in the corpus . \n\t', '\n\t\t Row l of matrix M represents concept vector l . \n\t', '\n\t\t We measure the similarity of two vectors by the cosine score : N 2 N 2 ~i=1 vZ ~i=1 wi where v~ and w~ are vectors and N is the dimension of the vector space . \n\t', '\n\t\t The more overlap there is between the neighbours of the two words whose vectors are compared , the higher the score . \n\t', '\n\t\t Contexts are represented as context vectors in Concept Space . \n\t', '\n\t\t A context vector is the sum of the vectors of concepts that occur in a context window . \n\t', '\n\t\t If many of the concepts in a window have a strong component for one of the topics , then the sum of the vectors , the context vector , will also have a strong component for the topic . \n\t', '\n\t\t Hence , the context vector indicates the strength of different topical or semantic components in a context . \n\t', '\n\t\t Senses are represented as sense vectors in Concept Space . \n\t', '\n\t\t A vector of sense si is the sum of the vectors of contexts in which the ambiguous word realises si . \n\t', '\n\t\t Since our topic signatures are classified naturally according to definitions in a bilingual dictionary , calculation of the vector for sense si is fairly straightforward : simply sum all the vectors of the contexts associated with sense si . \n\t', '\n\t\t After the training phase , we have obtained a sense vector vi* for each sense si of an ambiguous word w . \n\t', '\n\t\t Then , we perform the following steps to tag an occurrence t of w : N corr ( v , w ) = ~i=1 viwi 1 . \n\t', '\n\t\t Compute the context vector c~ for t in Concept Space by summing the vectors of the concepts in t\x92s context . \n\t', '\n\t\t Since the basic units of the test data are words rather than concepts , we have to convert all words in the test data into concepts . \n\t', '\n\t\t A simple way to achieve this is to replace a word v with all the concepts that contain v. 2 . \n\t', '\n\t\t Compute the cosine scores between all sense vectors of w and ~c , and then assign t to the sense si whose sense vector ~sj is closest to ~c . \n\t', '\n\t\t 3.2 Experiments and Results We tested our system on 6 nouns , as shown in Table 2 , which also shows information on the training and test data we used in the experiments . \n\t', '\n\t\t The training sets for motion , plant and tank are topic signatures extracted from the CGC ; whereas those for bass , crane and palm are obtained from both CGC and the People\x92s Daily On-line . \n\t', '\n\t\t This is because the Chinese translation equivalents of senses of the latter 3 words don\x92t occur frequently in CGC , and we had to seek more data from the Web. . \n\t', '\n\t\t Where applicable , we also limited the training data of each sense to a maximum of 6 , 000 instances for efficiency purposes . \n\t', ""\n\t\t Word^ Sense[] Training 11 Test[] 'Supervised'[ Precision Baseline bass[] 1. fish[ 418[] 12031 10[ 107 90.7 % 93.5 % 2. music 825 97 crane[ 1. bird[ 829[ 2301[ 24[ 95 74.7 % 76.6 % 2. machine 1472 71 motion[ 1. physical[ 6000[] 9265[ 141[ 201 70.1 % 69.7 % 2. legal 3265 60 palm 11 1 . \n\t"", '\n\t\t hand[ 852[] 12481 143[ 201 71.1 % 76.1 % 2. tree 396 58 plant[] 1. living[ 6000[ 12000E 86[ 188[ 54.3 % 11 70.2%[] 2. 2. factory 6000 102 tank[] 1. container[ 6000[] 93461 126[ 201 62.7 % 70.1 % 2. vehicle 3346 75 Table 2:Sizes of the training data and the test data , baseline performance , and the results . \n\t', '\n\t\t The test data is a binary sense-tagged corpus , the TWA Sense Tagged Data Set , manually produced by Rada Mihalcea and Li Yang \n\t\t']",Positive
"['\n\t\t We calculated a \x91supervised\x92 baseline from the annotated data by assigning the most frequent sense in the test data to all instances , although it could be argued that the baseline for unsupervised disambiguation should be computed by randomly assigning one of the senses to instances ( e.g. it would be 50 % for words with two senses ) . \n\t', '\n\t\t According to our previous description , the 2 , 500 most frequent concepts were selected as di- mensions . \n\t', '\n\t\t The number of features in a Concept Space depends on how many unique concepts actually occur in the training sets . \n\t', '\n\t\t Larger amounts of training data tend to yield a larger set of features . \n\t', '\n\t\t At the end of the training stage , for each sense , a sense vector was produced . \n\t', '\n\t\t Then we lemmatised the test data and extracted a set of context vectors for all instances in the same way . \n\t', '\n\t\t For each instance in the test data , the cosine scores between its context vector and all possible sense vectors acquired through training were calculated and compared , and then the sense scoring the highest was allocated to the instance . \n\t', '\n\t\t The results of the experiments are also given in Table 2 ( last column ) . \n\t', '\n\t\t Using our topic signatures , we obtained good results : the accuracy for all words exceeds the supervised baseline , except for motion which approaches it . \n\t', '\n\t\t The Chinese translations for motion are also ambiguous , which might be the reason that our WSD system performed less well on this word . \n\t', '\n\t\t However , as we mentioned , to avoid this problem , we could have expanded motion\x92s Chinese translations , using their Chinese monosemous synonyms , when we query the Chinese corpus or the Web. . \n\t', '\n\t\t Considering our system is unsupervised , the results are very promising . \n\t', '\n\t\t An indicative comparison might be with the work of \n\t\t']",Positive
"['\n\t\t 4 Discussion Although these results are promising , higher quality topic signatures would probably yield better results in our WSD experiments . \n\t', '\n\t\t There are a number of factors that could affect the acquisition process , which determines the quality of this resource . \n\t', '\n\t\t Firstly , since the translation was achieved by looking up in a bilingual dictionary , the deficiencies of the dictionary could cause problems . \n\t', '\n\t\t For example , the LDC Chinese-English Lexicon we used is not up to date , for example , lacking entries for words such as T-#L ( mobile phone ) , 3~ ~ ( the Internet ) , etc. . \n\t', '\n\t\t This defect makes our WSD algorithm unable to use the possibly strong topical information contained in those words . \n\t', '\n\t\t Secondly , errors generated during Chinese segmentation could affect the distributions of words . \n\t', '\n\t\t For example , a Chinese string ABC may be segmented as either A + BC or AB + C ; assuming the former is correct whereas AB + C was produced by the segmenter , distributions of words A , AB , BC , and C are all affected accordingly . \n\t', '\n\t\t Other factors such as cultural differences reflected in the different languages could also affect the results of this knowledge acquisition process . \n\t', '\n\t\t In our experiments , we adopted Chinese as a source language to retrieve English topic signatures . \n\t', '\n\t\t Nevertheless , our technique should also work on other distant language pairs , as long as there are existing bilingual lexicons and large monolingual corpora for the languages used . \n\t', '\n\t\t For example , one should be able to build French topic signatures using Chinese text , or Spanish topic signatures from Japanese text . \n\t', '\n\t\t In particular cases , where one only cares about translation ambiguity , this technique can work on any language pair . \n\t', '\n\t\t 5 Conclusion and Future Work We presented a novel method for acquiring English topic signatures from large quantities of Chinese text and English-Chinese and Chinese- English bilingual dictionaries . \n\t', '\n\t\t The topic signatures we acquired are a new type of resource , which can be useful in a number of NLP applications . \n\t', '\n\t\t Experimental results have shown its application to WSD is promising and the performance is competitive with other unsupervised algorithms . \n\t', '\n\t\t We intend to carry out more extensive evaluation to further explore this new resource\x92s properties and potential . \n\t', '\n\t\t Acknowledgements This research is funded by EU IST-2001- 34460 project MEANING : Developing Multilingual Web-Scale Language Technologies , and by the Department of Informatics at Sussex University . \n\t', '\n\t\t I am very grateful to Dr John Carroll , my supervisor , for his continual help and encouragement . \n\t', '\n\t\t References Eneko Agirre , Olatz Ansa , David Martinez , and Eduard Hovy . \n\t', '\n\t\t 2001 . \n\t', '\n\t\t Enriching WordNet concepts with topic signatures . \n\t', '\n\t\t In Proceedings of the NAACL workshop on WordNet and Other Lexical Resources : Applications , Extensions and Customizations . \n\t', '\n\t\t Pittsburgh , USA . \n\t', '\n\t\t Rebecca Bruce and Janyce Wiebe . \n\t', '\n\t\t 1994. Word-sense disambiguation using decomposable models . \n\t', '\n\t\t In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics , pages 139\x96 146 . \n\t', '\n\t\t Timothy Chklovski and Rada Mihalcea . \n\t', '\n\t\t 2002. Building a sense tagged corpus with open mind word expert . \n\t', '\n\t\t In Proceedings of the ACL 2002 Workshop on \x93Word Sense Disambiguation Recent Successes and Future Directions\x94 . \n\t', '\n\t\t Philadelphia , USA . \n\t', '\n\t\t Mona Diab and Philip Resnik . \n\t', '\n\t\t 2002. An unsupervised method for word sense taggin-1 using parallel cor- pora . \n\t', '\n\t\t In Proceedings of the 40t Anniversary Meeting ofthe Association for Computational Linguistics ( ACL-02 ) . \n\t', '\n\t\t Philadelphia , USA . \n\t', '\n\t\t William A. Gale , Kenneth W. Church , and David Yarowsky . \n\t', '\n\t\t 1992. Using bilingual materials to develop word sense disambiguation methods . \n\t', '\n\t\t In Proceedings of the International Conference on Theoretical and Methodological Issues in Machine Translation , pages 101\x96112 . \n\t', '\n\t\t Rada Mihalcea and Dan I. Moldovan . \n\t', '\n\t\t 1999. An automatic method for generating sense tagged corpora . \n\t', '\n\t\t In Proceedings of the 16th Conference of the American Association ofArtificialIntelligence . \n\t', '\n\t\t Rada Mihalcea . \n\t', '\n\t\t 2003. The role of non-ambiguous words in natural language disambiguation . \n\t', '\n\t\t In Proceedings of the Conference on Recent Advances in Natural Language Processing , RANLP 2003 . \n\t', '\n\t\t Borovetz , Bulgaria . \n\t', '\n\t\t George A. Miller , Richard Beckwith , Christiane Fellbaum , Derek Gross , and Katherine J. Miller . \n\t', '\n\t\t 1990. Introduction to WordNet : An on-line lexical database . \n\t', '\n\t\t Journal ofLexicography , 3(4):235\x96244 . \n\t', '\n\t\t Philip Resnik and David Yarowsky . \n\t', '\n\t\t 1999 . \n\t', '\n\t\t Distinguishing systems and distinguishing senses : New evaluation methods for word sense disambiguation . \n\t', '\n\t\t Natural Language Engineering , 5(2):113\x96133 . \n\t', '\n\t\t Philip Resnik . \n\t', '\n\t\t 1999. Mining the Web for bilingual text . \n\t', '\n\t\t In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics . \n\t', '\n\t\t Hinrich Sch¨utze . \n\t', '\n\t\t 1998. Automatic word sense discrimination . \n\t', '\n\t\t Computational Linguistics , 24(1):97\x96 123. \n\t', '\n\t\t iSTART : Paraphrase Recognition Chutima Boonthum Computer Science Department Old Dominion University , Norfolk , VA-23508 USA cboont@cs.odu.edu Abstract Paraphrase recognition is used in a number of applications such as tutoring systems , question answering systems , and information retrieval systems . \n\t', '\n\t\t The context of our research is the iSTART reading strategy trainer for science texts , which needs to understand and recognize the trainee\x92s input and respond appropriately . \n\t', '\n\t\t This paper describes the motivation for paraphrase recognition and develops a definition of the strategy as well as a recognition model for paraphrasing . \n\t', '\n\t\t Lastly , we discuss our preliminary implementation and research plan . \n\t', '\n\t\t 1 Introduction A web-based automated reading strategy trainer called iSTART ( Interactive Strategy Trainer for Active Reading and Thinking ) adaptively assigns individual students to appropriate reading training programs . \n\t', '\n\t\t It follows the SERT ( Self- Explanation Reading Training ) methodology developed by McNamara ( in press ) as a way to improve high school students\x92 reading ability by teaching them to use active reading strategies in self-explaining difficult texts . \n\t', '\n\t\t Details of the strategies can be found in McNamara ( in press ) and of iSTART in \n\t\t']",Positive
"['\n\t\t Then the trainer analyzes the student\x92s explanation and responds . \n\t', '\n\t\t The current system uses simple word- matching algorithms to evaluate the student\x92s input that do not yield results that are sufficiently reliable or accurate . \n\t', '\n\t\t We therefore propose a new system for handling the student\x92s explanation more effectively . \n\t', '\n\t\t Two major tasks of this semantically-based system are to ( 1 ) construct an internal representation of sentences and explanations and ( 2 ) recognize the reading strategies the student uses beginning with paraphrasing . \n\t', '\n\t\t Construct an Internal Representation : We transform the natural language explanation into a representation suitable for later analysis . \n\t', '\n\t\t The Sentence Parser gives us a syntactically and morphologically tagged representation . \n\t', '\n\t\t We transform the output of the Link Grammar parser ( CMU , 2000 ) that generates syntactical and morphological information into an appropriate knowledge representation using the Representation Generator . \n\t', '\n\t\t Recognize Paraphrasing : In what follows , we list the paraphrase patterns that we plan to cover and define a recognition model for each pattern . \n\t', '\n\t\t This involves two steps : ( 1 ) recognizing paraphrasing patterns , and ( 2 ) reporting the result . \n\t', '\n\t\t The Paraphrase Recognizer compares two internal representation ( one is of a given sentence and another is of the student\x92s explanation ) and finds paraphrase matches ( \x93concept-relationconcept\x94 triplet matches ) according to a paraphrasing pattern . \n\t', '\n\t\t The Reporter provides the final summary of the total paraphrase matches , noting unmatched information in either the sentence or the explanation . \n\t', '\n\t\t Based on the similarity measure , the report will include whether the student has fully or partially paraphrased a given sentence and whether it contains any additional information . \n\t', '\n\t\t 2 Paraphrase When two expressions describe the same situation , each is considered to be a paraphrase of the other . \n\t', '\n\t\t There is no precise paraphrase definition in general ; instead there are frequently-accepted paraphrasing patterns to which various authorities refer . \n\t', '\n\t\t Academic writing centers ( ASU Writing Center , 2000 ; BAC Writing Center ; USCA Writing Room ; and Hawes , 2003 ) provide a number of characterizations , such as using syno- nyms , changing part-of-speech , reordering ideas , breaking a sentence into smaller ones , using definitions , and using examples . \n\t', '\n\t\t McNamara ( in press ) , on the other hand , does not consider using definitions or examples to be part of paraphrasing , but rather considers them elaboration . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t Instead of attempting to find a single paraphrase definition , we will start with six commonly mentioned paraphrasing patterns : 1 . \n\t', '\n\t\t Synonym : substitute a word with its synonym , e.g. help , assist , aid ; 2 . \n\t', '\n\t\t Voice : change the voice of sentence from active to passive or vice versa ; 3 . \n\t', '\n\t\t Word-Form/Part-of-speech : change a word into a different form , e.g. change a noun to a verb , adverb , or adjective ; 4 . \n\t', '\n\t\t Break down Sentence : break a long sentence down into small sentences ; 5 . \n\t', '\n\t\t Definition/Meaning : substitute a word with its definition or meaning ; 6 . \n\t', '\n\t\t Sentence Structure : use different sentence structures to express the same thing . \n\t', '\n\t\t If the explanation has any additional information or misses some information that appeared in the original sentence , we should be able to detect this as well for use in discovering additional strategies employed . \n\t', '\n\t\t 3 Recognition Model To recognize paraphrasing , we convert natural language sentences into Conceptual Graphs ( CG , Sowa , 1983 ; 1992 ) and then compare two CGs for matching according to paraphrasing patterns . \n\t', '\n\t\t The matching process is to find as many \x93concept-relation-concept triplet\x94 matches as possible . \n\t', '\n\t\t A triplet match means that a triplet from the student\x92s input matches with a triplet from the given sentence . \n\t', '\n\t\t In particular , the left-concept , right-concept , and relation of both sub-graphs have to be exactly the same , or the same under a transformation based on a relationship of synonymy ( or other relation defined in WordNet ) , or the same because of idiomatic usage . \n\t', '\n\t\t It is also possible that several triplets of one sentence together match a single triplet of the other . \n\t', '\n\t\t At the end of this pattern matching , a summary result is provided : total paraphrasing matches , unpara- phrased information and additional information ( not appearing in the given sentence ) . \n\t', '\n\t\t 3.1 Conceptual Graph Generation A natural language sentence is converted into a conceptual graph using the Link Grammar parser . \n\t', '\n\t\t This process mainly requires mapping one or more Link connector types into a relation of the conceptual graph . \n\t', '\n\t\t A parse from the Link Grammar consists of triplets : starting word , an ending word , and a connector type between these two words . \n\t', '\n\t\t For example , [ 1 2 ( Sp ) ] means word-1 connects to word-2 with a subject connector or that word-1 is the subject of word-2 . \n\t', '\n\t\t The sentence \x93A walnut is eaten by a monkey\x94 is parsed as follows : [ (0=LEFT-WALL)(1=a)(2=walnut.n)(3=is.v) (4=eaten.v)(5=by)(6=a)(7=monkey.n)(8=.) ] [ [ 0 8 ( Xp ) ] [ 0 2 ( Wd ) ] [ 1 2 ( Dsu ) ] [ 2 3 ( Ss ) ] [ 3 4 ( Pv ) ] [ 4 5 ( MVp ) ] [ 5 7 ( Js ) ] [ 6 7 ( Ds ) ] ] We then convert each Link triplet into a corresponding CG triplet . \n\t', '\n\t\t Two words in the Link triplet can be converted into two concepts of the CG . \n\t', '\n\t\t To decide whether to put a word on the left or the right side of the CG triplet , we define a mapping rule for each Link connector type . \n\t', '\n\t\t For example , a Link triplet [ 1 2 ( S* ) ] will be mapped to the \x91Agent\x92 relation , with word-2 as the left-concept and word-1 as the right-concept : [ Word-2 ] \x97> ( Agent ) \x97> [ Word-1 ] . \n\t', '\n\t\t Sometimes it is necessary to consider several Link triplets in generating a single CG triplet . \n\t', '\n\t\t A CG of previous example is shown below : 0 [ 0 8 ( Xp ) ] -> #S# -> - N/A - 1 [ 0 2 ( Wd ) ] -> #S# -> - N/A - 2 [ 1 2 ( Dsu ) ] -> #S# -> [walnut.n]->(Article)->[a] 3 [ 2 3 ( Ss ) ] -> #M# S + Pv ( 4 ) # -> [eaten.v]->(Patient)->[walnut.n] 4 [ 3 4 ( Pv ) ] -> #M# Pv +MV(5)+O(6)# -> [ eaten.v ] -> ( Agent ) -> [ monkey.n ] 5 [ 4 5 ( MVp ) ] -> #S# eaten.v by 6 [ 5 7 ( Js ) ] -> #S# monkey.n by 7 [ 6 7 ( Ds ) ] -> #S# -> [ monkey.n ] -> ( Article ) -> [ a ] Each line ( numbered 0-7 ) shows a Link triplet and its corresponding CG triplet . \n\t', '\n\t\t These will be used in the recognition process . \n\t', '\n\t\t The \x91#S#\x92 and \x91#M\x92 indicate single and multiple mapping rules . \n\t', '\n\t\t 3.2 Paraphrase Recognition We illustrate our approach to paraphrase pattern recognition on single sentences : using synonyms ( single or compound-word synonyms and idiomatic expressions ) , changing the voice , using a different word form , breaking a long sentence into smaller sentences , substituting a definition for a word , and changing the sentence structure . \n\t', '\n\t\t Preliminaries : Before we start the recognition process , we need to assume that we have all the information about the text : each sentence has various content words ( excluding such \x91stop words\x92 as a , an , the , etc. ) ; each content word has a definition together with a list of synonyms , antonyms , and other relations provided by WordNet \n\t\t']",Positive
"['\n\t\t To prepare a given text and a sentence , we plan to have an automated process that generates necessary information as well as manual intervention to verify and rectify the automated result , if necessary . \n\t', '\n\t\t Single-Word Synonyms : First we discover that both CGs have the same pattern and then we check whether words in the same position are synonyms . \n\t', '\n\t\t Example : \x93Jenny helps Kay\x94 [ Help ] --> ( Agent ) --> [ Person : Jenny ] i~--> ( Patient ) --> [ Person : Kay ] vs. \x93Jenny assists Kay\x94 [ Assist ] --> ( Agent ) --> [ Person : Jenny ] i~--> ( Patient ) --> [ Person : Kay ] Compound-Word Synonyms : In this case , we need to be able to match a word and its compound-word synonym . \n\t', '\n\t\t For example , \x91install\x92 has \x91set up\x92 and \x91put in\x92 as its compound-word synonyms . \n\t', '\n\t\t The compound words are declared by the parser program . \n\t', '\n\t\t During the preliminary processing CGs are pre-generated . \n\t', '\n\t\t [ Install ] --> ( Object ) --> [ Thing ] ~ [ Set-Up ] --> ( Object ) --> [ Thing ] ~ [ Put-In ] --> ( Object ) --> [ Thing ] Then , this case will be treated like the single- word synonym . \n\t', '\n\t\t \x93Jenny installs a computer\x94 [ Install ] --> ( Agent ) --> [ Person : Jenny ] i~--> ( Object ) --> [ Computer ] vs. \x93Jenny sets up a computer\x94 [ Set-Up ] --> ( Agent ) --> [ Person : Jenny ] i~--> ( Object ) --> [ Computer ] Idiomatic Clause/Phrase : For each idiom , a CG will be generated and used in the comparison process . \n\t', '\n\t\t For example , the phrase \x91give someone a hand\x92 means \x91help\x92 . \n\t', '\n\t\t The preliminary process will generate the following conceptual graph : [ Help ] --> ( Patient ) --> [ Person : x ] ~ [ Give ] --> ( Patient ) --> [ Person : x ] i~--> ( Object ) --> [ Hand ] which gives us \x93Jenny gives Kay a hand\x94 [ Give ] --> ( Agent ) --> [ Person : Jenny ] i~--> ( Patient ) --> [ Person : Kay ] i~--> ( Object ) --> [ Hand ] In this example , one might say that a \x91hand\x92 might be an actual ( physical ) hand rather than a synonym phrase for \x91help\x92 . \n\t', '\n\t\t To reduce this particular ambiguity , the analysis of the context may be necessary . \n\t', '\n\t\t Voice : Even if the voice of a sentence is changed , it will have the same CG . \n\t', '\n\t\t For example , both \x93Jenny helps Kay\x94 and \x93Kay is helped by Jenny\x94 have the same graphs as follows : [ Help ] --> ( Agent ) --> [ Person : Jenny ] i~--> ( Patient ) --> [ Person : Kay ] At this time we are assuming that if two CGs are exactly the same , it means paraphrasing by changing voice pattern . \n\t', '\n\t\t However , we plan to introduce a modified conceptual graph that retains the original sentence structure so that we can verify that it was paraphrasing by change of voice and not simple copying . \n\t', '\n\t\t Part-of-speech : A paraphrase can be generated by changing the part-of-speech of some keywords . \n\t', '\n\t\t In the following example , the student uses \x93a historical life story\x94 instead of \x93life history\x94 , and \x91similarity\x92 instead of \x91similar\x92 . \n\t', '\n\t\t Original sentence : \x93All thunderstorms have a similar life history.\x94 Student\x92s Explanation : \x93All thunderstorms have similarity in their historical life story.\x94 To find this paraphrasing pattern , we look for the same word , or a word that has the same base- form . \n\t', '\n\t\t In this example , the sentences share the same base-form for \x91similar\x92 and \x91similarity\x92 as well as for \x91history\x92 and \x91historical\x92 . \n\t', '\n\t\t Breaking long sentence : A sentence can be explained by small sentences coupled up together in such a way that each covers a part of the original sentence . \n\t', '\n\t\t We integrate CGs of all sentences in the student\x92s input together before comparing it with the original sentence . \n\t', ""\n\t\t Original sentence : \x93All thunderstorms have a similar life history.\x94 [ Thunderstorm : b ' ] \x96 ( Feature ) --> [ History ] \x96 ( Attribute ) --> [ Life ] ( Attribute ) --> [ Similar ] Student\x92s Explanation : \x93Thunderstorms have life history . \n\t"", ""\n\t\t It is similar among all thunderstorms\x94 [ Thunderstorm ] \x96 ( Feature ) --> [ History ] \x96 ( Attribute ) --> [ Life ] [ It ] (pronoun)\x96 ( Attribute ) --> [ Similar ] ( Mod ) --> [ Thunderstorm : b ' ] ( among ) We will provisionally assume that the student uses only the words that appear in the sentence in this breaking down process . \n\t"", '\n\t\t One solution is to combine graphs from all sentences together . \n\t', '\n\t\t This can be done by merging graphs of the same concept . \n\t', '\n\t\t This process involves pronoun resolution . \n\t', '\n\t\t In this example , \x91it\x92 could refer to \x91life\x92 or \x91history\x92 . \n\t', '\n\t\t Our plan is to exercise all possible pronoun references and select one that gives the best paraphrasing recognition result . \n\t', '\n\t\t Definition/Meaning : A CG is pre-generated for a definition of each word and its associations ( synonyms , idiomatic expressions , etc. ) . \n\t', '\n\t\t To find a paraphrasing pattern of using the definition , for example , a \x91history\x92 means \x93the continuum of events occurring in succession leading from the past to the present and even into the future\x94 , we build a CG for this as shown below : [ Continuum ] \x96 ( Attribute ) --> [ Event : 3 ] [ Occur ] \x96 ( Patient ) --> [ Event : 3 ] ( Mod ) --> [ Succession ] ( in ) [ Lead ] \x96 ( Initiator ) --> [ Succession ] ( Source ) --> [ Time : Past ] ( from ) ( Path ) --> [ Time : Present ] ( to ) ( Path ) --> [ Time : Future ] ( into ) We refine this CG by incorporating CGs of the definition into a single integrated CG , if possible . \n\t', '\n\t\t ( Patient ) --> [ Event : 3 ] ( Mod ) --> [ Succession ] ( in ) ( Source ) --> [ Time : Past ] ( from ) ( Path ) --> [ Time : Present ] ( to ) ( Path ) --> [ Time : Future ] ( into ) From WordNet 2.0 , the synonyms of \x91past\x92 , \x91present\x92 , and \x91future\x92 found to be \x93begin , start , beginning process\x94 , \x93middle , go though , middle process\x94 , and \x93end , last , ending process\x94 , respectively . \n\t', '\n\t\t The following example shows how they can be used in recognizing paraphrases . \n\t', ""\n\t\t Original sentence : \x93All thunderstorms have a similar life history.\x94 [ Thunderstorm : b ' ] \x96 ( Feature ) --> [ History ] \x96 ( Attribute ) --> [ Life ] ( Attribute ) --> [ Similar ] Student\x92s Explanation : \x93Thunderstorms go through similar cycles . \n\t"", '\n\t\t They will begin the same , go through the same things , and end the same way.\x94 [ Go ] \x96 ( Agent ) --> [ Thunderstorm : # ] ( Path ) --> [ Cycle ] --> ( Attribute ) --> [ Similar ] [ Begin ] \x96 ( Agent ) --> [ Thunderstorm : # ] ( Attribute ) --> [ Same ] [ Go-Through ] \x96 ( Agent ) --> [ Thunderstorm : # ] ( Path ) --> [ Thing : 3 ] --> ( Attribute ) --> [ Same ] [ End ] \x96 ( Agent ) --> [ Thunderstorm : # ] ( Path ) --> [ Way : 3 ] --> ( Attribute ) --> [ Same ] From this CG , we found the use of \x91begin\x92 , \x91go- through\x92 , and \x91end\x92 , which are parts of the CG of history\x92s definition . \n\t', '\n\t\t These together with the correspondence of words in the sentences show that the student has used paraphrasing by using a definition of \x91history\x92 in the self-explanation . \n\t', '\n\t\t Sentence Structure : The same thing can be said in a number of different ways . \n\t', '\n\t\t For example , to say \x93There is someone happy\x94 , we can say \x93Someone is happy\x94 , \x93A person is happy\x94 , or \x93There is a person who is happy\x94 , etc. . \n\t', '\n\t\t As can be easily seen , all sentences have a similar CG trip- let of \x93[Person : 3 ] --> ( Char ) --> [Happy]\x94 in their CGs . \n\t', '\n\t\t But , we cannot simply say that they are paraphrases of each other ; therefore , need to study more on possible solutions . \n\t', '\n\t\t 3.3 Similarity Measure The similarity between the student\x92s input and the given sentence can be categorized into one of these four cases : 1 . \n\t', '\n\t\t Complete paraphrase without extra info . \n\t', '\n\t\t 2. Complete paraphrase with extra info . \n\t', '\n\t\t 3. Partial paraphrase without extra info . \n\t', '\n\t\t 4. Partial paraphrase with extra info . \n\t', '\n\t\t To distinguish between \x91complete\x92 and \x91partial\x92 paraphrasing , we will use the triplet matching result . \n\t', '\n\t\t What counts as complete depends on the context in which the paraphrasing occurs . \n\t', '\n\t\t If we consider the paraphrasing as a writing technique , the \x91complete\x92 paraphrasing would mean that all triplets of the given sentence are matched to those in the student\x92s input . \n\t', '\n\t\t Similarly , if any triplets in the given sentence do not have a match , it means that the student is \x91partially\x92 paraphrasing at best . \n\t', '\n\t\t On the other hand , if we consider the paraphrasing as a reading behavior or strategy , the \x91complete\x92 paraphrasing may not need all triplets of the given sentence to be matched . \n\t', '\n\t\t Hence , recognizing which part of the student\x92s input is a paraphrase of which part of the given sentence is significant . \n\t', '\n\t\t How can we tell that this explanation is an adequate paraphrase ? \n\t', '\n\t\t Can we use information provided in the given sentence as a measurement ? \n\t', '\n\t\t If so , how can we use it ? \n\t', '\n\t\t These questions still need to be answered . \n\t', '\n\t\t 4 Related Work A number of people have worked on paraphrasing such as the multilingual-translation recognition by \n\t\t']",Positive
"['\n\t\t Due to the space limitation we will mention only a few related works . \n\t', '\n\t\t ExtrAns ( Extracting answers from technical texts ) by \n\t\t']",Positive
"['\n\t\t They identify terminological paraphrases by using a term-based hierarchy with their synonyms and variations ; and syntactic paraphrases by constructing a common representation for different types of syntactic variation via meaning postulates . \n\t', '\n\t\t Absent a paraphrase , they loosen the criteria by using hyponyms , finding highest overlap of predicates , and simple keyword matching . \n\t', '\n\t\t Barzilay & \n\t\t']",Positive
"['\n\t\t They first find different paraphrasing rules by clustering sentences in comparable corpora using n-gram word-overlap . \n\t', '\n\t\t Then for each cluster , they use multi-sequence alignment to find intra-cluster paraphrasing rules : either morphosyntactic or lexical patterns . \n\t', '\n\t\t To identify inter- cluster paraphrasing , they compare the slot values without considering word ordering . \n\t', '\n\t\t In our system sentences are represented by conceptual graphs . \n\t', '\n\t\t Paraphrases are recognized through idiomatic expressions , definition , and sentence break up . \n\t', '\n\t\t Morpho-syntatic variations are also used but in more general way than the term hierarchy-based approach of ExtrAns . \n\t', '\n\t\t 5 Preliminary Implementation We have implemented two components to recognize paraphrasing with the CG for a single simple sentence : Automated Conceptual Graph Generator and Automated Paraphrasing Recognizer . \n\t', '\n\t\t Automated Conceptual Graph Generator : is a C++ program that calls the Link Grammar API to get the parse result for the input sentence , and generates a CG . \n\t', '\n\t\t We can generate a CG for a simple sentence using the first linkage result . \n\t', '\n\t\t Future versions will deal with complex sentence structure as well as multiple linkages , so that we can cover most paraphrases . \n\t', '\n\t\t Automated Paraphrasing Recognizer : The input to the Recognizer is a pair of CGs : one from the original sentence and another from the student\x92s explanation . \n\t', '\n\t\t Our goal is to recognize whether any paraphrasing was used and , if so , what was the paraphrasing pattern . \n\t', '\n\t\t Our first implementation is able to recognize paraphrasing on a single sentence for exact match , direct synonym match , first level antonyms match , hyponyms and hypernyms match . \n\t', '\n\t\t We plan to cover more relationships available in WordNet as well as definitions , idioms , and logically equivalent expressions . \n\t', '\n\t\t Currently , voice difference is treated as an exact match because both active voices have the same CGs and we have not yet modified the conceptual graph as indicated above . \n\t', '\n\t\t 6 Discussion and Remaining Work Our preliminary implementation shows us that paraphrase recognition is feasible and allows us to recognize different types of paraphrases . \n\t', '\n\t\t We continue to work on this and improve our recognizer so that it can handle more word relations and more types of paraphrases . \n\t', '\n\t\t During the testing , we will use data gathered during our previous iSTART trainer experiments . \n\t', '\n\t\t These are the actual explanations entered by students who were given the task of explaining sentences . \n\t', '\n\t\t Fortu- nately , quite a bit of these data have been evaluated by human experts for quality of explanation . \n\t', '\n\t\t Therefore , we can validate our paraphrasing recognition result against the human evaluation . \n\t', '\n\t\t Besides implementing the recognizer to cover all paraphrasing patterns addressed above , there are many issues that need to be solved and implemented during this course of research . \n\t', '\n\t\t The Representation for a simple sentence is the Conceptual Graph , which is not powerful enough to represent complex , compound sentences , multiple sentences , paragraphs , or entire texts . \n\t', '\n\t\t We will use Rhetorical Structure Theory ( RST ) to represent the relations among the CGs of these components of these more complex structures . \n\t', '\n\t\t This will also involve Pronoun Resolution as well as Discourse Chunking . \n\t', '\n\t\t Once a representation has been selected , we will implement an automated generator for such representation . \n\t', '\n\t\t The Recognizer and Paraphrase Reporter have to be completed . \n\t', '\n\t\t The similarity measures for writing technique and reading behavior must still be defined . \n\t', '\n\t\t Once all processes have been implemented , we need to verify that they are correct and validate the results . \n\t', '\n\t\t Finally , we can integrate this recognition process into the iSTART trainer in order to improve the existing evaluation system . \n\t', '\n\t\t Acknowledgements This dissertation work is under the supervision of Dr. Shunichi Toida and Dr. Irwin Levinstein . \n\t', '\n\t\t iSTART is supported by National Science Foundation grant REC-0089271 . \n\t', '\n\t\t References ASU Writing Center . \n\t', '\n\t\t 2000 . \n\t', '\n\t\t Paraphrasing : Restating Ideas in Your Own Words . \n\t', '\n\t\t Arizona State University , Tempe : AZ . \n\t', '\n\t\t BAC Writing Center . \n\t', '\n\t\t Paraphrasing . \n\t', '\n\t\t Boston Architectural Center . \n\t', '\n\t\t Boston : MA . \n\t', '\n\t\t Carnegie Mellon University . \n\t', '\n\t\t 2000. Link Grammar . \n\t', '\n\t\t R. Barzilay and L. Lee . \n\t', '\n\t\t 2003. Learning to Paraphrase : An Unsupervised Approach Using Multiple- Sequence Alignment . \n\t', '\n\t\t In HLT-NAACL , Edmonton : Canada , pp. 16-23 . \n\t', '\n\t\t C. Boonthum , S. Toida , and I. Levinstein . \n\t', '\n\t\t 2003 . \n\t', '\n\t\t Paraphrasing Recognition through Conceptual Graphs . \n\t', '\n\t\t Computer Science Department , Old Dominion University , Norfolk : VA . \n\t', '\n\t\t ( TR# is not available ) C. Boonthum . \n\t', '\n\t\t 2004. iSTART : Paraphrasing Recognition . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t Proposal : Computer Science Department , Old Dominion University , VA . \n\t', '\n\t\t C. Fellbaum . \n\t', '\n\t\t 1998. WordNet : an electronic lexical database . \n\t', '\n\t\t The MIT Press : MA . \n\t', '\n\t\t K. Hawes . \n\t', '\n\t\t 2003. Mastering Academic Writing : Write a Paraphrase Sentence . \n\t', '\n\t\t University of Memphis , Memphis : TN . \n\t', '\n\t\t I. Levinstein , D. McNamara , C. Boonthum , S. Pillarisetti , and K. Yadavalli . \n\t', '\n\t\t 2003. Web-Based Intervention for Higher-Order Reading Skills . \n\t', '\n\t\t In ED- MEDIA , Honolulu : HI , pp. 835-841 . \n\t', '\n\t\t D. Lin and P. Pantel . \n\t', '\n\t\t 2001. Discovery of Inference Rules for Question Answering . \n\t', '\n\t\t Natural Language Engineering 7(4):343-360 . \n\t', '\n\t\t W. Mann and S. Thompson , 1987 . \n\t', '\n\t\t Rhetorical Structure Theory : A Theory of Text Organization . \n\t', '\n\t\t The Structure of Discourse , Ablex . \n\t', '\n\t\t D. McNamara . \n\t', '\n\t\t ( in press ) . \n\t', '\n\t\t SERT : Self-Explanation Reading Training . \n\t', '\n\t\t Discourse Processes . \n\t', '\n\t\t D. Molla , R. Schwitter , F. Rinaldi , J. Dowdall , and M. Hess . \n\t', '\n\t\t 2003. ExtrAns : Extracting Answers from Technical Texts . \n\t', '\n\t\t IEEE Intelligent System 18(4) : 12-17 . \n\t', '\n\t\t M. Murata and H. Isahara . \n\t', '\n\t\t 2001. Universal Model for Paraphrasing \x96 Using Transformation Based on a Defined Criteria . \n\t', '\n\t\t In NLPRS : Workshop on Automatic Paraphrasing : Theories and Application . \n\t', '\n\t\t F. Rinaldi , J. Dowdall , K. Kaljurand , M. Hess , and D. Molla. 2003 . \n\t', '\n\t\t Exploiting Paraphrases in Question Answering System . \n\t', '\n\t\t In ACL : Workshop in Paraphrasing , Sapporo : Japan , pp. 25-32 . \n\t', '\n\t\t N. Smith . \n\t', '\n\t\t 2002. From Words to Corpora : Recognizing Translation . \n\t', '\n\t\t In EMNLP , Philadelphia : PA . \n\t', '\n\t\t J. Sowa . \n\t', '\n\t\t 1983. Conceptual Structures : Information Processing in Mind and Machine . \n\t', '\n\t\t Addison-Wesley , MA . \n\t', '\n\t\t J. Sowa . \n\t', '\n\t\t 1992. Conceptual Graphs as a Universal Knowledge Representation . \n\t', '\n\t\t Computers Math . \n\t', '\n\t\t Application , 23(2-5) : 75-93 . \n\t', '\n\t\t M. Stede . \n\t', '\n\t\t 1996. Lexical semantics and knowledge representation in multilingual sentence generation . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis : Department of Computer Science , University of Toronto , Canada . \n\t', '\n\t\t USCA Writing Room . \n\t', '\n\t\t Paraphrasing . \n\t', '\n\t\t The University of South Carolina : Aiken . \n\t', '\n\t\t Aiken : SC . \n\t', '\n\t\t Towards a Semantic Classification of Spanish Verbs Based on Subcategorisation Information Eva Esteve Ferrer Department of Informatics University of Sussex Brighton , BN1 9QH , UK E.Esteve-Ferrer@sussex.ac.uk Abstract We present experiments aiming at an automatic classification of Spanish verbs into lexical semantic classes . \n\t', '\n\t\t We apply well-known techniques that have been developed for the English language to Spanish , proving that empirical methods can be re-used through languages without substantial changes in the methodology . \n\t', '\n\t\t Our results on subcategorisation acquisition compare favourably to the state of the art for English . \n\t', '\n\t\t For the verb classification task , we use a hierarchical clustering algorithm , and we compare the output clusters to a manually constructed classification . \n\t', '\n\t\t 1 Introduction Lexical semantic classes group together words that have a similar meaning . \n\t', '\n\t\t Knowledge about verbs is especially important , since verbs are the primary means of structuring and conveying meaning in sentences . \n\t', '\n\t\t Manually built semantic classifications of English verbs have been used for different applications such as machine translation \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t A classification of Spanish verbs based on the same hypothesis has been developed by ( V´azquez et al. , 2000 ) . \n\t', '\n\t\t But manually constructing large-scale verb classifications is a labour-intensive task . \n\t', '\n\t\t For this reason , various methods for automatically classifying verbs using machine learning techniques have been attempted ( \n\t\t']",Positive
"['\n\t\t In this article we present experiments aiming at automatically classifying Spanish verbs into lexical semantic classes based on their subcategorisation frames . \n\t', '\n\t\t We adopt the idea that a description of verbs in terms of their syntactic behaviour is useful for acquiring their semantic properties . \n\t', '\n\t\t The classification task at hand is achieved through a process that requires different steps : we first extract from a partially parsed corpus the probabilities of the sub- categorisation frames for each verb . \n\t', '\n\t\t Then , the acquired probabilities are used as features describing the verbs and given as input to an unsupervised classification algorithm that clusters together the verbs according to the similarity of their descriptions . \n\t', '\n\t\t For the task of acquiring verb subcategorisation frames , we adapt to the specificities of the Spanish language well-known techniques that have been developed for English , and our results compare favourably to the sate of the art results obtained for English \n\t\t']",Positive
"['\n\t\t For the verb classification task , we use a hierarchical clustering algorithm , and we compare the output clusters to a manually constructed classification developed by ( V´azquez et al. , 2000 ) . \n\t', '\n\t\t 2 Acquisition of Spanish Subcategorisation Frames Subcategorisation frames encode the information of how many arguments are required by the verb , and of what syntactic type . \n\t', '\n\t\t Acquiring the subcategorization frames for a verb involves , in the first place , distinguishing which constituents are its arguments and which are adjuncts , elements that give an additional piece of information to the sentence . \n\t', '\n\t\t Moreover , sentences contain other constituents that are not included in the subcategorisation frames of verbs : these are sub-constituents that are not structurally attached to the verb , but to other constituents . \n\t', '\n\t\t 2.1 Methodology and Materials We experiment our methodology on two corpora of different sizes , both consisting of Spanish newswire text : a 3 million word corpus , hereafter called small corpus , and a 50 million word corpus , hereafter called large corpus . \n\t', '\n\t\t They are both POS tagged and partially parsed using the MS-analyzer , a partial parser for Spanish that includes named entities recognition \n\t\t']",Positive
"['\n\t\t In order to collect the frequency distributions of Spanish subcategorisation frames , we adapt a methodology that has been developed for English to the specificities of the Spanish language ( \n\t\t']",Positive
"['\n\t\t It consists in extracting from the corpus pairs made of a verb and its co-occurring constituents that are a possible pattern of a frame , and then filtering out the patterns that do not have a probability of co- occurrence with the verb high enough to be considered its arguments . \n\t', '\n\t\t We establish a set of 11 possible Spanish subcategorisation frames . \n\t', '\n\t\t These are the plausible combinations of a maximum of 2 of the following constituents : nominal phrases , prepositional phrases , temporal sentential clauses , gerundive sentential clauses , infinitival sentential clauses , and infinitival sentential clauses introduced by a preposition . \n\t', '\n\t\t The individual prepositions are also taken into account as part of the subcategorisation frame types . \n\t', '\n\t\t Adapting a methodology that has been thought for English presents a few problems , because English is a language with a strong word order constraint , while in Spanish the order of constituents is freer . \n\t', '\n\t\t Although the unmarked order of constituents is Subject Verb Object with the direct object preceding the indirect object , in naturally occurring language the constituents can be moved to non- canonical positions . \n\t', '\n\t\t Since we extract the patterns from a partially parsed corpus , which has no information on the attachment or grammatical function of the constituents , we have to take into account that the extraction is an approximation . \n\t', '\n\t\t There are various phenomena that can lead us to an erroneous extraction of the constituents . \n\t', '\n\t\t As an illustrative example , in Spanish it is possible to have an inversion in the order of the objects , as can be observed in sentence ( 1 ) , where the indirect object a Straw ( \x93to Straw\x94 ) precedes the direct object los alegatos ( \x93the pleas\x94 ) . \n\t', '\n\t\t ( 1 ) El gobierno chileno presentar´a hoy a Straw los alegatos ( ... ) . \n\t', '\n\t\t \x93The Chilean government will present today to Straw the pleas ( ... )\x94 . \n\t', '\n\t\t Dealing with this kind of phenomenon introduces some noise in the data . \n\t', '\n\t\t Matching a pattern for a subcategorisation frame from sentence ( 1 ) , for example , we would misleadingly induce the pattern [ PP(a) ] for the verb presentar , \x93present\x94 , when in fact the correct pattern for this sentence is [ NP PP(a) ] . \n\t', '\n\t\t The solution we adopt for dealing with the variations in the order of constituents is to take into account the functional information provided by clitics . \n\t', '\n\t\t Clitics are unstressed pronouns that refer to an antecedent in the discourse . \n\t', '\n\t\t In Spanish , clitic pronouns can only refer to the subject , the direct object , or the indirect object of the verb , and they can in most cases be disambiguated taking into account their agreement ( in person , number and gender ) with the verb . \n\t', '\n\t\t When we find a clitic pronoun in a sentence , we know that an argument position is already filled by it , and the rest of the constituents that are candidates for the position are either discarded or moved to another position . \n\t', '\n\t\t Sentence ( 2 ) shows an example of how the presence of clitic pronouns allows us to transform the patterns extracted . \n\t', '\n\t\t The sentence would normally match with the frame pattern [ PP(por) ] , but the presence of the clitic ( which has the form le ) allows us to deduce that the sentence contains an indirect object , realised in the sub- categorisation pattern with a prepositional phrase headed by a in second position . \n\t', '\n\t\t Therefore , we look for the following nominal phrase , la aparici´on del cad´aver , to fill the slot of the direct object , that otherwise would have not been included in the pattern . \n\t', '\n\t\t ( 2 ) Por la tarde , agentes del cuerpo nacional de policia le comunicaron por tel´efono la aparici´on del cad´aver . \n\t', '\n\t\t \x93In the afternoon , agents of the national police clitic IO reported by phone the apparition of the corpse.\x94 . \n\t', '\n\t\t The collection of pairs verb + pattern obtained with the method described in the last section needs to be filtered out , because we may have extracted constituents that are in fact adjuncts , or elements that are not attached to the verb , or errors in the extraction process . \n\t', '\n\t\t We filter out the spurious patterns with a Maximum Likelihood Estimate ( MLE ) , a method proposed by \n\t\t']",Positive
"['\n\t\t MLE is calculated as the ratio of the frequency of + over the frequency of . \n\t', '\n\t\t Pairs of verb+pattern that do not have a probability of co-occurring together higher than a certain threshold are filtered out . \n\t', '\n\t\t The threshold is determined empirically using held-out data ( 20 % of the total of the corpus ) , by choosing from a range of values between 0.02 and 0.1 the value that yields better results against a held-out gold standard of 10 verbs . \n\t', '\n\t\t In our experiments , this method yields a threshold value of 0.05 . \n\t', '\n\t\t 2.2 Experimental Evaluation We evaluate the obtained subcategorisation frames in terms of precision and recall compared to a gold No Prep . \n\t', '\n\t\t Groups Preposition Groups Corpus Prec Rec F Prec Rec F Small 65 62 63 63 61 62 Baseline 25 78 38 31 82 45 Large 70 60 65 71 61 66 Baseline 8 96 14 8 96 14 Table 1 : Results for the acquisition of subcategorisation frames . \n\t', '\n\t\t standard . \n\t', '\n\t\t The gold standard is manually constructed for a sample of 41 verbs . \n\t', '\n\t\t The verb sample is chosen randomly from our data with the condition that both frequent and infrequent verbs are represented , and that we have examples of all our subcategorisation frame types . \n\t', '\n\t\t We perform experiments on two corpora of different sizes , expecting that the differences in the results will show that a large amount of data does significantly improve the performance of any given system without any changes in the methodology . \n\t', '\n\t\t After the extraction process , the small corpus consists of 58493 pairs of verb+pattern , while the large corpus contains 1253188 pairs . \n\t', ""\n\t\t ' Since we in- clude in our patterns the heads of the prepositional phrases , the corpora contain a large number of pattern types ( 838 in the small corpora , and 2099 in the large corpora ) . \n\t"", '\n\t\t We investigate grouping semantically equivalent prepositions together , in order to reduce the number of pattern types , and therefore increment the probabilities on the patterns . \n\t', '\n\t\t The preposition groups are established manually . \n\t', '\n\t\t Table 1 shows the average results obtained on the two different corpora for the 41 test verbs . \n\t', '\n\t\t The baselines are established by considering all the frame patterns obtained in the extraction process as correct frames . \n\t', '\n\t\t The experiments on the large corpus give better results than the ones on the small one , and grouping similar prepositions together is useful only on the large corpus . \n\t', '\n\t\t This is probably due to the fact that the small corpus does not suffer from a too large number of frame types , and the effect of the groupings cannot be noticed . \n\t', '\n\t\t The F measure value of 66 % reported on the third line of table 1 , obtained on the large corpus with preposition groups , compares favourably to the results reported on \n\t\t']",Positive
"[""\n\t\t ' In all experiments , we post-process the data by eliminating prepositional constituents in the second position of the pattern that are introduced with the preposition de , \x93of\x94 . \n\t"", '\n\t\t This is moti- vated by the observation that in 96.8 % of the cases this prepo- sition is attached to the preceding constituent , and not to the verb.3 Clustering Verbs into Classes We use a bottom-up hierarchical clustering algorithm to group together 514 verbs into K classes . \n\t', '\n\t\t The algorithm starts by finding the similarities between all the possible pairs of objects in the data according to a similarity measure S . \n\t', '\n\t\t After having established the distance between all the pairs , it links together the closest pairs of objects by a linkage method L , forming a binary cluster . \n\t', '\n\t\t The linking process is repeated iteratively over the newly created clusters until all the objects are grouped into one cluster . \n\t', '\n\t\t K , S and L are parameters that can be set for the clustering . \n\t', '\n\t\t For the similarity measure S , we choose the Euclidean distance . \n\t', '\n\t\t For the linkage method L , we choose the Ward linkage method \n\t\t']",Positive
['\n\t\t Our choice of the parameter settings is motivated by the work of \n\t\t'],Positive
"['\n\t\t Applying a clustering method to the verbs in our data , we expect to find a natural division of the data that will be in accordance with the classification of verbs that we have set as our target classification . \n\t', '\n\t\t We perform different experiments with different values for K in order to test which of the different granularities yields better results . \n\t', '\n\t\t 3.1 The Target Classification In order to be able to evaluate the clusters output by the algorithm , we need to establish a manual classification of sample verbs . \n\t', '\n\t\t We assume the manual classification of Spanish verbs developed by ( V´azquez et al. , 2000 ) . \n\t', '\n\t\t In their classification , verbs are organised on the basis of meaning components , diathesis alternations and event structure . \n\t', '\n\t\t They classify a large number of verbs into three main classes ( Trajectory , Change and Attitude ) that are further subdivided into a total of 31 subclasses . \n\t', '\n\t\t Their classification follows the same basic hypotheses as Levin\x92s , but the resulting classes differ in some important aspects . \n\t', '\n\t\t For example , the Trajectory class groups together Levin\x92s Verbs of Motion ( move ) , Verbs of Communication ( tell ) and verbs of Change of Possession ( give ) , among others . \n\t', '\n\t\t Their justification for this grouping is that all the verbs in this class have a Trajectory meaning component , and that they all undergo the Underspecification alternation ( in Levin\x92s terminology , the Locative Preposition Drop and the Unspecified Object alternations ) . \n\t', '\n\t\t The size of the classes at the lower level of the classification hierarchy varies from 2 to 176 . \n\t', '\n\t\t 3.2 Materials The input to the algorithm is a description of each of the verbs in the form of a vector containing the probabilities of their subcategorisation frames . \n\t', '\n\t\t We obtain the subcategorisation frames with the method described in the previous section that gave better results : using the large corpus , and reducing the number of frame types by merging individual prepositions into groups . \n\t', '\n\t\t In order to reduce the number of frame types still further , we only take into account the ones that occur more than 10 times in the corpus . \n\t', '\n\t\t In this way , we have a set of 66 frame types . \n\t', '\n\t\t Moreover , for the purpose of the classification task , the subcategorisation frames are enhanced with extra information that is intended to reflect properties of the verbs that are relevant for the target classification . \n\t', '\n\t\t The target classification is based on three aspects of the verb properties : meaning components , diathesis alternations , and event structure , but the information provided by subcategorisation frames only reflects on the second of them . \n\t', '\n\t\t We expect to provide some information on the meaning components participating in the action by taking into account whether subjects and direct objects are recognised by the partial parser as named entities . \n\t', '\n\t\t Then , the possible labels for these constituents are \x93no NE\x94 , \x93persons\x94 , \x93locations\x94 , and \x93institutions\x94 . \n\t', '\n\t\t We introduce this new feature by splitting the probability mass of each frame among the possible labels , according to their frequencies . \n\t', '\n\t\t Now , we have a total of 97 features for each verb of our sample . \n\t', '\n\t\t 3.3 Clustering Evaluation Evaluating the results of a clustering experiment is a complex task because ideally we would like the output to fulfil different goals . \n\t', '\n\t\t One the one hand , the clusters obtained should reflect a good partition of the data , yielding consistent clusters . \n\t', '\n\t\t On the other hand , the partition of the data obtained should be as similar as possible to the manually constructed classification , the gold standard . \n\t', '\n\t\t We use the Silhouette measure \n\t\t']",Positive
"['\n\t\t For each clustering experiment , we calculate the mean of the silhouette value of all the data points , in order to get an indication of the overall quality of the clusters created . \n\t', '\n\t\t The main difficulty in evaluating unsupervised classification tasks against a gold standard lies in the fact that the class labels of the obtained clusters are unknown . \n\t', '\n\t\t Therefore , the evaluation is done according to the pairs of objects that the two groups have in common . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t It gives a value of 1 if the two classifications agree com- No Named Entities Task Mean Sil Baseline Radj 3-way 0.37 0 0.001 15-way 0.37 0 0.040 31-way 0.27 0 0.070 Table 2 : Clustering evaluation for the experiment without Named Entities Named Entities Task Mean Sil Baseline Radj 3-way 0.37 0 0.01 15-way 0.31 0 0.07 31-way 0.22 0 0.03 Table 3 : Clustering evaluation for the experiment with Named Entities pletely in which pairs of objects are clustered together and which are not , while complete disagreement between two classifications yields a value of -1 . \n\t', '\n\t\t 3.4 Experimental Results We perform various clustering experiments in order to test , on the one hand , the usefulness of our enhanced subcategorisation frames . \n\t', '\n\t\t On the other hand , we intend to discover which is the natural partition of the data that best accommodates our target classification . \n\t', '\n\t\t The target classification is a hierarchy of three levels , each of them dividing the data into 3 , 15 , or 31 levels . \n\t', '\n\t\t For this reason , we experiment on 3 , 15 , and 31 desired output clusters , and evaluate them on each of the target classification levels , respectively . \n\t', '\n\t\t Table 2 shows the evaluation results of the clustering experiment that takes as input bare subcategorisation frames . \n\t', '\n\t\t Table 3 shows the evaluation results of the experiment that includes named entity recognition in the features describing the verbs . \n\t', '\n\t\t In both tables , each line reports the results of a classification task . \n\t', '\n\t\t The average Silhouette measure is shown in the second column . \n\t', '\n\t\t We can observe that the best classification tasks in terms of the Silhouette measure are the 3-way and 15-way classifications . \n\t', '\n\t\t The baseline is calculated , for each task , as the average value of the Adjusted Rand measure for 100 random cluster assignations . \n\t', '\n\t\t Although all the tasks perform better than the baseline , the increase is so small that it is clear that some improvements have to be done on the experiments . \n\t', '\n\t\t According to the Adjusted Rand measure , the clustering algorithm seems to perform better in the tasks with a larger number of classes . \n\t', '\n\t\t On the other hand , the enhanced features are useful on the 15-way and 3-way classifications , but they are harmful in the 31-way classification . \n\t', '\n\t\t In spite of these results , a qualitative observation of the output clusters reveals that they are intuitively plausible , and that the evaluation is penalised by the fact that the target classes are of very different sizes . \n\t', '\n\t\t On the other hand , our data takes into account syntactic information , while the target classification is not only based on syntax , but also on other aspects of the properties of the verbs . \n\t', '\n\t\t These results compare poorly to the performance achieved by \n\t\t']",Positive
"['\n\t\t Nevertheless , our results are comparable to a subset of experiments reported in \n\t\t']",Positive
"['\n\t\t 4 Conclusions and Future Work We have presented a series of experiments that use an unsupervised learning method to classify Spanish verbs into semantic classes based on subcategorisation information . \n\t', '\n\t\t We apply well-known techniques that have been developed for the English language to Spanish , confirming that empirical methods can be re-used through languages without substantial changes in the methodology . \n\t', '\n\t\t In the task of acquiring subcategorisation frames , we achieve state of the art results . \n\t', '\n\t\t On the contrary , the task of inducing semantic classes from syntactic information using a clustering algorithm leaves room for improvement . \n\t', '\n\t\t The future work for this task goes on two directions . \n\t', '\n\t\t On the one hand , the theoretical basis of the manual verb classification suggests that , although the syntactic behaviour of verbs is an important criteria for a semantic classification , other properties of the verbs should be taken into account . \n\t', '\n\t\t Therefore , the description of verbs could be further enhanced with features that reflect on meaning components and event structure . \n\t', '\n\t\t The incorporation of name entity recognition in the experiments reported here is a first step in this direction , but it is probably a too sparse feature in the data to make any significant contributions . \n\t', '\n\t\t The event structure of predicates could be statistically approximated from text by grasping the aspect of the verb . \n\t', '\n\t\t The aspect of the verbs could , in turn , be approximated by developing features that would consider the usage of certain tenses , or the presence of certain types of adverbs that imply a restriction on the aspect of the verb . \n\t', '\n\t\t Adverbs such as \x94suddenly\x94 , \x94continuously\x94 , \x94often\x94 , or even adverbial sentences such as \x94every day\x94 give information on the event structure of predicates . \n\t', '\n\t\t As they are a closed class of words , a typology of adverbs could be established to approximate the event structure of the verb \n\t\t']",Positive
"['\n\t\t On the other hand , an observation of the verb clusters output by the algorithm suggests that they are intuitively more plausible than what the evaluation measures indicate . \n\t', '\n\t\t For the purposes of possible applications , a hard clustering of verbs does not seem to be necessary , especially when even manually constructed classifications adopt arbitrary decisions and do not agree with each other : knowing which verbs are semantically similar to each other in a more \x93fuzzy\x94 way might be even more useful . \n\t', '\n\t\t For this reason , a new approach could be envisaged for this task , in the direction of the work by \n\t\t']",Positive
"['\n\t\t For the purpose of evaluation , the gold standard classification could also be organised in the form of similarity rankings , based on the distance between the verbs in the hierarchy . \n\t', '\n\t\t Then , the rankings for each verb could be evaluated . \n\t', '\n\t\t The two directions appointed here , enriching the verb descriptions with new features that grasp other properties of the verbs , and envisaging a similarity ranking of verbs instead of a hard clustering , are the next steps to be taken for this work . \n\t', '\n\t\t Acknowledgements The realisation of this work was possible thanks to the funding of the Swiss FNRS project number 11- 65328.01 . \n\t', '\n\t\t References Jordi Atserias , Josep Carmona , Irene Castell´on , Sergi Cervell , Montserrat Civit , Lluis M`arquez , M. Antonia Marti , Lluis Padr´o , Roser Placer , Horacio Rodriguez , Mariona Taul´e , and Jordi Turmo . \n\t', '\n\t\t 1998. Morphosyntactic analysis and parsing of unrestricted spanish text . \n\t', '\n\t\t In Proceedings of the First International Conference on Language Resources and Evaluation ( LREC\x9298 ) , pages 1267\x961272 , Granada/Spain . \n\t', '\n\t\t Michael Brent . \n\t', '\n\t\t 1993. From grammar to lexicon : Unsupervised learning of lexical syntax . \n\t', '\n\t\t Computational Linguistics , 19(2):243\x96262 . \n\t', '\n\t\t Bonnie Dorr . \n\t', '\n\t\t 1997. Large-scale dictionary construction for foreign language tutoring and interlingual machine translation . \n\t', '\n\t\t Machine Translation , 12(4):1\x9655 . \n\t', '\n\t\t Eva Esteve Ferrer and Paola Merlo . \n\t', '\n\t\t 2003. Automatic classification of english verbs . \n\t', '\n\t\t Technical report , Universit´e de Gen`eve . \n\t', '\n\t\t Leonard Kaufman and Peter J. Rousseeuw . \n\t', '\n\t\t 1990. Finding Groups in Data - An Introduction to Cluster Analysis . \n\t', '\n\t\t Probability and Mathematical Statistics . \n\t', '\n\t\t Jonh Wiley and Sons , Inc. , New York . \n\t', '\n\t\t Anna Korhonen . \n\t', '\n\t\t 2002a . \n\t', '\n\t\t Semantically motivated subcategorization acquisition . \n\t', '\n\t\t In Proceedings of the Workshop of the ACL Special Interest Group on the Lexicon on Unsupervised Lexical Acquisition , pages 51\x9658 , Philadelphia,PA , July . \n\t', '\n\t\t Anna Korhonen . \n\t', '\n\t\t 2002b . \n\t', '\n\t\t Subcategorisation Acquisition . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , University of Cambridge . \n\t', '\n\t\t distributed as UCAM-CL-TR-530 . \n\t', '\n\t\t Beth Levin . \n\t', '\n\t\t 1993. English Verb Classes and Alternations . \n\t', '\n\t\t University of Chicago Press , Chicago , IL . \n\t', '\n\t\t Christopher Manning . \n\t', '\n\t\t 1993. Automatic acquisition of a large subcategorization dictionary from corpora . \n\t', '\n\t\t In Proceedings of the 31st Annual Meeting of the ACL , pages 235\x96242 , Columbus/Ohio . \n\t', '\n\t\t Paola Merlo and Suzanne Stevenson . \n\t', '\n\t\t 2001. Automatic verb classification based on statistical distributions of argument structure . \n\t', '\n\t\t Computational Linguistics , 27(3):373\x96408 . \n\t', '\n\t\t Gerold Schneider . \n\t', '\n\t\t 2003. A low-complexity , broad coverage probabilistic dependency parser for english . \n\t', '\n\t\t In Proceedings of NAACL/HLT 2003 Student Session , pages 31\x9636 , Edmonton/Canada . \n\t', '\n\t\t Sabine Schulte im Walde . \n\t', '\n\t\t 2003. Experiments on the Automatic Induction of German Semantic Verb Classes . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , Institut fur Maschinelle Sprachverarbeitung , Universitat Stuttgart . \n\t', '\n\t\t Published as AIMS Report 9(2) . \n\t', '\n\t\t Suzanne Stevenson and Eric Joanis . \n\t', '\n\t\t 2003. Semi- supervised verb class discovery using noisy features . \n\t', '\n\t\t In Proceedings of the Seventh Conference on Natural Language Learning ( CoNLL-2003 ) , page , Edmonton/Canada . \n\t', '\n\t\t Gloria V´azquez , Ana Fern´andez , Irene Castell´on , and M. Antonia Mart´í . \n\t', '\n\t\t 2000. Clasificaci´on verbal : Alternancias de di´atesis . \n\t', '\n\t\t Quaderns de Sintagma . \n\t', '\n\t\t Universitat de Lleida , 3 . \n\t', '\n\t\t Joe H. Ward . \n\t', '\n\t\t 1963. Hierarchical grouping to optimize an objective function . \n\t', '\n\t\t Journal of the American Statistical Association , 58:236\x96244 . \n\t', '\n\t\t Julie Weeds and David Weir . \n\t', '\n\t\t 2003. A general framework for distributional similarity . \n\t', '\n\t\t In Proceedings of the Conference on Empirical Methods in Natural Language Processing ( EMNLP2003 ) , Sapporo/Japan . \n\t', '\n\t\t Improving the Accuracy of Subcategorizations Acquired from Corpora Naoki Yoshinaga Department of Computer Science , University of Tokyo 7-3-1 Hongo , Bunkyo-ku , Tokyo , 113-0033 yoshinag@is.s.u-tokyo.ac.jp Abstract This paper presents a method of improving the accuracy of subcategorization frames ( SCFs ) acquired from corpora to augment existing lexicon resources . \n\t', '\n\t\t I estimate a confidence value of each SCF using corpus-based statistics , and then perform clustering of SCF confidence- value vectors for words to capture co- occurrence tendency among SCFs in the lexicon . \n\t', '\n\t\t I apply my method to SCFs acquired from corpora using lexicons of two large-scale lexicalized grammars . \n\t', '\n\t\t The resulting SCFs achieve higher precision and recall compared to SCFs obtained by naive frequency cut-off . \n\t', '\n\t\t 1 Introduction Recently , a variety of methods have been proposed for acquisition of subcategorization frames ( SCFs ) from corpora ( surveyed in \n\t\t']",Positive
"['\n\t\t One interesting possibility is to use these techniques to improve the coverage of existing large- scale lexicon resources such as lexicons of lexicalized grammars . \n\t', '\n\t\t However , there has been little work on evaluating the impact of acquired SCFs with the exception of \n\t\t']",Positive
"['\n\t\t The problem when we integrate acquired SCFs into existing lexicalized grammars is lower quality of the acquired SCFs , since they are acquired in an unsupervised manner , rather than being manually coded . \n\t', '\n\t\t If we attempt to compensate for the poor precision by being less strict in filtering out less likely SCFs , then we will end up with a larger number of noisy lexical entries , which is problematic for parsing with lexicalized grammars \n\t\t']",Positive
['\n\t\t We thus need some method of selecting the most reliable set of SCFs from the system output as demonstrated in \n\t\t'],Positive
"['\n\t\t In this paper , I present a method of improving the accuracy of SCFs acquired from corpora in order to augment existing lexicon resources . \n\t', '\n\t\t I first estimate a confidence value that a word can have each SCF , using corpus-based statistics . \n\t', '\n\t\t To capture latent co-occurrence tendency among SCFs in the target lexicon , I next perform clustering of SCF confidence-value vectors of words in the acquired lexicon and the target lexicon . \n\t', '\n\t\t Since each centroid value of the obtained clusters indicates whether the words in that cluster have each SCF , we can eliminate SCFs acquired in error and predict possible SCFs according to the centroids . \n\t', '\n\t\t I applied my method to SCFs acquired from a corpus of newsgroup posting about mobile phones \n\t\t']",Positive
"['\n\t\t I then compared the resulting SCFs with SCFs obtained by naive frequency cut-off to observe the effects of clustering . \n\t', '\n\t\t 2 Background 2.1 SCF Acquisition for Lexicalized Grammars I start by acquiring SCFs for a lexicalized grammar from corpora by the method described in \n\t\t']",Positive
"['\n\t\t #S(EPATTERN :TARGET |yield| :SUBCAT ( VSUBCAT NP ) :CLASSES ( ( 24 51 161 ) 5293 ) :RELIABILITY 0 :FREQSCORE 0.26861903 :FREQCNT 1 :TLTL ( VV0 ) :SLTL ( ( |route| NN1 ) ) :OLT1L ( ( |result| NN2 ) ) :OLT2L NIL :OLT3L NIL :LRL 0 ) ) Figure 1 : An acquired SCF for a verb \x93yield\x94 In their study , they first acquire fine-grained SCFs using the unsupervised method proposed by \n\t\t']",Positive
"['\n\t\t Figure 1 shows an example of one acquired SCF entry for a verb \x93yield.\x94 Each SCF entry has several fields about the observed SCF . \n\t', '\n\t\t I explain here only its portion related to this study . \n\t', '\n\t\t The TARGET field is a word stem , the first number in the CLASSES field indicates an SCF type , and the FREQCNT field shows how often words derivable from the word stem appeared with the SCF type in the training corpus . \n\t', '\n\t\t The obtained SCFs comprise the total 163 SCF types which are originally based on the SCFs in the ANLT \n\t\t']",Positive
"['\n\t\t In this example , the SCF type 24 corresponds to an SCF of transitive verb . \n\t', '\n\t\t They then obtain SCFs for the target lexicalized grammar ( the LinGO ERG \n\t\t']",Positive
"['\n\t\t They reported that they could achieve a coverage improvement of 4.5 % but that average parse time was doubled . \n\t', '\n\t\t This is because they did not use any filtering method for the acquired SCFs to suppress an increase of the lexical ambiguity . \n\t', '\n\t\t We definitely need some method to control the quality of the acquired SCFs . \n\t', '\n\t\t Their method is extendable to any lexicalized grammars , if we could have a translation map from these 163 types to the SCF types in the grammar . \n\t', '\n\t\t 2.2 Clustering of Verb SCF Distributions There is some related work on clustering of verbs according to their SCF probability distributions \n\t\t']",Positive
['\n\t\t Schulte im Walde and ( true ) probability distribution NP None NP_to-PP NP_PP PP subcategorization frame Figure 2 : SCF probability distributions for apply \n\t\t'],Positive
"['\n\t\t These studies aim at obtaining verb semantic classes , which are closely related to syntactic behavior of argument selection \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t In this study , I assume that there are classes whose element words have identical SCF types . \n\t', '\n\t\t I then obtain these classes by clustering acquired SCFs , using information available in the target lexicon , and directly use the obtained classes to eliminate implausible SCFs . \n\t', '\n\t\t 3 Method 3.1 Estimation of Confidence Values for SCFs I first create an SCF confidence-value vector vi for each word wi , an object for clustering . \n\t', '\n\t\t Each element vij in vi represents a confidence value of SCF sj for a word wi , which expresses how strong the evidence is that the word wi has SCF sj . \n\t', '\n\t\t Note that a confidence value confi j is not a probability that a word wi appears with SCF sj but a probability of existence of SCF sj for the word wi . \n\t', '\n\t\t In this study , I assume that a word wi appears with each SCF sj with a certain ( non-zero ) probability Oij(= p(sij |wi ) > 0 where Ej Oij = 1 ) , but only SCFs whose probabilities exceed a certain threshold are recognized in the lexicon . \n\t', '\n\t\t I hereafter call this threshold recognition threshold . \n\t', '\n\t\t Figure 2 depicts a probability distribution of SCF for apply . \n\t', '\n\t\t In this context , I can regard a confidence value of each SCF as a probability that the probability of that SCF exceeds the recognition threshold . \n\t', '\n\t\t recognition threshold 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 apply One intuitive way to estimate a confidence value is to assume an observed probability , i.e. , relative frequency , is equal to a probability eij of SCF sj for a word wi ( eij = freqij / ~j freqij where freqij is a frequency that a word wi appears with SCF sj in corpora ) . \n\t', '\n\t\t When the relative frequency of sj for a word wi exceeds the recognition threshold , its confidence value confij is set to 1 , and otherwise confi j is set to 0 . \n\t', '\n\t\t However , an observed probability is unreliable for infrequent words . \n\t', '\n\t\t Moreover , when we want to encode confidence values of reliable SCFs in the target grammar , we cannot distinguish the confidence values of those SCFs with confidence values of acquired SCFs . \n\t', '\n\t\t The other promising way to estimate a confidence value , which I adopt in this study , is to as- sume a probability eij as a stochastic variable in the context of Bayesian statistics \n\t\t']",Positive
"['\n\t\t In this context , a posteriori distribution of the probability eij of an SCF sj for a word wi is given by : p(eijJD) = P(eij)P( Jeij ) P(D) P(eij)P(DJeij) ( 1 ) fo P(eij)P(DJeij)deij where P(eij) is a priori distribution , and D is the data we have observed . \n\t', '\n\t\t Since every occurrence of SCFs in the data D is independent with each other , the data D can be regarded as Bernoulli trials . \n\t', '\n\t\t When we observe the data D that a word wi appears n times in total and x(< n ) times with SCF sj,1 its conditional distribution is represented by binominal distribution : P(DJeij) = \\x/et~(1-eij)(n-x) ( 2 ) To calculate this a posteriori distribution , I need to define the a priori distributionP(eij) . \n\t', '\n\t\t The question is which probability distribution of eij can appropriately reflects prior knowledge . \n\t', '\n\t\t In other words , it should encode knowledge we use to estimate SCFs for unknown words . \n\t', '\n\t\t I simply determine it from distributions of observed probability values of sj for words seen in corpora2 by using 1The values of FREQCNT is used to obtain n and x. 2I estimated a priori distribution separately for each type of SCF from words that appeared more than 50 times in the training corpus in the following experiments . \n\t', '\n\t\t a method described in \n\t\t']",Positive
"['\n\t\t In their study , they assume a priori distribution as the beta distribution defined as : p(eijJa,Q) = ea-1 ij(1- eij)Q-1 B(a,Q) , ( 3 ) where B(a,Q) = f01 eta-1(1 - eij)Q-1deij . \n\t', '\n\t\t The value of a and Q is determined by moment esti- mation.3 By substituting Equations 2 and 3 into Equation 1 , I finally obtain the a posteriori distribution p(eij JD ) as : p(eij Ja , Q , D ) = c. eta+a-1(1- eij)n-x+Q-1 , ( 4 ) where c = ( x ) /(B(a,Q) f01 P(eij)P(DJ eij)deij ) . \n\t', '\n\t\t When I regard the recognition threshold as t , I can calculate a confidence value confij that a word wi can have sj by integrating the a posteriori distribution p(eij JD ) from the threshold t to 1 : 1 confij=~ c.e~+a-1(1-eij)n-x+Q-1deij.(5) r By using this confidence value , I represent an SCF confidence-value vector vi for a word wi in the acquired SCF lexicon ( vij = confij ) . \n\t', '\n\t\t In order to combine SCF confidence-value vectors for words acquired from corpora and those for words in the lexicon of the target grammar , I also represent an SCF confidence-value vector v~i for a word w~i in the target grammar by : ~1- E w~i has sj in the lexicon ( 6 ) s otherwise , where c expresses an unreliability of the lexicon . \n\t', '\n\t\t In this study , I trust the lexicon as much as possible by setting E to the machine epsilon . \n\t', '\n\t\t 3.2 Clustering of SCF Confidence-Value Vectors I next present a clustering algorithm of words according to their SCF confidence-value vectors . \n\t', '\n\t\t Given k initial representative vectors called centroids , my algorithm iteratively updates clusters by assigning each data object to its closest centroid 3 The expectation and variance of the beta distribution are made equal to those of the observed probability values . \n\t', '\n\t\t v~ij = Input : a set of SCF confidence-value vectors V={v1,v2,...,vn}^Rm a distance function d:Rm×Zm^R a function to compute a centroid 9 : { vj1 , vj2 , ... , vjl}^Zm initial centroids C = { c1,c2,...,ck } ^ Zm Output : a set of clusters ICjl while cluster members are not stable do foreach cluster Cj Cj={vi^cl,d(vi,cj)^d(vi,cl)} ( 1 ) end foreach foreach clusters Cj cj=(Cj) ( 2 ) end foreach end while return ICjl Figure 3 : Clustering algorithm for SCF confidence-value vectors and recomputing centroids until cluster members become stable , as depicted in Figure 3 . \n\t', '\n\t\t Although this algorithm is roughly based on the k-Means algorithm , it is different from k-Means in important respects . \n\t', '\n\t\t I assume the elements of the centroids of the clusters as a discrete value of 0 or 1 because I want to obtain clusters whose element words have the exactly same set of SCFs . \n\t', '\n\t\t I then derive a distance function d to calculate a probability that a data object vi should have an SCF set represented by a centroid cm as follows : d(vi,cm) = rl vij · H ( 1 ^vij ) . \n\t', '\n\t\t ( 7 ) cmj=1 cmj=0 By using this function , I can determine the closest d(vi , cm ) ( ( 1 ) in Figure 3 ) . \n\t', '\n\t\t After every assignment , I calculate a next centroid cm of each cluster Cm ( ( 2 ) in Figure 3 ) by comparing a probability that the words in the cluster have an SCF sj and a probability that the words in the cluster do not have the SCF sj as follows : 1 when H vij > H ( 1 ^vij ) vi ^Cm vi ^Cm ( 8 ) 0 otherwise . \n\t', '\n\t\t I next address the way to determine the number of clusters and initial centroids . \n\t', '\n\t\t In this study , I assume that the most of the possible set of SCFs for words are included in the lexicon of the target grammar,4 and make use of the existing sets of 4When the lexicon is less accurate , I can determine the number of clusters using other algorithms \n\t\t']",Positive
"['\n\t\t SCFs for the words in the lexicon to determine the number of clusters and initial centroids . \n\t', '\n\t\t I first extract SCF confidence-value vectors from the lexicon of the grammar . \n\t', '\n\t\t By eliminating duplications from them and regarding c = 0 in Equation 6 , I obtain initial centroids cm. I then initialize the number of clusters k to the number of cm. I finally update the acquired SCFs using the obtained clusters and the confidence values of SCFs in this order . \n\t', '\n\t\t I call the following procedure centroid cut-off t when the confidence values are estimated under the recognition threshold t . \n\t', '\n\t\t Since the value cmj of a centroid cm in a cluster Cm represents whether the words in the cluster can have SCF sj , I first obtain SCFs by collecting SCF sj for a word wi ^ Cm when cmj is 1 . \n\t', '\n\t\t I then eliminate implausible SCFs sj for wi from the resulting SCFs according to their confidence values confij . \n\t', '\n\t\t In the following , I compare centroid cut-off with frequency cut-off and confidence cut-off t , which use relative frequencies and confidence values calculated under the recognition threshold t , respectively . \n\t', '\n\t\t Note that these cut-offs use only corpus-based statistics to eliminate SCFs . \n\t', '\n\t\t 4 Experiments I applied my method to SCFs acquired from 135,902 sentences of mobile phone newsgroup postings archived by Google.com , which is the same data used in \n\t\t']",Positive
"['\n\t\t The number of acquired SCFs was 14,783 for 3,864 word stems , while the number of SCF types in the data was 97 . \n\t', '\n\t\t I then translated the 163 SCF types into the SCF types of the XTAG English grammar ( XTAG Research Group , 2001 ) and the LinGO ERG \n\t\t']",Positive
"['\n\t\t To evaluate my method , I split each lexicon of the two grammars into the training SCFs and the testing SCFs . \n\t', '\n\t\t The words in the testing SCFs were included in the acquired SCFs . \n\t', '\n\t\t When I apply my method to the acquired SCFs using the training SCFs and evaluate the resulting SCFs with the 5I used the same version of the LinGO ERG as \n\t\t']",Positive
"['\n\t\t cluster as argmax Cm cmj = 1 0.8 0.6 0.4 0.2 0 1 0.8 0.6 0.4 0.2 0 A : frequency cut-off B : confidence cut-off 0.01 C : confidence cut-off 0.03 D : confidence cut-off 0.05 B C D A A D B C A : frequency cut-off B : confidence cut-off 0.01 C : confidence cut-off 0.03 D : confidence cut-off 0.05 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Precision Precision XTAG ERG Figure 4 : Precision and recall of the resulting SCFs using confidence cut-offs and frequency cut-off : the XTAG English grammar ( left ) the LinGO ERG ( right ) XTAG ERG Figure 5 : Precision and recall of the resulting SCFs using confidence cut-off and centroid cut-off : the XTAG English grammar ( left ) the LinGO ERG ( right ) 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 Precision Precision 0.8 0.6 0.4 0.2 0 1 0.8 0.6 0.4 0.2 0 1 A B D A : frequency cut-off B : centroid cut-off* 0.05 C : centroid cut-off 0.05 D : confidence cut-off 0.05 C A : frequency cut-off B : centroid cut-off* 0.05 C : centroid cut-off 0.05 D : confidence cut-off 0.05 D A C B testing SCFs , we can estimate to what extent my method can preserve reliable SCFs for words unknown to the grammar.6 The XTAG lexicon was split into 9,437 SCFs for 8,399 word stems as training and 423 SCFs for 280 word stems as testing , while the ERG lexicon was split into 1,608 SCFs for 1,062 word stems as training and 292 SCFs for 179 word stems as testing . \n\t', '\n\t\t I extracted SCF confidence-value vectors from the training SCFs and the acquired SCFs for the words in the testing SCFs . \n\t', '\n\t\t The number of the resulting data objects was 8,679 for XTAG and 1,241 for ERG . \n\t', '\n\t\t The number of initial centroids7 extracted from the training SCFs was 49 for XTAG and 53 for ERG . \n\t', '\n\t\t I then performed clustering of 8,679 data objects into 49 clusters and 1,241 data objects into 6I here assume that the existing SCFs for the words in the lexicon is more reliable than the other SCFs for those words . \n\t', '\n\t\t 7I used the vectors that appeared for more than one word . \n\t', '\n\t\t 53 clusters , and then evaluated the resulting SCFs by comparing them to the testing SCFs . \n\t', '\n\t\t I first compare confidence cut-off with frequency cut-off to observe the effects of Bayesian estimation . \n\t', '\n\t\t Figure 4 shows precision and recall of the SCFs obtained using frequency cut-off and confidence cut-off 0.01 , 0.03 , and 0.05 by varying threshold for the confidence values and the relative frequencies from 0 to 1.8 The graph indicates that the confidence cut-offs achieved higher recall than the frequency cut-off , thanks to the a priori distributions . \n\t', '\n\t\t When we compare the three confidence cut-offs , we can improve precision using higher recognition thresholds while we can improve recall using lower recognition thresholds . \n\t', '\n\t\t This is quite consistent with our expectations . \n\t', '\n\t\t Precision=Correct SCFs for the words in the resulting SCFs 8 All SCFs for the words in the resulting SCFs Recall =Correct SCFs for the words in the resulting SCFs All SCFs for the words in the test SCFs I then compare centroid cut-off with confidence cut-off to observe the effects of clustering . \n\t', '\n\t\t Figure 5 shows precision and recall of the resulting SCFs using centroid cut-off 0.05 and the confidence cut-off 0.05 by varying the threshold for the confidence values . \n\t', '\n\t\t In order to show the effects of the use of the training SCFs , I also performed clustering of SCF confidence-value vectors in the acquired SCFs with random initialization ( k = 49 ( for XTAG ) and 53 ( for ERG ) ; centroid cut-off 0.05* ) . \n\t', '\n\t\t The graph shows that clustering is meaningful only when we make use of the reliable SCFs in the manually-coded lexicon . \n\t', '\n\t\t The centroid cutoff using the lexicon of the grammar boosted precision compared to the confidence cut-off . \n\t', '\n\t\t The difference between the effects of my method on XTAG and ERG would be due to the finer-grained SCF types of ERG . \n\t', '\n\t\t This resulted in lower precision of the acquired SCFs for ERG , which prevented us from distinguishing infrequent ( correct ) SCFs from SCFs acquired in error . \n\t', '\n\t\t However , since unusual SCFs tend to be included in the lexicon , we will be able to have accurate clusters for unknown words with smaller SCF variations as we achieved in the experiments with XTAG . \n\t', '\n\t\t 5 Concluding Remarks and Future Work In this paper , I presented a method to improve the quality of SCFs acquired from corpora using existing lexicon resources . \n\t', '\n\t\t I applied my method to SCFs acquired from corpora using lexicons of the XTAG English grammar and the LinGO ERG , and have shown that it can eliminate implausible SCFs , preserving more reliable SCFs . \n\t', '\n\t\t In the future , I need to evaluate the quality of the resulting SCFs by manual analysis and by using the extended lexicons to improve parsing . \n\t', '\n\t\t I will investigate other clustering methods such as hierarchical clustering , and use other information for clustering such as semantic preference of arguments of SCFs to have more accurate clusters . \n\t', '\n\t\t Acknowledgments I thank Yoshimasa Tsuruoka and Takuya Matsuzaki for their advice on probabilistic modeling , Alex Fang for his help in using the acquired SCFs , and Anna Korhonen for her insightful suggestions on evaluation . \n\t', '\n\t\t I am also grateful to Jun\x92ichi Tsujii , Yusuke Miyao , John Carroll and the anonymous reviewers for their valuable comments . \n\t', '\n\t\t This work was supported in part by JSPS Research Fellowships for Young Scientists and in part by CREST , JST ( Japan Science and Technology Agency ) . \n\t', '\n\t\t References B. Boguraev and T. Briscoe . \n\t', '\n\t\t 1987. Large lexicons for natural language processing : utilising the grammar coding system of LDOCE . \n\t', '\n\t\t Computational Linguistics , 13(4):203\x96218 . \n\t', '\n\t\t T. Briscoe and J. Carroll . \n\t', '\n\t\t 1997. Automatic extraction of subcategorization from corpora . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t the fifth ANLP , pages 356\x96363 . \n\t', '\n\t\t J. Carroll and A. C. Fang . \n\t', '\n\t\t 2004. The automatic acquisition of verb subcategorizations and their impact on the performance of an HPSG parser . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t thefirst ijc-NLP , pages 107\x96114 . \n\t', '\n\t\t A. Copestake . \n\t', '\n\t\t 2002. Implementing typed feature structure grammars . \n\t', '\n\t\t CSLI publications . \n\t', '\n\t\t E. W. Forgy . \n\t', '\n\t\t 1965. Cluster analysis of multivariate data : Efficiency vs. interpretability of classifications . \n\t', '\n\t\t Biometrics , 21:768\x96780 . \n\t', '\n\t\t A. Gelman , J. B. Carlin , H. S. Stern , and D. B. Rubin , editors . \n\t', '\n\t\t 1995. Bayesian Data Analysis . \n\t', '\n\t\t Chapman and Hall . \n\t', '\n\t\t R. Grishman , C. Macleod , and A. Meyers . \n\t', '\n\t\t 1994. Comlex syntax : Building a computational lexicon . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t the 15th COLING , pages 268\x96272 . \n\t', '\n\t\t G. Hamerly . \n\t', '\n\t\t 2003. Learning structure and concepts in data through data clustering . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , University of California , San Diego . \n\t', '\n\t\t A. Korhonen , Y. Krymolowski , and Z. Marx . \n\t', '\n\t\t 2003. Clustering polysemic subcategorization frame distributions semantically . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t the 41stACL , pages 64\x9671 . \n\t', '\n\t\t A. Korhonen . \n\t', '\n\t\t 2002. Subcategorization Acquisition . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , University of Cambridge . \n\t', '\n\t\t B. Levin . \n\t', '\n\t\t 1993. English Verb Classes and Alternations . \n\t', '\n\t\t Chicago University Press . \n\t', '\n\t\t A. Sarkar , F. Xia , and A. K. Joshi . \n\t', '\n\t\t 2000. Some experiments on indicators of parsing complexity for lexicalized grammars . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t the 18th COLING workshop , pages 37\x9642 . \n\t', '\n\t\t S. Schulte im Walde and C. Brew . \n\t', '\n\t\t 2002. Inducing German semantic verb classes from purely syntactic subcategorisation information . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t the 41stACL , pages 223\x9623 0 . \n\t', '\n\t\t Y. Tsuruoka and T. Chikayama . \n\t', '\n\t\t 2001 . \n\t', '\n\t\t Estimating reliability of contextual evidences in decision-list classifiers under Bayesian learning . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t the sixth NLPRS , pages 701\x96 707 . \n\t', '\n\t\t XTAG Research Group . \n\t', '\n\t\t 2001. A Lexicalized Tree Adjoining Grammar for English . \n\t', '\n\t\t Technical Report IRCS-01-03 , IRCS , University of Pennsylvania . \n\t', '\n\t\t Robust VPE detection using Automatically Parsed Text Leif Arda Nielsen Department of Computer Science King\x92s College London nielsen@dcs.kcl.ac.uk Abstract This paper describes a Verb Phrase Ellipsis ( VPE ) detection system , built for robustness , accuracy and domain independence . \n\t', '\n\t\t The system is corpus-based , and uses machine learning techniques on free text that has been automatically parsed . \n\t', '\n\t\t Tested on a mixed corpus comprising a range of genres , the system achieves a 70 % F1-score . \n\t', '\n\t\t This system is designed as the first stage of a complete VPE resolution system that is input free text , detects VPEs , and proceeds to find the antecedents and resolve them . \n\t', '\n\t\t 1 Introduction Ellipsis is a linguistic phenomenon that has received considerable attention , mostly focusing on its interpretation . \n\t', '\n\t\t Most work on ellipsis \n\t\t']",Negative
"['\n\t\t The detection of elliptical sentences or the identification of the antecedent and elided clauses within them are usually not dealt with , but taken as given . \n\t', '\n\t\t Noisy or missing input , which is unavoidable in NLP applications , is not dealt with , and neither is focusing on specific domains or applications . \n\t', '\n\t\t It therefore becomes clear that a robust , trainable approach is needed . \n\t', '\n\t\t An example of Verb Phrase Ellipsis ( VPE ) , which is detected by the presence of an auxiliary verb without a verb phrase , is seen in example 1 . \n\t', '\n\t\t VPE can also occur with semi-auxiliaries , as in example 2. ( 1 ) John3 { loves his3 wife12 . \n\t', '\n\t\t Bill3 does1 too . \n\t', '\n\t\t ( 2 ) But although he was terse , he didn\x92t { rage at me12 the way I expected him to1 . \n\t', '\n\t\t Several steps of work need to be done for ellipsis resolution : 1 . \n\t', '\n\t\t Detecting ellipsis occurrences . \n\t', '\n\t\t First , elided verbs need to be found . \n\t', '\n\t\t 2. Identifying antecedents . \n\t', '\n\t\t For most cases of ellipsis , copying of the antecedent clause is enough for resolution \n\t\t']",Positive
"['\n\t\t 3. Resolving ambiguities . \n\t', '\n\t\t For cases where ambiguity exists , a method for generating the full list of possible solutions , and suggesting the most likely one is needed . \n\t', '\n\t\t This paper describes the work done on the first stage , the detection of elliptical verbs . \n\t', '\n\t\t First , previous work done on tagged corpora will be summarised . \n\t', '\n\t\t Then , new work on parsed corpora will be presented , showing the gains possible through sentence-level features . \n\t', '\n\t\t Finally , experiments using unannotated data that is parsed using an automatic parser are presented , as our aim is to produce a stand-alone system . \n\t', '\n\t\t We have chosen to concentrate on VP ellipsis due to the fact that it is far more common than other forms of ellipsis , but pseudo-gapping , an example of which is seen in example 3 , has also been included due to the similarity of its resolution to VPE \n\t\t']",Positive
"['\n\t\t Do so/it/that and so doing anaphora are not handled , as their resolution is different from that of VPE \n\t\t']",Positive
"['\n\t\t ( 3 ) John writes plays , and Bill does novels . \n\t', '\n\t\t 2 Previous work Hardt\x92s ( 1997 ) algorithm for detecting VPE in the Penn Treebank ( see Section 3 ) achieves precision levels of 44 % and recall of 53 % , giving an F11 of 48 % , using a simple search technique , which relies on the parse annotation having identified empty expressions correctly . \n\t', '\n\t\t In previous work \n\t\t']",Positive
"['\n\t\t These earlier results are not directly comparable to Hardt\x92s , due to the different corpora used . \n\t', '\n\t\t The expanded set of results are summarised in Table 1 , for Transformation Based Learning ( TBL ) \n\t\t']",Positive
"['\n\t\t Algorithm Recall Precision F1 TBL 69.63 85.14 76.61 Decision Tree 60.93 79.39 68.94 MBL 72.58 71.50 72.04 GIS-MaxEnt 71.72 63.89 67.58 L-BFGS-MaxEnt 71.93 80.58 76.01 Table 1 : Comparison of algorithms 1Precision , recall and F1 are defined as : Recall = No(correct ellipses found ) ( 1 ) No(all ellipses in test ) Precision = No(correct ellipses found ) ( 2 ) No(all ellipses found ) F1 = 2 × Precision × Recall ( 3 ) Precision + Recall 2Downloadable from http://www.nlplab.cn/zhangle/maxent toolkit.html For all of these experiments , the training features consisted of lexical forms and Part of Speech ( POS ) tags of the words in a three word forward/backward window of the auxiliary being tested . \n\t', '\n\t\t This context size was determined empirically to give optimum results , and will be used throughout this paper . \n\t', '\n\t\t The L-BFGS-MaxEnt uses Gaussian Prior smoothing which was optimized for the BNC data , while the GIS-MaxEnt has a simple smoothing option available , but this deteriorates results and is not used . \n\t', '\n\t\t MBL was used with its default settings . \n\t', '\n\t\t While TBL gave the best results , the software we used \n\t\t']",Negative
"['\n\t\t Decision trees , on the other hand , tend to oversimplify due to the very sparse nature of ellipsis , and produce a single rule that classifies everything as nonVPE . \n\t', '\n\t\t This leaves Maximum Entropy and MBL for further experiments . \n\t', '\n\t\t 3 Corpus description The British National Corpus ( BNC ) \n\t\t']",Positive
"['\n\t\t A range of V sections of the BNC , containing around 370k words3 with 645 samples of VPE was used as training data . \n\t', '\n\t\t The separate test data consists of around 74k words4 with 200 samples of VPE . \n\t', '\n\t\t The Penn Treebank \n\t\t']",Positive
"['\n\t\t A mixture of sections from the Wall Street Journal and Brown corpus were used . \n\t', '\n\t\t The training section5 consists of around 540k words and contains 522 samples of VPE . \n\t', '\n\t\t The test section6 consists of around 140k words and contains 150 samples of VPE . \n\t', '\n\t\t 4 Experiments using the Penn Treebank To experiment with what gains are possible through the use of more complex data such as 3Sections CS6 , A2U , J25 , FU6 , H7F , HA3 , A19 , A0P , G1A , EWC , FNS , C8T 4Sections EDJ , FR3 5Sections WSJ 00 , 01 , 03 , 04 , 15 , Brown CF , CG , CL , CM , CN , CP 6Sections WSJ 02 , 10 , Brown CK , CR parse trees , the Penn Treebank is used for the second round of experiments . \n\t', '\n\t\t The results are presented as new features are added in a cumulative fashion , so each experiment also contains the data contained in those before it . \n\t', '\n\t\t Words and POS tags The Treebank , besides POS tags and category headers associated with the nodes of the parse tree , includes empty category information . \n\t', '\n\t\t For the initial experiments , the empty category information is ignored , and the words and POS tags are extracted from the trees . \n\t', '\n\t\t The results in Table 2 are seen to be considerably poorer than those for BNC , despite the comparable data sizes . \n\t', '\n\t\t This can be accounted for by the coarser tagset employed . \n\t', '\n\t\t Algorithm Recall Precision F1 MBL 47.71 60.33 53.28 GIS-MaxEnt 34.64 79.10 48.18 L-BFGS-MaxEnt 60.13 76.66 67.39 Table 2 : Initial results with the Treebank Close to punctuation A very simple feature , that checks for auxiliaries close to punctuation marks was tested . \n\t', '\n\t\t Table 3 shows the performance of the feature itself , characterised by very low precision , and results obtained by using it . \n\t', '\n\t\t It gives a 2 % increase in F1 for MBL , 3 % for GIS-MaxEnt , but a 1.5 % decrease for L-BFGS-MaxEnt . \n\t', '\n\t\t This brings up the point that the individual success rate of the features will not be in direct correlation with gains in overall results . \n\t', '\n\t\t Their contribution will be high if they have high precision for the cases they are meant to address , and if they produce a different set of results from those already handled well , complementing the existing features . \n\t', '\n\t\t Overlap between features can be useful to have greater confidence when they agree , but low precision in the feature can increase false positives as well , decreasing performance . \n\t', '\n\t\t Also , the small size of the test set can contribute to fluctuations in results . \n\t', '\n\t\t Heuristic Baseline A simple heuristic approach was developed to form a baseline . \n\t', '\n\t\t The method takes all auxiliaries Algorithm Recall Precision F1 close-to-punctuation 30.06 2.31 4.30 MBL 50.32 61.60 55.39 GIS-MaxEnt 37.90 79.45 51.32 L-BFGS-MaxEnt 57.51 76.52 65.67 Table 3 : Effects of using the close-to-punctuation feature ( SINV ( ADVP-PRD-TPC-2 ( RB so ) ) ( VP ( VBZ is ) ( ADVP-PRD ( -NONE- *T*-2 ) ) ) ( NP-SBJ ( PRP $ its ) ( NN balance ) ( NN sheet ) ) ) Figure 1 : Fragment of sentence from Treebank as possible candidates and then eliminates them using local syntactic information in a very simple way . \n\t', '\n\t\t It searches forwards within a short range of words , and if it encounters any other verbs , adjectives , nouns , prepositions , pronouns or numbers , classifies the auxiliary as not elliptical . \n\t', '\n\t\t It also does a short backwards search for verbs . \n\t', '\n\t\t The forward search looks 7 words ahead and the backwards search 3 . \n\t', '\n\t\t Both skip \x91asides\x92 , which are taken to be snippets between commas without verbs in them , such as : \x93 ... papers do , however , show ...\x94 . \n\t', '\n\t\t This feature gives a 4.5 % improvement for MBL ( Table 4 ) , 4 % for GIS-MaxEnt and 3.5 % for L-BFGSMaxEnt . \n\t', '\n\t\t Algorithm Recall Precision F1 heuristic 48.36 27.61 35.15 MBL 55.55 65.38 60.07 GIS-MaxEnt 43.13 78.57 55.69 L-BFGS-MaxEnt 62.09 77.86 69.09 Table 4 : Effects of using the heuristic feature Surrounding categories The next feature added is the categories of the previous branch of the tree , and the next branch . \n\t', '\n\t\t So in the example in Figure 1 , the previous category of the elliptical verb is ADVP-PRD-TPC-2 , and the next category NP-SBJ . \n\t', '\n\t\t The results of using this feature are seen in Table 5 , giving a 3.5 % boost to MBL , 2 % to GIS-MaxEnt , and 1.6 % to L-BFGSMaxEnt . \n\t', '\n\t\t Algorithm Recall Precision F1 MBL 58.82 69.23 63.60 GIS-MaxEnt 45.09 81.17 57.98 L-BFGS-MaxEnt 64.70 77.95 70.71 Table 5 : Effects of using the surrounding categories Auxiliary-final VP For auxiliary verbs parsed as verb phrases ( VP ) , this feature checks if the final element in the VP is an auxiliary or negation . \n\t', '\n\t\t If so , no main verb can be present , as a main verb cannot be followed by an auxiliary or negation . \n\t', '\n\t\t This feature was used by \n\t\t']",Positive
['\n\t\t Algorithm Recall Precision F1 Auxiliary-final VP 72.54 35.23 47.43 MBL 63.39 71.32 67.12 GIS-MaxEnt 54.90 77.06 64.12 L-BFGS-MaxEnt 71.89 76.38 74.07 Table 6 : Effects of using the Auxiliary-final VP feature Empty VP \n\t\t'],Positive
"['\n\t\t Our findings are in line with Hardt\x92s , who reports 48 % F1 , with the difference being due to the different sections of the Treebank used . \n\t', '\n\t\t It was observed that this search may be too restrictive to catch some examples of VPE in the corpus , and pseudo-gapping . \n\t', '\n\t\t Modifying the search pattern to be \x91(VP ( -NONE- *?*)\x92 instead improves the feature itself by 10 % in F1 and gives the results seen in Table 7 , increasing MBL\x92s F 1 by 10 % , GIS-MaxEnt by 14 % and L-BFGS-MaxEnt by 11.7 % . \n\t', '\n\t\t Algorithm Recall Precision F1 Empty VP 54.90 97.67 70.29 MBL 77.12 77.63 77.37 GIS-MaxEnt 69.93 88.42 78.10 L-BFGS-MaxEnt 83.00 88.81 85.81 Table 7 : Effects of using the improved Empty VP feature Empty categories Finally , including empty category information completely , such that empty categories are treated as words and included in the context . \n\t', '\n\t\t Table 8 shows that adding this information results in a 4 % increase in F1 for MBL , 4.9 % for GIS-MaxEnt , and 2.5 % for L-BFGS-MaxEnt . \n\t', '\n\t\t Algorithm Recall Precision F1 MBL 83.00 79.87 81.41 GIS-MaxEnt 76.47 90.69 82.97 L-BFGS-MaxEnt 86.27 90.41 88.29 Table 8 : Effects of using the empty categories 5 Experiments with Automatically Parsed data The next set of experiments use the BNC and Treebank , but strip POS and parse information , and parse them automatically using two different parsers . \n\t', '\n\t\t This enables us to test what kind of performance is possible for real-world applications . \n\t', '\n\t\t 5.1 Parsers used Charniak\x92s parser ( 2000 ) is a combination probabilistic context free grammar and maximum entropy parser . \n\t', '\n\t\t It is trained on the Penn Treebank , and achieves a 90.1 % recall and precision average for sentences of 40 words or less . \n\t', '\n\t\t Robust Accurate Statistical Parsing ( RASP ) \n\t\t']",Positive
"['\n\t\t RASP is trained on a range of corpora , and uses a more complex tagging system ( CLAWS-2 ) , like that of the BNC. . \n\t', '\n\t\t This parser , on our data , generated full parses for 70 % of the sentences , partial parses for 28 % , while 2 % were not parsed , returning POS tags only . \n\t', '\n\t\t 5.2 Reparsing the Treebank The results of experiments using the two parsers ( Table 9 ) show generally similar performance . \n\t', '\n\t\t Compared to results on the original treebank with similar data ( Table 6 ) , the results are 4-6 % lower , or in the case of GIS-MaxEnt , 4 % lower or 2 % higher , depending on parser . \n\t', '\n\t\t This drop in performance is not surprising , given the errors introduced by the parsing process . \n\t', '\n\t\t As the parsers do not generate empty-category information , their overall results are 14-20 % lower , compared to those in Table 8 . \n\t', '\n\t\t The success rate for the features used ( Table 10 ) stay the same , except for auxiliary-final VP , which is determined by parse structure , is only half as successful for RASP . \n\t', '\n\t\t Conversely , the heuristic baseline is more successful for RASP , as it relies on POS tags , which is to be expected as RASP has a more detailed tagset . \n\t', '\n\t\t Feature Charniak close-to-punct heuristic baseline auxiliary-final VP RASP close-to-punct heuristic baseline auxiliary-final VP Table 10 : Performance of features on re-parsed Treebank data 5.3 Parsing the BNC Experiments using parsed versions of the BNC corpora ( Table 11 ) show similar results to the original results ( Table 1 ) - except L-BFGS-MaxEnt which scores 4-8 % lower - meaning that the added information from the features mitigates the errors introduced in parsing . \n\t', '\n\t\t The performance of the features ( Table 12 ) remain similar to those for the re- parsed treebank experiments . \n\t', '\n\t\t Feature Charniak close-to-punct heuristic baseline auxiliary-final VP RASP close-to-punct heuristic baseline auxiliary-final VP Table 12 : Performance of features on parsed BNC data 5.4 Combining BNC and Treebank data Combining the re-parsed BNC and Treebank data diversifies and increases the size of the test data , making conclusions drawn empirically more reliable , and the wider range of training data makes it more robust . \n\t', '\n\t\t This gives a training set of 1167 VPE\x92s and a test set of 350 VPE\x92s . \n\t', '\n\t\t The results in Table 13 show little change from the previous experiments . \n\t', '\n\t\t 6 Conclusion and Future work This paper has presented a robust system for VPE detection . \n\t', '\n\t\t The data is automatically tagged and parsed , syntactic features are extracted and machine learning is used to classify instances . \n\t', '\n\t\t Three different machine learning algorithms , Memory Based Learning , GIS-based and L-BFGS-based maximum entropy modeling are used . \n\t', '\n\t\t They give similar results , with L-BFGS-MaxEnt generally giving the highest performance . \n\t', '\n\t\t Two parsers were used , Charniak\x92s and RASP , achieving similar results . \n\t', '\n\t\t To summarise the findings : \x95 Using the BNC , which is tagged with a complex tagging scheme but has no parse data , it is possible to get 76 % F 1 using lexical forms and POS data alone \x95 Using the Treebank , the coarser tagging scheme reduces performance to 67 % . \n\t', '\n\t\t Adding extra features , including sentence- level ones , raises this to 74 % . \n\t', '\n\t\t Adding empty category information gives 88 % , compared to previous results of 48 % \n\t\t']",Negative
"['\n\t\t Next , we will experiment with an algorithm \n\t\t']",Positive
"['\n\t\t Cross-validation experiments will be performed to negate the effects the small test set may cause . \n\t', '\n\t\t As machine learning is used to combine various features , this method can be extended to other forms of ellipsis , and other languages . \n\t', '\n\t\t However , a number of the features used are specific to English VPE , and would have to be adapted to such cases . \n\t', '\n\t\t It is difficult to extrapolate how successful Rec Prec F1 34.00 2.47 4.61 45.33 25.27 32.45 51.33 36.66 42.77 71.05 2.67 5.16 74.34 28.25 40.94 22.36 25.18 23.69 Rec Prec F1 48.00 5.52 9.90 44.00 34.50 38.68 53.00 42.91 47.42 55.32 4.06 7.57 84.77 35.15 49.70 16.24 28.57 20.71 MBL GIS-MaxEnt L-BFGS-MaxEnt Rec Prec F1 Rec Prec F1 Rec Prec F1 Charniak Words + POS 54.00 62.30 57.85 38.66 79.45 52.01 56.66 71.42 63.19 + features 58.00 65.41 61.48 50.66 73.78 60.07 65.33 72.05 68.53 RASP Words + POS 55.92 66.92 60.93 43.42 56.89 49.25 51.63 79.00 62.45 + features 57.23 71.31 63.50 61.84 72.30 66.66 62.74 73.84 67.84 Table 9 : Results on re-parsed data from the Treebank MBL GIS-MaxEnt L-BFGS-MaxEnt Rec Prec F1 Rec Prec F1 Rec Prec F1 Charniak Words + POS 66.50 63.63 65.03 55.00 75.86 63.76 71.00 70.64 70.82 + features 67.50 67.16 67.33 65.00 75.58 69.89 71.00 73.19 72.08 RASP Words + POS 61.92 63.21 62.56 64.46 54.04 58.79 65.34 70.96 68.04 + features 71.06 73.29 72.16 73.09 61.01 66.51 70.29 67.29 68.76 Table 11 : Results on parsed data from the BNC MBL GIS-MaxEnt L-BFGS-MaxEnt Rec Prec F1 Rec Prec F1 Rec Prec F1 Charniak Words + POS 62.28 69.20 65.56 54.28 77.86 63.97 65.14 69.30 67.15 + features 65.71 71.87 68.65 63.71 72.40 67.78 70.85 69.85 70.35 RASP Words + POS 63.61 67.47 65.48 59.31 55.94 57.37 57.46 71.83 63.84 + features 68.48 69.88 69.17 67.61 71.47 69.48 70.14 72.17 71.14 Table 13 : Results on parsed data using the combined dataset such approaches would be based on current work , but it can be expected that they would be feasible , albeit with lower performance . \n\t', '\n\t\t References Eric Brill . \n\t', '\n\t\t 1995. Transformation-based error-driven learning and natural language processing : A case study in part-of-speech tagging . \n\t', '\n\t\t Computational Linguistics , 21(4):543\x96565 . \n\t', '\n\t\t E. Briscoe and J. Carroll . \n\t', '\n\t\t 2002 . \n\t', '\n\t\t Robust accurate statistical annotation of general text . \n\t', '\n\t\t In Proceedings of the 3rd International Conference on Language Resources and Evaluation , Las Palmas , Gran Canaria . \n\t', '\n\t\t Eugene Charniak . \n\t', '\n\t\t 2000. A maximum-entropy-inspired parser . \n\t', '\n\t\t In Meeting of the North American Chapter of the ACL , page 132 . \n\t', '\n\t\t Walter Daelemans , Jakub Zavrel , Ko van der Sloot , and Antal van den Bosch . \n\t', '\n\t\t 2002. Tilburg memory based learner , version 4.3 , reference guide . \n\t', '\n\t\t Downloadable from http://ilk.kub.nl/downloads/pub/papers/ilk0210.ps.gz . \n\t', '\n\t\t Mary Dalrymple , Stuart M. Shieber , and Fernando Pereira . \n\t', '\n\t\t 1991. Ellipsis and higher-order unification . \n\t', '\n\t\t Linguistics and Philosophy , 14:399\x96452 . \n\t', '\n\t\t Robert Fiengo and Robert May . \n\t', '\n\t\t 1994. Indices and Identity . \n\t', '\n\t\t MIT Press , Cambridge , MA . \n\t', '\n\t\t Daniel Hardt . \n\t', '\n\t\t 1993. VP Ellipsis : Form , Meaning , and Processing . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , University of Pennsylvania . \n\t', '\n\t\t Daniel Hardt . \n\t', '\n\t\t 1997. An empirical approach to vp ellipsis . \n\t', '\n\t\t Computational Linguistics , 23(4) . \n\t', '\n\t\t Mark Johnson . \n\t', '\n\t\t 2002. A simple pattern-matching algorithm for recovering empty nodes and their antecedents . \n\t', '\n\t\t In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics . \n\t', '\n\t\t Andrew Kehler and Gregory Ward . \n\t', '\n\t\t 1999. On the semantics and pragmatics of \x91identifier so\x92 . \n\t', '\n\t\t In Ken Turner , editor , The Semantics/Pragmatics Interface from Different Points of View ( Current Research in the Semantics/Pragmatics Interface Series , Volume I ) . \n\t', '\n\t\t Amsterdam : Elsevier . \n\t', '\n\t\t Andrew Kehler . \n\t', '\n\t\t 1993. A discourse copying algorithm for ellipsis and anaphora resolution . \n\t', '\n\t\t In Proceedings of the Sixth Conference of the Euro- pean Chapter ofthe Associationfor Computational Linguistics ( EACL-93 ) , Utrecht , the Netherlands . \n\t', '\n\t\t Torbjorn Lager . \n\t', '\n\t\t 1999. The mu-tbl system : Logic programming tools for transformation-based learning . \n\t', '\n\t\t In Third International Workshop on Computational Natural Language Learning ( CoNLL\x9299 ) . \n\t', '\n\t\t Downloadable from http://www.ling.gu.se/ lager/mutbl.html . \n\t', '\n\t\t Shalom Lappin . \n\t', '\n\t\t 1993. The syntactic basis of ellipsis resolution . \n\t', '\n\t\t In S. Berman and A. Hestvik , editors , Proceedings ofthe Stuttgart Ellipsis Workshop , Arbeitspapiere des Sonderforschungsbereichs 340 , Bericht Nr. 29-1992 . \n\t', '\n\t\t University of Stuttgart , Stuttgart . \n\t', '\n\t\t Shalom Lappin . \n\t', '\n\t\t 1996. The interpretation of ellipsis . \n\t', '\n\t\t In Shalom Lappin , editor , The Handbook of Contemporary Semantic Theory , pages 145\x96175 . \n\t', '\n\t\t Oxford : Blackwell . \n\t', '\n\t\t G. Leech . \n\t', '\n\t\t 1992. 100 million words of english : The British National Corpus . \n\t', '\n\t\t Language Research , 28(1):1\x9613 . \n\t', '\n\t\t Robert Malouf . \n\t', '\n\t\t 2002. A comparison of algorithms for maximum entropy parameter estimation . \n\t', '\n\t\t In Proceedings of the Sixth Conference on Natural Language Learning ( CoNLL-2002 ) , pages 49\x9655 . \n\t', '\n\t\t M. Marcus , G. Kim , M. Marcinkiewicz , R. MacIntyre , M. Bies , M. Ferguson , K. Katz , and B. Schasberger . \n\t', '\n\t\t 1994. The Penn Treebank : Annotating predicate argument structure . \n\t', '\n\t\t In Proceedings of the Human Language Technology Workshop . \n\t', '\n\t\t Morgan Kaufmann , San Francisco . \n\t', '\n\t\t Leif Arda Nielsen . \n\t', '\n\t\t 2003a . \n\t', '\n\t\t A corpus-based study of verb phrase ellipsis . \n\t', '\n\t\t In Proceedings of the 6th Annual CLUKResearch Colloquium . \n\t', '\n\t\t Leif Arda Nielsen . \n\t', '\n\t\t 2003b . \n\t', '\n\t\t Using machine learning techniques for VPE detection . \n\t', '\n\t\t In Proceedings ofRANLP . \n\t', '\n\t\t R. Quinlan . \n\t', '\n\t\t 1993. C4.5 : Programs for Machine Learning . \n\t', '\n\t\t San Mateo , CA : Morgan Kaufmann. Adwait Ratnaparkhi . \n\t', '\n\t\t 1998. Maximum Entropy Models for Natural Language Ambiguity Resolution . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , University of Pennsylvania . \n\t', '\n\t\t Stuart Shieber , Fernando Pereira , and Mary Dalrymple . \n\t', '\n\t\t 1996. Interactions of scope and ellipsis . \n\t', '\n\t\t Linguistics and Philosophy , 19(5):527\x96552 . \n\t', '\n\t\t A Machine Learning Approach to German Pronoun Resolution Beata Kouchnir Department of Computational Linguistics T¨ubingen University 72074 T¨ubingen , Germany kouchnir@sfs.uni-tuebingen.de Abstract This paper presents a novel ensemble learning approach to resolving German pronouns . \n\t', '\n\t\t Boosting , the method in question , combines the moderately accurate hypotheses of several classifiers to form a highly accurate one . \n\t', '\n\t\t Experiments show that this approach is superior to a single decision-tree classifier . \n\t', '\n\t\t Furthermore , we present a standalone system that resolves pronouns in unannotated text by using a fully automatic sequence of preprocessing modules that mimics the manual annotation process . \n\t', '\n\t\t Although the system performs well within a limited textual domain , further research is needed to make it effective for open-domain question answering and text summarisation . \n\t', '\n\t\t 1 Introduction Automatic coreference resolution , pronominal and otherwise , has been a popular research area in Natural Language Processing for more than two decades , with extensive documentation of both the rule-based and the machine learning approach . \n\t', '\n\t\t For the latter , good results have been achieved with large feature sets ( including syntactic , semantic , grammatical and morphological information ) derived from handannotated corpora . \n\t', '\n\t\t However , for applications that work with plain text ( e.g. question answering , text summarisation ) , this approach is not practical . \n\t', '\n\t\t The system presented in this paper resolves German pronouns in free text by imitating the manual annotation process with off-the-shelf language sofware . \n\t', '\n\t\t As the avalability and reliability of such software is limited , the system can use only a small number of features . \n\t', '\n\t\t The fact that most German pronouns are morphologically ambiguous proves an additional challenge . \n\t', '\n\t\t The choice of boosting as the underlying machine learning algorithm is motivated both by its theoretical concept as well as its performance for other NLP tasks . \n\t', '\n\t\t The fact that boosting uses the method of ensemble learning , i.e. combining the decisions of several classifiers , suggests that the combined hypothesis will be more accurate than one learned by a single classifier . \n\t', '\n\t\t On the practical side , boosting has distinguished itself by achieving good results with small feature sets . \n\t', '\n\t\t 2 Related Work Although extensive research has been conducted on statistical anaphora resolution , the bulk of the work has concentrated on the English language . \n\t', '\n\t\t Nevertheless , comparing different strategies helped shape the system described in this paper . \n\t', '\n\t\t \n\t\t']",Positive
['\n\t\t RESOLVE was trained on data from MUC-5 English Joint Venture ( EJV ) corpus and used the C4.5 decision tree algorithm \n\t\t'],Positive
"['\n\t\t The system achieved an F-measure of 86.5 for full coreference resolution ( no values were given for pronouns ) . \n\t', '\n\t\t Although a number this high must be attributed to the specific textual domain , RESOLVE also outperformed the authors\x92 rule-based algorithm by 7.6 percentage points , which encouraged further reseach in this direction . \n\t', '\n\t\t Unlike the other systems presented in this section , \n\t\t']",Positive
"['\n\t\t The model is trained on a subset of the Wall Street Journal , comprising 21 million tokens . \n\t', '\n\t\t The reported F-measure for pronoun resolution is 81.5 . \n\t', '\n\t\t However , \n\t\t']",Negative
['\n\t\t \n\t\t'],Positive
"['\n\t\t Their system was trained on both the MUC-6 and the MUC-7 datasets , for which it achieved F-scores of 62.6 and 60.4 , respectively . \n\t', '\n\t\t Although these results are far worse than the ones reported in \n\t\t']",Positive
['\n\t\t As \n\t\t'],Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t However , since using this many features proved to be detrimental to performance , all features that induced low precision rules were discarded , leaving only 19 . \n\t', '\n\t\t The final system outperformed that of \n\t\t']",Negative
"['\n\t\t For pronouns , the reported results are 74.6 and 57.8 , respectively . \n\t', '\n\t\t The experiment presented in \n\t\t']",Positive
"['\n\t\t The research is based on the Heidelberg Text Corpus ( see Section 4 ) , which makes it ideal for comparison with our system . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t The results for personal and possessive pronouns are 82.79 and 84.94 , respectively . \n\t', '\n\t\t 3 Boosting All of the systems described in the previous section use a single classifier to resolve coreference . \n\t', '\n\t\t Our intuition , however , is that a combination of classifiers is better suited for this task . \n\t', '\n\t\t The concept of ensemble learning \n\t\t']",Positive
['\n\t\t One of the most popular ensemble learning methods is boosting \n\t\t'],Positive
"['\n\t\t It is based on the observation that finding many weak hypotheses is easier than finding one strong hypothesis . \n\t', '\n\t\t This is achieved by running a base learning algorithm over several iterations . \n\t', '\n\t\t Initially , an importance weight is distributed uniformly among the training examples . \n\t', '\n\t\t After each iteration , the weight is redistributed , so that misclassified examples get higher weights . \n\t', '\n\t\t The base learner is thus forced to concentrate on difficult examples . \n\t', '\n\t\t Although boosting has not yet been applied to coreference resolution , it has outperformed stateof-the-art systems for NLP tasks such as partofspeech tagging and prepositional phrase attachment \n\t\t']",Positive
['\n\t\t The implementation used for this project is BoosTexter \n\t\t'],Positive
"['\n\t\t In addition to labels , BoosTexter assigns confidence weights that reflect the reliability of the decisions . \n\t', '\n\t\t 4 System Description Our system resolves pronouns in three stages : preprocessing , classification , and postprocessing . \n\t', '\n\t\t Figure 1 gives an overview of the system architecture , while this section provides details of each component . \n\t', '\n\t\t 4.1 Training and Test Data The system was trained with data from the Heidelberg Text Corpus ( HTC ) , provided by the European Media Laboratory in Heidelberg , Germany . \n\t', '\n\t\t Figure 1 : System Architecture The HTC is a collection of 250 short texts ( 30-700 tokens ) describing architecture , historical events and people associated with the city of Heidelberg . \n\t', '\n\t\t To examine its domain (in)dependence , the system was tested on 40 unseen HTC texts as well as on 25 articles from the Spiegel magazine , the topics of which include current events , science , arts and entertainment , and travel . \n\t', '\n\t\t 4.2 The MMAX Annotation Tool The manual annotation of the training data was done with the MMAX ( Multi-Modal Annotation in XML ) annotation tool ( M¨uller and Strube , 2001 ) . \n\t', '\n\t\t The fist step of coreference annotation is to identify the markables , i.e. noun phrases that refer to real-word entities . \n\t', '\n\t\t Each markable is annotated with the following attributes : np form : proper noun , definite NP , indefinite NP , personal pronoun , possessive pronoun , or demonstrative pronoun . \n\t', '\n\t\t person , number and gender . \n\t', '\n\t\t The possible values are 1s , 1p , 2s , 2p , 3m , 3f , 3n , 3p . \n\t', '\n\t\t semantic class : human , physical object ( includes animals ) , or abstract . \n\t', '\n\t\t When the semantic class is ambiguous , the \x94abstract\x94 option is chosen . \n\t', '\n\t\t type : if the entity that the markable refers to is new to the discourse , the value is \x94none\x94 . \n\t', '\n\t\t If the markable refers to an already mentioned entity , the value is \x94anaphoric\x94 . \n\t', '\n\t\t An anaphoric markable has another attribute for its relation to the antecedent . \n\t', '\n\t\t The values for this attribute are \x94direct\x94 , \x94pronominal\x94 , and \x94ISA\x94 ( hyponym-hyperonym ) . \n\t', '\n\t\t To mark coreference , MMAX uses coreference sets , such that every new reference to an already mentioned entity is added to the set of that entity . \n\t', '\n\t\t Implicitly , there is a set for every entity in the discourse - if an entity occurs only once , its set contains one markable . \n\t', '\n\t\t grammatical role : subject , object ( direct or indirect ) , or other . \n\t', '\n\t\t 4.3 Feature Vector agreement : this attribute is a combination of The features used by our system are summarised in Table 4.3 . \n\t', '\n\t\t The individual features for anaphor Feature Description pron the pronoun ana npform NP form of the anaphor ana gramrole grammatical role of the anaphor ana agr agreement of the anaphor ana semclass* semantic class of the anaphor ante npform NP form of the antecedent ante gramrole grammatical role of the antecedent ante agr agreement of the antecedent ante semclass* semantic class of the antecedent dist distance in markables between anaphor and antecedent ( 1 .. 20 ) same agr same agreement of anaphor and antecedent ? \n\t', '\n\t\t same gramrole same grammatical role of anaphor and antecedent ? \n\t', '\n\t\t same semclass* same semantic class of anaphor and antecedent ? \n\t', '\n\t\t Table 1 : Features used by our system . \n\t', '\n\t\t *-ed features were only used for 10-fold cross-validation on the manually annotated data and antecedent - NP form , grammatical role , semantic class - are extracted directly from the annotation . \n\t', '\n\t\t The relational features are generated by comparing the individual ones . \n\t', '\n\t\t The binary target function - coreferent , non-coreferent - is determined by comparing the values of the member attribute . \n\t', '\n\t\t If both markables are members of the same set , they are coreferent , otherwise they are not . \n\t', '\n\t\t Due to lack of resources , the semantic class attribute cannot be annotated automatically , and is therefore used only for comparison with \n\t\t']",Positive
"['\n\t\t 4.4 Noun Phrase Chunking , NER and POS-Tagging To identify markables automatically , the system uses the noun phrase chunker described in \n\t\t']",Positive
"['\n\t\t The chunker is based on a head-lexicalised probabilistic context free grammar ( H-L PCFG ) and achieves an F-measure of 92 for range only and 83 for range and label , whereby a range of a noun chunk is defined as \x94all words from the beginning of the noun phrase to the head noun\x94 . \n\t', '\n\t\t This is different from manually annotated markables , which can be complex noun phrases . \n\t', '\n\t\t Despite good overall performance , the chunker fails on multi-word proper names in which case it marks each word as an individual chunk . \n\t', ""\n\t\t ' Since many pronouns refer to named entities , the chunker needs to be supplemented by a named entity recogniser . \n\t"", '\n\t\t Although , to our knowledge , there currently does not exist an off-the-shelf named entity recogniser for German , we were able to obtain the system submitted by \n\t\t']",Positive
"['\n\t\t In order to run the recogniser , the data needs to be tokenised , tagged and lemmatised , all of which is done by the TreeTagger \n\t\t']",Positive
"['\n\t\t 4.5 Markable Creation After the markables are identified , they are automatically annotated with the attributes described in Section 4.4 . \n\t', '\n\t\t The NP form can be reliably determined by examining the output of the noun chunker and the named entity recogniser . \n\t', '\n\t\t Pronouns and named entities are already labeled during chunking . \n\t', '\n\t\t The remaining markables are labelled as definite NPs if their first words are definite articles or possessive determiners , and as indefinite NPs otherwise . \n\t', '\n\t\t Grammatical role is determined by the case assigned to the markable - subject if nominative , object if accusative . \n\t', '\n\t\t Although datives and genitives can also be objects , they are more likely to be adjuncts and are therefore assigned the value \x94other\x94 . \n\t', '\n\t\t For non-pronominal markables , agreement is determined by lexicon lookup of the head nouns . \n\t', '\n\t\t Number ambiguities are resolved with the help of the case information . \n\t', '\n\t\t Most proper names , except for a few common ones , do not appear in the lexicon and have to remain ambiguous . \n\t', ""\n\t\t Although it is impossible to fully resolve the agreement ambiguities of pronominal markables , they can be classi- ' An example is [ Verteidigunsminister Donald ] [ Rumsfeld ] ( [ Minister of Defense Donald ] [ Rumsfeld ] ) . \n\t"", '\n\t\t fied as either feminine/plural or masculine/neuter . \n\t', '\n\t\t Therefore we added two underspecified values to the agreement attribute : 3f 3p and 3m 3n . \n\t', '\n\t\t Each of these values was made to agree with both of its subvalues . \n\t', '\n\t\t 4.6 Antecedent Selection After classification , one non-pronominal antecedent has to be found for each pronoun . \n\t', '\n\t\t As BoosTexter assigns confidence weights to its predictions , we have a choice between selecting the antecedent closest to the anaphor ( closest-first ) and the one with the highest weight ( best-first ) . \n\t', '\n\t\t Furthermore , we have a choice between ignoring pronominal antecedents ( and risking to discard all the correct antecedents within the window ) and resolving them ( and risking multiplication of errors ) . \n\t', '\n\t\t In case all of the instances within the window have been classified as non-coreferent , we choose the negative instance with the lowest weight as the antecedent . \n\t', '\n\t\t The following section presents the results for each of the selection strategies . \n\t', '\n\t\t 5 Evaluation Before evaluating the actual system , we compared the performance of boosting to that of C4.5 , as reported in \n\t\t']",Negative
"['\n\t\t Trained on the same corpus and evaluated with the 10-fold crossvalidation method , boosting significantly outperforms C4.5 on both personal and possessive pronouns ( see Table 2 ) . \n\t', '\n\t\t These results support the intuition that ensemble methods are superior to single classifiers . \n\t', '\n\t\t To put the performance of our system into perspective , we established a baseline and an upper bound for the task . \n\t', '\n\t\t The baseline chooses as the antecedent the closest non-pronominal markable that agrees in number and gender with the pronoun . \n\t', '\n\t\t The upper bound is the system\x92s performance on the manually annotated ( gold standard ) data without the semantic features . \n\t', '\n\t\t For the baseline , accuracy is significantly higher for the gold standard data than for the two test sets ( see Table 3 ) . \n\t', '\n\t\t This shows that agreement is the most important feature , which , if annotated correctly , resolves almost half of the pronouns . \n\t', '\n\t\t The classification results of the gold standard data , which are much lower than the ones in Table 2 also PPER PPOS \n\t\t']",Negative
"['\n\t\t As for the test sets , while the classifier significantly outperformed the baseline for the HTC set , it did nothing for the Spiegel set . \n\t', '\n\t\t This shows the limitations of an algorithm trained on overly restricted data . \n\t', '\n\t\t Among the selection heuristics , the approach of resolving pronominal antecedents proved consistently more effective than ignoring them , while the results for the closest-first and best-first strategies were mixed . \n\t', '\n\t\t They imply , however , that the bestfirst approach should be chosen if the classifier performed above a certain threshold ; otherwise the closest-first approach is safer . \n\t', '\n\t\t Overall , the fact that 67.2 of the pronouns were correctly resolved in the automatically annotated HTC test set , while the upper bound is 82.0 , validates the approach taken for this system . \n\t', '\n\t\t 6 Conclusion and Future Work The pronoun resolution system presented in this paper performs well for unannotated text of a limited domain . \n\t', '\n\t\t While the results are encouraging considering the knowledge-poor approach , experiments with a more complex textual domain show that the system is unsuitable for wide-coverage tasks such as question answering and summarisation . \n\t', '\n\t\t To examine whether the system would yield comparable results in unrestricted text , it needs to be trained on a more diverse and possibly larger corpus . \n\t', '\n\t\t For this purpose , T¨uba-D/Z , a treebank consisting of German newswire text , is presently being annotated with coreference information . \n\t', '\n\t\t As the syntactic annotation of the treebank is richer than that of the HTC corpus , additional features may be derived from it . \n\t', '\n\t\t Experiments with T¨ubaD/Z will show whether the performance achieved for the HTC test set is scalable . \n\t', '\n\t\t For future versions of the system , it might also HTC-Gold HTC-Test Spiegel Baseline accuracy 46.7 % 30.9 % 31.1 % Classification F score 77.9 62.8 30.4 Best-first , ignoring pronominal ant . \n\t', '\n\t\t 82.0 % 67.2 % 28.3 % Best-first , resolving pronominal ant . \n\t', '\n\t\t 72.2 % 49.1 % 21.7 % Closest-first , ignoring pronominal ant . \n\t', '\n\t\t 82.0 % 57.3 % 34.4 % Closest-first , resolving pronominal ant . \n\t', '\n\t\t 72.2 % 49.1 % 22.8 % Table 3 : Accuracy of the different selection heuristics compared with baseline accuracy and classification F-score . \n\t', '\n\t\t HTC-Gold and HTC-Test stand for manually and automatically annotated test sets , respectively . \n\t', '\n\t\t be beneficial to use full parses instead of chunks . \n\t', '\n\t\t As most German verbs are morphologically unambiguous , an analysis of them could help disambiguate pronouns . \n\t', '\n\t\t However , due to the relatively free word order of the German language , this approach requires extensive reseach . \n\t', '\n\t\t References Steven Abney , Robert E. Schapire , and Yoram Singer . \n\t', '\n\t\t 1999. Boosting applied to tagging and PP attachment . \n\t', '\n\t\t In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora . \n\t', '\n\t\t Xavier Carreras , Lluis M`arquez , and Lluis Padr´o . \n\t', '\n\t\t 2002. Named entity extraction using AdaBoost . \n\t', '\n\t\t In Proceedings of CoNLL-2002 , pages 167\x96170 , Taipei , Taiwan . \n\t', '\n\t\t James R. Curran and Stephen Clark . \n\t', '\n\t\t 2003. Language- independent NER using a maximum entropy tagger . \n\t', '\n\t\t In Proceedings of CoNLL-2003 , pages 164\x96167 , Edmonton , Canada . \n\t', '\n\t\t Thomas G. Dietterich . \n\t', '\n\t\t 2000. Ensemble methods in machine learning . \n\t', '\n\t\t In First International Workshop on Multiple Classifier Systems , Lecture Notes in Computer Science , pages 1\x9615 . \n\t', '\n\t\t Springer , New York . \n\t', '\n\t\t Gerard Escudero , Lluis M`arquez , and German Rigau . \n\t', '\n\t\t 2000. Boosting applied to word sense disambiguation . \n\t', '\n\t\t In Proceedings of the 12th European Conference on Machine Learning , pages 129\x96141 . \n\t', '\n\t\t Joseph F. McCarthy and Wendy G. Lehnert . \n\t', '\n\t\t 1995. Using decision trees for coreference resolution . \n\t', '\n\t\t In Proceedings of the 14th International Joint Conference on Artificial Intelligence ( IJCAI\x9295 ) , pages 1050\x96 1055 , Montreal , Canada . \n\t', '\n\t\t Thomas S. Morton . \n\t', '\n\t\t 2000. Coreference for nlp applications . \n\t', '\n\t\t In Proceedings of the 38th Annual Meeting of the Associationfor Computational Linguistics ( ACL\x9200 ) , Hong Kong . \n\t', '\n\t\t Christoph M¨uller and Michael Strube . \n\t', '\n\t\t 2001. Annotating anaphoric and bridging relations with MMAX . \n\t', '\n\t\t In Proceedings ofthe 2nd SIGdial Workshop on Discourse and Dialogue , pages 90\x9695 , Aalborg , Denmark . \n\t', '\n\t\t Vincent Ng and Claire Cardie . \n\t', '\n\t\t 2002. Improving machine learning approaches to coreference resolution . \n\t', '\n\t\t In Proceedings ofthe 40th Annual Meeting ofthe Association for Computational Linguistics ( ACL\x9202 ) , pages 104\x96111 , Philadelphia , PA , USA . \n\t', '\n\t\t J. Ross Quinlan . \n\t', '\n\t\t 1993. C4.5 : Programs for Machine Learning . \n\t', '\n\t\t Morgan Kaufman , San Mateo , CA . \n\t', '\n\t\t Robert E. Schapire and Yoram Singer . \n\t', '\n\t\t 2000. Boostexter : A boosting-based system for text categorization . \n\t', '\n\t\t Machine Learning , 39(2/3):135\x96168 . \n\t', '\n\t\t Robert E. Schapire . \n\t', '\n\t\t 2002. The boosting approach to machine learning : an overview . \n\t', '\n\t\t In Proceedings of the MSRI Workshop on Nonlinear Estimation and Classification . \n\t', '\n\t\t Helmut Schmid and Sabine Schulte im Walde . \n\t', '\n\t\t 2000 . \n\t', '\n\t\t Robust German noun chunking with a probabilistic context-free grammar . \n\t', '\n\t\t In Proceedings of the 18th International Conference on Computational Linguistics ( COLING-00 ) , pages 726\x96732 , Saarbr¨ucken , Germany . \n\t', '\n\t\t Helmut Schmid . \n\t', '\n\t\t 1995. Improvements in part-ofspeech tagging with an application to German . \n\t', '\n\t\t In Proceedings of the ACL SIGDAT-Workshop . \n\t', '\n\t\t Wee Meng Soon , Hwee Tou Ng , and Daniel Chung Yong Lim . \n\t', '\n\t\t 2001. A machine learning approach to coreference resolution of noun phrases . \n\t', '\n\t\t Computational Linguistics , 27(4):521\x96544 . \n\t', '\n\t\t Michael Strube , Stefan Rapp , and Christoph M¨uller . \n\t', '\n\t\t 2002. The influence of minimum edit distance on reference resolution . \n\t', '\n\t\t In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing ( EMNLP\x9202 ) , pages 312\x96319 , Philadelphia , PA , USA . \n\t', '\n\t\t Beyond N in N-gram Tagging Robbert Prins Alfa-Informatica University of Groningen P.O. Box 716 , NL-9700 AS Groningen The Netherlands r.p.prins@let.rug.nl Abstract The Hidden Markov Model ( HMM ) for part-of-speech ( POS ) tagging is typically based on tag trigrams . \n\t', '\n\t\t As such it models local context but not global context , leaving long-distance syntactic relations unrepresented . \n\t', '\n\t\t Using n-gram models for n > 3 in order to incorporate global context is problematic as the tag sequences corresponding to higher order models will become increasingly rare in training data , leading to incorrect estimations of their probabilities . \n\t', '\n\t\t The trigram HMM can be extended with global contextual information , without making the model infeasible , by incorporating the context separately from the POS tags . \n\t', '\n\t\t The new information incorporated in the model is acquired through the use of a wide-coverage parser . \n\t', '\n\t\t The model is trained and tested on Dutch text from two different sources , showing an increase in tagging accuracy compared to tagging using the standard model . \n\t', '\n\t\t 1 Introduction The Hidden Markov Model ( HMM ) used for partof-speech ( POS ) tagging is usually a second-order model , using tag trigrams , implementing the idea that a limited number of preceding tags provide a considerable amount of information on the identity of the current tag . \n\t', '\n\t\t This approach leads to good results . \n\t', '\n\t\t For example , the TnT trigram HMM tagger achieves state-of-the-art tagging accuracies on English and German \n\t\t']",Positive
"['\n\t\t In general , however , as the model does not consider global context , mistakes are made that concern long-distance syntactic relations . \n\t', '\n\t\t 2 A restriction of HMM tagging The simplifying assumption , which is the basis for HMM tagging , that the context of a given tag can be fully represented by just the previous two tags , leads to tagging errors where syntactic features that fall outside of this range , and that are needed for determining the identity of the tag at hand , are ignored . \n\t', '\n\t\t One such error in tagging Dutch is related to finiteness of verbs . \n\t', '\n\t\t This is discussed in the next paragraph and will be used in explaining the proposed approach . \n\t', '\n\t\t Other possible applications of the technique include assignment of case in German , and assignment of chunk tags in addition to partof-speech tags . \n\t', '\n\t\t These will be briefly discussed at the end of this paper . \n\t', '\n\t\t 2.1 An example from Dutch In experiments on tagging Dutch text performed in the context of \n\t\t']",Positive
"['\n\t\t In Dutch , the plural finite form of a verb is similar in appearance to the infinitive form of the verb . \n\t', '\n\t\t In example ( 1-a ) the second verb in the sentence , vliegen , is correctly tagged as an infinitive , but in example ( 1-b ) the added adverb creates a surrounding in which the tagger incorrectly labels the verb as the finite plural form . \n\t', '\n\t\t ( 1 ) a. Jan zag\x96past sg vogels vliegen\x96inf Jan saw birds fly b. *Jan zag\x96past sg vogels vliegen\x96pl Jan saw birds fly gisteren yesterday Since a clause normally contains precisely one finite verb , this mistake could be avoided by remembering whether the finite verb for the current clause has already occurred , and using this information in classifying a newly observed verb as either finite or nonfinite . \n\t', '\n\t\t The trigram tagger has normally \x93forgotten\x94 about any finite verb upon reaching a second verb , and is led into a mistake by other parts of the context even if the two verbs are close to each other . \n\t', '\n\t\t Basing the model on n-grams bigger than trigrams is not a solution as the n-grams would often not occur in the training data , making the associated probabilities hard to estimate . \n\t', '\n\t\t 3 Extending the model Instead of considering longer n-grams , the model can be extended with specific long-distance context information . \n\t', '\n\t\t Analogous to how sequences of tags can be modeled as a probabilistic network of events , modeling the probability of a tag given a number of preceding tags , in the same way we can model the syntactic context . \n\t', '\n\t\t For the example problem presented in section 2 . \n\t', '\n\t\t 1 , this network would consist of two states : pre and post . \n\t', '\n\t\t In state pre the finite verb for the current clause has not yet been seen , while in state post is has . \n\t', '\n\t\t In general , the context feature C with values C1 ... j and its probability distribution is to be incorporated in the model . \n\t', '\n\t\t In describing how the extra context information is added to the HMM , we will first look at how the standard model for POS tagging is constructed . \n\t', '\n\t\t Then the probability distribution on which the new model is based is introduced . \n\t', '\n\t\t A distinction is made between a naive approach where the extra context is added to the model by extending the tagset , and a method where the context is added separately from the tags which results in a much smaller increase in the number of probabilities to be estimated from the training data . \n\t', '\n\t\t 3.1 Standard model In the standard second order HMM used for POS tagging ( as described for example in chapter 10.2 of ( Manning and Sch¨utze , 1999 ) ) , a single state corresponds to two POS tags , and the observed symbols are words . \n\t', '\n\t\t The transitions between states are governed by probabilities that combine the probabilities for state transitions ( tag sequences ti-2 , ti-1 , ti ) and output of observed symbols ( words wi ) : P(ti , wiI ti-2 , ti-1 ) This probability distribution over tags and words is factorized into two separate distributions , using the chain rule P(A,BIC) = P(AIC)-P(BIC,A) : P(ti , wi I ti-2 , ti-1 ) = P(ti I ti-2 , ti-1 ) - P(wi I ti-2 , ti-1 , ti ) Finally , the POS tagging assumption that the word only depends on the current tag is applied : P(ti , wi I ti-2 , ti-1 ) N P(ti I ti-2 , ti-1 ) - P(wi I ti ) If T is the size of the tagset , w the size of the vocabulary , and n the length of the tag n-grams used , then the number of parameters in this standard model is Tn + Tw. 3.2 Extended model As a starting point in adding the extra feature to the model , the same probability distribution used as a basis for the standard model is used : P(ti , wiI ti-2 , ti-1 ) Naive method : extending the tagset . \n\t', '\n\t\t The contextual information C with j possible values could be added to the model by extending the set of tags , so that every tag t in the tagset is replaced by a set of tags 1tc1 , tc2 , ... , tcj } . \n\t', '\n\t\t If T is the size of the original tagset , then the number of parameters in this extended model would be Tn jn + Tjw , the number of tag n-grams being multiplied by eight in our example . \n\t', '\n\t\t In experiments this increase in the number of parameters led to less accurate probability estimates . \n\t', '\n\t\t C(t1t2) C(t1t2)+cXD(t1t2) ( 0 A3 = Sl if C(t1t2) = 0 if C(t1t2) > 0 Better method : adding context to states as a separate feature . \n\t', '\n\t\t In order to avoid the problem associated with the naive method , the context feature is added to the states of the model separately from the tags . \n\t', '\n\t\t This way it is possible to combine probabilities from the different distributions in an appropriate manner , restricting the increase in the number of parameters . \n\t', '\n\t\t For example , it is now stated that as far as the context feature is concerned , the model is first order . \n\t', '\n\t\t The probabilities associated with state transitions are defined as follows , where ci is the value of the new context feature at position i : P(ti , wi , cilti-2 , ti-1 , ci-1 ) As before , the probability distribution is factorized into separate distributions : P(ti , wi , ci l ti-2 , ti-1 , ci-1 ) = P(ti l ti-2 , ti-1 , ci-1 ) . \n\t', '\n\t\t P(ci lti-2 , ti-1 , ci-1 , ti ) . \n\t', '\n\t\t P(wi lti-2 , ti-1 , ci-1 , ti , ci ) The assumption made in the standard POS tagging model that words only depend on the corresponding tag is applied , as well as the assumption that the current context value only depends on the current tag and the previous context value : P(ti , wi , ci l ti-2 , ti-1 , ci-1 ) ~ P(ti l ti-2 , ti-1 , ci-1 ) . \n\t', '\n\t\t P(ci lci-1 , ti ) . \n\t', '\n\t\t P(wi l ti ) The total numbers of parameters for this model is 7-nj+7-j2+7-~ . \n\t', '\n\t\t In the case of the example problem this means the number of tag n-grams is multiplied by two . \n\t', '\n\t\t The experiments described in section 5 will make use of this model . \n\t', '\n\t\t 3.3 Training the model The model\x92s probabilities are estimated from annotated training data . \n\t', '\n\t\t Since the model is extended with global context , this has to be part of the annotation . \n\t', '\n\t\t The Alpino wide-coverage parser for Dutch \n\t\t']",Positive
"['\n\t\t For the example concerning finite plural verbs and infinitives , this means the parser labels every word in the sentence with one of the two possible context values . \n\t', '\n\t\t When the parser encounters a root clause ( including imperative clauses and questions ) or a subordinate clause ( including relative clauses ) , it assigns the context value pre . \n\t', '\n\t\t When a finite verb is encountered , the value post is assigned . \n\t', '\n\t\t Past the end of a root clause or subordinate clause the context is reset to the value used before the embedded clause began . \n\t', '\n\t\t In all other cases , the value assigned to the previous position is continued . \n\t', '\n\t\t From the text annotated with POS tags and context labels the n-gram probabilities and lexical probabilities needed by the model are estimated based on the frequencies of the corresponding sequences . \n\t', '\n\t\t 4 The tagger 4.1 Tagging method The trigram HMM tagger used in the experiments of section 5 computes the a posteriori probability for every tag . \n\t', '\n\t\t This value is composed of the forward and backward probability of the tag at hand as defined in the forward-backward algorithm for HMM-training . \n\t', '\n\t\t This idea is also described in \n\t\t']",Positive
"['\n\t\t The trigram data is combined with bigram and unigram data through linear interpolation to reduce the problem of sparse data . \n\t', '\n\t\t 4.1.1 Smoothing Applying the method known as linear interpolation , probabilities of unigrams , bigrams and trigrams are combined in a weighted sum using weights A1 , A2 and A3 respectively . \n\t', '\n\t\t The weights are computed for every individual case using the notion of n-gram diversity \n\t\t']",Positive
"['\n\t\t The diversity of an n-gram is the number of different tags that appear in the position following this n-gram in the training data . \n\t', '\n\t\t The weight A3 assigned to the trigram t1t2t3 is computed on the basis of the diversity and frequency of the prefix bigram W2 , t2 , using the following equation , where c regulates the importance of diversity ( c = 6 was used in the experiments described below ) , and C(x) and D(x) are respectively the count and diversity of x : The bigram weight A2 is computed as a fraction of 1 \x97 A3 using the bigram version of the above equation . \n\t', '\n\t\t The remaining weight 1 \x97 A3 \x97 A2 is used as the unigram weight A1 . \n\t', '\n\t\t 4.1.2 Unknown words The tagger uses a lexicon that has been created from the training data to assign an initial set of possible tags to every word . \n\t', '\n\t\t Words that were not seen during training are not in the lexicon , so that another method has to be used to assign initial tags to these words . \n\t', '\n\t\t A technique described and implemented by Jan Daciuk \n\t\t']",Positive
"['\n\t\t 5 Tagging experiment 5.1 Experiment setup 5.1.1 Method An extended model was created featuring context information on the occurrence of the finite verb form . \n\t', '\n\t\t The tagger is used to tag a set of sentences , assigning one tag to each word , first using the standard model and then using the extended model . \n\t', '\n\t\t The results are compared in terms of tagging accuracy . \n\t', '\n\t\t The experiment is conducted twice with different data sets used for both training and testing . \n\t', '\n\t\t 5.1.2 Data The first set consists of a large amount of Dutch newspaper text that was annotated with syntactical tags by the Alpino parser . \n\t', '\n\t\t This is referred to as the \x93Alpino\x94 data . \n\t', '\n\t\t The second and much smaller set of data is the Eindhoven corpus tagged with the Wotan tagset \n\t\t']",Positive
"['\n\t\t This data set was also used in ( van Halteren et al. , 2001 ) , therefore the second experiment will allow for a comparison of the results with previous work on tagging Dutch . \n\t', '\n\t\t This data will be referred to as the \x93Wotan\x94 data . \n\t', '\n\t\t For both sets the contextual information concerning finite verbs is added to the training data by the Alpino parser as described in section 3.3 . \n\t', '\n\t\t Due to memory restrictions , the parser was not able to parse 265 of the 36K sentences of Wotan training data . \n\t', '\n\t\t These sentences received no contextual labels and thus not all of the training data used in ( van Halteren et al. , 2001 ) could be used in the Wotan experiment . \n\t', '\n\t\t Training data for the Alpino experiment is four years of daily newspaper text , amounting to about 2M sentences ( 25M words ) . \n\t', '\n\t\t Test data is a collection of 3686 sentences ( 59K words ) from the Parool newspaper . \n\t', '\n\t\t The data is annotated with a tagset consisting of 2825 tags . \n\t', '\n\t\t ( The large size of the Alpino tagset is mainly due to a large number of infrequent tags representing specific uses of prepositions . \n\t', '\n\t\t ) In the Wotan experiment , 36K sentences ( 628K words ) are used for training ( compared to 640K words in ( van Halteren et al. , 2001 ) ) , and 4176 sentences ( 72K words ) are used for testing . \n\t', '\n\t\t The Wotan data is annotated with a tagset consisting of 345 tags ( although a number of 341 is reported in ( van Halteren et al. , 2001 ) ) . \n\t', '\n\t\t 5.1.3 Baseline method As a baseline method every word is assigned the tag it was most often seen with in the training data . \n\t', '\n\t\t Thus the baseline method is to tag each word w with a tag t such that P(tlw) is maximized . \n\t', '\n\t\t Unknown words are represented by all words that occurred only once . \n\t', '\n\t\t The baseline accuracies are 85.9 % on the Alpino data and 84.3 % on the Wotan data . \n\t', '\n\t\t 5.2 Results 5.2.1 \x93Alpino\x94 experiment The results on the Alpino data are shown in table 1 . \n\t', '\n\t\t Using the standard model , accuracy is 93.34 % ( 3946 mistakes ) . \n\t', '\n\t\t Using the extended model , accuracy is 93.62 % ( 3779 mistakes ) . \n\t', '\n\t\t This amounts to an overall error reduction of 4.23 % . \n\t', '\n\t\t In table 2 and 3 the 6 most frequent tagging errors are listed for tagging using the standard and extended model respectively . \n\t', '\n\t\t Mistakes where verb ( p l ) is mixed up with verb ( in f ) sum up to 241 instances ( 6.11 % of all mistakes ) when using the standard model , as opposed to 82 cases ( 2.17 % ) using the extended model , an error reduction of 65.98 % . \n\t', '\n\t\t 5.2.2 \x93Wotan\x94 experiment The results on the Wotan data can be seen in table 4 . \n\t', '\n\t\t Using the standard model , accuracy is 92.05 % ( 5715 mistakes ) . \n\t', '\n\t\t This result is very simi- baseline accuracy 85.9 % model standard extended bigram accuracy 92.49 % 92.94 % trigram accuracy 93.34 % 93.62 % errors 3946 3779 error reduction 167 = 4.23 % pl/inf errors 241 ( 6.11 % ) 82 ( 2.17 % ) pl/inf error red . \n\t', '\n\t\t 159 = 65.98 % Table 1 : Tagging results on Alpino data freq assigned correct 159 verb(int) verb(pl) 82 verb(pl) verb(int) 68 proper name(both) 1-proper name(both) 57 proper name(both) noun(de,sg) 53 verb(psp) adjective(no e,adv ) 45 proper name(both) 2-proper name(both) Table 2 : Most frequent tagging mistakes on Alpino data , using standard model lar to the 92.06 % reported by Van Halteren , Zavrel and Daelemans in ( van Halteren et al. , 2001 ) who used the TnT trigram tagger \n\t\t']",Positive
"['\n\t\t Using the extended model , accuracy is 92.26 % ( 5564 mistakes ) . \n\t', '\n\t\t This amounts to an overall error reduction of 2.64 % . \n\t', '\n\t\t Mistakes where the plural verb is mixed up with the infinitive sum up to 316 instances ( 5.53 % of all mistakes ) when using the standard model , as opposed to 199 cases ( 3.58 % ) using the extended model , an error reduction of 37.03 % . \n\t', '\n\t\t 5.3 Discussion of results Extending the standard trigram tagging model with syntactical information aimed at resolving the most frequent type of tagging error led to a considerable reduction of this type of error in stand-alone POS tagging experiments on two dif- freq assigned correct 69 proper name(both) 1-proper name(both) 57 proper name(both) noun(de,sg) 53 verb(int) verb(pl) 47 verb(psp) adjective(no e,adv ) 45 proper name(both) 2-proper name(both) 42 punct(ligg streep ) skip Table 3 : Most frequent tagging mistakes on Alpino data , using extended model baseline accuracy 84.3 % model standard extended bigram accuracy 91.45 % 91.73 % trigram accuracy 92.05 % 92.26 % errors 5715 5564 error reduction 151 = 2.64 % pl/inf errors 316 ( 5.53 % ) 199 ( 3.58 % ) pl/inf error red . \n\t', '\n\t\t 117 = 37.03 % Table 4 : Tagging results on Wotan data ferent data sets . \n\t', '\n\t\t At the same time , other types of errors were also reduced . \n\t', '\n\t\t The relative error reduction for the specific type of error involving finite and infinite verb forms is almost twice as high in the case of the Alpino data as in the case of the Wotan data ( respectively 65.98 % and 37.03 % ) . \n\t', '\n\t\t There are at least two possible explanations for this difference . \n\t', '\n\t\t The first is a difference in tagsets . \n\t', '\n\t\t Although the Wotan tagset is much smaller than the Alpino tagset , the former features a more detailed treatment of verbs . \n\t', '\n\t\t In the Alpino data , the difference between plural finite verb forms and nonfinite verb forms is represented through just two tags . \n\t', '\n\t\t In the Wotan data , this difference is represented by 20 tags . \n\t', '\n\t\t An extended model that predicts which of the two forms should be used in a given situation is therefore more complex in the case of the Wotan data . \n\t', '\n\t\t A further important difference between the two data sets is the available amount of training data ( 25 million words for the Alpino experiment compared to 628 thousand words for the Wotan experiment ) . \n\t', '\n\t\t In general a stochastic model such as the HMM will become more accurate when more training data is available . \n\t', '\n\t\t The Wotan experiment was repeated with increasing amounts of training data , and the results indicated that using more data would improve the results of both the standard and the extended model . \n\t', '\n\t\t The advantage of the extended model over the standard model increases slightly as more data is available , suggesting that the extended model would benefit more from extra data than the standard model . \n\t', '\n\t\t 6 Conclusion and future work This work has presented how the HMM for POS tagging was extended with global contextual information without increasing the number of parameters beyond practical limits . \n\t', '\n\t\t Two tagging experiments , using a model extended with a binary feature concerning the occurrence of finite verb forms , resulted in improved accuracies compared to using the standard model . \n\t', '\n\t\t The annotation of the training data with context labels was acquired automatically through the use of a wide-coverage parser . \n\t', '\n\t\t The tagger described here is used as a POS tag filter in wide-coverage parsing of Dutch \n\t\t']",Positive
"['\n\t\t In addition to reducing lexical ambiguity , it would be interesting to see if structural ambiguity can be reduced . \n\t', '\n\t\t In the approach under consideration , the tagger supplies the parser with an initial syntactic structure in the form of a partial bracketing of the input , based on the recognition of larger syntactic units or \x92chunks\x92 . \n\t', '\n\t\t Typically chunk tags will be assigned on the basis of words and their POS tags . \n\t', '\n\t\t An alternative approach is to use an extended model that assigns chunk tags and POS tags simultaneously , as was done for finite verb occurrence and POS tags in the current work . \n\t', '\n\t\t In this way , relations between POS tags and chunk tags can be modeled in both directions . \n\t', '\n\t\t Another possible application is tagging of German . \n\t', '\n\t\t German features different cases , which can lead to problems for statistical taggers . \n\t', '\n\t\t This is illustrated in \n\t\t']",Positive
"['\n\t\t The preference for just one assignment of the nominative case might be learned by including case information in the model . \n\t', '\n\t\t Acknowledgements . \n\t', '\n\t\t This research was carried out as part of the PIONIER Project Algorithms for Linguistic Processing , funded by NWO ( Dutch Organization for Scientific Research ) and the University of Groningen . \n\t', '\n\t\t I would like to thank Hans van Halteren for supplying the Eindhoven corpus data set as used in ( van Halteren et al. , 2001 ) . \n\t', '\n\t\t References J. Berghmans . \n\t', '\n\t\t 1994. Wotan , een automatische grammatikale tagger voor het Nederlands . \n\t', '\n\t\t Master\x92s thesis , Dept. of Language and Speech , University of Nijmegen . \n\t', '\n\t\t Gosse Bouma , Gertjan van Noord , and Robert Malouf . \n\t', '\n\t\t 2001. Wide coverage computational analysis of Dutch . \n\t', '\n\t\t In Walter Daelemans , Khalil Sima\x92an , Jorn Veenstra , and Jakub Zavrel , editors , Computational Linguistics in the Netherlands , CLIN 2000 , pages 45\x9659 , Amsterdam . \n\t', '\n\t\t Rodopi . \n\t', '\n\t\t Thorsten Brants . \n\t', '\n\t\t 2000. TnT \x96 a statistical part-ofspeech tagger . \n\t', '\n\t\t In Proceedings of the 6th Applied Natural Language Processing Conference , Seattle , WA . \n\t', '\n\t\t E. Charniak , G. Carroll , J. Adcock , A. Cassandra , Y. Gotoh , J. Katz , M. Littman , and J. McCann. 1996 . \n\t', '\n\t\t Taggers for parsers . \n\t', '\n\t\t Arti~cial Intelligence , 85(1-2):45\x9657 . \n\t', '\n\t\t Michael Collins . \n\t', '\n\t\t 1999. Head-Driven Statistical Models for Natural Language Parsing . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , University of Pennsylvania , Philadelphia , Pennsylvania . \n\t', '\n\t\t Jan Daciuk . \n\t', '\n\t\t 1999. Treatment of unknown words . \n\t', '\n\t\t In Proceedings of the Workshop on Implementing Automata WIA\x9299 , pages IX\x961 \x96 IX\x969 , Potsdam , Germany , July . \n\t', '\n\t\t Erhard W. Hinrichs and Julia Trushkina . \n\t', '\n\t\t 2003. N- gram and PCFG models for morpho-syntactic tagging of German . \n\t', '\n\t\t In Proceedings of The 2nd Workshop on Treebanks and Linguistic Theories ( TLT 2003 ) , pages 81\x9692 , V¨axj¨o , Sweden , November . \n\t', '\n\t\t Frederick Jelinek . \n\t', '\n\t\t 1998. Statistical Methods for Speech Recognition . \n\t', '\n\t\t MIT Press . \n\t', '\n\t\t Christopher D. Manning and Hinrich Sch¨utze . \n\t', '\n\t\t 1999. Foundations of Statistical Natural Language Processing . \n\t', '\n\t\t MIT Press , Cambridge Mass. Robbert Prins and Gertjan van Noord . \n\t', '\n\t\t 2004. Reinforcing parser preferences through tagging . \n\t', '\n\t\t Traitement Automatique des Langues ( TAL ) , special issue on Evolutions in Parsing . \n\t', '\n\t\t Accepted for publication , 2004 . \n\t', '\n\t\t H. van Halteren , J. Zavrel , and W. Daelemans . \n\t', '\n\t\t 2001. Improving accuracy in word class tagging through the combination of machine learning systems . \n\t', '\n\t\t Computational Linguistics , 27(2):199\x96230 . \n\t', '\n\t\t A Framework for Unsupervised Natural Language Morphology Induction Christian Monson Language Technologies Institute Carnegie Mellon University 5000 Forbes Ave. . \n\t', '\n\t\t Pittsburgh , PA , USA 15213 cmonson@cs.cmu.edu Abstract This paper presents a framework for unsupervised natural language morphology induction wherein candidate suffixes are grouped into candidate inflection classes , which are then arranged in a lattice structure . \n\t', '\n\t\t With similar candidate inflection classes placed near one another in the lattice , I propose this structure is an ideal search space in which to isolate the true inflection classes of a language . \n\t', '\n\t\t This paper discusses and motivates possible search strategies over the inflection class lattice structure . \n\t', '\n\t\t 1 Introduction Many natural language processing tasks , including parsing and machine translation , frequently require a morphological analysis of the language(s) at hand . \n\t', '\n\t\t The task of a morphological analyzer is to identify the lexeme , citation form , or inflection class of surface word forms in a language . \n\t', '\n\t\t Striving to bypass the time consuming , labor intensive task of constructing a morphological analyzer by hand , unsupervised morphology induction techniques seek to automatically discover the morphological structure of a natural language through the analysis of corpora . \n\t', '\n\t\t This paper presents a framework for automatic natural language morphology induction inspired by the traditional and linguistic concept of inflection classes . \n\t', '\n\t\t \n\t\t']",Positive
['\n\t\t This paper presents a discussion of the candidate inflection class framework as a generalization of corpus tries used in early work \n\t\t'],Positive
"['\n\t\t This paper employs English to illustrate its main conjectures and a Spanish newswire corpus of 40,011 tokens and 6,975 types for concrete examples . \n\t', '\n\t\t 2 Previous Work It is possible to organize much of the recent work on unsupervised morphology induction by considering the bias each approach has toward discovering morphologically related words that are also orthographically similar . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t Next along the spectrum of orthographic similarity bias is the work of Schone and Jurafsky ( 2000 ; 2001 ) , who first acquire a list of potential morphological variants using an orthographic similarity technique due to \n\t\t']",Positive
"['\n\t\t They then apply latent semantic analysis ( LSA ) to score the potential morphological variants with a semantic distance . \n\t', '\n\t\t Word forms with small semantic distance are proposed as morphological variants of one anther . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t Segmenting word forms in a corpus , Goldsmith creates an inventory of stems and suffixes . \n\t', '\n\t\t Suffixes which can interchangeably concatenate onto a set of stems form a signature . \n\t', '\n\t\t After defining the space of signatures , Goldsmith searches for that choice of word segmentations resulting in a minimum description length local optimum . \n\t', '\n\t\t Finally , the work of Harris ( 1955 ; 1967 ) , and later \n\t\t']",Positive
"['\n\t\t Couched in modern terms , their work involves first building tries over a corpus vocabulary and then selecting , as morpheme boundaries , those character boundaries with corresponding high branching count in the tries . \n\t', '\n\t\t The work in this paper also has a strong bias toward discovering morphologically related words that share a similar orthography . \n\t', '\n\t\t In particular , the morphology model I use is , akin to Goldsmith , limited to suffix substitution . \n\t', '\n\t\t The novel proposal I bring to the table , however , is a formalization of the full search space of all candidate inflection classes . \n\t', '\n\t\t With this framework in place , defining search strategies for morpheme discovery becomes a natural and straightforward activity . \n\t', '\n\t\t 3 Inflection Classes as Motivation When learning the morphology of a foreign language , it is common for a student to study tables of inflection classes . \n\t', '\n\t\t Carstairs-McCarthy formalizes the concept of an inflection class in chapter 16 of The Handbook of \n\t\t']",Positive
"['\n\t\t In his terminology , a language with inflectional morphology contains lexemes which occur in a variety of word forms . \n\t', '\n\t\t Each word form carries two pieces of information : 1 ) Lexical content and 2 ) Morphosyntactic properties . \n\t', '\n\t\t For example , the English word form gave expresses the lexeme GIVE plus the morphosyntactic property Past , while gives expresses GIVE plus the properties 3rd Person , Singular , and Non-Past . \n\t', '\n\t\t A set of morphosyntactic properties realized with a single word form is defined to be a cell , while a paradigm is a set of cells exactly filled by the word forms of some lexeme . \n\t', '\n\t\t A particular natural language may have many paradigms . \n\t', '\n\t\t In English , a language with very little inflectional morphology , there are at least two paradigms , a noun paradigm consisting of two cells , Singular and Plural , and a paradigm for verbs , consisting of the five cells given ( with one choice of naming convention ) as the first column of Table 1 . \n\t', '\n\t\t Lexemes that belong to the same paradigm may still differ in their morphophonemic realizations of various cells in that paradigm\x97each paradigm may have several associated inflection classes which specify , for the lexemes belonging to that inflection class , the surface instantiation for each cell of the paradigm . \n\t', '\n\t\t Three of the many inflection classes within the English verb paradigm are found in Table 1 under the columns labeled A through C . \n\t', '\n\t\t The task the morphology induction system presented in this paper engages is exactly the discovery of the inflection classes of a natural language . \n\t', '\n\t\t Unlike the analysis in Table 1 , however , the rest of this paper treats word forms as simply strings of characters as opposed to strings of phonemes . \n\t', '\n\t\t 4 Empirical Inflection Classes There are two stages in the approach to unsupervised morphology induction proposed in this paper . \n\t', '\n\t\t First , a search space over a set of candidate Verb Inflection Classes Paradigm A B C Basic blame roam solve show sow saw sing ring 3rd Person Singular Non -past -/z/ -/z/ -/z/ blames roams solves shows sows saws sings rings Past -/d/ -/d/ V /eI/ blamed roamed solved showed sowed sawed sang rang Perfective or Passive -/d/ -/n/ V / / blamed roamed solved shown sown sawn sung rung Progressive -/i / -/i / -/i / blaming roaming solving showing sowing sawing singing ringing Table 1 : A few inflection classes of the Eng- lish verb paradigm inflection classes is defined , and second , this space is searched for those candidates most likely to be part of a true inflection class in the language . \n\t', '\n\t\t I have written a program to create the search space but the search strategies described in this paper have yet to be implemented . \n\t', '\n\t\t 4.1 Candidate Inflection Class Search Space To define a search space wherein inflection classes of a natural language can be identified , my algorithm accepts as input a monolingual corpus for the language and proposes candidate morpheme boundaries at every character boundary in every word form in the corpus vocabulary . \n\t', '\n\t\t I call each string before a candidate morpheme boundary a candidate stem or c-stem , and each string after a boundary a c-suffix . \n\t', '\n\t\t I define a candidate inflection class ( CIC ) to be a set of c-suffixes for which there exists at least one c-stem , t , such that each c-suffix in the CIC concatenated to t produces a word form in the vocabulary . \n\t', '\n\t\t I let the set of c-stems which generate a CIC , C , be called the adherent c-stems of C ; the size of the set of adherent c-stems of C be C\x92s adherent size ; and the size of the set of c- suffixes in C be the level of C. I then define a lattice of relations between CIC\x92s . \n\t', '\n\t\t In particular , two types of relations are defined : 1 ) C-suffix set inclusion relations relate pairs of CIC\x92s when the c-suffixes of one CIC are a superset of the c-suffixes of the other , and 2 ) Morpheme boundary relations occur between CIC\x92s which propose different mor- pheme boundaries within the same word forms . \n\t', '\n\t\t Figure 1 diagrams a portion of a CIC lattice over a toy vocabulary consisting of a subset of the word forms found under inflection class A from Table 1 . \n\t', '\n\t\t The c-suffix set inclusion relations , represented vertically by solid lines , connect such CIC\x92s as e.es.ed and e.ed , both of which originate from the c-stem blam , since the first is a superset of the second . \n\t', '\n\t\t Morpheme boundary relations , drawn horizontally with dashed lines , connect such CIC\x92s as me.mes.med and e.es.ed , each derived from exactly the triple of word forms blame , blames , and blamed , but differing in the placement of the hypothesized morpheme boundary Hierarchical links , connect any given CIC to often more than one parent and more than one child . \n\t', '\n\t\t The empty CIC ( not pictured in Figure 1 ) can be considered the child of all level one CIC\x92s ( including the Ø CIC ) , but there is no universal parent of all top level CIC\x92s . \n\t', '\n\t\t Horizontal morpheme boundary links , dashed lines , connect a CIC , C , with a neighbor to the right if each c-suffix in C begins with the same character . \n\t', '\n\t\t This entails that there is at most one morpheme boundary link leading to the right of each CIC . \n\t', '\n\t\t There may be , however , as many links leading to the left as there are characters in the orthography . \n\t', '\n\t\t The only CIC with depicted multiple left links in Figure 1 is Ø , which has left links to the CIC\x92s e , s , and d . \n\t', '\n\t\t A number of left links emanating from the CIC\x92s in Figure 1 are not shown ; among others absent from the figure is the left link from the CIC e.es leading to the CIC ve.ves with the adherent sol . \n\t', '\n\t\t While many ridiculous CIC\x92s are found in Figure 1 , such as ame.ames.amed from the vocabulary items blame , blames , and blamed and the c- stem bl , there are also CIC\x92s that seem very reasonable , such as Ø.s from the c-stems blame and tease . \n\t', '\n\t\t The key task in automatic morphology induction is to autonomously separate the nonsense CIC\x92s from the useful ones , thus identifying linguistically plausible inflection classes . \n\t', '\n\t\t To better visualize what a CIC lattice looks like when derived from real data , Figure 2 contains a portion of a hierarchical lattice automatically generated from the Spanish newswire corpus . \n\t', '\n\t\t Each entry in Figure 2 contains the c-suffixes comprising the CIC , the adherent size of the CIC , and a sample of adherent c-stems . \n\t', '\n\t\t The lattice in Figure 2 covers : 1 ) The productive Spanish inflection class for adjectives , a.as.o.os , covering the four cells feminine singular , feminine plural , masculine singular , and masculine plural , respectively ; Hierarchical c-suffix set inclusion links Morpheme boundary links Figure 1 : Portion of a CIC lattice from the toy vocabulary : blame , blames , blamed , roams , roamed , roaming , solve , solves , solving 2 ) All possible CIC subsets of the adjective CIC , e.g. a.as.o , a.os , etc. ; and 3 ) The imposter CIC a.as.o.os.tro , together with its rogue descendents , a.tro and tro . \n\t', '\n\t\t Other CIC\x92s that are descendents of a.as.o.os.tro and that contain the c-suffix tro do not supply additional adherents and hence are not present either in Figure 2 or in my program\x92s representation of the CIC lattice . \n\t', '\n\t\t The CIC\x92s a.as.tro and os.tro , for example , both have only the one adherent , cas , already possessed by their common ancestor a.as.o.os.tro . \n\t', '\n\t\t 4.2 Search With the space of candidate inflection classes defined , it seems natural to treat this lattice of CIC\x92s as a hypothesis space of valid inflection classes and to search this space for CIC\x92s most likely to be true inflection classes in a language . \n\t', '\n\t\t There are many possible search strategies applicable to the CIC lattice . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t Using the same Spanish newswire corpus as this paper , the implemented algorithms have achieved F1 measures above 0.5 when identifying CIC\x92s belonging to true inflection classes in Spanish . \n\t', '\n\t\t In Ø.s.d blame e.es.ed blam Ø.s blame solve e.es blam solv me.mes bla Ø.d blame me.med bla e.ed blam s.d blame es.ed blam e blam solv me bla es blam solv s blame roam solve mes bla ed blam roam d blame roame med bla roa mes.med bla blame blames blamed roams roamed roaming solve solves solving me.mes.med bla this paper I discuss some theoretical motivations underlying CIC lattice search . \n\t', '\n\t\t Since there are two types of relations in the CIC lattices I construct , search can be broken into two phases . \n\t', '\n\t\t One phase searches the c-suffix set inclusion relations , and the other phase searches the morpheme boundary relations . \n\t', '\n\t\t The search algorithms discussed in \n\t\t']",Positive
"['\n\t\t In previous related work , morpheme boundary relations and c-suffix set inclusion relations are implicitly present but not explicitly referred to . \n\t', '\n\t\t For example , \n\t\t']",Positive
"['\n\t\t Goldsmith\x92s triage search strategies , which make small changes in the segmentation positions in words , primarily search the morpheme boundary relations , while the vertical search is primarily performed by heuristics that suggest initial word segmentations . \n\t', '\n\t\t To illustrate , if , using the Spanish newswire corpus from this paper , Goldsmith\x92s algorithm decided to segment the word form castro as cas-tro , then there is an implicit vote for the CIC a.as.o.os.tro in Figure 2 . \n\t', '\n\t\t If , on the other hand , his algorithm decided not to segment castro then there is a vote for the lower level CIC a.as.o.os . \n\t', '\n\t\t The next two subsections motivate search over the morpheme boundary relations and the c-suffix set inclusion relations respectively . \n\t', '\n\t\t 4.2.1 Searching Morpheme Boundary Relations Harris ( 1955 ; 1967 ) and \n\t\t']",Positive
"['\n\t\t The rationale behind their procedure is that the phoneme , or grapheme , sequence within a morpheme is completely restricted , while at a morpheme boundary any number of new morphemes ( many with different initial phonemes ) could occur . \n\t', '\n\t\t To assess the flavor of Harris\x92 algorithms , the bottom branch of the trie in Figure 3 begins with roam and subsequently encounters a branching factor of three , leading to the trie nodes Ø , i , and s . \n\t', '\n\t\t Such a high branching factor suggests there may be a morpheme boundary after roam . \n\t', '\n\t\t One way to view the horizontal morpheme boundary links in a CIC lattice is as a character trie generalization where identical sub-tries within the full vocabulary trie are conflated . \n\t', '\n\t\t Figure 3 illustrates the correspondences between a trie and a portion of a CIC lattice for a small vocabulary con- Figure 2 : Hierarchical CIC lattice automati- cally derived from Spanish sisting of the word forms : rest , rests , resting , retreat , retreats , retreating , retry , retries , retrying , roam , roams , and roaming . \n\t', '\n\t\t Each circled sub-trie of the trie in the top portion of the figure corresponds to one of the four CIC\x92s in the bottom portion of the figure . \n\t', '\n\t\t For example , the right- branching children of the y node in retry form a sub-trie consisting of Ø and ing , but this same subtrie is also found following the t node in rest , the t node in retreat , and the m node in roam . \n\t', '\n\t\t The CIC lattice conflates all these sub-tries into the single CIC Ø.ing with the four adherents rest , retreat , retry , and roam . \n\t', '\n\t\t Taking this congruency further , branching factor in the trie corresponds roughly to the level of a CIC . \n\t', '\n\t\t A level 3 CIC such as Ø.ing.s corresponds to sub-tries with initial branching factor of 3 . \n\t', '\n\t\t If separate c-suffixes in a CIC happen to begin with the same character , then a lower branching factor may correspond to a higher level CIC . \n\t', '\n\t\t Similarly , the number of sub-tries which conflate to form a CIC corresponds to the number of adherents belonging to the CIC . \n\t', '\n\t\t a.as.o.os.tro 1 cas a 1237 huelg ib id iglesi ... as 404 huelg huelguist incluid industri ... o 1139 hub hug human huyend ... os 534 humorístic human hígad impedid ... tro 16 catas ce cen cua ... a.tro 2 cas cen a.as 199 huelg incluid industri inundad ... a.as.o 59 cas citad jurídic l ... a.o 214 id indi indonesi inmediat ... as.o 85 intern jurídic just l ... a.as.os 50 afectad cas jurídic l ... a.as.o.os 43 african cas jurídic l ... a.os 134 impedid impuest indonesi inundad ... a.o.os 105 impuest indonesi italian jurídic ... as.os 68 cas implicad inundad jurídic ... as.o.os 54 cas implicad jurídic l ... o.os 268 human implicad indici indocumentad ... \n\t', '\n\t\t Figure 3 : A trie ( top ) with some repeated sub- tries circled . \n\t', '\n\t\t These sub-tries are then conflated into the corresponding CIC lattice ( bottom ) . \n\t', '\n\t\t It is interesting to note that while Harris\x92 style phoneme successor criteria do often correctly identify morpheme boundaries , they posses one inherent class of errors . \n\t', '\n\t\t Because Harris treats all word forms with the same initial string as identical , any morpheme boundary decision is global for all words that happen to begin with the same string . \n\t', '\n\t\t For example , Harris cannot differentiate between the forms casa and castro . \n\t', '\n\t\t If a morpheme boundary is ( correctly ) placed after the cas in casa , then a morpheme boundary must be placed ( incorrectly ) after the cas in castro . \n\t', '\n\t\t Using a CIC lattice , however , allows an algorithm to first choose which branches of a trie are relevant and then select morpheme boundaries given the relevant sub-trie . \n\t', '\n\t\t Exploring the vertical CIC lattice in Figure 2 , a search algorithm might hope to discover that the tro trie branch is irrelevant and search for a morpheme boundary along the sub-tries ending in a.as.o.os . \n\t', '\n\t\t Perhaps the morpheme boundary search would use the branching factor of this restricted trie as a discriminative criterion . \n\t', '\n\t\t 4.2.2 Searching C-suffix Set Inclusion Relations Since trie branches correspond to CIC level , I turn now to outline a search method over the vertical c-suffix set inclusion relations . \n\t', '\n\t\t This search method makes particular use of CIC adherent counts through the application of statistical independence tests . \n\t', '\n\t\t The goal of a vertical search algorithm is to avoid c-suffixes which occur not as true suffixes that are part of an inflection class , but instead as random strings that happen to be able to attach to a given initial string . \n\t', '\n\t\t To formalize the idea of randomness I treat each c-suffix , F , as a Boolean random variable which is true when F attaches to a given c-stem and false when F does not attach to that c-stem . \n\t', '\n\t\t I then make the simplifying assumption that c-stems are independent identically distributed draws from the population of all possible c-stems . \n\t', '\n\t\t Since my algorithm identifies all possible initial substrings of a vocabulary as c-stems , the c-stems are clearly not truly independent\x97some c-stems are actually sub- strings of other c-stems . \n\t', '\n\t\t Nevertheless , natural language inflection classes , in the model of this paper , consist of c-suffixes which interchangeably attach to the same c-stems . \n\t', '\n\t\t Hence , given the assumption of c-suffixes as random variables , the true inflection classes of a language are most likely those groups of c-suffixes which are positively correlated . \n\t', '\n\t\t That is , if knowing that c-suffix F1 concatenates onto c-stem T increases the probability that the suffix F2 also concatenates onto T , then F1 and F2 are likely from the same inflection class . \n\t', '\n\t\t On the other hand , if F1 and F2 are statistically independent , or knowing that F1 concatenates to T does not change the probability that F2 can attach to T , then it is likely that F1 or F2 ( or both ) is a c-suffix that just randomly happens to be able to concatenate onto a T . \n\t', '\n\t\t And finally , if F1 and F2 are negatively correlated , i.e. they occur interchangeably on the same c-stem less frequently than random chance , then it may be that F1 and F2 come from different inflection classes within the same paradigm or are even associated with completely separate paradigms . \n\t', '\n\t\t There are a number of statistical tests designed to assess the probability that two discrete random variables are independent . \n\t', '\n\t\t Here I will look at the ^2 independence test , which computes the probability that two random variables are independent by calculating a statistic Q distributed as ^2 by comparing the expected distributions of the two random variables , assuming their independence with their actual distribution . \n\t', '\n\t\t The larger the values of Q , the lower the probability that the random variables are independent . \n\t', '\n\t\t Summing the results of each c-stem independent trial of the c-suffix Boolean random variables , re- t.ting Ø.ing rest retreat retry roam res retrea r o e a s t m r t Ø Ø i y e s s i i Ø i n n e a n g g s t Ø g i s n g t.ts.ting res retrea Ø.s.ing rest retreat roam sults in Bernoulli distributed random variables whose joint distributions can be described as two by two contingency tables . \n\t', '\n\t\t Table 2 gives such contingency tables for the pairs of random variable c-suffixes ( a , as ) and ( a , tro ) . \n\t', '\n\t\t These tables can be calculated by examining specific CIC\x92s in the lattices . \n\t', '\n\t\t To fill the contingency table for ( a , as ) I proceed as follows : The number of times a occurs jointly with as is exactly the adherent size of the a.as CIC , 199 . \n\t', '\n\t\t The marginal number of occurrences of a , 1237 , can be read from the CIC a , and similarly the marginal number of occurrences of as , 404 , can be read from the CIC as . \n\t', '\n\t\t The bottom right-hand cell in the tables in Table 2 is the total number of trials , or in this case , the number of unique c-stems . \n\t', '\n\t\t This quantity is easily calculated by summing the adherent sizes of all level one CIC\x92s together . \n\t', '\n\t\t In the Spanish newswire corpus there are 22950 unique c-stems . \n\t', '\n\t\t The remaining cells in the contingency table can be calculated by assuring the rows and columns sum up to their marginals . \n\t', '\n\t\t Using these numbers we can calculate the Q statistic : Q(a , as ) = 1552 and Q(a , tro ) = 1.587 . \n\t', '\n\t\t These values suggest that a and as are not independent while a and tro are . \n\t', '\n\t\t 5 Future Work There is clearly considerable work left to do within the CIC framework presented in this paper . \n\t', '\n\t\t I intend to implement the search strategies outlined in this paper . \n\t', '\n\t\t I also plan to apply these techniques to describe the morphologies of a variety of languages beyond English and Spanish . \n\t', '\n\t\t Acknowledgements The research presented in this paper was funded in part by NSF grant number IIS-0121631 . \n\t', '\n\t\t References Andrew Carstairs-McCarthy . \n\t', '\n\t\t 1998. \x93Paradigmatic Structure : Inflectional Paradigms and Morphological Classes.\x94 The Handbook of Morphology . \n\t', '\n\t\t Eds . \n\t', '\n\t\t Andrew Spencer and Arnold M. Zwicky . \n\t', '\n\t\t Blackwell Publishers Inc. , Massachusetts , USA , 322-334 . \n\t', '\n\t\t Éric Gaussier . \n\t', '\n\t\t 1999. Unsupervised learning of derivational morphology from inflectional lexicons . \n\t', '\n\t\t In Proceedings of ACL \x9299 Workshop : Unsupervised Learning in Natural Language Processing . \n\t', '\n\t\t John Goldsmith . \n\t', '\n\t\t 2001. Unsupervised learning of the morphology of a natural language . \n\t', '\n\t\t Computational Linguistics , 27(2) : 153-198 . \n\t', '\n\t\t a ~a marginal as 199 205 404 ~as 1038 21508 22546 marginal 1237 21713 22950 a ~a marginal tro 2 14 16 ~tro 1235 21699 22934 marginal 1237 21713 22950 Table 2 : Contingency tables for a few c-suffixes Margaret A. Hafer and Stephen F. Weiss . \n\t', '\n\t\t 1974. Word segmentation by letter successor varieties . \n\t', '\n\t\t Information Storage and Retrieval , 10:371-385 . \n\t', '\n\t\t Zellig Harris . \n\t', '\n\t\t 1955. From phoneme to morpheme . \n\t', '\n\t\t Language , 31:190-222 . \n\t', '\n\t\t Reprinted in Harris 1970 . \n\t', '\n\t\t Zellig Harris . \n\t', '\n\t\t 1967. Morpheme boundaries within words : Report on a computer test . \n\t', '\n\t\t Transformation and Discourse Analysis Papers 73 , Department of Linguistics , University of Pennsylvania . \n\t', '\n\t\t Reprinted in Harris 1970 . \n\t', '\n\t\t Zellig Harris . \n\t', '\n\t\t 1970. Papers in Structural and Transformational Linguistics . \n\t', '\n\t\t D. Reidel , Dordrecht , Holland . \n\t', '\n\t\t Christian Monson , Alon Lavie , Jaime Carbonell , and Lori Levin . \n\t', '\n\t\t 2004. Unsupervised Induction of Natural Language Morphology Inflection Classes . \n\t', '\n\t\t In Proceedings of the Seventh Meeting of the ACL Special Interest Group in Computational Phonology ( SIGPHON\x9204 ) . \n\t', '\n\t\t Patrick Schone and Daniel Jurafsky . \n\t', '\n\t\t 2000. Knowledge-free Induction of Morphology Using Latent Semantic Analysis . \n\t', '\n\t\t In Proceedings of the Fourth Conference on Computational Natural Language Learning and of the Second Learning Language in Logic Workshop , 67-72 . \n\t', '\n\t\t Patrick Schone and Daniel Jurafsky . \n\t', '\n\t\t 2001. Knowledge-free Induction of Inflectional Morphologies . \n\t', '\n\t\t In Proceedings of the North American Chapter of the Association of Computational Linguistics . \n\t', '\n\t\t 183-191 . \n\t', '\n\t\t David Yarowsky , Grace Ngai , and Richard Wicentowski . \n\t', '\n\t\t 2001. Inducing multilingual text analysis tools via robust projection across aligned corpora . \n\t', '\n\t\t In Proceedings of the Human Language Technology Conference , 161-168 . \n\t', '\n\t\t TransType2 \x96 An Innovative Computer-Assisted Translation System José Esteban and José Lorenzo Antonio S. Guy Lapalme Atos Origin Valderrábanos RALI Laboratory Albarracín 25 Bitext.com Université de Montréal 28037 Madrid , Spain General Oráa 3 C.P. 6128 , Succ Centreville jfernando.esteban@atosorigin.com 28006 Madrid , Spain Montréal , Québec jose.lorenzo@atosorigin.com asv@bitext.com Canada H3C 3J7 lapalme@iro.umontreal.ca Abstract TT2 is an innovative tool for speeding up and facilitating the work of translators by automatically suggesting translation completions . \n\t', '\n\t\t Different versions of the system are being developed for English , French , Spanish and German by an international team of researchers from Europe and Canada . \n\t', '\n\t\t Two professional translation agencies are currently evaluating successive prototypes . \n\t', '\n\t\t 1 Introduction TransType2 (TT2)1 is an innovative tool for speeding up and facilitating the work of translators by automatically suggesting translation completions . \n\t', ""\n\t\t The system uses probabilistic translation and language models to calculate completions that are compatible with translator 's input and , furthermore , revises its suggestions in real time with each new character the translator enters . \n\t"", '\n\t\t If the system provides a correct suggestion , the translator has only to accept it , thereby saving time in producing the target text . \n\t', ""\n\t\t Otherwise , the translator ignores the system 's suggestions and continues to type his or her intended translation . \n\t"", '\n\t\t TT2 is based on a new Machine Assisted Translation paradigm that sits between fully automatic MT and translation memory in order to significantly increase translator productivity on non-repetitive texts . \n\t', '\n\t\t TT2 is unique in the way in which it combines the strengths of MT technology with the competence of the human translator . \n\t', '\n\t\t The project is an extension of the TransType project that was developed from 1997 to 2000 by the RALI at Université de Montréal ( Foster 1997 , Langlais 2002 ) , which demonstrated the interest of target text mediated computer aided translation . \n\t', '\n\t\t Different versions of the system are being developed for English , French , Spanish and German ( with English as the pivot ) . \n\t', '\n\t\t To ensure that TT2 corresponds to translators\x92 needs , two professional translation agencies are currently evaluating successive prototypes . \n\t', '\n\t\t To date , translation technology has not been able to keep pace with the demand for high-quality translation . \n\t', '\n\t\t TT2 has the ability to significantly increase translator productivity and thus has enormous commercial potential . \n\t', '\n\t\t TT2 is a RTD project funded by the European Commission under the Information Society Technologies Programme and includes five European partners : Atos Origin ( Spain ) : administrative and technical coordinator , system design and integration . \n\t', '\n\t\t Lehrstuhl fiir Informatik VI , Computer Science Department , RWTH Aachen - University of Technology ( Germany ) : statistical translation , speech recognition . \n\t', '\n\t\t Instituto Tecnológico de Informática , Universidad Politécnica de Valencia , ( Spain ) : finite-state techniques for translation and speech recognition . \n\t', '\n\t\t Xerox Research Centre Europe , Grenoble ( France ) : corpus provider and statistical translation modeling . \n\t', '\n\t\t Celer Soluciones , Madrid ( Spain ) : evaluation in the operational context of a translation bureau . \n\t', '\n\t\t And two Canadian partners : RALI Laboratory , University of Montreal ( Canada ) : user-interface , statistical modeling , evaluation coordination . \n\t', '\n\t\t Société Gamma , Ottawa ( Canada ) : evaluation in the operational context of a translation bureau . \n\t', '\n\t\t 1 For further details , see http://tt2.sema.es Figure 1. User-view of TT2 with the source text on the left highlighting the sentence under translation . \n\t', '\n\t\t The translator types in the right pane in which TT2 suggests completions that appear in the menu in real-time . \n\t', '\n\t\t Completions can be accepted either by clicking an item from the menu or by the keyboard . \n\t', '\n\t\t This picture displays in red ( appearing in gray in black and white ) characters that have been suggested and accepted by the translator . \n\t', '\n\t\t 2 TT2 as seen by a translator TransType is a tool that observes a translator as he or she is typing , tries to predict what will be typed next and displays its predictions to the user . \n\t', '\n\t\t The translator can incorporate these suggestions into the current target text if they are useful , or simply ignore them by continuing typing . \n\t', '\n\t\t The system will then adapt itself to the new text typed by the translator . \n\t', ""\n\t\t The suggestions can potentially improve a translator 's productivity both by speeding up the keying in of the target text and by contributing to the translation process itself . \n\t"", ""\n\t\t If the underlying machine translation technology is good enough , TransType2 's contributions may reduce the need to consult conventional tools such as a bilingual dictionary , term bank , or translation memory . \n\t"", '\n\t\t The user interface ( Figure 1 ) allows a real-time interaction with the output of the translation/language model to help a translator produce a translation . \n\t', ""\n\t\t TransType2 's main window is divided into two panes , one containing the source text and another containing the target text . \n\t"", '\n\t\t The panes are displayed side by side , with their contents divided into aligned segments . \n\t', '\n\t\t They are also synchronized , so that scrolling one moves the other in parallel . \n\t', ""\n\t\t Many aspects of the main window 's behavior and appearance , such as the orientation of the source and target panes , can be changed using the commands accessible from the menu or keyboard shortcuts . \n\t"", '\n\t\t The source pane is read-only in which the only operation allowed is the selection of a new sentence that triggers a new translation in the target window . \n\t', '\n\t\t The target window is a normal text editing window , except that after each character typed by the user , the system displays a pop-up menu of suggestions for completing the current input . \n\t', '\n\t\t If the user types a return or a tab , this suggestion is inserted in the text . \n\t', '\n\t\t Suggestions can be scrolled up or down with arrow keys or selected with the mouse . \n\t', '\n\t\t At initialization time , the user selects the prediction engine to be used according to one of six source-to-target translation pairs and one of the following domains : technical manuals , European Community official documents and official reports of the debates of the House of Commons of Canada ( Hansards ) . \n\t', '\n\t\t 3 System Architecture The TT2 system consists of two major subsystems that interact closely : user interface ( UI ) , written in Java , provides the typing and pointing modalities ; a second UI supplements those with speech for operating the prototype via short commands uttered by the user . \n\t', '\n\t\t The user interface also produces a trace of all user- actions that can later be replayed by a special program or analyzed in order to evaluate the effectiveness of TransType2 both in terms of number of keystrokes needed for typing a translation and the various patterns of use . \n\t', '\n\t\t prediction engine ( PE ) , written in C/C++ , of which there are multiple realizations available , several per language pair and specific domain ( either technical documentation , EC official documents or Hansards ) . \n\t', '\n\t\t The translation engines developed by research partners are : RALI ( FrenchHEnglish ) is a maximum-entropy minimum-divergence translation model \n\t\t']",Positive
"['\n\t\t ITI ( FrenchHEnglish , SpanishHEnglish ) are based on finite-state techniques \n\t\t']",Positive
"['\n\t\t RWTH ( FrenchHEnglish , SpanishHEnglish , GermanHEnglish ) are statistical based \n\t\t']",Positive
"['\n\t\t The main communications between the UI and the PE are the following : 1 . \n\t', '\n\t\t To initialize the PE , the UI calls a generic create method API function with the appropriate parameters required by each PE and checks its successful completion . \n\t', '\n\t\t 2. Once the user has selected the file he/she wants to work with , the UI produces a list of text segments ( sentences ) and displays them in the source text pane of the interface . \n\t', '\n\t\t 3. The selection of a source sentence is communicated to the PE by the UI . \n\t', '\n\t\t The sentence becomes the source text context prediction for the PE until the user selects another sentence . \n\t', '\n\t\t 4. The UI communicates to the PE every single modification of the target text : insertion/removal of a new character ( letter , digit , punctuation sign or white space ) and cursor movements within the target text . \n\t', '\n\t\t The UI communicates left-right onecharacter-at-a-time movements in the target text area . \n\t', '\n\t\t However , the PE does not take into account the text to the right of the cursor for making its predictions . \n\t', '\n\t\t 5. In response to the request , the PE initiates the search for completions that are eventually returned to the UI for their display . \n\t', '\n\t\t 6. As part of the general exit procedure , the UI calls a generic destroy method API function with the appropriate parameters required by each PE and checks its successful completion . \n\t', '\n\t\t All communication exchanges between the UI and the PE are initiated by the UI , while the PE is in charge of responding by doing some actual work . \n\t', '\n\t\t This is particularly the case in 5 ( producing a list of completions ) , while the others are more of an informative nature ( cases 3 and 4 ) or can hardly considered communication exchanges at all : cases 1 , 2 ( loading a text file and producing a list of sentences ) and 6 ( termination ) . \n\t', '\n\t\t Prediction engines and the speech recognizers are developed and tested under an operating platform ( Linux ) different than the one chosen for user testing ( MS Windows ) . \n\t', '\n\t\t This duality implies that prediction engines and speech recognizers , while developed under Linux , should be able to run under Windows . \n\t', '\n\t\t The users ( i.e. the two translation bureaus ) voiced early in the project that TT2 system should run at least under Windows , although preferably it should also run under Linux . \n\t', '\n\t\t TT2 runs currently on both platforms , the dissemination and awareness of the TT2 prototype are broader , and go further than the initial objectives proposed inside the IST project . \n\t', '\n\t\t Given that developers of the prediction engines and speech recognizers were in favor of using C/C++ as their principal programming language , two practical alternatives were discussed : \x95 Write code without operating platform dependencies and according to standards , that would allow compilers for both platforms to build functionally equivalent binary versions . \n\t', '\n\t\t \x95 Employ tools that lessen to a certain extent the requirement of written C/C++ platform independent code , while allowing the porting of code from the Linux to the Windows platform . \n\t', '\n\t\t This was the preferred option and the three PE\x92s actually make use of one of such tool : Cygwin2 . \n\t', '\n\t\t Cygwin provides a C/C++ compiler for the Windows platform and a library ( cygwin1.dll ) that gives support to Linux/Unix operating system services under the Windows environment . \n\t', '\n\t\t The partners responsible for developing the user interface have opted for JAVA as the programming language because of its graphical user capabilities , in particular its text components , which are fully configurable and compatible with external C/C++ programs . \n\t', '\n\t\t This option solves the portability problem , since the resulting code will run under any JAVA-enabled operating system . \n\t', '\n\t\t 2 http://www.cygwin.com/ 4 System requirements Generally speaking , running the TT2 system demands a high-end personal computer or workstation in order to be able to provide translation completions in real-time and also to be able to incorporate multi-modal user input . \n\t', '\n\t\t The minimum user equipment is a high-end personal computer running under Windows with a minimum of 1 GB of RAM ; however , 2 GB of RAM and Windows XP Professional operating system is preferable . \n\t', '\n\t\t If a Linux operating system is used , the kernel version must be 2.4.20 or higher . \n\t', '\n\t\t It is also required to have installed the Java 2 Runtime Environment , preferably version 1.3.1_ 09 . \n\t', '\n\t\t To produce the PE , cygwin1.dll version 1.5.5-1 is required . \n\t', '\n\t\t The interface requirements of both scenarios include standard keyboard and mouse equipment ; video display capable of resolutions of 1024x768 pixels or higher and voice input hardware ( microphone , a headset preferably , and sound card ) if the optional speech recognition module is used . \n\t', '\n\t\t 5 Evaluation TT2 is based on the premise that we can improve the productivity of translators by reducing the number of keystrokes needed for entering a translation . \n\t', '\n\t\t Professionals at two translation bureaus are currently testing the prototypes . \n\t', '\n\t\t Even though translators are not used to working with this kind of environment , some of them need about 50 % less keystrokes to enter a translation and can thus produce a translation faster . \n\t', '\n\t\t Many user interface improvements suggested by the translators will be included in the next prototypes . \n\t', '\n\t\t 6 Conclusion TT2 is the outcome of a successful cooperation between European countries and Canada to develop an innovative approach to machine aided translation . \n\t', '\n\t\t It is based on advances in statistical machine translation research and on a seamless integration in a word processing environment of the same type as the one currently used by translators . \n\t', '\n\t\t 7 Acknowledgements TT2 is a RTD project funded by the European Commission under the Information Society Technologies Programme ( IST-2001-32091 ) . \n\t', '\n\t\t In Canada it is funded by the National Science and Engineering Research Council and the Ministère du Développement Économique et Régional du Québec ( Mission Recherche ) . \n\t', '\n\t\t References E. Cubel , J. González , A. Lagarda , F. Casacuberta , A. Juan and E.Vidal . \n\t', '\n\t\t Adapting finite-state translation to the TransType2 project . \n\t', '\n\t\t Proceedings of the 8th International Workshop of the European Association for Machine Translation and the 4th Controlled Language Applications Workshop Dublin City University Joint Conference , Ireland , 2003 . \n\t', '\n\t\t Foster G. , Isabelle P. , Plamondon P. Target-Text Mediated Interactive Machine Translation , Machine Translation , 12:1-2 , 175-194 , 1997 . \n\t', '\n\t\t Foster G. , A Maximum Entropy / Minimum Divergence Translation Model , Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics , pp. 37-42 , Hong- Kong , October 2000 . \n\t', '\n\t\t Philippe Langlais , Guy Lapalme and Marie Loranger . \n\t', ""\n\t\t TransType : Development-Evaluation Cycles to Boost Translator 's Productivity . \n\t"", '\n\t\t Machine Translation ( Special Issue on Embedded MT Systems ) , vol. 17 , num . \n\t', '\n\t\t 2 , pp. 77-98 , Feb 2002 . \n\t', '\n\t\t F.J. Och , R. Zens , H. Ney . \n\t', '\n\t\t Efficient Search for Interactive Statistical Machine Translation . \n\t', '\n\t\t Proceedings of the 10th Conference of the European Chapter of the Association for Computational Linguistics ( EACL ) . \n\t', '\n\t\t Budapest , Hungary , pp. 387-393 , April 2003 . \n\t', '\n\t\t Antonio S. Valderrábanos , José Esteban and Luis Iraola. TransType2 - A New Paradigm for Translation Automation . \n\t', '\n\t\t MT Summit 2003 , New Orleans , USA . \n\t', '\n\t\t Improving Domain-Specific Word Alignment for Computer Assisted Translation WU Hua , WANG Haifeng Toshiba ( China ) Research and Development Center 5/F. , Tower W2 , Oriental Plaza No. 1 , East Chang An Ave. , Dong Cheng District Beijing , China , 100738 { wuhua , wanghaifeng}@rdc.toshiba.com.cn Abstract This paper proposes an approach to improve word alignment in a specific domain , in which only a small-scale domain-specific corpus is available , by adapting the word alignment information in the general domain to the specific domain . \n\t', '\n\t\t This approach first trains two statistical word alignment models with the large-scale corpus in the general domain and the small-scale corpus in the specific domain respectively , and then improves the domain-specific word alignment with these two models . \n\t', '\n\t\t Experimental results show a significant improvement in terms of both alignment precision and recall . \n\t', '\n\t\t And the alignment results are applied in a computer assisted translation system to improve human translation efficiency . \n\t', '\n\t\t 1 Introduction Bilingual word alignment is first introduced as an intermediate result in statistical machine translation ( SMT ) \n\t\t']",Positive
"['\n\t\t In previous alignment methods , some researchers modeled the alignments with different statistical models \n\t\t']",Negative
['\n\t\t Some researchers use similarity and association measures to build alignment links \n\t\t'],Negative
"['\n\t\t However , All of these methods require a large-scale bilingual corpus for training . \n\t', '\n\t\t When the large-scale bilingual corpus is not available , some researchers use existing dictionaries to improve word alignment \n\t\t']",Positive
"['\n\t\t However , few works address the problem of domain-specific word alignment when neither the large-scale domain-specific bilingual corpus nor the domain-specific translation dictionary is available . \n\t', '\n\t\t This paper addresses the problem of word alignment in a specific domain , where only a small domain-specific corpus is available . \n\t', '\n\t\t In the domain-specific corpus , there are two kinds of words . \n\t', '\n\t\t Some are general words , which are also frequently used in the general domain . \n\t', '\n\t\t Others are domain-specific words , which only occur in the specific domain . \n\t', '\n\t\t In general , it is not quite hard to obtain a large-scale general bilingual corpus while the available domain-specific bilingual corpus is usually quite small . \n\t', '\n\t\t Thus , we use the bilingual corpus in the general domain to improve word alignments for general words and the corpus in the specific domain for domain-specific words . \n\t', '\n\t\t In other words , we will adapt the word alignment information in the general domain to the specific domain . \n\t', '\n\t\t In this paper , we perform word alignment adaptation from the general domain to a specific domain ( in this study , a user manual for a medical system ) with four steps . \n\t', '\n\t\t ( 1 ) We train a word alignment model using the large-scale bilingual corpus in the general domain ; ( 2 ) We train another word alignment model using the small-scale bilingual corpus in the specific domain ; ( 3 ) We build two translation dictionaries according to the alignment results in ( 1 ) and ( 2 ) respectively ; ( 4 ) For each sentence pair in the specific domain , we use the two models to get different word alignment results and improve the results according to the translation dictionaries . \n\t', '\n\t\t Experimental results show that our method improves domain-specific word alignment in terms of both precision and recall , achieving a 21.96 % relative error rate reduction . \n\t', '\n\t\t The acquired alignment results are used in a generalized translation memory system ( GTMS , a kind of computer assisted translation systems ) \n\t\t']",Positive
"['\n\t\t This kind of system facilitates the re-use of existing translation pairs to translate documents . \n\t', '\n\t\t When translating a new sentence , the system tries to provide the pre-translated examples matched with the input and recommends a translation to the human translator , and then the translator edits the suggestion to get a final translation . \n\t', '\n\t\t The conventional TMS can only recommend translation examples on the sentential level while GTMS can work on both sentential and sub-sentential levels by using word alignment results . \n\t', '\n\t\t These GTMS are usually employed to translate various documents such as user manuals , computer operation guides , and mechanical operation manuals . \n\t', '\n\t\t 2 Word Alignment Adaptation 2.1 Bi-directional Word Alignment In statistical translation models \n\t\t']",Negative
"['\n\t\t Thus , some multi-word units cannot be correctly aligned . \n\t', '\n\t\t In order to deal with this problem , we perform translation in two directions ( English to Chinese , and Chinese to English ) as described in \n\t\t']",Positive
"['\n\t\t The GIZA++ toolkit 1 is used to perform statistical word alignment . \n\t', '\n\t\t For the general domain , we use SG1 and SG2 to represent the alignment sets obtained with English as the source language and Chinese as the target language or vice versa . \n\t', '\n\t\t For alignment links in both sets , we use i for English words and j for Chinese words . \n\t', '\n\t\t SG1 = { ( Aj , j ) | Aj = { aj } , aj >_ 0 } SG2 = { ( i , Ai ) |Ai = { ai } , ai >_ 0 } Where , ak ( k = i , j ) is the position of the source word aligned to the target word in position k . \n\t', '\n\t\t The set Ak(k=i,j) indicates the words aligned to the same source word k . \n\t', '\n\t\t For example , if a Chinese word in position j is connect to an English word in position i , then aj = i . \n\t', '\n\t\t And if a Chinese word in position j is connect to English words in position i and k , then Aj = { i , k Based on the above two alignment sets , we obtain their intersection set , union set 2 and subtraction set . \n\t', '\n\t\t Intersection : SG = SG1 ^ SG2 Union : PG = SG1 ^ SG2 Subtraction : MG = PG ^ SG For the specific domain , we use SFl and SF2 to represent the word alignment sets in the two directions . \n\t', '\n\t\t The symbols SF , PF and MF represents the intersection set , union set and the subtraction set , respectively . \n\t', '\n\t\t 2.2 Translation Dictionary Acquisition When we train the statistical word alignment model with a large-scale bilingual corpus in the general domain , we can get two word alignment results for the training data . \n\t', '\n\t\t By taking the intersection of the two word alignment results , we build a new alignment set . \n\t', '\n\t\t The alignment links in this intersection set are extended by iteratively adding 1 It is located at http://www.isi.edu/~och/GIZA++.html 2 In this paper , the union operation does not remove the replicated elements . \n\t', '\n\t\t For example , if set one includes two elements { 1 , 2 } and set two includes two elements { 1 , 3 } , then the union of these two sets becomes { 1 , 1 , 2 , 3 } . \n\t', '\n\t\t word alignment links into it as described in \n\t\t']",Positive
"['\n\t\t Based on the extended alignment links , we build an English to Chinese translation dictionary D1 with translation probabilities . \n\t', '\n\t\t In order to filter some noise caused by the error alignment links , we only retain those translation pairs whose translation probabilities are above a threshold ^1 or co-occurring frequencies are above a threshold ^2 . \n\t', '\n\t\t When we train the IBM statistical word alignment model with a limited bilingual corpus in the specific domain , we build another translation dictionary D2 with the same method as for the dictionary D1 . \n\t', '\n\t\t But we adopt a different filtering strategy for the translation dictionary D2 . \n\t', '\n\t\t We use log-likelihood ratio to estimate the association strength of each translation pair because \n\t\t']",Positive
"['\n\t\t Thus , we get the translation dictionary D2 by keeping those entries whose log-likelihood ratio scores are greater than a threshold ^3 . \n\t', '\n\t\t 2.3 Word Alignment Adaptation Algorithm Based on the bi-directional word alignment , we define SI as SI = SG ^ SF and UG as UG = PG ^ PF ^ SI . \n\t', '\n\t\t The word alignment links in the set SI are very reliable . \n\t', '\n\t\t Thus , we directly accept them as correct links and add them into the final alignment set WA . \n\t', '\n\t\t Input : Alignment set SIand UG Output : Updated alignment set WA Figure 1 . \n\t', '\n\t\t Word Alignment Adaptation Algorithm ( 1 ) For alignment links in SI , we directly add them into the final alignment set WA . \n\t', '\n\t\t ( 2 ) For each English word i in the UG , we firstfind its different alignment links , and then do the following : a ) If there are alignment links found in dictionary D1 , add the link with the largest probability to WA . \n\t', '\n\t\t b ) Otherwise , if there are alignment links found in dictionary D2 , add the link with the largest log-likelihood ratio score to WA . \n\t', '\n\t\t c ) If both a ) and b ) fail , but three links select the same target words for the English word i , we add this link into WA . \n\t', '\n\t\t d ) Otherwise , if there are two different links for this word : one target is a single word , and the other target is a multi-word unit and the words in the multi-word unit have no link in , add this multi-word alignment link to . \n\t', '\n\t\t WA WA . \n\t', '\n\t\t } For each source word in the set uG , there are two to four different alignment links . \n\t', '\n\t\t We first use translation dictionaries to select one link among them . \n\t', '\n\t\t We first examine the dictionary D1 and then D2 to see whether there is at least an alignment link of this word included in these two dictionaries . \n\t', '\n\t\t If it is successful , we add the link with the largest probability or the largest log-likelihood ratio score to the final set WA . \n\t', '\n\t\t Otherwise , we use two heuristic rules to select word alignment links . \n\t', '\n\t\t The detailed algorithm is described in Figure 1 . \n\t', '\n\t\t Figure 2 . \n\t', '\n\t\t Alignment Example Figure 2 shows an alignment result obtained with the word alignment adaptation algorithm . \n\t', '\n\t\t For example , for the English word \x93x-ray\x94 , we have two different links in UG . \n\t', '\n\t\t One is ( x-ray , X ) and the other is ( x-ray , X^^ ) . \n\t', '\n\t\t And the single Chinese words \x93^\x94 and \x93^\x94 have no alignment links in the set WA . \n\t', '\n\t\t According to the rule d ) , we select the link ( x-ray , X ^^ ) . \n\t', '\n\t\t The Chinese sentences in both the training set and the testing set are automatically segmented into words . \n\t', '\n\t\t In order to exclude the effect of the segmentation errors on our alignment results , we correct the segmentation errors in our testing set . \n\t', '\n\t\t The alignments in the testing set are manually annotated , which includes 1,478 alignment links . \n\t', '\n\t\t 3.2 Overall Performance We use evaluation metrics similar to those in \n\t\t']",Positive
"['\n\t\t However , we do not classify alignment links into sure links and possible links . \n\t', '\n\t\t We consider each alignment as a sure link . \n\t', '\n\t\t If we use SG to represent the alignments identified by the proposed methods and SC to denote the reference alignments , the methods to calculate the precision , recall , and f-measure are shown in Equation ( 1 ) , ( 2 ) and ( 3 ) . \n\t', '\n\t\t According to the definition of the alignment error rate ( AER ) in \n\t\t']",Positive
"['\n\t\t Thus , the higher the f-measure is , the lower the alignment error rate is . \n\t', '\n\t\t Thus , we will only give precision , recall and AER values in the experimental results . \n\t', '\n\t\t precision = |SG ^SC | | SG | ( 1 ) SG ^SC | | |SC | ( 2 ) 3 Evaluation recall = We compare our method with three other methods . \n\t', '\n\t\t The first method \x93Gen+Spec\x94 directly combines the corpus in the general domain and in the specific domain as training data . \n\t', '\n\t\t The second method \x93Gen\x94 only uses the corpus in the general domain as training data . \n\t', '\n\t\t The third method \x93Spec\x94 only uses the domain-specific corpus as training data . \n\t', '\n\t\t With these training data , the three methods can get their own translation dictionaries . \n\t', '\n\t\t However , each of them can only get one translation dictionary . \n\t', '\n\t\t Thus , only one of the two steps a ) and b ) in Figure 1 can be applied to these methods . \n\t', '\n\t\t The difference between these three methods and our method is that , for each word , our method has four candidate alignment links while the other three methods only has two candidate alignment links . \n\t', '\n\t\t Thus , the steps c ) and d ) in Figure 1 should not be applied to these three methods . \n\t', '\n\t\t 3.1 Training and Testing Data We have a sentence aligned English-Chinese bilingual corpus in the general domain , which includes 320,000 bilingual sentence pairs , and a sentence aligned English-Chinese bilingual corpus in the specific domain ( a medical system manual ) , which includes 546 bilingual sentence pairs . \n\t', '\n\t\t From this domain-specific corpus , we randomly select 180 pairs as testing data . \n\t', '\n\t\t The remained 366 pairs are used as domain-specific training data . \n\t', '\n\t\t Method Precision Recall AER Ours 0.8363 0.7673 0.1997 Gen+Spec 0.8276 0.6758 0.2559 Gen 0.8668 0.6428 0.2618 Spec 0.8178 0.4769 0.3974 Table 1 . \n\t', '\n\t\t Word Alignment Adaptation Results We get the alignment results shown in Table 1 by setting the translation probability threshold to ^1 = 0.1 , the co-occurring frequency threshold to ^2 = 5 and log-likelihood ratio score to ^3 = 50 . \n\t', '\n\t\t From the results , it can be seen that our approach performs the best among others , achieving much higher recall and comparable precision . \n\t', '\n\t\t It also achieves a 21.96 % relative error rate reduction compared to the method \x93Gen+Spec\x94 . \n\t', '\n\t\t This indicates that separately modeling the general words and domain-specific words can effectively improve the word alignment in a specific domain . \n\t', '\n\t\t 2* | | SG ^ SC = fmeasure ( 3 ) SG | | +|SC| AER =1^2*| SG ^SC | =1 | SG | +|SC| fmeasure ( 4 ) 4 Computer Assisted Translation System A direct application of the word alignment result to the GTMS is to get translations for sub-sequences in the input sentence using the pre-translated examples . \n\t', '\n\t\t For each sentence , there are many sub-sequences . \n\t', '\n\t\t GTMS tries to find translation examples that match the longest sub-sequences so as to cover as much of the input sentence as possible without overlapping . \n\t', '\n\t\t Figure 3 shows a sentence translated on the sub-sentential level . \n\t', '\n\t\t The three panels display the input sentence , the example translations and the translation suggestion provided by the system , respectively . \n\t', '\n\t\t The input sentence is segmented to three parts . \n\t', '\n\t\t For each part , the GTMS finds one example to get a translation fragment according to the word alignment result . \n\t', '\n\t\t By combining the three translation fragments , the GTMS produces a correct translation suggestion \x93~~~jk)~ CT 49~~o \x94 Without the word alignment information , the conventional TMS cannot find translations for the input sentence because there are no examples closely matched with it . \n\t', '\n\t\t Thus , word alignment information can improve the translation accuracy of the GTMS , which in turn reduces editing time of the translators and improves translation efficiency . \n\t', '\n\t\t Figure 3 . \n\t', '\n\t\t A Snapshot of the Translation System 5 Conclusion This paper proposes an approach to improve domain-specific word alignment through alignment adaptation . \n\t', '\n\t\t Our contribution is that our approach improves domain-specific word alignment by adapting word alignment information from the general domain to the specific domain . \n\t', '\n\t\t Our approach achieves it by training two alignment models with a large-scale general bilingual corpus and a small-scale domain-specific corpus . \n\t', '\n\t\t Moreover , with the training data , two translation dictionaries are built to select or modify the word alignment links and further improve the alignment results . \n\t', '\n\t\t Experimental results indicate that our approach achieves a precision of 83.63 % and a recall of 76.73 % for word alignment on a user manual of a medical system , resulting in a relative error rate reduction of 21.96 % . \n\t', '\n\t\t Furthermore , the alignment results are applied to a computer assisted translation system to improve translation efficiency . \n\t', '\n\t\t Our future work includes two aspects . \n\t', '\n\t\t First , we will seek other adaptation methods to further improve the domain-specific word alignment results . \n\t', '\n\t\t Second , we will use the alignment adaptation results in other applications . \n\t', '\n\t\t References Lars Ahrenberg , Magnus Merkel and Mikael Andersson . \n\t', '\n\t\t 1998. A Simple Hybrid Aligner for Generating Lexical Correspondences in Parallel Tests . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics , pages 29-35 . \n\t', '\n\t\t Peter F. Brown , Stephen A. Della Pietra , Vincent J. Della Pietra and Robert L. Mercer . \n\t', '\n\t\t 1993. The Mathematics of Statistical Machine Translation : Parameter Estimation . \n\t', '\n\t\t Computational Linguistics , 19(2) : 263-311 . \n\t', '\n\t\t Colin Cherry and Dekang Lin . \n\t', '\n\t\t 2003. A Probability Model to Improve Word Alignment . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 41st Annual Meeting of the Association for Computational Linguistics , pages 88-95 . \n\t', '\n\t\t Ted Dunning . \n\t', '\n\t\t 1993 . \n\t', '\n\t\t Accurate Methods for the Statistics of Surprise and Coincidence . \n\t', '\n\t\t Computational Linguistics , 19(1) : 61-74 . \n\t', '\n\t\t Sue J. Ker , Jason S. Chang . \n\t', '\n\t\t 1997. A Class-based Approach to Word Alignment . \n\t', '\n\t\t Computational Linguistics , 23(2) : 313-343 . \n\t', '\n\t\t Franz Josef Och and Hermann Ney . \n\t', '\n\t\t 2000. Improved Statistical Alignment Models . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the 38th Annual Meeting of the Association for Computational Linguistics , pages 440-447 . \n\t', '\n\t\t Michel Simard and Philippe Langlais . \n\t', '\n\t\t 2001. Sub-sentential Exploitation of Translation Memories . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of MT Summit VIII , pages 335-339 . \n\t', '\n\t\t Dan Tufis and Ana Maria Barbu . \n\t', '\n\t\t 2002. Lexical Token Alignment : Experiments , Results and Application . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t of the Third International Conference on Language Resources and Evaluation , pages 458-465 . \n\t', '\n\t\t Dekai Wu . \n\t', '\n\t\t 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora . \n\t', '\n\t\t Computational Linguistics , 23(3) : 377-403 . \n\t', '\n\t\t Constructing Transliteration Lexicons from Web Corpora Jin-Shea Kuo1 , 2 Ying-Kuei Yang2 1Chung-Hwa Telecommunication 2E . \n\t', '\n\t\t E. Dept. , National Taiwan University of Science Laboratories , Taiwan , R. O. C. , 326 and Technology , Taiwan , R.O.C. , 106 jskuo@cht.com.tw ykyang@mouse.ee.ntust.edu.tw Abstract This paper proposes a novel approach to automating the construction of transliterated-term lexicons . \n\t', '\n\t\t A simple syllable alignment algorithm is used to construct confusion matrices for cross-language syllable-phoneme conversion . \n\t', '\n\t\t Each row in the confusion matrix consists of a set of syllables in the source language that are ( correctly or erroneously ) matched phonetically and statistically to a syllable in the target language . \n\t', '\n\t\t Two conversions using phoneme-to-phoneme and text-to-phoneme syllabification algorithms are automatically deduced from a training corpus of paired terms and are used to calculate the degree of similarity between phonemes for transliterated-term extraction . \n\t', '\n\t\t In a large-scale experiment using this automated learning process for conversions , more than 200,000 transliterated-term pairs were successfully extracted by analyzing query results from Internet search engines . \n\t', '\n\t\t Experimental results indicate the proposed approach shows promise in transliterated-term extraction . \n\t', '\n\t\t 1 Introduction Machine transliteration plays an important role in machine translation . \n\t', '\n\t\t The importance of term transliteration can be realized from our analysis of the terms used in 200 qualifying sentences that were randomly selected from English-Chinese mixed news pages . \n\t', '\n\t\t Each qualifying sentence contained at least one English word . \n\t', '\n\t\t Analysis showed that 17.43 % of the English terms were transliterated , and that most of them were content words ( words that carry essential meaning , as opposed to grammatical function words such as conjunctions , prepositions , and auxiliary verbs ) . \n\t', '\n\t\t In general , a transliteration process starts by first examining a pre-compiled lexicon which contains many transliterated-term pairs collected manually or automatically . \n\t', '\n\t\t If a term is not found in the lexicon , the transliteration system then deals with this out-ofvocabulary ( OOV ) term to try to generate a transliterated-term via a sequence of pipelined conversions \n\t\t']",Positive
"['\n\t\t Before this issue can be dealt with , a large quantity of transliterated-term pairs are required to train conversion models . \n\t', '\n\t\t Preparing a lexicon composed of transliterated term pairs is time- and labor-intensive . \n\t', '\n\t\t Constructing such a lexicon automatically is the most important goal of this paper . \n\t', '\n\t\t The problem is how to collect transliterated-term pairs from text resources . \n\t', ""\n\t\t Query logs recorded by Internet search engines reveal users ' intentions and contain much information about users ' behaviors . \n\t"", '\n\t\t \n\t\t']",Positive
"['\n\t\t Under this method , a large initial number of term pairs were compiled manually . \n\t', '\n\t\t It is time-consuming to prepare such an initial training set , and the resource used is not publicly accessible . \n\t', '\n\t\t The Internet is one of the largest distributed databases in the world . \n\t', '\n\t\t It comprises various kinds of data and at the same time is growing rapidly . \n\t', '\n\t\t Though the World Wide Web is not systematically organized , much invaluable information can be obtained from this large text corpus . \n\t', '\n\t\t Many researchers dealing with natural language processing , machine translation , and information retrieval have focused on exploiting such non-parallel Web data \n\t\t']",Positive
"['\n\t\t Also , online texts contain the latest terms that may not be found in existing dictionaries . \n\t', '\n\t\t Regularly exploring Web corpora is a good way to update dictionaries . \n\t', '\n\t\t Transliterated-term extraction using non-parallel corpora has also been conducted \n\t\t']",Positive
"['\n\t\t Automated speech recognition-generated confusion matrices ( AGCM ) have been used successfully to bootstrap term extraction from Web pages collected by a software spider . \n\t', '\n\t\t AGCM were used successfully not only to alleviate pronunciation variation , especially the sociolinguistic causes , but also to construct a method for cross- language syllable-phoneme conversion ( CLSPC ) . \n\t', '\n\t\t This is a mapping from a source-language syllable into its target-language counterpart . \n\t', '\n\t\t The problem is how to produce such conversions if AGCM are not available for the targeted language pair . \n\t', '\n\t\t To generate confusion matrices from automated speech recognition requires the effort of collecting many speech corpora for model training , costing time and labor . \n\t', '\n\t\t Automatically constructing a CLSPC without AGCM is the other main focus of this paper . \n\t', '\n\t\t Web pages , which are dynamically updated and publicly accessible , are important to many researchers . \n\t', '\n\t\t However , if many personally guided spiders were simultaneously collecting Web pages , they might cause a network traffic jam . \n\t', '\n\t\t Internet search engines , which update their data periodically , provide search services that are also publicly accessible . \n\t', '\n\t\t A user can select only the pages of interest from Internet search engines ; this mitigates the possibility that a network traffic jam will be caused by many personally guided spiders . \n\t', '\n\t\t Possibly aligned candidate strings in two languages , which may belong to two completely different language families , are selected using local context analysis from non-parallel corpora \n\t\t']",Positive
"['\n\t\t In order to determine the degree of similarity between possible candidate strings , a method for converting such aligned terms cross-linguistically into the same representation in syllables is needed . \n\t', '\n\t\t A syllable is the basic pronunciation unit used in this paper . \n\t', '\n\t\t The tasks discussed in this paper are first to align syllables cross-linguistically , then to construct a cross- linguistic relation , and third to use the trained relation to extract transliterated-term pairs . \n\t', '\n\t\t The remainder of the paper is organized as follows : Section 2 describes how English-Chinese transliterated-term pairs can be extracted automatically . \n\t', '\n\t\t Experimental results are presented in Section 3 . \n\t', '\n\t\t Section 4 analyzes on the performance achieved by the extraction . \n\t', '\n\t\t Conclusions are drawn in Section 5. 2 . \n\t', '\n\t\t The Proposed Approach An algorithm based on minimizing the edit distance between words with the same representation has been proposed \n\t\t']",Negative
"['\n\t\t However , the mapping between cross-linguistic phonemes is obtained only after the cross-linguistic relation is constructed . \n\t', '\n\t\t Such a relation is not available at the very beginning . \n\t', '\n\t\t A simple and fast approach is proposed here to overcome this problem . \n\t', '\n\t\t Initially , 200 verified correct English-Chinese transliterated-term pairs are collected manually . \n\t', '\n\t\t One of the most important attributes of these term pairs is that the numbers of syllables in the source-language term and the target- language term are equal . \n\t', '\n\t\t The syllables of both languages can also be decomposed further into phonemes . \n\t', '\n\t\t The algorithm that adopts equal syllable numbers to align syllables and phonemes cross- linguistically is called the simple syllable alignment algorithm ( SSAA ) . \n\t', '\n\t\t This algorithm generates syllable and phoneme mapping tables between the source and target languages . \n\t', '\n\t\t These two mapping tables can be used to calculate similarity between candidate strings in transliterated-term extraction . \n\t', '\n\t\t With the mapping , the transliterated-term pairs can be extracted . \n\t', '\n\t\t The obtained term pairs can be selected according to the criterion of equal syllable segments . \n\t', '\n\t\t These qualified term pairs can then be merged with the previous set to form a larger set of qualified term pairs . \n\t', '\n\t\t The new set of qualified term pairs can be used again to construct a new cross-linguistic mapping for the next term extraction . \n\t', '\n\t\t This process iterates until no more new term pairs are produced or until other criteria are met . \n\t', '\n\t\t The conversions used in the last round of the training phase are then used to extract large-scale transliterated-term pairs from query results . \n\t', '\n\t\t Two types of cross-linguistic relations , phonemeto-phoneme ( PP ) and text-to-phoneme ( TP ) , can be used depending on whether a source-language letterto-sound system is available or not . \n\t', '\n\t\t 2.1 Construction of a Relation Using Phoneme-toPhoneme Mapping If a letter-to-phoneme system is available , a phoneme-based syllabification algorithm ( PSA ) is used for constructing a cross-linguistic relation , then a phoneme-to-phoneme ( PP ) mapping is selected . \n\t', '\n\t\t Each word in the located English string is converted into phonemes using MBRDICO \n\t\t']",Positive
"['\n\t\t In order to compare English terms with Chinese terms in syllables , the generated English phonemes are syllabified into consonant-vowel pairs . \n\t', '\n\t\t Each consonant-vowel pair is then converted into a Chinese syllable . \n\t', '\n\t\t The PSA used here is basically the same as the classical one \n\t\t']",Positive
"['\n\t\t Traditionally , an English syllable is composed of an initial consonant cluster followed by a vowel and then a final consonant cluster . \n\t', '\n\t\t However , in order to convert English syllables to Chinese ones , the final consonant cluster is appended only when it is a nasal . \n\t', '\n\t\t The other consonants in the final consonant cluster are then segmented into isolated consonants . \n\t', '\n\t\t Such a syllable may be viewed as the basic pronunciation unit in transliterated-term extraction . \n\t', '\n\t\t After English phonemes are grouped into syllables , the English syllables can be converted into Chinese ones according to the results produced by using SSAA . \n\t', '\n\t\t The accuracy of the conversion can improve progressively if the cross-linguistic relation is deduced from a large quantity of transliterated-term pairs . \n\t', '\n\t\t Take the word "" polder "" as an example . \n\t', '\n\t\t First , it is converted into /polda/ using the letter-to-phoneme system , and then according to the phoneme-based syllabification algorithm ( PSA ) , it is divided into /po/ , /l/ , and /do/ , where /l/ is an isolated consonant . \n\t', '\n\t\t Second , these English syllables are then converted into Chinese syllables using the trained cross- linguistic relation ; for example , /po/ , /l/ , and /dO/ are converted into /po/ , /er/ , and /de/ ( in Pin-yin ) , respectively . \n\t', '\n\t\t /l/ is a syllable with only an isolated consonant . \n\t', '\n\t\t A final is appended to its converted Chinese syllable in order to make it complete because not all Chinese initials are legal syllables . \n\t', '\n\t\t The other point worth noting is that /l/ , a consonant in English , is converted into its Chinese equivalent , /er/ , but , /er/ is a final ( a kind of complex vowel ) in Chinese . \n\t', '\n\t\t 2.2 Construction of a Relation Using Text-toPhoneme Mapping If a source language letter-to-phoneme system is not available , a simple text-based syllabification algorithm ( TSA ) is used and a text-to-phoneme ( TP ) mapping is selected . \n\t', '\n\t\t An English word is frequently composed of multiple syllables ; whereas , every Chinese character is a monosyllable . \n\t', '\n\t\t First , each English character in an English term is identified as a consonant , a vowel or a nasal . \n\t', '\n\t\t For example , the characters \x93a\x94 , \x93b\x94 and \x93n\x94 are viewed as a vowel , a consonant and a nasal , respectively . \n\t', '\n\t\t Second , consecutive characters of the same attribute form a cluster . \n\t', '\n\t\t However , some characters , such as \x93ch\x94 , \x93ng\x94 and \x93ph\x94 , always combine together to form complex consonants . \n\t', '\n\t\t Such complex consonants are also taken into account in the syllabification process . \n\t', '\n\t\t A Chinese syllable is composed of an initial and a final . \n\t', '\n\t\t An initial is similar to a consonant in English , and a final is analogous to a vowel or a combination of a vowel and a nasal . \n\t', '\n\t\t Using the proposed simple syllable alignment algorithm , a conversion using TP mapping can be produced . \n\t', '\n\t\t The conversion can also be used in transliterated-term extraction from nonparallel web corpora . \n\t', '\n\t\t The automated construction of a cross-linguistic mapping eliminates the dependency on AGCM reported in \n\t\t']",Positive
"['\n\t\t The cross-linguistic relation constructed using TSA and TP is called CTP ; on the other hand , the cross- linguistic relation using PSA and PP is called CPP . \n\t', '\n\t\t 3 The Experimental Results 3.1 Training Cross-language Syllable-phoneme Conversions An English-Chinese text corpus of 500MB in 15,822,984 pages , which was collected from the Internet using a web spider and was converted to plain text , was used as a training set . \n\t', '\n\t\t This corpus is called SET 1 . \n\t', '\n\t\t From SET 1 , 80,094 qualifying sentences that occupied 5MB were extracted . \n\t', '\n\t\t A qualifying sentence was a sentence composed of at least one English string . \n\t', '\n\t\t Two experiments were conducted using either CPP or CTP on SET 1 . \n\t', '\n\t\t Figure 1 shows the progress of extracting transliterated-term pairs achieved using CPP mapping . \n\t', '\n\t\t A noteworthy phenomenon was that phoneme conversion produced more term pairs than syllable conversion did at the very beginning of training . \n\t', '\n\t\t This is because , initially , the quality of the syllable combinations is not good enough . \n\t', '\n\t\t The phonemes exerted finer-grained control than syllables did . \n\t', '\n\t\t However , when the generated syllable combinations improved in quality , the situation changed . \n\t', '\n\t\t Finally , extraction performed using syllable conversion outperformed that achieved using phoneme conversion . \n\t', '\n\t\t Note also that the results produced by using phonemes quickly approached the saturation state . \n\t', '\n\t\t This is because the English phoneme set is small . \n\t', '\n\t\t When phonemes were used independently to perform term extraction , fewer extracted term pairs were produced than were produced using syllables or a combination of syllables and phonemes . \n\t', '\n\t\t Iter #1 Iter #2 Iter #3 Iter #4 Iter #5 Iter #6 Figure 1 . \n\t', '\n\t\t The progress of extracting transliterated- term pairs using CPP conversion Figure 2 shows the progress of extracting transliterated-term pairs using CTP . \n\t', '\n\t\t The same situation also occurred at the very beginning of training . \n\t', '\n\t\t Comparing the results generated using CPP and CTP , CPP outperformed CTP in terms of the quantity of extracted term pairs because the combinations obtained using TSA are larger than those obtained using PSA . \n\t', '\n\t\t This is also revealed by the results generated at iteration 1 and shown in Figures 1 and 2 . \n\t', '\n\t\t Figure 2 . \n\t', '\n\t\t The progress of extracting transliterated- term pairs using CTP conversion . \n\t', '\n\t\t 7000 6500 6000 5500 5000 4500 4000 3500 3000 2500 2000 1500 1000 500 0 Syllable ( S ) Phoneme ( P ) S+P Iter #1 Iter #2 Iter #3 Iter #4 Iter #5 Iter #6 6000 5500 5000 4500 4000 3500 3000 2500 2000 1500 1000 500 0 Syllable ( S ) Phoneme ( P ) S+P 3.2 Transliterated-term Extraction The Web is growing rapidly . \n\t', '\n\t\t It is a rich information source for many researchers . \n\t', '\n\t\t Internet search engines have collected a huge number of Web pages for public searching \n\t\t']",Positive
"['\n\t\t Submitting queries to these search engines and analyzing the results can help researchers to understand the usages of transliterated-term pairs . \n\t', '\n\t\t Query results are text snippets shown in a page returned from an Internet search engine in response to a query . \n\t', '\n\t\t These text snippets may be composed of texts that are extracted from the beginning of pages or from the texts around the keywords matched in the pages . \n\t', '\n\t\t Though a snippet presents only a portion of the full text , it provides an alternative way to summarize the pages matched . \n\t', '\n\t\t Initially , 200 personal names were randomly selected from the names in the 1990 census conducted by the US Census Bureau1 as queries to be submitted to Internet search engines . \n\t', '\n\t\t CPP and CTP were obtained in the last round of the training phase . \n\t', '\n\t\t The estimated numbers of distinct qualifying term pairs ( EDQTP ) obtained by analyzing query results and by using CPP and CTP mappings for 7 days are shown in Table 1 . \n\t', '\n\t\t A qualifying term pair means a term pair that is verified manually to be correct . \n\t', '\n\t\t EDQTP are term pairs that are not verified manually but are estimated according to the precision achieved during the training phase . \n\t', '\n\t\t Finally , a text corpus called SET2 was obtained by iteratively submitting queries to search engines . \n\t', '\n\t\t SET2 occupies 3.17GB and is composed of 67,944 pages in total . \n\t', '\n\t\t The term pairs extracted using CTP were much fewer in number than those extracted using CPP . \n\t', '\n\t\t This is because the TSA used in this study , though effective , is very simple and rudimentary . \n\t', '\n\t\t A finer-grained syllabification algorithm would improve performance . \n\t', '\n\t\t EDQTP Table 1 . \n\t', '\n\t\t The term pairs extracted from Internet search engines using PP and TP mappings . \n\t', '\n\t\t 4 Discussion Comparing the performances achieved by CPP and CTP , the results obtained by using CPP were better than those with CTP . \n\t', '\n\t\t The reason is that TSA is very simple . \n\t', '\n\t\t A better TSA would produce better results . \n\t', '\n\t\t Though TSA is simple , it is still effective in automatically extracting a large quantity of term 1http://www.census.gov/genealogy/names/ pairs . \n\t', '\n\t\t Also , TSA has an advantage over PSA is that no letter-to-phoneme system is required . \n\t', '\n\t\t It could be helpful when applying the proposed approach to other language pairs , where such a mapping may not be available . \n\t', '\n\t\t 5 Conclusions An approach to constructing transliterated-term lexicons has been presented in this paper . \n\t', '\n\t\t A simple alignment algorithm has been used to automatically construct confusion matrices for cross-language syllable-phoneme conversion using phoneme-tophoneme ( PP ) and text-to-phoneme ( TP ) syllabification algorithms . \n\t', '\n\t\t The proposed approach not only reduces the need for using automated speech recognition-generated confusion matrices , but also eliminates the need for a letter-to-phoneme system for source-language terms if TP is used to construct a cross-language syllable-phoneme conversion and to successfully extract transliterated- term pairs from query results returned by Internet search engines . \n\t', '\n\t\t The performance achieved using PP and TP has been compared and discussed . \n\t', '\n\t\t The overall experimental results show that this approach is very promising for transliterated-term extraction . \n\t', '\n\t\t References Al-Onaizan Y. and Knight K. 2002 . \n\t', '\n\t\t Machine Transliteration of Names in Arabic Text , In Proceedings of ACL Workshop on Computational Approaches to Semitic Languages , pp. 34-46 . \n\t', '\n\t\t Brill E. , Kacmarcik G. , Brockett C. 2001 . \n\t', '\n\t\t Automatically Harvesting Katakana-English Term Pairs from Search Engine Query Logs , In Proceedings of Natural Language Processing Pacific Rim Symposium , pp. 393- 399 . \n\t', '\n\t\t Brin S. and Page L. 1998 . \n\t', '\n\t\t The Anatomy of a Large-scale Hypertextual Web Search Engine , In Proceedings of 7th International World Wide Web Conference , pp. 107-117 . \n\t', '\n\t\t Fung P. and Yee L.-Y. 1998 . \n\t', '\n\t\t An IR Approach for Translating New Words from Nonparallel , Comparable Texts . \n\t', '\n\t\t In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 7th International Conference on Computational Linguistics , pp. 414-420 . \n\t', '\n\t\t Jurafsky D. and Martin J. H. 2000 . \n\t', '\n\t\t Speech and Language Processing , pp. 102-120 , Prentice-Hall , New Jersey . \n\t', '\n\t\t Knight K. and Graehl J. 1998 . \n\t', '\n\t\t Machine Transliteration , Computational Linguistics , Vol. 24 , No. 4 , pp.599-612 . \n\t', '\n\t\t Kuo J. S. and Yang Y. K. 2003 . \n\t', '\n\t\t Automatic Transliterated- term Extraction Using Confusion Matrix from Nonparallel Corpora , In Proceedings of ROCLING XV Computational Linguistics Conference , pp. 17-32 . \n\t', '\n\t\t Pagel V. , Lenzo K. , and Black A. 1998 . \n\t', '\n\t\t Letter to Sound Rules for Accented Lexicon Compression , In Proceedings of ICSLP , pp. 2015-2020 . \n\t', '\n\t\t CPP CTP 201,732 110,295 \n\t', '\n\t\t Subsentential Translation Memory for Computer Assisted Writing and Translation Jian-Cheng Wu Department of Computer Science National Tsing Hua University 101 , Kuangfu Road , Hsinchu , 300 , Taiwan , ROC D928322@oz.nthu.edu.tw Thomas C. Chuang Department of Computer Science Van Nung Institute of Technology No. 1 Van-Nung Road Chung-Li Tao-Yuan , Taiwan , ROC tomchuang@cc.vit.edu.tw Wen-Chi Shei , Jason S. Chang Department of Computer Science National Tsing Hua University 101 , Kuangfu Road , Hsinchu , 300 , Taiwan , ROC jschang@cs.nthu.edu.tw Abstract This paper describes a database of translation memory , TotalRecall , developed to encourage authentic and idiomatic use in second language writing . \n\t', '\n\t\t TotalRecall is a bilingual concordancer that support search query in English or Chinese for relevant sentences and translations . \n\t', '\n\t\t Although initially intended for learners of English as Foreign Language ( EFL ) in Taiwan , it is a gold mine of texts in English or Mandarin Chinese . \n\t', '\n\t\t TotalRecall is particularly useful for those who write in or translate into a foreign language . \n\t', '\n\t\t We exploited and structured existing high-quality translations from bilingual corpora from a Taiwan-based Sinorama Magazine and Official Records of Hong Kong Legislative Council to build a bilingual concordance . \n\t', '\n\t\t Novel approaches were taken to provide high- precision bilingual alignment on the subsentential and lexical levels . \n\t', '\n\t\t A browser- based user interface was developed for ease of access over the Internet . \n\t', '\n\t\t Users can search for word , phrase or expression in English or Mandarin . \n\t', '\n\t\t The Web-based user interface facilitates the recording of the user actions to provide data for further research . \n\t', '\n\t\t 1 Introduction Translation memory has been found to be more effective alternative to machine translation for translators , especially when working with batches of similar texts . \n\t', '\n\t\t That is particularly true with so- called delta translation of the next versions for publications that need continuous revision such as an encyclopaedia or user\x92s manual . \n\t', '\n\t\t On another area of language study , researchers on English Language Teaching ( ELT ) have increasingly looked to concordancer of very large corpora as a new re-source for translation and language learning . \n\t', '\n\t\t Concordancers have been indispensable for lexicographers . \n\t', '\n\t\t But now language teachers and students also embrace the concordancer to foster data-driven , student-centered learning . \n\t', '\n\t\t A bilingual concordance , in a way , meets the needs of both communities , the computer assisted translation ( CAT ) and computer assisted language learning ( CALL ) . \n\t', '\n\t\t A bilingual concordancer is like a monolingual concordance , except that each sentence is followed by its translation counterpart in a second language . \n\t', '\n\t\t \x93Existing translations contain more solutions to more translation problems than any other existing resource.\x94 \n\t\t']",Positive
"['\n\t\t The same can be argued for language learning ; existing texts offer more answers for the learner than any teacher or reference work do . \n\t', '\n\t\t However , it is important to provide easy access for translators and learning writers alike to find the relevant and informative citations quickly . \n\t', '\n\t\t For instance , the English-French concordance system , TransSearch provides a familiar interface for the users \n\t\t']",Positive
"['\n\t\t The user type in the expression in question , a list of citations will come up and it is easy to scroll down until one finds translation that is useful much like using a search engine . \n\t', '\n\t\t TransSearch exploits sentence alignment techniques \n\t\t']",Positive
"['\n\t\t In this paper , we describe a bilingual concordancer which facilitate search and visualization with fine granularity . \n\t', '\n\t\t TotalRecall exploits subsentential and word alignment to provide a new kind of bilingual concordancer . \n\t', '\n\t\t Through the interactive interface and clustering of short subsentential bi-lingual citations , it helps translators and non-native speakers find ways to translate or express them-selves in a foreign language . \n\t', '\n\t\t 2 Aligning the corpus Central to TotalRecall is a bilingual corpus and a set of programs that provide the bilingual analyses to yield a translation memory database out of the bilingual corpus . \n\t', '\n\t\t Currently , we are working with A : Database selection B : English query C : Chinese query D : Number of items per page E : Normal view F : Clustered summary according to translation G : Order by counts or lengths H : Submit bottom I : Help file J : Page index K : English citation L : Chinese citation M : Date and title N : All citations in the cluster O : Full text context P : Side-by-side sentence alignment Figure 2 . \n\t', '\n\t\t The results of searching for \x93hard\x94 bilingual corpora from a Taiwan-based Sinorama Magazine and Official Records of Hong Kong Legislative Council . \n\t', '\n\t\t A large bilingual collection of Studio Classroom English lessons will be provided in the near future . \n\t', '\n\t\t That would allow us to offer bilingual texts in both translation directions and with different levels of difficulty . \n\t', '\n\t\t Currently , the articles from Sinorama seems to be quite usefully by its own , covering a wide range of topics , reflecting the personalities , places , and events in Taiwan for the past three decades . \n\t', '\n\t\t The concordance database is composed of bilingual sentence pairs , which are mutual translation . \n\t', '\n\t\t In addition , there are also tables to record additional information , including the source of each sentence pairs , metadata , and the information on phrase and word level alignment . \n\t', '\n\t\t With that additional information , TotalRecall provides various functions , including 1. viewing of the full text of the source with a simple click . \n\t', '\n\t\t 2. highlighted translation counterpart of the query word or phrase . \n\t', '\n\t\t 3. ranking that is pedagogically useful for translation and language learning . \n\t', '\n\t\t We are currently running an operational system with Sinorama Magazine articles and HK LEGCO records . \n\t', '\n\t\t These bilingual texts that go into TotalRecall must be rearranged and structured . \n\t', '\n\t\t We describe the main steps below : 2.1 Subsentential alignment While the length-based approach \n\t\t']",Negative
"['\n\t\t Also sentence alignment tends to produce pairs of a long Chinese sentence and several English sentences . \n\t', '\n\t\t Such pairs of mutual translation make it difficult for the user to read and grasp the answers embedded in the retrieved citations . \n\t', '\n\t\t We develop a new approach to aligning English and Mandarin texts at sub-sentential level in parallel corpora based on length and punctuation marks . \n\t', '\n\t\t The subsentential alignment starts with parsing each article from corpora and putting them into the database . \n\t', '\n\t\t Subsequently articles are segmented into subsentential segments . \n\t', '\n\t\t Finally , segments in the two languages which are mutual translation are aligned . \n\t', '\n\t\t Sentences and subsentenial phrases and clauses are broken up by various types of punctuation in the two languages . \n\t', '\n\t\t For fragments much shorter than sentences , the variances of length ratio are larger leading to unacceptably low precision rate for alignment . \n\t', '\n\t\t We combine length-based and punctuation-based approach to cope with the difficulties in subsentential alignment . \n\t', '\n\t\t Punctuations in one language translate more or less consistently into punctuations in the other language . \n\t', '\n\t\t Therefore the information is useful in compensating for the weakness of length-based approach . \n\t', '\n\t\t In addition , we seek to further improve the accuracy rates by employing cognates and lexical information . \n\t', '\n\t\t We experimented with an implementation of the pro-posed method on a very large Mandarin-English parallel corpus of records of Hong Kong Legislative Council with satisfactory results . \n\t', '\n\t\t Experiment results show that the punctuation-based approach outperforms the length-based approach with precision rates approaching 98 % . \n\t', '\n\t\t Subsentential alignment results From 1983 to 1991 , the average rate of wage growth for all trades and industries was only 1.6 % . \n\t', '\n\t\t 1.6 % This was far lower than the growth in labour productivity , which averaged 5.3 % . \n\t', '\n\t\t 5.3 % But , it must also be noted that the average inflation rate was as high as 7.7 % during the same period . \n\t', '\n\t\t ^^^^^^^^^^^^ 7.7 % As I have said before , even when the economy is booming , the workers are unable to share the fruit of economic success . \n\t', '\n\t\t Figure 1 The result of subsentential alignment and collocation alignment . \n\t', '\n\t\t 2.2 Word and Collocation Alignment After sentences and their translation counterparts are identified , we proceeded to carry out finer- grained alignment on the word level . \n\t', '\n\t\t We employed the Competitive Linking Algorithm \n\t\t']",Positive
"['\n\t\t We also extract English collocations and their translation equivalent based on the result of word alignment . \n\t', '\n\t\t These alignment results were subsequently used to cluster citations and highlight translation equivalents of the query . \n\t', '\n\t\t 3 Aligning the corpus TotalRecall allows a user to look for instances of specific words or expressions and its translation counterpart . \n\t', '\n\t\t For this purpose , the system opens up two text boxes for the user to enter queries in any or both of the two languages involved . \n\t', '\n\t\t We offer some special expressions for users to specify the following queries : \x95 Single or multi-word query \x96 spaces between words in a query are considered as \x93and.\x94 For disjunctive query , use \x93||\x94 to de-note \x93or.\x94 \x95 Every word in the query will be expanded to all surface forms for search . \n\t', '\n\t\t That includes singular and plural forms , and various tense of the verbs . \n\t', '\n\t\t \x95 TotalRecall automatically ignore high frequency words in a stoplist such as \x93the,\x94 \x93to,\x94 and \x93of.\x94 \x95 It is also possible to ask for exact match by submitting query in quotes . \n\t', '\n\t\t Any word within the quotes will not be ignored . \n\t', '\n\t\t It is useful for searching named entities . \n\t', '\n\t\t Once a query is submitted , TotalRecall displays the results on Web pages . \n\t', '\n\t\t Each result appears as a pair of segments in English and Chinese , in sideby-side format . \n\t', '\n\t\t A \x93context\x94 hypertext link is included for each citation . \n\t', '\n\t\t If this link is selected , a new page appears displaying the original document of the pair . \n\t', '\n\t\t If the user so wishes , she can scroll through the following or preceding pages of context in the original document . \n\t', '\n\t\t TotalRecall present the results in a way that makes it easy for the user to grasp the information returned to her : \x95 When operating in the monolingual mode , TotalRecall presents the citation according to lengths . \n\t', '\n\t\t \x95 When operating in the bilingual mode , TotalRecall clusters the citations according to the translation counterparts and presents the user with a summary page of one example each for different translations . \n\t', '\n\t\t The query words and translation counterparts are high-lighted . \n\t', '\n\t\t 4 Conclusion In this paper , we describe a bilingual concordance designed as a computer assisted translation and language learning tool . \n\t', '\n\t\t Currently , TotalRecll uses Sinorama Magazine and HKLEGCO corpora as the databases of translation memory . \n\t', '\n\t\t We have already put a beta version on line and experimented with a focus group of second language learners . \n\t', '\n\t\t Novel features of TotalRecall include highlighting of query and corresponding translations , clustering and ranking of search results according translation and frequency . \n\t', '\n\t\t TotalRecall enable the non-native speaker who is looking for a way to express an idea in English or Mandarin . \n\t', '\n\t\t We are also adding on the basic functions to include a log of user activities , which will record the users\x92 query behavior and their background . \n\t', '\n\t\t We could then analyze the data and find useful information for future research . \n\t', '\n\t\t Acknowledgement We acknowledge the support for this study through grants from National Science Council and Ministry of Education , Taiwan ( NSC 91-2213-E007-061 and MOE EX-92-E-FA06-4-4 ) and a special grant for preparing the Sinorama Corpus for distri-bution by the Association for Computational Lin-guistics and Chinese Language Processing . \n\t', '\n\t\t References Brown P. , Cocke J. , Della Pietra S. , Jelinek F. , Lafferty J. , Mercer R. , & Roossin P. ( 1990 ) . \n\t', '\n\t\t A statistical approach to machine translation . \n\t', '\n\t\t Computational Linguistics , vol. 16 . \n\t', '\n\t\t Gale , W. & K. W. Church , "" A Program for Aligning Sen-tences in Bilingual Corpora "" Proceedings of the 29th An-nual Meeting of the Association for Computational Linguistics , Berkeley , CA , 1991 . \n\t', '\n\t\t Isabelle , Pierre , M. Dymetman , G. Foster , J-M. Jutras , E. Macklovitch , F. Perrault , X. Ren and M. Simard . \n\t', '\n\t\t 1993. Translation Analysis and Translation Automation . \n\t', '\n\t\t In Pro-ceedings of the Fifth International Conference on Theoreti-cal and Methodological Issues in Machine Translation , Kyoto , Japan , pp. 12-20 . \n\t', '\n\t\t I. Dan Melamed . \n\t', '\n\t\t 2000. Models of translational equivalence among words . \n\t', '\n\t\t Computational Linguistics , 26(2):221\x96249 , June . \n\t', '\n\t\t Customizing Parallel Corpora at the Document Level Monica ROGATI and Yiming YANG Computer Science Department , Carnegie Mellon University 5000 Forbes Avenue Pittsburgh , PA 15213 mrogati@cs.cmu.edu , yiming@cs.cmu.edu Abstract Recent research in cross-lingual information retrieval ( CLIR ) established the need for properly matching the parallel corpus used for query translation to the target corpus . \n\t', '\n\t\t We propose a document-level approach to solving this problem : building a custom-made parallel corpus by automatically assembling it from documents taken from other parallel corpora . \n\t', '\n\t\t Although the general idea can be applied to any application that uses parallel corpora , we present results for CLIR in the medical domain . \n\t', '\n\t\t In order to extract the best- matched documents from several parallel corpora , we propose ranking individual documents by using a length-normalized Okapi-based similarity score between them and the target corpus . \n\t', '\n\t\t This ranking allows us to discard 50-90 % of the training data , while avoiding the performance drop caused by a good but mismatched resource , and even improving CLIR effectiveness by 4-7 % when compared to using all available training data . \n\t', '\n\t\t 1 Introduction Our recent research in cross-lingual information retrieval ( CLIR ) established the need for properly matching the parallel corpus used for query translation to the target corpus \n\t\t']",Positive
"['\n\t\t In particular , we showed that using a general purpose machine translation ( MT ) system such as SYSTRAN , or a general purpose parallel corpus - both of which perform very well for news stories \n\t\t']",Negative
"['\n\t\t To explore solutions to this problem , we used cosine similarity between training and target corpora as respective weights when building a translation model . \n\t', '\n\t\t This approach treats a parallel corpus as a homogeneous entity , an entity that is self-consistent in its domain and document quality . \n\t', '\n\t\t In this paper , we propose that instead of weighting entire resources , we can select individual documents from these corpora in order to build a parallel corpus that is tailor-made to fit a specific target collection . \n\t', '\n\t\t To avoid confusion , it is helpful to remember that in IR settings the true test data are the queries , not the target documents . \n\t', '\n\t\t The documents are available off-line and can be ( and usually are ) used for training and system development . \n\t', '\n\t\t In other words , by matching the training corpora and the target documents we are not using test data for training . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t We are not aware of any additional related work . \n\t', '\n\t\t In addition to proposing individual documents as the unit for building custom-made parallel corpora , in this paper we start exploring the criteria used for individual document selection by examining the effect of ranking documents using the length-normalized Okapi-based similarity score between them and the target corpus . \n\t', '\n\t\t 2 Evaluation Data 2.1 Medical Domain Corpus : Springer The Springer corpus consists of 9640 documents ( titles plus abstracts of medical journal articles ) each in English and in German , with 25 queries in both languages , and relevance judgments made by native German speakers who are medical experts and are fluent in English . \n\t', '\n\t\t We split this parallel corpus into two subsets , and used the first subset ( 4,688 documents ) for training , and the remaining subset ( 4,952 documents ) as the test set in all our experiments . \n\t', '\n\t\t This configuration allows us to experiment with CLIR in both directions ( EN-DE and DE-EN ) . \n\t', '\n\t\t We applied an alignment algorithm to the training documents , and obtained a sentence- aligned parallel corpus with about 30K sentences in each language . \n\t', '\n\t\t 2.2 Training Corpora In addition to Springer , we have used four other English-German parallel corpora for training : \x95 NEWS is a collection of 59K sentence aligned news stories , downloaded from the web ( 1996-2000 ) , and available at http://www.isi.edu/~koehn/publications/denews/ \x95 WAC is a small parallel corpus obtained by mining the web \n\t\t']",Positive
"['\n\t\t Its documents are sentence aligned European Parliament proceedings . \n\t', '\n\t\t This is a large collection that has been successfully used for CLEF , when the target corpora were collections of news stories \n\t\t']",Positive
"['\n\t\t \x95 MEDTITLE is an English-German parallel corpus consisting of 549K paired titles of medical journal articles . \n\t', '\n\t\t These titles were gathered from the PubMed online database ( http://www.ncbi.nlm.nih.gov/PubMed/ ) . \n\t', '\n\t\t Table 1 presents a summary of the five training corpora characteristics . \n\t', '\n\t\t Name Size ( sent ) Domain NEWS 59K news WAC 60K mixed EUROPAR 665K politics L SPRINGE 30K medical R MEDTITL 550K medical E Table 1 . \n\t', '\n\t\t Characteristics of Parallel Training Corpora 3 Selecting Documents from Parallel Corpora While selecting and weighing entire training corpora is a problem already explored by \n\t\t']",Positive
"['\n\t\t We seek to construct a custom parallel corpus , by choosing individual documents which best match the testing collection . \n\t', '\n\t\t We compute the similarity between the test collection ( in German or English ) and each individual document in the parallel corpora for that respective language . \n\t', '\n\t\t We have a choice of similarity metrics , but since this computation is simply retrieval with a long query , we start with the Okapi model \n\t\t']",Positive
"['\n\t\t Although the Okapi model takes into account average document length , we compare it with its length-normalized version , measuring per-word similarity . \n\t', '\n\t\t The two measures are identified in the results section by \x93Okapi\x94 and \x93Normalized\x94 . \n\t', '\n\t\t Once the similarity is computed for each document in the parallel corpora , only the top N most similar documents are kept for training . \n\t', '\n\t\t They are an approximation of the domain(s) of the test collection . \n\t', '\n\t\t Selecting N has not been an issue for this corpus ( values between 10-75 % were safe ) . \n\t', '\n\t\t However , more generally , this parameter can be tuned to a different test corpus as any other parameter . \n\t', '\n\t\t Alternatively , the document score can also be incorporated into the translation model , eliminating the need for thresholding . \n\t', '\n\t\t 4 CLIR Method We used a corpus-based approach , similar to that in \n\t\t']",Positive
"['\n\t\t Let L1 be the source language and L2 be the target language . \n\t', '\n\t\t The cross- lingual retrieval consists of the following steps : 1 . \n\t', '\n\t\t Expanding a query in L1 using blind feedback 2 . \n\t', '\n\t\t Translating the query by taking the dot product between the query vector ( with weights from step 1 ) and a translation matrix obtained by calculating translation probabilities or term-term similarity using the parallel corpus . \n\t', '\n\t\t 3. Expanding the query in L2 using blind feedback 4 . \n\t', '\n\t\t Retrieving documents in L2 Here , blind feedback is the process of retrieving documents and adding the terms of the top-ranking documents to the query for expansion . \n\t', '\n\t\t We used simplified Rocchio positive feedback as implemented by Lemur \n\t\t']",Positive
"['\n\t\t For the results in this paper , we have used Pointwise Mutual Information ( PMI ) instead of IBM Model 1 \n\t\t']",Positive
"['\n\t\t 5 Results and Discussion 5.1 Empirical Settings For the retrieval part of our system , we adapted Lemur \n\t\t']",Positive
"['\n\t\t Several parameters were tuned , none of them on the test set . \n\t', '\n\t\t In our corpus- based approach , the main parameters are those used in query expansion based on pseudo- relevance , i.e. , the maximum number of documents and the maximum number of words to be used , and the relative weight of the expanded portion with respect to the initial query . \n\t', '\n\t\t Since the Springer training set is fairly small , setting aside a subset of the data for parameter tuning was not desirable . \n\t', '\n\t\t We instead chose parameter values that were stable on the CLEF collection \n\t\t']",Positive
"['\n\t\t The relative weight of the expanded portion with respect to the initial query was set to 0.5 . \n\t', '\n\t\t The results were evaluated using mean average precision ( AvgP ) , a standard performance measure for IR evaluations . \n\t', '\n\t\t In the following sections , DE-EN refers to retrieval where the query is in German and the documents in English , while EN-DE refers to retrieval in the opposite direction . \n\t', '\n\t\t 5.2 Using the Parallel Corpora Separately Can we simply choose a parallel corpus that performed very well on news stories , hoping it is robust across domains ? \n\t', '\n\t\t Natural approaches also include choosing the largest corpus available , or using all corpora together . \n\t', '\n\t\t Figure 1 shows the effect of these strategies . \n\t', '\n\t\t Figure 1. CLIR results on the Springer test set by AvgP . \n\t', '\n\t\t EN-DE DE-EN using PMI with different training corpora . \n\t', '\n\t\t We notice that choosing the largest collection ( EUROPARL ) , using all resources available without weights ( ALL ) , and even choosing a large collection in the medical domain ( MEDTITLE ) are all sub-optimal strategies . \n\t', '\n\t\t Given these results , we believe that resource selection and weighting is necessary . \n\t', '\n\t\t Thoroughly exploring weighting strategies is beyond the scope of this paper and it would involve collection size , genre , and translation quality in addition to a measure of domain match . \n\t', '\n\t\t Here , we start by selecting individual documents that match the domain of the test collection . \n\t', '\n\t\t We examine the effect this choice has on domain-specific CLIR . \n\t', '\n\t\t 5.3 Using Okapi weights to build a custom parallel corpus Figures 2 and 3 compare the two document selection strategies discussed in Section 3 to using all available documents , and to the ideal ( but not truly optimal ) situation where there exists a \x93best\x94 resource to choose and this collection is known . \n\t', '\n\t\t By \x93best\x94 , we mean one that can produce optimal results on the test corpus , with respect to the given metric In reality , the true \x93best\x94 resource is unknown : as seen above , many intuitive choices for the best collection are not optimal . \n\t', '\n\t\t Figure 2. CLIR DE-EN performance vs. . \n\t', '\n\t\t Percent of Parallel Documents Used . \n\t', '\n\t\t \x93Best Corpus\x94 is given by an oracle and is usually unknown . \n\t', '\n\t\t Figure 3. CLIR EN-DE performance vs. . \n\t', '\n\t\t Percent of Parallel Documents Used . \n\t', '\n\t\t \x93Best Corpus\x94 is given by an oracle and is usually unknown SPRINGER MEDTITLE WAC NEWS EUROPARL ALL 60 55 50 45 40 1 10 100 Percent Used ( log ) Okapi Normalized All Corpora Best Corpus Okapi Normalized All Corpora Best Corpus 70 1 10 100 Percent Used ( log ) 65 60 55 50 70 60 50 40 30 20 10 0 Notice that the normalized version performs better and is more stable . \n\t', '\n\t\t Per-word similarity is , in this case , important when the documents are used to train translation scores : shorter parallel documents are better when building the translation matrix . \n\t', '\n\t\t Our strategy accounts for a 4-7 % improvement over using all resources with no weights , for both retrieval directions . \n\t', '\n\t\t It is also very close to the \x93oracle\x94 condition , which chooses the best collection in advance . \n\t', '\n\t\t More importantly , by using this strategy we are avoiding the sharp performance drop when using a mismatched , although very good , resource ( such as EUROPARL ) . \n\t', '\n\t\t 6 Future Work We are currently exploring weighting strategies involving collection size , genre , and estimating translation quality in addition to a measure of domain match . \n\t', '\n\t\t Another question we are examining is the granularity level used when selecting resources , such as selection at the document or cluster level . \n\t', '\n\t\t Similarity and overlap between resources themselves is also worth considering while exploring tradeoffs between redundancy and noise . \n\t', '\n\t\t We are also interested in how these approaches would apply to other domains . \n\t', '\n\t\t 7 Conclusions We have examined the issue of selecting appropriate training resources for cross-lingual information retrieval . \n\t', '\n\t\t We have proposed and evaluated a simple method for creating a customized parallel corpus from other available parallel corpora by matching the domain of the test documents with that of individual parallel documents . \n\t', '\n\t\t We noticed that choosing the largest collection , using all resources available without weights , and even choosing a large collection in the medical domain are all sub-optimal strategies . \n\t', '\n\t\t The techniques we have presented here are not restricted to CLIR and can be applied to other areas where parallel corpora are necessary , such as statistical machine translation . \n\t', '\n\t\t The trained translation matrix can also be reused and can be converted to any of the formats required by such applications . \n\t', '\n\t\t 8 Acknowledgements We would like to thank Ralf Brown for collecting the MEDTITLE and SPRINGER data . \n\t', '\n\t\t This research is sponsored in part by the National Science Foundation ( NSF ) under grant IIS9982226 , and in part by the DOD under award 114008-N66001992891808 . \n\t', '\n\t\t Any opinions and conclusions in this paper are the authors\x92 and do not necessarily reflect those of the sponsors . \n\t', '\n\t\t References Brown , P.F , Pietra , D. , Pietra , D , Mercer , R.L. 1993.The Mathematics of Statistical Machine Translation : Parameter Estimation . \n\t', '\n\t\t In Computational Linguistics , 19:263-312 Koehn , P. Europarl : A Multilingual Corpus for Evaluation of Machine Translation . \n\t', '\n\t\t Draft , Unpublished . \n\t', '\n\t\t Nie , J. Y. , Simard , M. and Foster , G .. 2000 . \n\t', '\n\t\t Using parallel web pages for multi-lingual IR . \n\t', '\n\t\t In C. Peters(Ed.) , Proceedings of the CLEF 2000 forum Ogilvie , P. and Callan , J. 2001 . \n\t', '\n\t\t Experiments using the Lemur toolkit . \n\t', '\n\t\t In Proceedings of the Tenth Text Retrieval Conference ( TREC-10 ) . \n\t', '\n\t\t Peters , C. 2003 . \n\t', '\n\t\t Results of the CLEF 2003 Cross-Language System Evaluation Campaign . \n\t', '\n\t\t Working Notes for the CLEF 2003 Workshop , 21-22 August , Trondheim , Norway Robertson , S.E. and all . \n\t', '\n\t\t 1993. Okapi at TREC . \n\t', '\n\t\t In The First TREC Retrieval Conference , Gaithersburg , MD . \n\t', '\n\t\t pp. 21-30 Rogati , M and Yang , Y. 2003 . \n\t', '\n\t\t Multilingual Information Retrieval using Open , Transparent Resources in CLEF 2003 . \n\t', '\n\t\t In C. Peters ( Ed . \n\t', '\n\t\t ) , Results of the CLEF2003 cross-language evaluation forum Rogati , M and Yang , Y. 2004 . \n\t', '\n\t\t Resource Selection for Domain Specific Cross-Lingual IR . \n\t', ""\n\t\t In Proceedings of ACM SIGIR Conference on Research and Development in Information Retrieval ( SIGIR'04 ) . \n\t"", '\n\t\t An Automatic Filter for Non-Parallel Texts Chris Pike Computer Science Department New York University 715 Broadway , 7th Floor New York , NY 10003 USA lastname @cs.nyu.edu I. Dan Melamed Computer Science Department New York University 715 Broadway , 7th Floor New York , NY 10013 USA lastname @cs.nyu.edu Abstract Numerous cross-lingual applications , including state-of-the-art machine translation systems , require parallel texts aligned at the sentence level . \n\t', '\n\t\t However , collections of such texts are often polluted by pairs of texts that are comparable but not parallel . \n\t', '\n\t\t Bitext maps can help to discriminate between parallel and comparable texts . \n\t', '\n\t\t Bitext mapping algorithms use a larger set of document features than competing approaches to this task , resulting in higher accuracy . \n\t', '\n\t\t In addition , good bitext mapping algorithms are not limited to documents with structural mark-up such as web pages . \n\t', '\n\t\t The task of filtering non-parallel text pairs represents a new application of bitext mapping algorithms . \n\t', '\n\t\t 1 Introduction In June 2003 , the U.S. government organized a \x93Surprise Language Exercise\x94 for the NLP community . \n\t', '\n\t\t The goal was to build the best possible language technologies for a \x93surprise\x94 language in just one month \n\t\t']",Positive
"['\n\t\t One of the main technologies pursued was machine translation ( MT ) . \n\t', '\n\t\t Statistical MT ( SMT ) systems were the most successful in this scenario , because their construction typically requires less time than other approaches . \n\t', '\n\t\t On the other hand , SMT systems require large quantities of parallel text as training data . \n\t', '\n\t\t A significant collection of parallel text was obtained for this purpose from multiple sources . \n\t', '\n\t\t SMT systems were built and tested ; results were reported . \n\t', '\n\t\t Much later we were surprised to discover that a significant portion of the training data was not parallel text ! \n\t', '\n\t\t Some of the document pairs were on the same topic but not translations of each other . \n\t', '\n\t\t For today\x92s sentence-based SMT systems , this kind of data is noise . \n\t', '\n\t\t How much better would the results have been if the noisy training data were automatically filtered out ? \n\t', '\n\t\t This question is becoming more important as SMT systems increase their reliance on automatically collected parallel texts . \n\t', '\n\t\t There is abundant literature on aligning parallel texts at the sentence level . \n\t', '\n\t\t To the best of our knowledge , all published methods happily misalign nonparallel inputs , without so much as a warning . \n\t', '\n\t\t There is also some recent work on distinguishing parallel texts from pairs of unrelated texts \n\t\t']",Positive
"['\n\t\t In this paper , we propose a solution to the more difficult problem of distinguishing parallel texts from texts that are comparable but not parallel . \n\t', '\n\t\t Definitions of \x93comparable texts\x94 vary in the literature . \n\t', '\n\t\t Here we adopt a definition that is most suitable for filtering SMT training data : Two texts are \x93comparable\x94 if they are not alignable at approximately the sentence level . \n\t', '\n\t\t This definition is also suitable for other applications of parallel texts , such as machine-assisted translation and computer- assisted foreign language learning . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t STRAND relies on mark-up within a document to reveal the document\x92s structure . \n\t', '\n\t\t STRAND then predicts that documents with the same structure are parallel . \n\t', '\n\t\t Tsim uses a machine-readable bilingual dictionary to find word-to-word matches between two halves of a bitext . \n\t', '\n\t\t It then computes a similarity score based on the maximum cardinality bipartite matching between the two halves . \n\t', '\n\t\t We chose to compare our method with tsim because we were interested in an approach that works with both marked up and plain text documents . \n\t', '\n\t\t 2 A Modification to SIMR Our work is based on a modification of the SIMR bitext mapping algorithm \n\t\t']",Positive
"['\n\t\t The SIMR algorithm attempts to construct a piecewise linear approximation to the True Bitext Map ( TBM ) of a bitext by greedily searching for small chains of points of correspondence . \n\t', '\n\t\t Each chain forms one section of the approximation . \n\t', '\n\t\t SIMR uses a two- phase approach to generating chains . \n\t', '\n\t\t First , it generates a set of potential points of correspondence within a search rectangle . \n\t', '\n\t\t Next , it searches the Figure 1 : On the left is part of a bitext map generated by SIMR for non-parallel texts . \n\t', '\n\t\t On the right is part of a bitext map for parallel texts . \n\t', '\n\t\t points of correspondence for chains whose points meet requirements for linearity , injectivity , and maximum angle deviation . \n\t', '\n\t\t If no such chain is found , the search rectangle is expanded and the search repeats . \n\t', '\n\t\t Our method of detecting translations is based on the premise that SIMR will find fewer points of correspondence in comparable texts than it will in parallel texts . \n\t', '\n\t\t This is because points of correspondence are more likely to occur in closely corresponding locations in the two halves of a bitext than in two documents that are merely comparable . \n\t', '\n\t\t Therefore , the bitext map of parallel texts will usually be much denser than the bitext map of comparable texts . \n\t', '\n\t\t Figure 1 above contrasts the bitext maps output by SIMR for non-parallel and parallel texts . \n\t', '\n\t\t To maximize the percentage of correctly classified document pairs , we need to maximize the difference between the map densities of parallel and comparable texts . \n\t', '\n\t\t SIMR\x92s built in restrictions on the chains it will accept severely limit the number of points of correspondence SIMR accepts from most non-parallel texts . \n\t', '\n\t\t Despite this SIMR still generated bitext maps for some non-parallel documents that had densities very close to the densities of parallel documents . \n\t', '\n\t\t Chains of spurious points tended to form over a longer section of the bitext than correct chains . \n\t', '\n\t\t Therefore we introduced an additional parameter that limited the length of chains that SIMR would accept . \n\t', '\n\t\t This modification of SIMR is called SIMR-cl . \n\t', '\n\t\t Chains are not perfectly linear . \n\t', '\n\t\t Therefore we cannot calculate chain length by simply taking the distance between the first and last points in the chain . \n\t', '\n\t\t Instead we find the smallest possible rectangle for which all points in the chain are interior points . \n\t', '\n\t\t We then calculate the length of the chain as the distance from the lower left corner to the upper right hand corner of the rectangle . \n\t', '\n\t\t When SIMR finds an acceptable chain the search rectangle is moved so that the point on the lower left is no longer included in the search . \n\t', '\n\t\t As a result , when SIMR is finding a large number of chains , the length of those chains will remain relatively short . \n\t', '\n\t\t Therefore , in parallel texts SIMR will find many chains and limiting the chain length will have a minimal effect on the number of chains SIMR will find . \n\t', '\n\t\t On a non-parallel text , however , SIMR will find fewer sets of points of correspondence meeting the criteria for a chain . \n\t', '\n\t\t The result is longer chains , which can be filtered by our new parameter . \n\t', '\n\t\t E.g. , the non-parallel bitext map in Figure 1 , which was created without the chain length parameter , has on average 630 characters between points . \n\t', '\n\t\t In contrast , running SIMR on the same pair of nonparallel documents with a maximum chain length of 700 yielded only 22 points of correspondence , or 3032 characters between points on average . \n\t', '\n\t\t 3 Training Training SIMR-cl , much like SIMR , requires a state space search algorithm , and an objective function to evaluate the current state . \n\t', '\n\t\t We chose to use simulated annealing to perform our state space search . \n\t', '\n\t\t The first step in training is to generate a set of parameter values that make up the current state . \n\t', '\n\t\t SIMR-cl uses the standard SIMR parameters plus the additional chain length parameter discussed above . \n\t', '\n\t\t Once the current state is set SIMR-cl generates a bitext map and calculates the density of the map . \n\t', '\n\t\t The bitext map density is defined as the number of points in the bitext map divided by the length of the main diagonal of the bitext space . \n\t', '\n\t\t We call this the SIMR-cl score . \n\t', '\n\t\t Our objective function seeks to drive the parameters to a state where we can select a single threshold value that will classify all candidate bitexts in the development set correctly . \n\t', '\n\t\t That is , all parallel texts should have a SIMR-cl score greater than the threshold , and all non-parallel texts should have a SIMR-cl score less than the threshold . \n\t', '\n\t\t We cannot achieve this by simply measuring the percentage of correctly classified candidate text pairs , because any given change to the parameters is not likely to change the classification of any candidate bitexts . \n\t', '\n\t\t In order to measure the amount of error we borrowed the concept of margin slack from the support vector machines literature . \n\t', '\n\t\t For simplicity we used a margin of zero , which reduces the margin slack of a SIMR-cl score to the difference between the threshold density , and the density of a misclassified candidate pair . \n\t', '\n\t\t Any correctly classified candidate pair is defined to have a margin slack of zero . \n\t', '\n\t\t From there we defined our objective as minimizing the sum of the margin slack of all candidate pairs . \n\t', '\n\t\t All that is left at this point is to select an optimal threshold . \n\t', '\n\t\t We performed a line search for the best possible threshold for each parameter set . \n\t', '\n\t\t 4 Experiments In our first two experiments we limited the points of correspondence to orthographic cognates . \n\t', '\n\t\t We used the Longest Common Subsequence Ratio ( LCSR ) to measure similarity \n\t\t']",Positive
"['\n\t\t The LCSR ratio is the length of the longest common subsequence of two tokens , divided by the length of the longer token . \n\t', '\n\t\t In our English-Hindi experiments we used an English-Hindi dictionary because the languages are written in different character sets , limiting the effectiveness of orthographic cognates . \n\t', '\n\t\t 4.1 STRAND data Before evaluating our approach on the more difficult task of discriminating parallel texts from comparable texts , we compared it to previous approaches on the easier task of discriminating parallel texts from unrelated texts . \n\t', '\n\t\t For this purpose , we used the STRAND corpus , which consists of 326 candidate bitexts in French and English1 \n\t\t']",Positive
"['\n\t\t As a precursor to generating a bitext map of a candidate pair we tokenized the STRAND documents and generated the axis files required by SIMR-cl . \n\t', '\n\t\t We attempted several schemes on training data and found that generating one token per HTML tag gave us the best results . \n\t', '\n\t\t While the end performance of the two approaches was comparable , we did find that tsim had an advantage over SIMR-cl in training . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t We repeated their experiments using 1/4 of the available candidate pairs for training and found no improvement , indicating that tsim can be optimally trained using a small development set . \n\t', '\n\t\t By contrast , using 32 training instances , SIMRcl achieved only 86 % agreement with the human judges , compared to tsim\x92s 96 % . \n\t', '\n\t\t When trained with 1/4 of the candidate pairs , SIMR-cl achieved 96 % accuracy . \n\t', '\n\t\t 4.2 Filtering of Comparable Texts We were unable to find a suitable corpus containing both parallel and comparable texts . \n\t', '\n\t\t Expert opinion suggests that no such corpora are publicly available 2 . \n\t', '\n\t\t Therefore we proceeded by simulation . \n\t', '\n\t\t We constructed 3 sets of two corpora from the Romanian/English Multext-East 1984 corpus ( Tufis , 1We removed all document pairs which were not in French/English . \n\t', '\n\t\t 2Doug Oard , personal cummunication , 2004. text length 164 820 1640 tsim 66 % 66 % 66 % SIMR-cl 90 % 96.5 % 98.5 % Table 1 : Percentage of documents correctly classified by tsim and SIMR-cl on parallel and comparable corpora with texts of varying lengths , by average number of words in the English text . \n\t', '\n\t\t 1999 ) . \n\t', '\n\t\t We constructed parallel texts by breaking the corpus into aligned chunks of 10 , 50 , and 100 segments . \n\t', '\n\t\t We then simulated comparable texts by pairing non-aligned , consecutive chunks of the same length . \n\t', '\n\t\t We chose to use consecutive chunks because there is a better chance for overlap between words in adjacent segments than in segments far apart . \n\t', '\n\t\t After breaking the corpus into chunks , 1/3 of the chunks were used as a training set and the remaining 2/3 were used as a test set . \n\t', '\n\t\t We had 63 training and 130 test pairs of size 100 , 126 training and 259 test pairs of size 50 , and 642 training and 1285 test pairs of size 10 . \n\t', '\n\t\t On average each English segment was 16 words in length . \n\t', '\n\t\t Since a Romanian/English bilingual dictionary was not readily available , we created a dictionary for tsim by searching all aligned segments for cognates . \n\t', '\n\t\t We then performed the same optimization process for tsim and SIMR-cl using documents containing 10 , 50 , and 100 segments . \n\t', '\n\t\t After performing our optimizations , we found that the LCSR parameters optimized for tsim generated a dictionary containing 3380 pairs . \n\t', '\n\t\t Using this parameter set , tsim correctly classified 66 % of the documents in the 1984 corpus . \n\t', '\n\t\t The accuracy was the same for all bitext lengths . \n\t', '\n\t\t Much like tsim , we found that for SIMR-cl the optimal parameter set was independent of the length of the bitexts being compared . \n\t', '\n\t\t SIMR-cl did however perform better on longer texts . \n\t', '\n\t\t Regardless , SIMR-cl outperformed tsim on all text lengths , as shown in table 1 . \n\t', '\n\t\t 4.3 The Surprise Language Data Encouraged by our success on French/English and on Romanian/English , we applied our method to the Hindi/English data used during the surprise language exercise . \n\t', '\n\t\t We did not have Hindi/English bitexts that were reliably classified as parallel or not , so we could not optimize SIMR-cl\x92s parameters specifically for this language pair . \n\t', '\n\t\t However , we were interested in determining how sensitive the parameters were to changes in the input language pair and text genre . \n\t', '\n\t\t So we simply reused the param- eters that were found to be optimal on the Romanian/English 1984 corpus . \n\t', '\n\t\t With these parameters , we ran SIMR-cl on just over half of the Hindi/English collection , the part that was collected from Indian government web pages . \n\t', '\n\t\t Our method classified 6 of the document pairs as non-parallel . \n\t', '\n\t\t Some of these 6 document pairs were relatively long , together they accounted for 7 % of the English word count in this part of the collection . \n\t', '\n\t\t We asked a Hindi speaker to compare the Hindi and English text in each of these 6 document pairs . \n\t', '\n\t\t For each text pair , we asked our informant : 1 . \n\t', '\n\t\t Do the texts express the same ideas ? \n\t', '\n\t\t If yes , was one of the texts probably written as a translation of the other ? \n\t', '\n\t\t If yes , was the translation done roughly at the sentence level ? \n\t', '\n\t\t The informant decided that in all 6 cases , the pair of texts expressed the same ideas . \n\t', '\n\t\t However in 4 of the pairs , the two texts were probably written independently , rather than one as a translation of the other . \n\t', '\n\t\t In the remaining two texts , the informant found large omissions on the English side , larger than what typical alignment algorithms can handle . \n\t', '\n\t\t In these latter two documents , our Hindi informant also discovered an interesting phenomenon that we were not expecting \x97 the sections that were translated were summarized to some degree . \n\t', '\n\t\t I.e. , even in sections where the order of ideas was largely the same in the two languages , the English wording was much more terse ( the informant said \x94compressed\x94 ) , and omitted many details . \n\t', '\n\t\t In summary , our method achieved 100 % precision in filtering out document pairs that were comparable but not parallel . \n\t', '\n\t\t We then asked our informant to examine 3 document pairs that our method accepted as parallel . \n\t', '\n\t\t After a cursory inspection , the informant answered yes to all 3 questions above for each of these pairs . \n\t', '\n\t\t Unfortunately , it would have been very time-consuming to evaluate recall rigourously , because it would entail exhaustive reading of pairs of documents in parallel , to ensure that there were no non-parallel segments . \n\t', '\n\t\t 5 Conclusions We have shown that SIMR-cl , a modified version of the SIMR bitext mapping algorithm , can reliably discriminate between parallel and comparable texts . \n\t', '\n\t\t We have demonstrated that SIMR-cl is effective on three language pairs , including two where no bilingual dictionary was available . \n\t', '\n\t\t In addition , we have presented tentative evidence that the parameters of SIMR-cl are not very sensitive to particular language pairs or text genres on this task . \n\t', '\n\t\t Our results suggest several new avenues for future research . \n\t', '\n\t\t First , it would be useful to combine our method for filtering out non-parallel texts with methods for detecting omissions in translations \n\t\t']",Positive
"['\n\t\t Some of the translations found on the web today might be made more literal by deleting the untranslated parts . \n\t', '\n\t\t Second , we seem to have discovered the existence of training data for a machine learning approach to translation with summarization . \n\t', '\n\t\t Third , our results suggest that the density of a bitext map is highly correlated with its accuracy , and that this correlation is largely invariant across language pairs and text genres . \n\t', '\n\t\t If this is true , then it should be possible to train bitext mapping algorithms without any hand-aligned training data , by using map density as the objective function instead of RMS error . \n\t', '\n\t\t Acknowledgements Thanks to Philip Resnik and Noah Smith for sharing STRAND data , human judgements , and tsim scores . \n\t', '\n\t\t Thanks also to Noah Smith for providing a tsim implementation . \n\t', '\n\t\t This research was sponsored by the DARPA TIDES program , by an NSF CAREER award , and by an equipment gift from Sun Microsystems . \n\t', '\n\t\t References I. Dan Melamed . \n\t', '\n\t\t 1995. Automatic evaluation and uniform filter cascades for inducing n-best translation lexicons . \n\t', '\n\t\t In Proceedings of the 3rd ACL Workshop on Very Large Corpora ( WVLC ) , Cambridge , Massachusetts . \n\t', '\n\t\t I. Dan Melamed . \n\t', '\n\t\t 1996. Automatic detection of omissions in translations . \n\t', '\n\t\t In Proceedings of the International Conference on Computational Linguistics ( COLING ) 1996 , pages 764\x96769 , Copenhagen , Denmark , August . \n\t', '\n\t\t I. Dan Melamed . \n\t', '\n\t\t 1999. Bitext maps and alignment via pattern recognition . \n\t', '\n\t\t Computational Linguistics , 25(1):107\x96139 , March . \n\t', '\n\t\t D. Oard . \n\t', '\n\t\t 2003. The surprise language excercises . \n\t', '\n\t\t In ACM Transactions on Asian Language Information Processing ( TALIP ) , pages 79\x9684 , New York , NY , June . \n\t', '\n\t\t P. Resnik and N. A. Smith . \n\t', '\n\t\t 2003. The web as a parallel corpus . \n\t', '\n\t\t Computational Linguistics , pages 349\x96380 , September . \n\t', '\n\t\t D. Tufis . \n\t', '\n\t\t 1999. Multext-east 1984 corpus . \n\t', '\n\t\t http://nl.ijs.si/ME/ . \n\t', '\n\t\t Exploiting Aggregate Properties of Bilingual Dictionaries For Distinguishing Senses of English Words and Inducing English Sense Clusters Charles SCHAFER and David YAROWSKY Department of Computer Science and Center for Language and Speech Processing Johns Hopkins University Baltimore , MD , 21218 , USA {cschafer,yarowsky}@cs.jhu.edu Abstract We propose a novel method for inducing monolingual semantic hierarchies and sense clusters from numerous foreign-language-to-English bilingual dictionaries . \n\t', '\n\t\t The method exploits patterns of non-transitivity in translations across multiple languages . \n\t', '\n\t\t No complex or hierarchical structure is assumed or used in the input dictionaries : each is initially parsed into the \x93lowest common denominator\x94 form , which is to say , a list of pairs of the form ( foreign word , English word ) . \n\t', '\n\t\t We then propose a monolingual synonymy measure derived from this aggregate resource , which is used to derive multilinguallymotivated sense hierarchies for monolingual English words , with potential applications in word sense classification , lexicography and statistical machine translation . \n\t', '\n\t\t 1 Introduction In this work we consider a learning resource comprising over 80 foreign-language-to-English bilingual dictionaries , collected by downloading electronic dictionaries from the Internet and also scanning and running optical character recognition ( OCR ) software on paper dictionaries . \n\t', '\n\t\t Such a diverse parallel lexical data set has not , to our knowledge , previously been assembled and examined in its aggregate form as a lexical semantics training resource . \n\t', '\n\t\t We show that this aggregate data set admits of some surprising applications , including discovery of synonymy relationships between words and automatic induction of high-quality hierarchical word sense clusterings for English . \n\t', '\n\t\t We perform and describe several experiments deriving synonyms and sense groupings from the aggregate bilingual dictionary , and subsequently suggest some possible applications for the results . \n\t', '\n\t\t Finally , we propose that sense taxonomies of the kind introduced here , being of different provenance from those produced explicitly by lexicographers or using unsupervised corpus-driven methods , have significant value because they add diversity to the set of available resources . \n\t', '\n\t\t 2 Resources First we collected , from Internet sources and via scanning and running OCR on print dictionaries , 82 dictionaries between English and a total of 44 distinct foreign languages from a variety of language families . \n\t', '\n\t\t Over 213K distinct English word types were present in a total of 5.5M bilingual dictionary entries , for an av- Figure 1 : Detecting asynonymy via unbalanced synonymy relationships among 3 words . \n\t', '\n\t\t The derived synonymy relation S holds between fair and blond , and between fair and just . \n\t', '\n\t\t S does not hold between blond and fair . \n\t', '\n\t\t We can infer thatfair has at least 2 senses and , further , we can represent them by blond and just . \n\t', '\n\t\t English French Spanish German fair blond , juste blondo , blond , gerecht licito , recto blond blond blondo blond just juste licito ; recto gerecht Figure 2 : This excerpt from the data set illustrates the kind of support the aggregate bilingual dictionary provides for partitioning the meanings offair into distinct senses : blond and just . \n\t', '\n\t\t erage of 26 and a median of 3 foreign entries per English word . \n\t', '\n\t\t Roughly 15K English words had at least 100 foreign entries ; over 64K had at least 10 entries . \n\t', '\n\t\t No complex or hierarchical structure was assumed or used in our input dictionaries . \n\t', '\n\t\t Each was initially parsed into the \x93lowest common denominator\x94 form . \n\t', '\n\t\t This consisted of a list of pairs of the form ( foreign word , English word ) . \n\t', '\n\t\t Because bilingual dictionary structure varies widely , and even the availability and compatibility of part-of-speech tags for entries is uncertain , we made the decision to compile the aggregate resource only with data that could be extracted from every individual dictionary into a universally compatible format . \n\t', '\n\t\t The unique pairs extracted from each dictionary were then converted to 4- tuples of the form : <foreign language , dictionary name , foreign word , English word> before being inserted into the final , combined dictionary data set . \n\t', '\n\t\t 3 A Synonymy Relation We began by using the above-described data set to obtain a synonymy relation between English words . \n\t', '\n\t\t In general , in a paper bilingual dictionary , each for- fair blond and just S S are synonymous with differing senses of blond just S fair eign word can be associated with a list of English words which are possible translations ; in our reduced format each entry lists a single foreign word and single possible English translation , though taking a union of all English translations for a particular foreign word recreates this list . \n\t', '\n\t\t We use the notion of coentry to build the synonymy relation between English words . \n\t', '\n\t\t The per-entry coentry count Cper\x97entry(e1,e2) for two English words e1 and e2 is simply the number of times e1 and e2 both appear as the translation of the same foreign word ( over all foreign words , dictionaries and languages ) . \n\t', '\n\t\t The per-dictionary coentry count Cper\x97dict(e1,e2) , ignores the number of individual coentries within a particular dictionary and merely counts as 1 any number of coentries inside a particular dictionary . \n\t', '\n\t\t Finally , per-language coentry count Cper\x97lang(e1,e2) counts as 1 any number of coentries for e1 and e2 for a particular language . \n\t', '\n\t\t Thus , for the following snippet from the database : Eng . \n\t', '\n\t\t Wd . \n\t', '\n\t\t Foreign Wd . \n\t', '\n\t\t Foreign Language Dict . \n\t', '\n\t\t ID hit schlagen schlagen GERMAN GERMAN ger.dict1 ger.dict1 pound hit schlag schlag GERMAN GERMAN ger.dict1 ger.dict1 pound hit schlag schlag GERMAN GERMAN ger.dict2 ger.dict2 pound hit battere battere ITAL ITAL ital.dict1 ital.dict1 pound Cper\x97entry(hit,pound) = 4 , while Cper\x97dict(hit,pound) = 3 , since the two individual coentries in ger.dict1 are only counted once . \n\t', '\n\t\t Cper\x97lang(hit,pound) = 2 ; hit and pound are coentries in the Italian and German languages . \n\t', '\n\t\t We found the more conservative per-dictionary and per-language counts to be a useful device , given that some dictionary creators appear sometimes to copy and paste identical synonym sets in a fairly indiscriminate fashion , spuriously inflating the Cper\x97entry(e1,e2) counts . \n\t', '\n\t\t Our algorithm for identifying synonyms was simple : we sorted all pairs of English words by decreasing Cper\x97dict(e1,e2) and , after inspection of the resulting list , cut it off at a per-dictionary and per-language count threshold1 yielding qualitatively strong results . \n\t', '\n\t\t For all word pairs e1,e2 above threshold , we say the symmetric synonymy relation S(e1,e2) holds . \n\t', '\n\t\t The following tables provide a clarifying example showing how synonymy can be inferred from multiple bilingual dictionaries in a way which is impossible with a single such dictionary ( because of idiosyncratic foreign language polysemy ) . \n\t', '\n\t\t Lang . \n\t', '\n\t\t Dict . \n\t', '\n\t\t ID Foreign Wd English Translations GERMAN ger.dict1 absetzen deposit drop deduct sell GERMAN ger.dict1 ablagerung deposit sediment settlement The table above displays entries from one German-English dictionary . \n\t', '\n\t\t How can we tell that \x93sediment\x94 is a better synonym for \x93deposit\x94 than \x93sell\x94 ? \n\t', '\n\t\t We can build and examine the 1 The threshold was 10 and 5 respectively for per-dictionary and per- language coentry counts . \n\t', '\n\t\t coentry counts Cper\x97lang(deposit,sediment) and Cper\x97lang(deposit,sell) using dictionaries from many languages , as illustrated below : FRENCH fre.dict1 d´ep\x88ot arsenal deposit depository depot entrusting filing sludge store trust submission repository scale sediment TURKISH tk.dict1 tortu sediment deposit faeces remainder dregs crust CZECH cz.dict1 sedlina clot deposit sediment warp Polysemy which is specific to German \x96 \x93deposit\x94 and \x93sell\x94 senses coexisting in a particular word form \x93absetzen\x94 \x96 will result in total coentry counts Cper\x97lang(deposit,sell) , over all languages and dictionaries , which are low . \n\t', '\n\t\t In fact , \x93deposit\x94 and \x93sell\x94 are coentries under only 2 out of 44 languages in our database ( German and Swedish , which are closely related ) . \n\t', '\n\t\t On the other hand , near-synonymous English translations of a particular sense across a variety of languages will result in high coentry counts , as is the case with Cper\x97lang(deposit,sediment) . \n\t', '\n\t\t As illustrated in the tables , German , French , Czech and Turkish all support the synonymy hypothesis for this pair of English words . \n\t', '\n\t\t \x93deposit\x94 Coentries Per Entry Per Dict . \n\t', '\n\t\t Per Lang . \n\t', '\n\t\t sell 4 4 2 sediment 68 40 18 The above table , listing the various coentry counts for \x93deposit\x94 , demonstrates the empirical motivation in the aggregate dictionary for the synonymy relationship between deposit and sediment , while the aggregate evidence of synonymy between deposit and sell is weak , limited to 2 languages , and is most likely the result of a word polysemy restricted to a few Germanic languages . \n\t', '\n\t\t 4 Different Senses : Asymmetries of Synonymy Relations After constructing the empirically derived synonymy relation S described in the previous section , we observed that one can draw conclusions from the topology of the graph of S relationships ( edges ) among words ( vertices ) . \n\t', '\n\t\t Specifically , consider the case of three words e1,e2 , e3 for which S(e1,e2) and S(e1,e3) hold , but S(e2,e3) does not . \n\t', '\n\t\t Figure 1 illustrates this situation with an example from data ( e1 = \x93fair\x94 ) , and more examples are listed in Table 1 . \n\t', '\n\t\t As Figure 1 suggests and inspection of the random extracts presented in Table 1 will confirm , this topology can be interpreted as indicating that e2 and e3 exemplify differing senses of e1 . \n\t', '\n\t\t We decided to investigate and apply it with more generality . \n\t', '\n\t\t This will be discussed in the next section . \n\t', '\n\t\t 5 Inducing Sense Taxonomies : Clustering with Synonym Similarity With the goal of using the aggregate bilingual dictionary to induce interesting and useful sense distinctions of English words , we investigated the following strategy . \n\t', '\n\t\t chart-topper/recording/hit single sense . \n\t', '\n\t\t The following table also illustrates the clarity with which major sense distinctions are reflected in the aggregate dictionary . \n\t', '\n\t\t The induced clustering for strike ( tree as well as flat cluster boundaries ) is presented in Figure 4. attack bang hit knock walkout find attack -4 18 7 0 0 bang - 38 43 2 0 0 hit - 44 2 29 knock - 2 0 walkout - 0 find - We used the CLUTO clustering toolkit \n\t\t']",Positive
"['\n\t\t Example results for vital and strike are in Figures 3 and 4 respectively4 . \n\t', '\n\t\t Figure 4 also presents flat clusters automatically derived from the tree , as well as a listing of some foreign words associated with particular clusters . \n\t', '\n\t\t Figure 3 : Induced sense hierarchy for the word \x93vital\x94 6 Related Work There is a distinguished history of research extracting lexical semantic relationships from bilingual dictionaries \n\t\t']",Positive
['\n\t\t There is also a longstanding goal of mapping translations and senses in multiple languages in a linked ontology structure \n\t\t'],Positive
['\n\t\t The recent work of \n\t\t'],Positive
"['\n\t\t The current paper can be distinguished on a number of dimensions , including our much greater range of participating languages , and the fundamental algorithmic linkage between multilingual translation distributions and monolingual synonymy clusters . \n\t', '\n\t\t 4In both \x93vital\x94 and \x93strike\x94 examples , the rendered hierarchical clusterings were pruned ( automatically ) in order to fit in this paper . \n\t', '\n\t\t yet syn2 ( W ) W still syn1 ( W ) quiet want tender desire delicate lack offer hide kind conceal nice skin sort crack charge clear wrong decline cast run fabric fair base charge strain assault filter flow cloth blond foundation deny hurl bright harm crackle impeach stretch manage load structure just ignoble fall mould open incorrect fissure load keen rough enthusiastic coarse sharp difficult form paint stain mold lean cast fast fling firm speedy mildew raise figure fashion incline arouse digit dye spot meagre increase shape picture tincture cast call shape claim toss shout ground fellow earth associate groundwork guy stop arrest plug Table 1 : A representative sampling of high-confidence sense distinctions derived via unbalanced synonymy relationships among three words , W and two of its synonyms syn1(W) & syn2(W) , such that Cper\x97dict(W,syn1(W)) and Cper\x97dict(W,syn2(W)) are high , whereas Cper\x97dict(syn1(W),syn2(W)) is low ( 0 ) . \n\t', '\n\t\t Ex- tracted from nax~list sorted by descending Cpnexr~\x97dict(nWsyn1(W)) * Cper\x97dict(W syn2(W)) / Cper\x97dict(syn1(W),syn2(W)) ( counts were smoothed to prevent division by zero ) . \n\t', '\n\t\t For each target word Wt in English having a sufficiently high dictionary occurrence count to allow interesting results2 , a list of likely synonym words W3 was induced by the method described in Section 33 . \n\t', '\n\t\t Additionally , we generated a list of all words Wc having non- zero Cper\x97dict(Wt,Wc) . \n\t', '\n\t\t The synonym words W3 \x96 the sense exemplars for target words Wt \x96 were clustered based on vectors of coentry counts Cper\x97dict(W3,Wc) . \n\t', '\n\t\t This restriction on vector dimension to only words that have nonzero co- entries with the target word helps to exclude distractions such as coentries of W3 corresponding to a sense which doesn\x92t overlap with Wt . \n\t', '\n\t\t The example given in the following table shows an excerpt of the vectors for synonyms of strike . \n\t', '\n\t\t The hit synonym overlaps strike in the beat/bang/knock sense . \n\t', '\n\t\t Restricting the vector dimension as described will help prevent noise from hit\x92s common 2For our experiments , English words occurring in at least 15 distinct source dictionaries were considered . \n\t', '\n\t\t 3Again , the threshold for synonyms was 10 and 5 respectively for per-dictionary and per-language coentry counts . \n\t', '\n\t\t Figure 4 : Induced sense hierarchy for the word \x93strike\x94 and some translations of individual \x93strike\x94 synonyms . \n\t', '\n\t\t Flat clusters automatically derived from the tree are denoted by the horizontal lines . \n\t', '\n\t\t 7 Analysis and Conclusions This is the first presentation of a novel method for the induction of word sense inventories , which makes use of aggregate information from a large collection of bilingual dictionaries . \n\t', '\n\t\t One possible application of the induced sense inventories presented here is as an aid to manual construction of monolingual dictionaries or thesauri , motivated by translation distinctions across numerous world languages . \n\t', '\n\t\t While the desired granularity of sense distinction will vary according to the requirements of taste and differing applications , treating our output as a proposal to be assessed and manually modified would be a valuable labor-saving tool for lexicographers . \n\t', '\n\t\t Another application of this work is a supplemental resource for statistical machine translation ( SMT ) . \n\t', '\n\t\t It is possible , as shown graphically in Figure 4 , to recover the foreign words associated with a cluster ( not just a single word ) . \n\t', '\n\t\t Given that the clusters provide a more complete coverage of English word types for a given sense than the English side of a particular bilingual dictionary , clusters could be used to unify bitext co- occurrence counts of foreign words with English senses in a way that typical bilingual dictionaries cannot . \n\t', '\n\t\t Unifying counts in this way would be a useful way of reducing data sparsity in SMT training . \n\t', '\n\t\t Finally , evaluation of induced sense taxonomies is always problematic . \n\t', '\n\t\t First of all , there is no agreed \x93correct\x94 way to classify the possible senses of a particular word . \n\t', '\n\t\t To some degree this is because human experts disagree on particular judgments of classification , though a larger issue , as pointed out in Resnik and Yarowsky 1997 , is that what constitutes an appropriate set of sense distinctions for a word is , emphatically , a function of the task at hand . \n\t', '\n\t\t The sense-distinction requirements of English-to-French machine translation differ from those of English-to-Arabic machine translation ( due to differing degrees of parallel polysemy across the language pairs ) , and both differ from those of English dictionary construction . \n\t', '\n\t\t We believe that the translingually-motivated word-sense taxonomies developed here will prove useful for the a variety of tasks including those mentioned above . \n\t', '\n\t\t The fact that they are derived from a novel resource , not constructed explicitly by humans or derived in fully unsupervised fashion from text corpora , makes them worthy of study and incorporation in future lexicographic , machine translation , and word sense disambiguation efforts . \n\t', '\n\t\t References J. Chen and J. Chang . \n\t', '\n\t\t 1998. Topical Clustering of MRD Senses Based on Information Retrieval Techniques . \n\t', '\n\t\t Computational Linguistic , 29(2):61-95 . \n\t', '\n\t\t A. Copestake , E. Briscoe , P. Vossen , A. Ageno , I. Castellan , F. Ribas , G. Rigau , H. Rodriguez and A. Samiotou . \n\t', '\n\t\t 1995. Acquisition of Lexical Translation Relations from MRDs . \n\t', '\n\t\t Machine Translation : Special Issue on the Lexicon , 9(3):33-69 . \n\t', '\n\t\t G. Karypis . \n\t', '\n\t\t 2002. CLUTO : A Clustering Toolkit . \n\t', '\n\t\t Tech Report 02-017 , Dept. of Computer Science , University ofMinnesota . \n\t', '\n\t\t Available at http://www.cs.umn.edu\x98cluto S. Ploux and H. Ji . \n\t', '\n\t\t 2003. A Model for Matching Semantic Maps Between Languages ( French/English , English/French ) . \n\t', '\n\t\t Computational Linguistics , 29(2):155- 178 . \n\t', '\n\t\t P. Resnik and D. Yarowsky . \n\t', '\n\t\t 1997. A Perspective on Word Sense Disambiguation Methods and Their Evaluation . \n\t', '\n\t\t In Proceedings of SIGLEX-1997 , pp. 79-86 . \n\t', '\n\t\t O. Risk . \n\t', '\n\t\t 1989. Sense Disambiguation of Word Translations in Bilingual Dictionaries : Trying to Solve The Mapping Problem Automatically . \n\t', '\n\t\t RC 14666 , IBM T.J. Watson Research Center . \n\t', '\n\t\t Yorktown Heights . \n\t', '\n\t\t P. Vossen ( ed . \n\t', '\n\t\t 1998. EUROWORDNET : A Multilingual Database with Lexical Semantic Networks . \n\t', '\n\t\t Kluwer Academic Publishers . \n\t', '\n\t\t Dordrecht , The Netherlands . \n\t', '\n\t\t Interactive grammar development with WCDG Kilian A. Foth Michael Daum Wolfgang Menzel Natural Language Systems Group Hamburg University D-22527 Hamburg Germany {foth,micha,menzel}@nats.informatik.uni-hamburg.de Abstract The manual design of grammars for accurate natural language analysis is an iterative process ; while modelling decisions usually determine parser behaviour , evidence from analysing more or different input can suggest unforeseen regularities , which leads to a reformulation of rules , or even to a different model of previously analysed phenomena . \n\t', '\n\t\t We describe an implementation of Weighted Constraint Dependency Grammar that supports the grammar writer by providing display , automatic analysis , and diagnosis of dependency analyses and allows the direct exploration of alternative analyses and their status under the current grammar . \n\t', '\n\t\t 1 Introduction For parsing real-life natural language reliably , a grammar is required that covers most syntactic structures , but can also process input even if it contains phenomena that the grammar writer has not foreseen . \n\t', '\n\t\t Two fundamentally different ways of reaching this goal have been employed various times . \n\t', '\n\t\t One is to induce a probability model of the target language from a corpus of existing analyses and then compute the most probable structure for new input , i.e. the one that under some judiciously chosen measure is most similar to the previously seen structures . \n\t', '\n\t\t The other way is to gather linguistically motivated general rules and write a parsing system that can only create structures adhering to these rules . \n\t', '\n\t\t Where an automatically induced grammar requires large amounts of training material and the development focuses on global changes to the probability model , a handwritten grammar could in principle be developed without any corpus at all , but considerable effort is needed to find and formulate the individual rules . \n\t', '\n\t\t If the formalism allows the ranking of grammar rules , their relative importance must also be determined . \n\t', '\n\t\t This work is usually much more cyclical in character ; after grammar rules have been changed , intended and unforeseen consequences of the change must be checked , and further changes or entirely new rules are suggested by the results . \n\t', '\n\t\t We present a tool that allows a grammar writer to develop and refine rules for natural language , parse new input , or annotate corpora , all in the same environment . \n\t', '\n\t\t Particular support is available for interactive grammar development ; the effect of individual grammar rules is directly displayed , and the system explicitly explains its parsing decisions in terms of the rules written by the developer . \n\t', '\n\t\t 2 The WCDG parsing system The WCDG formalism ( Schr¨oder , 2002 ) describes natural language exclusively as dependency structure , i.e. ordered , labelled pairs of words in the input text . \n\t', '\n\t\t It performs natural language analysis under the paradigm of constraint optimization , where the analysis that best conforms to all rules of the grammar is returned . \n\t', '\n\t\t The rules are explicit descriptions of well-formed tree structures , allowing a modular and fine-grained description of grammatical knowledge . \n\t', '\n\t\t For instance , rules in a grammar of English would state that subjects normally precede the finite verb and objects follow it , while temporal NP can either precede or follow it . \n\t', '\n\t\t In general , these constraints are defeasible , since many rules about language are not absolute , but can be preempted by more important rules . \n\t', '\n\t\t The strength of constraining information is controlled by the grammar writer : fundamental rules that must always hold , principles of different import that have to be weighed against each other , and general preferences that only take effect when no other disambiguating knowledge is available can all be formulated in a uniform way . \n\t', '\n\t\t In some cases preferences can also be used for disambiguation by approximating information that is currently not available to the system ( e.g. knowledge on attachment preferences ) . \n\t', '\n\t\t Even the very weak preferences have an influence on the parsing process ; apart from serving as tiebreakers for structures where little context is available ( e.g. with fragmentary input ) , they provide an Figure 1 : Display of a simplified feature hierarchy initial direction for the constraint optimization process even if they are eventually overruled . \n\t', '\n\t\t As a consequence , even the best structure found usually incurs some minor constraint violations ; as long as the combined evidence of these default expectation failures is small , the structure can be regarded as perfectly grammatical . \n\t', '\n\t\t The mechanism of constraint optimization simultaneously achieves robustness against extra- grammatical and ungrammatical input . \n\t', '\n\t\t Therefore WCDG allows for broad-coverage parsing with high accuracy ; it is possible to write a grammar that is guaranteed to allow at least one structure for any kind of input , while still preferring compliant over deviant input wherever possible . \n\t', '\n\t\t This graceful degradation under reduced input quality makes the formalism suitable for applications where deviant input is to be expected , e.g. second language learning . \n\t', '\n\t\t In this case the potential for error diagnosis is also very valuable : if the best analysis that can be found still violates an important constraint , this directly indicates not only where an error occurred , but also what might be wrong about the input . \n\t', '\n\t\t 3 XCDG : A Tool for Parsing and Modelling An implementation of constraint dependency grammar exists that has the character of middleware to allow embedding the parsing functionality into other natural language applications . \n\t', '\n\t\t The program XCDG uses this functionality for a graphical tool for grammar development . \n\t', '\n\t\t In addition to providing an interface to a range of different parsing algorithms , graphical display of grammar elements and parsing results is possible ; for instance , the hierarchical relations between possible attributes of lexicon items can be shown . \n\t', '\n\t\t See Figure 1 for an excerpt of the hierarchy of German syntactical categories used ; the terminals correspond to those used the Stuttgart-T¨ubingen Tagset of German \n\t\t']",Positive
"['\n\t\t More importantly , mean and end results of pars- ing runs can be displayed graphically . \n\t', '\n\t\t Dependency structures are represented as trees , while additional relations outside the syntax structure are shown as arcs below the tree ( see the referential relationship REF in Figure 2 ) . \n\t', '\n\t\t As well as end results , intermediate structures found during parsing can be displayed . \n\t', '\n\t\t This is often helpful in understanding the behaviour of the heuristic solution methods employed . \n\t', '\n\t\t Together with the structural analysis , instances of broken rules are displayed below the dependency graph ( ordered by decreasing weights ) , and the dependencies that trigger the violation are highlighted on demand ( in our case the PP-modification between the preposition in and the infinite form verkaufen ) . \n\t', '\n\t\t This allows the grammar writer to easily check whether or not a rule does in fact make the distinction it is supposed to make . \n\t', '\n\t\t A unique identifier attached to each rule provides a link into the grammar source file containing all constraint definitions . \n\t', '\n\t\t The unary constraint \x92mod-Distan z\x92 in the example of Figure 2 is a fairly weak constraint which penalizes attachments the stronger the more distant a dependent is placed from its head . \n\t', '\n\t\t Attaching the preposition to the preceding noun Bund would be preferred by this constraint , since the distance is shorter . \n\t', '\n\t\t However , it would lead to a more serious constraint violation because noun attachments are generally dispreferred . \n\t', '\n\t\t To facilitate such experimentation , the parse window doubles as a tree editor that allows structural , lexical and label changes to be made to an analysis by drag and drop . \n\t', '\n\t\t One important application of the integrated parsing and editing tool is the creation of large-scale dependency treebanks . \n\t', '\n\t\t With the ability to save and load parsing results from disk , automatically computed analyses can be checked and hand- corrected where necessary and then saved as annotations . \n\t', '\n\t\t With a parser that achieves a high performance on unseen input , a throughput of over 100 annotations per hour has been achieved . \n\t', '\n\t\t 4 Grammar development with XCDG The development of a parsing grammar based on declarative constraints differs fundamentally from that of a derivational grammar , because its rules forbid structures instead of licensing them : while a context-free grammar without productions licenses nothing , a constraint grammar without constraints would allow everything . \n\t', '\n\t\t A new constraint must therefore be written whenever two analyses of the same string are possible under the existing constraints , but human judgement clearly prefers one over the other . \n\t', '\n\t\t Figure 2 : Xcdg Tree Editor Most often , new constraints are prompted by inspection of parsing results under the existing grammar : if an analysis is computed to be grammatical that clearly contradicts intuition , a rule must be missing from the grammar . \n\t', '\n\t\t Conversely , if an error is signalled where human judgement disagrees , the relevant grammar rule must be wrong ( or in need of clarifying exceptions ) . \n\t', '\n\t\t In this way , continuous improvement of an existing grammar is possible . \n\t', '\n\t\t XCDG supports this development style through the feature of hypothetical evaluation . \n\t', '\n\t\t The tree display window does not only show the result returned by the parser ; the structure , labels and lexical selections can be changed manually , forcing the parser to pretend that it returned a different analysis . \n\t', '\n\t\t Recall that syntactic structures do not have to be specifically allowed by grammar rules ; therefore , every conceivable combination of subordinations , labels and lexical selections is admissible in principle , and can be processed by XCDG , although its score will be low if it contradicts many constraints . \n\t', '\n\t\t After each such change to a parse tree , all con- straints are automatically re-evaluated and the updated grammar judgement is displayed . \n\t', '\n\t\t In this way it can quickly be checked which of two alternative structures is preferred by the grammar . \n\t', '\n\t\t This is useful in several ways . \n\t', '\n\t\t First , when analysing parsing errors it allows the grammar author to distinguish search errors from modelling errors : if the intended structure is assigned a better score than the one actually returned by the parser , a search error occurred ( usually due to limited processing time ) ; but if the computed structure does carry the higher score , this indicates an error of judgement on the part of the grammar writer , and the grammar needs to be changed in some way if the phenomenon is to be modelled adequately . \n\t', '\n\t\t If a modelling error does occur , it must be because a constraint that rules against the intended analysis has overruled those that should have selected it . \n\t', '\n\t\t Since the display of broken constraints is ordered by severity , it is immediately obvious which of the grammar rules this is . \n\t', '\n\t\t The developer can then decide whether to weaken that rule or extend it so that it makes an exception for the current phenomenon . \n\t', '\n\t\t It is also possible that the intended analysis really does conflict with a particular linguistic principle , but in doing so follows a more important one ; in this case , this other rule must be found and strengthened so that it will overrule the first one . \n\t', '\n\t\t The other rule can likewise be found by re-creating the original automatic analysis and see which of its constraint violations needs to be given more weight , or , alternatively , which entirely new rule must be added to the grammar . \n\t', '\n\t\t In the decision whether to add a new rule to a constraint grammar , it must be discovered under what conditions a particular phenomenon occurs , so that a generally relevant rule can be written . \n\t', '\n\t\t The possession of a large amount of analysed text is often useful here to verify decisions based on mere introspection . \n\t', '\n\t\t Working together with an external program to search for specific structures in large treebanks , XCDG can display multiple sentences in stacked widgets and highlight all instances of the same phenomenon to help the grammar writer decide what the relevant conditions are . \n\t', '\n\t\t Using this tool , a comprehensive grammar of modern German has been constructed \n\t\t']",Positive
"['\n\t\t It achieves a structural recall of 87.7 % on sentences from the NEGRA corpus ( Foth et al. , submitted ) , but can be applied to texts of many other types , where structural recall varies between 80\x9690 % . \n\t', '\n\t\t To our knowledge , no other system has been published that achieves a comparable correctness for open-domain German text . \n\t', '\n\t\t Parsing time is rather high due to the computational effort of multidimensional optimization ; processing time is usually measured in seconds rather than milliseconds for each sentence . \n\t', '\n\t\t 5 Conclusions We demonstrate a tool that lets the user parse , display and manipulate dependency structures according to a variant of dependency grammar in a graphical environment . \n\t', '\n\t\t We have found such an integrated environment invaluable for the development of precise and large grammars of natural language . \n\t', '\n\t\t Compared to other approaches , c.f. \n\t\t']",Negative
"['\n\t\t This additional information can then be immediately used in subsequent development cycles . \n\t', '\n\t\t A similar tool , called Annotate , has been de- scribed in \n\t\t']",Negative
"['\n\t\t This tool facilitates syntactic corpus annotation in a semiautomatic way by using a part-of-speech tagger and a parser running in the background . \n\t', '\n\t\t In comparison , Annotate is primarily used for corpus annotation , whereas XCDG supports the development of the parser itself also . \n\t', '\n\t\t Due to its ability to always compute the single best analysis of a sentence and to highlight possible shortcomings of the grammar , the XCDG system provides a useful framework in which human design decisions on rules and weights can be effectively combined with a corpus-driven evaluation of their consequences . \n\t', '\n\t\t An alternative for a symbiotic cooperation in grammar development has been devised by \n\t\t']",Negative
"['\n\t\t Although the resulting grammar produced highly competitive results , it nevertheless requires a treebank being given in advance , while our approach also supports a simultaneous treebank compilation . \n\t', '\n\t\t References Thorsten Brants and Oliver Plaehn . \n\t', '\n\t\t 2000. Interactive corpus annotation . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 2nd Int. Conf . \n\t', '\n\t\t on Language Resources and Engineering , LREC 2000 , pages 453\x96459 , Athens . \n\t', '\n\t\t Kilian Foth , Michael Daum , and Wolfgang Menzel . \n\t', '\n\t\t submitted . \n\t', '\n\t\t A broad-coverage parser for German based on defeasible constraints . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 7. Konferenz zur Verarbeitung nat¨urlicher Sprache , KONVENS-2004 , Wien , Austria . \n\t', '\n\t\t Kilian A. Foth . \n\t', '\n\t\t 2004. Writing weighted constraints for large dependency grammars . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t Recent Advances in Dependency Grammars , COLING 2004 , Geneva , Switzerland . \n\t', '\n\t\t Julia Hockenmaier and Mark Steedman . \n\t', '\n\t\t 2002. Generative models for statistical parsing with combinatory categorial grammar . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 40th Annual Meeting of the ACL , ACL-2002 , Philadelphia , PA . \n\t', '\n\t\t Ronald M. Kaplan and John T. Maxwell . \n\t', '\n\t\t 1996. LFG grammar writer\x92s workbench . \n\t', '\n\t\t Technical report , Xerox PARC . \n\t', '\n\t\t Anne Schiller , Simone Teufel , Christine St¨ockert , and Christine Thielen . \n\t', '\n\t\t 1999. Guidelines f¨ur das Tagging deutscher Textcorpora . \n\t', '\n\t\t Technical report , Universit¨at Stuttgart / Universit¨at T¨ubingen . \n\t', '\n\t\t Ingo Schr¨oder . \n\t', '\n\t\t 2002. Natural Language Parsing with Graded Constraints . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , Department of Informatics , Hamburg University , Hamburg , Germany . \n\t', '\n\t\t Wide Coverage Symbolic Surface Realization Charles B. Callaway Istituto per la Ricerca Scientifica e Tecnologica Istituto Trentino di Cultura , Italy ( ITC-irst ) callaway@itc.it Abstract Recent evaluation techniques applied to corpus- based systems have been introduced that can predict quantitatively how well surface realizers will generate unseen sentences in isolation . \n\t', '\n\t\t We introduce a similar method for determining the coverage on the Fuf/Surge symbolic surface realizer , report that its coverage and accuracy on the Penn TreeBank is higher than that of a similar statistics-based generator , describe several benefits that can be used in other areas of computational linguistics , and present an updated version of Surge for use in the NLG community . \n\t', '\n\t\t 1 Introduction Surface realization is the process of converting the semantic and syntactic representation of a sentence or series of sentences into the text , or surface form , of a particular language \n\t\t']",Positive
"['\n\t\t Most surface realizers have been symbolic , grammar-based systems using syntactic linguistic theories like HPSG . \n\t', '\n\t\t These systems were often developed as either proof-of-concept implementations or to support larger end-to-end NLG systems which have produced limited amounts of domain-specific texts . \n\t', '\n\t\t As such , determining the generic coverage of a language has been substituted by the goal of producing the necessary syntactic coverage for a particular project . \n\t', '\n\t\t As described in \n\t\t']",Positive
"['\n\t\t Instead , large syntactically annotated corpora such as the Penn TreeBank \n\t\t']",Positive
['\n\t\t We conducted a similar corpus-based experiment \n\t\t'],Positive
['\n\t\t We describe a direct comparison with HALOGEN \n\t\t'],Negative
"['\n\t\t We also present a longitudinal comparison of two versions of FUF/SURGE showing a significant improvement in its coverage and accuracy after new grammar and morphology rules were added . \n\t', '\n\t\t This improved version of SURGE is available for use in the NLG community . \n\t', ""\n\t\t 2 Related Work in Wide Coverage Generation Verifying wide coverage generation depends on ( 1 ) a large , well structured corpus , ( 2 ) a transformation algorithm that converts annotated sentences into the surface realizer 's expected input form , ( 3 ) the surface realizer itself , and ( 4 ) an automatic metric for determining the accuracy of the generated sentences . \n\t"", '\n\t\t Large , well structured , syntactically marked corpora such as the Penn TreeBank \n\t\t']",Positive
"['\n\t\t Realization of text from corpora has been approached in several ways . \n\t', ""\n\t\t In the case of Ratnaparkhi 's generator for flight information in the air travel domain \n\t\t""]",Positive
['\n\t\t FERGUS \n\t\t'],Positive
"['\n\t\t The system uses an underlying tree- ( S ( NP-SBJ ( ( cat clause ) ( NP ( JJ overall ) ( process ( ( type ascriptive ) ( tense past ) ) ) ( NNS sales ) ) ) ( participants ( VP ( VBD were ) ( ( carrier ( ( cat common ) ( lex "" sale "" ) ( number plural ) ( ADJP-PRD ( describer ( ( cat adj ) ( lex "" overall "" ) ) ) ) ) ( RB roughly ) ( attribute ( ( cat ap ) ( lex "" flat "" ) ( JJ flat ) ) ) ) ( modifier ( ( cat adv ) ( lex "" roughly "" ) ) ) ) ) ) ) Figure 1 : A Penn TreeBank Sentence and Corresponding SUrGE Input Representation based syntactic model to generate a set of possible candidate realizations , and then chooses the best candidate with a trigram model of the Treebank text . \n\t', '\n\t\t An evaluation of three versions of FErGUS on randomly chosen Wall Street Journal sentences of the TreeBank showed simple string accuracy up to 58.9 % . \n\t', ""\n\t\t Finally , Langkilde 's work on HALOGEN \n\t\t""]",Positive
"['\n\t\t The system uses the transformed semantic input to create millions of possible realizations ( most of which are grammatical but unwieldy ) in a lattice structure and then also uses n-grams to select the most probable as its output sentence . \n\t', '\n\t\t Langkilde evaluated the system using the standard train-and-test methodology with Section 23 of the TreeBank as the unseen set . \n\t', '\n\t\t These systems represent a statistical approach to wide coverage realization , turning to automatic methods to evaluate coverage and quality based on corpus statistics . \n\t', ""\n\t\t However , a symbolic realizer can use the same evaluation technique if a method exists to transform the corpus annotation into the realizer 's input representation . \n\t"", '\n\t\t Thus symbolic realizers can also use the same types of evaluations employed by the parsing and MT communities , allowing for meaningful comparisons of their performance on metrics such as coverage and accuracy . \n\t', '\n\t\t 3 The Penn TreeBank The Penn TreeBank \n\t\t']",Positive
"['\n\t\t The corpus is divided into 24 sections , with each section having on average 2000 sentences . \n\t', '\n\t\t The representation of an example sentence is shown at the left of Figure 1 . \n\t', '\n\t\t In general , many sentences contained in the TreeBank are not typical of those produced by current NLG systems . \n\t', '\n\t\t For instance , newspaper text requires extensive quoting for conveying dialogue , special formatting for stock reports , and methods for dealing with contractions . \n\t', ""\n\t\t These types of constructions are not available in current general purpose , rule-based generators : \x95 Direct and indirect quotations from re- porters ' interviews \n\t\t""]",Positive
"['\n\t\t \x95 Incomplete quotations : Then retailers "" will probably push them out altogether , "" he says . \n\t', '\n\t\t \x95 Simple lists of facts from stock reports : 8 13/16 % high , 8 1/2 % low , 8 5/8 % near closing bid , 8 3/4 % offered . \n\t', ""\n\t\t \x95 Both formal and informal language : You 've either got a chair or you do n't . \n\t"", '\n\t\t \x95 A variety of punctuation mixed with text : $ 55,730,000 of school financing bonds , 1989 Series B ( 1987 resolution ) . \n\t', '\n\t\t \x95 Combinations of infrequent syntactic rules : Then how should we think about service ? \n\t', '\n\t\t \x95 Irregular and rare words : "" I was upset with Roger , I fumpered and schmumpered , "" says Mr. Peters . \n\t', '\n\t\t By adding rules for these phenomena , NLG realizers can significantly increase their coverage . \n\t', '\n\t\t For instance , approximately 15 % of Penn TreeBank sentences contain either direct , indirect or incomplete written dialogue . \n\t', '\n\t\t Thus for a newspaper domain , excluding dialogue from the grammar greatly limits potential coverage . \n\t', '\n\t\t Furthermore , using a corpus for testing a surface realizer is akin to having a very large regression test set , with the added benefit of being able to robustly generate real-world sentences . \n\t', '\n\t\t In order to compare a symbolic surface realizer with its statistical counterparts , we tested an enhanced version of an off-the-shelf symbolic generation system , the FUF/SURGE \n\t\t']",Positive
"['\n\t\t To obtain a meaningful comparison , we utilized the same approach as Realizer Sentences Coverage Matches Covered Matches Total Matches Accuracy SURGE 2.2 2416 48.1 % 102 8.8 % 4.2 % 0.8542 SURGE+ 2416 98.9 % 1474 61.7 % 61.0 % 0.9483 HalOGEN 2416 83.3 % 1157 57.5 % 47.9 % 0.9450 Table 1 : Comparing two SURGE versions with HALOGEN [ Langkilde 20021 . \n\t', '\n\t\t HALOGEN , treating Section 23 of the Treebank as an unseen test set . \n\t', '\n\t\t We created an analogous transformation algorithm \n\t\t']",Positive
"['\n\t\t 4 Coverage and Accuracy Evaluation Of the three statistical systems presented above , only \n\t\t']",Positive
"['\n\t\t Because of the sheer number of sentences ( 2416 ) , and to enable a direct comparison with HALOGEN , we similarly used the simple string accuracy \n\t\t']",Positive
"['\n\t\t Unlike typical statistical and machine learning experiments , the grammar was "" trained "" by hand , though the evaluation of the resulting sentences was performed automatically . \n\t', '\n\t\t This resulted in numerous generalized syntactic and morphology rules being added to the SURGE grammar , as well as specialized rules pertaining to specific domain elements from the texts . \n\t', '\n\t\t Table 1 shows a comparative coverage and accuracy analysis of three surface realizers on Section 23 of the Penn TreeBank : the original SURGE 2.2 distribution , our modified version of SURGE , and the HALOGEN system described in \n\t\t']",Positive
"['\n\t\t The surface realizers are measured in terms of : \x95 Coverage : The number of sentences for which the realizer returned a recognizable string rather than failure or an error . \n\t', '\n\t\t \x95 Matches : The number of identical sen- tences ( including punctuation/capitals ) . \n\t', '\n\t\t \x95 Percent of covered matches : How often the realizer returned a sentence match given that a sentence is produced . \n\t', '\n\t\t \x95 Percent of matches for all sentences : A measure of matches from all inputs , which penalizes systems that improve accuracy at the expense of coverage ( Matches / 2416 , or Coverage * Covered Matches ) . \n\t', '\n\t\t \x95 Accuracy : The aggregate simple string accuracy score for all covered sentences ( as opposed to the entire sentence set ) . \n\t', '\n\t\t The first thing to note is the drastic improvement between the two versions of SURGE . \n\t', '\n\t\t As the analysis in Section 3 showed , studying the elements of a particular domain are very important in determining what parts of a grammar should be improved . \n\t', '\n\t\t For instance , the TreeBank contains many constructions which are not handled by SURGE 2.2 , such as quotations , which account for 15 % of the sentences . \n\t', '\n\t\t When SURGE 2.2 encounters a quotation , it fails to produce a text string , accounting for a large chunk of the sentences not covered ( 51.9 % compared to 1.1 % for our enhanced version of SURGE ) . \n\t', '\n\t\t Additionally , a number of morphology enhancements , such as contractions and punctuation placement contributed to the much higher percentage of exact matches . \n\t', '\n\t\t While some of these are domain-specific , many are broader generalizations which although useful , were not included in the original grammar because they were not encountered in previous domains or arose only in complex sentences . \n\t', '\n\t\t On all four measures the enhanced version of SURGE performed much better than the statistical approach to surface realization embodied in HALOGEN . \n\t', '\n\t\t The accuracy measure is especially surprising given that statistical and machine learning approaches employ maximization algorithms to ensure that grammar rules are chosen to get the highest possible accuracy . \n\t', '\n\t\t However , given that the difference in accuracy from Surge 2.2 is relatively small while its quality is obviously poor , using such accuracy measures alone is a bad way to compare surface realizers . \n\t', '\n\t\t Finally , the coverage difference between the enhanced version of SURGE and that of HALOGEN is especially striking . \n\t', '\n\t\t Some explanations may be that statistical systems are not yet capable of handling certain linguistic phenomena like long-distance dependencies ( due to n-gram ap- proaches ) , or given that statistical systems are typically robust and very unlikely to produce no output , that there were problems in the transformation algorithm that converted individual sentence representations from the corpus . \n\t', '\n\t\t 5 Additional Benefits The evaluation approach presented here has other advantages besides calculating the coverage and accuracy of a grammar . \n\t', '\n\t\t For instance , in realizers where linguists must add new lexical resources by hand , such a system allows them to generate text by first creating sample sentences in the more familiar TreeBank notation . \n\t', '\n\t\t Sentences could also be directly generated by feeding an example text to a parser capable of producing TreeBank structures . \n\t', '\n\t\t This would be especially useful in new domains to quickly see what new specialized syntax they might need . \n\t', '\n\t\t Additionally , the transformation program can be used as an error-checker to assist in annotating sentences in a new corpus . \n\t', '\n\t\t Rules could be ( and have been ) added alongside the normal transformation rules that detect when errors are encountered , categorize them , and make them available to the corpus creator for correction . \n\t', '\n\t\t This can extend beyond the syntax level , detecting even morphology errors such as incorrect verbs , typos , or dialect differences . \n\t', '\n\t\t Finally , such an approach can help test parsing systems without the need for the time- consuming process of annotating corpora in the first place . \n\t', '\n\t\t If a parser creates a TreeBank representation for a sentence , the generation system can then attempt to regenerate that same sentence automatically . \n\t', '\n\t\t Exact matches are highly likely to have been correctly parsed , and more time can be spent locating and resolving parses that returned very low accuracy scores . \n\t', '\n\t\t 6 Conclusions and Future Work Recent statistical systems for generation have focused on surface realizers , offering robustness , wide coverage , and domain- and language- independence given certain resources . \n\t', '\n\t\t This paper represents the analogous effort for a symbolic generation system using an enhanced version of the FUF/SURGE systemic realizer . \n\t', '\n\t\t We presented a grammatical coverage and accuracy experiment showing the symbolic system had a much higher level of coverage of English and better accuracy as represented by the Penn TreeBank . \n\t', '\n\t\t The improved SURGE grammar , version 2.4 , will be made freely available to the NLG community . \n\t', '\n\t\t While we feel that both coverage and accuracy could be improved even more , additional gains would not imply a substantial improvement in the quality of the grammar itself . \n\t', '\n\t\t The reason is that most problems affecting accuracy lie in transforming the TreeBank representation as opposed to the grammar , which has remained relatively stable . \n\t', '\n\t\t References S. Bangalore and O. Rambow . \n\t', '\n\t\t 2000. Exploiting a probabilistic hierarchical model for generation . \n\t', '\n\t\t In COLING-2000 : Proceedings of the 18th International Conference on Computational Linguistics , Saarbruecken , Germany . \n\t', '\n\t\t John A. Bateman . \n\t', '\n\t\t 1995. KPML : The KOMETpenman ( multilingual ) development environment . \n\t', '\n\t\t Technical Report Release 0.8 , Institut f^ur Integrierte Publikations- und Informationssysteme ( IPSI ) , GMD , Darmstadt . \n\t', '\n\t\t Charles Callaway . \n\t', '\n\t\t 2001. A computational feature analysis for multilingual character-tocharacter dialogue . \n\t', '\n\t\t In Proceedings of the Second International Conference on Intelligent Text Processing and Computational Linguistics , pages 251264 , Mexico City , Mexico . \n\t', '\n\t\t Charles B. Callaway . \n\t', '\n\t\t 2003. Evaluating coverage for large symbolic NLG grammars . \n\t', '\n\t\t In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence , pages 811817 , Acapulco , Mexico , August . \n\t', '\n\t\t George Doddington . \n\t', '\n\t\t 2002. Automatic evaluation of machine translation quality using n- gram co-occurrence statistics . \n\t', '\n\t\t In Proceedings of the 2002 Conference on Human Language Technology , San Diego , CA , March . \n\t', '\n\t\t Michael Elhadad . \n\t', '\n\t\t 1991. FUF : The universal unifier user manual version 5.0 . \n\t', '\n\t\t Technical Report CUCS-038-91 , Dept. of Computer Science , Columbia University . \n\t', '\n\t\t Irene Langkilde-Geary . \n\t', '\n\t\t 2002. An empirical verification of coverage and correctness for a general-purpose sentence generator . \n\t', '\n\t\t In Second International Natural Language Generation Conference , Harriman , NY , July . \n\t', '\n\t\t M. Marcus , B. Santorini , and M. Marcinkiewicz . \n\t', '\n\t\t 1993. Building a large annotated corpus of English : The PennTreeBank . \n\t', '\n\t\t Computational Linguistics , 26(2) . \n\t', '\n\t\t Adwait Ratnaparkhi . \n\t', '\n\t\t 2000. Trainable methods for surface natural language generation . \n\t', '\n\t\t In Proceedings of the First North American Conference of the ACL , Seattle , WA , May . \n\t', '\n\t\t Part-of-Speech Tagging Considering Surface Form for an Agglutinative Language Do-Gil Lee and Hae-Chang Rim Dept. of Computer Science & Engineering Korea University 1 , 5-ka , Anam-dong , Seongbuk-ku Seoul 136-701 , Korea { dglee , rim}@nlp.korea.ac.kr Abstract The previous probabilistic part-of-speech tagging models for agglutinative languages have considered only lexical forms of morphemes , not surface forms of words . \n\t', '\n\t\t This causes an inaccurate calculation of the probability . \n\t', '\n\t\t The proposed model is based on the observation that when there exist words ( surface forms ) that share the same lexical forms , the probabilities to appear are different from each other . \n\t', '\n\t\t Also , it is designed to consider lexical form of word . \n\t', '\n\t\t By experiments , we show that the proposed model outperforms the bigram Hidden Markov model (HMM)-based tagging model . \n\t', '\n\t\t based tagging model . \n\t', '\n\t\t 2 Korean POS tagging model In this section , we first describe the standard morpheme-unit tagging model and point out a mistake of this model . \n\t', '\n\t\t Then , we describe the proposed model . \n\t', '\n\t\t 2.1 Standard morpheme-unit model This section describes the HMM-based morpheme- unit model . \n\t', '\n\t\t The morpheme-unit POS tagging model is to find the most likely sequence of morphemes M and corresponding POS tags T for a given sentence W , as follows \n\t\t']",Positive
"['\n\t\t In English POS tagging , word is used as a linguistic unit . \n\t', '\n\t\t However , the number of possible words in agglutinative languages such as Korean is almost infinite because words can be freely formed by gluing morphemes together . \n\t', '\n\t\t Therefore , morpheme-unit tagging is preferred and more suitable in such languages than word-unit tagging . \n\t', '\n\t\t Figure 1 shows an example of morpheme structure of a sentence , where the bold lines indicate the most likely morpheme-POS sequence . \n\t', '\n\t\t A solid line represents a transition between two morphemes across a word boundary and a dotted line represents a transition between two morphemes in a word . \n\t', '\n\t\t The previous probabilistic POS models for agglutinative languages have considered only lexical forms of morphemes , not surface forms of words . \n\t', '\n\t\t This causes an inaccurate calculation of the probability . \n\t', '\n\t\t The proposed model is based on the observation that when there exist words ( surface forms ) that share the same lexical forms , the probabilities to appear are different from each other . \n\t', '\n\t\t Also , it is designed to consider lexical form of word . \n\t', '\n\t\t By experiments , we show that the proposed model outperforms the bigram Hidden Markov model (HMM)- =argmax P(ml,u , tl,u I Wl,n ) ( 1 ) ml,u,tl,u ~argmax P(ml,u , tl,u ) ( 2 ) ml,u,tl,u In the equation , u(>= n ) denotes the number of morphemes in the sentence . \n\t', '\n\t\t A sequence of W = Wl,n = WlW2 \x95 \x95 \x95 Wn is a sentence of n words , and a sequence of M = ml,u = mlm2 \x95 \x95 \x95 mu and a sequence of T = tl,u = tlt2 \x95 \x95 \x95 tu denote a sequence of u lexical forms of morphemes and a sequence of u morpheme categories ( POS tags ) , respectively . \n\t', '\n\t\t To simplify Equation 2 , a Markov assumption is usually used as follows : P(ti I ti-l,p)P(ti I mi ) ( 3 ) where , to is a pseudo tag which denotes the beginning of word and is also written as BOW . \n\t', '\n\t\t p denotes a type of transition from the previous tag to the current tag . \n\t', '\n\t\t It has a binary value according to the type of the transition ( either intra-word or inter- word transition ) . \n\t', '\n\t\t As can be seen , the word1 sequence Wl,n is discarded in Equation 2 . \n\t', '\n\t\t This leads to an inaccurate 1A word is a surface form . \n\t', '\n\t\t F(W) ~f argmax ml,u,tl,u ~u i=l BOS na/NNP na/VV na/VX nal/VV neun/PX neun/EFD n-da/EFC n-da/EFF hag-gyo/NNC e/PA ga/VV ga/VX gal/VV EOS Figure 1 : Morpheme structure of the sentence \x93na-neun hag-gyo-e gan-da\x94 ( I go to school ) calculation of the probability . \n\t', '\n\t\t A lexical form of a word can be mapped to more than one surface word . \n\t', '\n\t\t In this case , although the different surface forms are given , if they have the same lexical form , then the probabilities will be the same . \n\t', '\n\t\t For example , a lexical form mong-go/nc+leul/jc2 , can be mapped from two surface forms mong-gol and mong-go-leul . \n\t', '\n\t\t By applying Equation 1 and Equation 2 to both words , the following equations can be derived : P(mong-go , nc , leul , jc I mong-gol ) ti P(mong-go , nc , leul , jc ) ( 4 ) P ( mong-go , nc , leul , j c I mong-go-leul ) ti P(mong-go , nc , leul , jc ) ( 5 ) As a result , we can acquire the following equation from Equation 4 and Equation 5 : P( mong-go , nc , leul , jc I mong-gol ) = P(mong-go , nc , leul , jc I mong-go-leul ) ( 6 ) That is , they assume that probabilities of the results that have the same lexical form are the same . \n\t', '\n\t\t However , we can easily show that Equation 6 is mistaken : Actually , P(mong-go , nc , leul , jcI mong-go-leul ) = 1 and P(mong-gol , ncI mong-gol ) =~ 0 . \n\t', '\n\t\t Hence , P(mong-go , nc , leul , jc I mong-gol ) < P(mong-go , nc , leul , jc I mong-go-leul ) . \n\t', '\n\t\t To overcome the disadvantage , we propose a new tagging model that can consider the surface form . \n\t', '\n\t\t 2.2 The proposed model This section describes the proposed model . \n\t', '\n\t\t To simplify the notation , we introduce a variable R , which means a tagging result of a given sentence and consists of M and T. F ( W ) def argmax P(M,TIW) ( 7 ) M,T =argmax P(RIW) ( 8 ) R 2mong-go means Mongolia , nc is a common noun , and jc is a objective case postposition . \n\t', '\n\t\t The probability P ( R I W ) is given as follows : P(R I W ) = P(r1,n I w1,n ) ( 9 ) P(ri I w1,n , r1,i-1 ) ( 10 ) P(ri I wi , ri-1 ) ( 11 ) where , ri denotes the tagging result of ith word ( wi ) , and ro denotes a pseudo variable to indicate the beginning of word . \n\t', '\n\t\t Equation 9 becomes Equation 10 by the chain rule . \n\t', '\n\t\t To be a more tractable form , Equation 10 is simplified by a Markov assumption as Equation 11 . \n\t', '\n\t\t The probability P(ri I wi , ri-1 ) cannot be calculated directly , so it is derived as follows : P(wi , ri-1 , ri ) P ( ri Iwi , ri-1 ) =12 P(wi , ri-1 ) P(wi)P(ri I wi)P(ri-1 I ri ) ( 13 ) ti P(wi)P(ri-1) = P(riIwi)P(ri-1 I ri ) ( 14 ) P(ri-1) = P(riIwi) P(ri-1 , ri ) ( 15 ) P(ri-1)P(ri ) Equation 12 is derived by Bayes rule , Equation 13 by a chain rule and an independence assumption , and Equation 15 by Bayes rule . \n\t', '\n\t\t In Equation 15 , we call the left term \x93morphological analysis model\x94 and right one \x93transition model\x94 . \n\t', '\n\t\t The morphological analysis model P(ri I wi ) can be implemented in a morphological analyzer . \n\t', '\n\t\t If a morphological analyzer can provide the probability , then the tagger can use the values as they are . \n\t', '\n\t\t Actually , we use the probability that a morphological analyzer , ProKOMA \n\t\t']",Positive
"['\n\t\t Although it is not necessary to discuss the morphological analysis model in detail , we should note that surface forms are considered here . \n\t', '\n\t\t The transition model is a form of point-wise mutual information . \n\t', ""\n\t\t = ~n i=1 ~n ti i=1 P(1i-1~~1 ) ' i-1 ' 1( i ' 1r ) ( 16 ) P(1i-1 ' 1 i-P~~i ' T ) P(m1,j1'ti1,3- . \n\t"", ""\n\t\t )P(mi1,k' ti1,k ) where , a superscript i in mi1 k and ti1 k denotes the position of the word in a sentence . \n\t"", '\n\t\t The denominator means a joint probability that the morphemes and the tags in a word appear together , and the numerator means a joint probability that all the morphemes and the tags between two words appear together . \n\t', '\n\t\t Due to the sparse data problem , they cannot also be calculated directly from the test data . \n\t', '\n\t\t By a Markov assumption , the denominator and the numerator can be broken down into Equation 18 and Equation 19 , respectively . \n\t', '\n\t\t where , Pinter ( ti1 I ti~ 1 ) means a transition probability between the last morpheme of the (i~1)th word and the first morpheme of the ith word . \n\t', '\n\t\t By applying Equation 18 and Equation 19 to Equation 17 , we obtain the following equation : Pinter ( ti1 Itj 1 ) P ( ti1IBOW ) ( 20 ) For a given sentence , Figure 2 shows the bigram HMM-based tagging model , and Figure 3 the proposed model . \n\t', '\n\t\t The main difference between the two models is the proposed model considers surface forms but the HMM does not . \n\t', '\n\t\t 3 Experiments For evaluation , two data sets are used : ETRI POS tagged corpus and KAIST POS tagged corpus . \n\t', '\n\t\t We divided the test data into ten parts . \n\t', '\n\t\t The performances of the model are measured by averaging over the ten test sets in the 10-fold cross-validation experiment . \n\t', '\n\t\t Table 1 shows the summary of the corpora . \n\t', '\n\t\t Table 1 : Summary of the data Generally , POS tagging goes through the following steps : First , run a morphological analyzer , where it generates all the possible interpretations for a given input text . \n\t', '\n\t\t Then , a POS tagger takes the results as input and chooses the most likely one among them . \n\t', '\n\t\t Therefore , the performance of the tagger depends on that of the preceding morphological analyzer . \n\t', '\n\t\t If the morphological analyzer does not generate the exact result , the tagger has no chance to select the correct one , thus an answer inclusion rate of the morphological analyzer becomes the upper bound of the tagger . \n\t', '\n\t\t The previous works preprocessed the dictionary to include all the exact answers in the morphological analyzer\x92s results . \n\t', '\n\t\t However , this evaluation method is inappropriate to the real application in the strict sense . \n\t', '\n\t\t In this experiment , we present the accuracy of the morphological analyzer instead of preprocessing the dictionary . \n\t', '\n\t\t ProKOMA\x92s results with the test data are listed in Table 2 . \n\t', '\n\t\t Table 2 : Morphological analyzer\x92s results with the test data Corpus ETRI KAIST Answer inclusion rate ( % ) 95.82 95.95 Average # of results per word 2.16 1.81 1-best accuracy ( % ) 88.31 90.12 In the table , 1-best accuracy is defined as the number of words whose result with the highest probability is matched to the gold standard over the entire words in the test data . \n\t', '\n\t\t This can also be a tagging model that does not consider any outer context . \n\t', '\n\t\t To compare the proposed model with the standard model , the results of the two models are given in Table 3 . \n\t', '\n\t\t As can be seen , our model outperforms the HMM model . \n\t', '\n\t\t Moreover , the HMM model is even worse than the ProKOMA\x92s 1-best accuracy . \n\t', '\n\t\t This tells that the standard HMM by itself is not a good model for agglutinative languages . \n\t', ""\n\t\t 4 Conclusion We have presented a new POS tagging model that can consider the surface form for Korean , which Corpus ETRI KAIST Total # of words 288,291 175,468 Total # of sentences 27,855 16,193 # of tags 27 54 P(ri-1 ' ri ) P(ri-1 )P(ri) P(m1,j1' 1J 'mi1,k'ti1,k ) 17 ) P(m1,k,t1,k) _ ~k P(ti I ti-1)P(mi I ti ) ( 18 ) i=1 ~j(P(tl-1 tl=i ) 1 \\P(ml-1 ti-1)l ~-1 X Pinter(ti1 I tj 1 ) X P(mi1 ti1 ) ~X P((Tni tI ti ) ) ) ( 19 ) m-2( m m P(m i-1 i-1 i i 1,9 ' t1,j ' m1,k' t1,k ) k P(ri-1 , ri ) P(ri-1)P(ri) na neun hag-gyo e ga n-da BOS NNP PX NNC PA VV EFF EOS Figure 2 : Lattice of the bigram HMM-based model na-neun hag-gyo-e gan-da BOS na/NNP+neun/PX hag-gyo/NNC+e/PA ga/VV+n-da/EFF EOS Figure 3 : Lattice of the proposed model Table 3 : Tagging accuracies ( % ) of the standard HMM and the proposed model Corpus ETRI KAIST The standard HMM The proposed model 87.47 89.83 90.66 92.01 is an agglutinative language . \n\t"", '\n\t\t Although the model leaves much room for improvement , it outperforms the HMM based model according to the experimental results . \n\t', '\n\t\t Acknowledgement This work was supported by Korea Research Foundation Grant ( KRF-2003-041-D20485 ) References J.-D. Kim , S.-Z. Lee , and H.-C. Rim . \n\t', '\n\t\t 1998. A morpheme-unit POS tagging model considering word-spacing . \n\t', '\n\t\t In Proceedings of the 1998 Conference on Hangul and Korean Information Processing , pages 3\x968 . \n\t', '\n\t\t D.-G. Lee and H.-C. Rim . \n\t', '\n\t\t 2004. ProKOMA : A probabilistic Korean morphological analyzer . \n\t', '\n\t\t Technical Report KU-NLP-04-01 , Department of Computer Science and Engineering , Korea University . \n\t', '\n\t\t S.-Z. Lee , Jun\x92ichi Tsujii , and H.-C. Rim . \n\t', '\n\t\t 2000. Hidden markov model-based Korean part-ofspeech tagging considering high agglutinativity , word-spacing , and lexical correlativity . \n\t', '\n\t\t In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics . \n\t', '\n\t\t Is Conceptual Combination Influenced by Word Order ? \n\t', '\n\t\t Phil Maguire Department of Computer Science University College Dublin Belfield , Dublin 4 , Ireland Phil.Maguire@ucd.ie Abstract We describe two experiments using French noun-noun combinations which parallel a study carried out by Gagné ( 2001 ) using English combinations . \n\t', '\n\t\t The order of the modifier and head noun are reversed in French , allowing us to investigate whether the influence of relation priming that Gagné found is due to the order of the modifier and head noun or whether it is due to their different functional roles . \n\t', '\n\t\t While our findings indicate that interpretation is influenced by previous exposure to combinations incorporating one of the same constituent nouns , the results show that primes with the same modifier have a greater influence when associated with a different relation to the target . \n\t', '\n\t\t This pattern of influence is similar to that found in English and suggests that the modifier is exclusively involved in relation selection , irrespective of its order in a combination . \n\t', '\n\t\t 1 Introduction The combination of two existing words is a productive strategy used by speakers to convey new concepts and extend the limits of the vernacular . \n\t', '\n\t\t The process of understanding these novel compounds is worthy of study , both because it is intimately associated with the creativity of language use and because it provides a constrained domain in which to test cognitive theories of conceptual representation and language comprehension . \n\t', '\n\t\t In English compounds , the first word or modifier attaches further meaning to the second word or head , thus creating a reference to the intended concept . \n\t', '\n\t\t In order to interpret a nominal compound such as \x93mountain stream\x94 , people must find a relation to link the compound\x92s head and modifier . \n\t', '\n\t\t Several different theories have been proposed as to how people find the correct relation with which to link the constituent nouns . \n\t', '\n\t\t Gagné and Shoben\x92s ( 1997 ) Competition Among Relations In Nominals ( CARIN ) theory maintains that there is a fixed , relatively small taxonomy of standard relations that can be used to link the Arthur Cater Department of Computer Science University College Dublin Belfield , Dublin 4 , Ireland Arthur.Cater@ucd.ie modifier and head noun concepts . \n\t', '\n\t\t According to this theory , the representation of the modifier concept includes statistical knowledge about those relations with which the modifier tends to be used during conceptual combinations . \n\t', '\n\t\t The most available standard relation is the one most frequently used to interpret other compounds containing that same modifier . \n\t', '\n\t\t For instance , the modifier \x93mountain\x94 is most often associated with the <head LOCATED modifier> relation thus making the combination \x93mountain stream\x94 easier to interpret than \x93mountain magazine\x94 which uses the <head ABOUT modifier> relation . \n\t', '\n\t\t Important evidence in support of the CARIN model is the finding that the modifier\x92s relational distribution influences the ease with which a combined concept can be interpreted . \n\t', '\n\t\t Gagné and \n\t\t']",Positive
"['\n\t\t This raises the question as to why it should be the case that the frequencies of relations associated with the modifier affect ease of interpretation , but not those of the head noun . \n\t', '\n\t\t Gagné and \n\t\t']",Positive
"['\n\t\t A second possibility they suggest is that the modifier noun has certain associated properties which give it a semantic privilege in determining the meaning of a combination . \n\t', '\n\t\t One way to test both of these hypotheses is to examine the interpretation of combinations in a language in which the order of the nouns is the reverse of that in English . \n\t', '\n\t\t We adopt such an approach by examining the interpretation of combinations in the French language in order to determine which of the above possibilities can account for Gagné and Shoben\x92s findings . \n\t', '\n\t\t The following experiments parallel a speeded sensibility study by Gagné ( 2001 ) which investigated the ways in which recent exposure to a similar combination influences the processing of a subsequent combination . \n\t', '\n\t\t Gagné found that when the prime and the target had the same head noun , there was no significant difference in reaction times between the cases where they shared the same relation and cases where they did not . \n\t', '\n\t\t However , when the modifier was the repeated constituent , primes that used the same relation exerted more influence than those that used a different relation . \n\t', '\n\t\t Thus , \x93mountain stream\x94 was more effective than \x93mountain magazine\x94 at priming \x93mountain goat\x94 while \x93kitchen chair\x94 and \x93wood chair\x94 were equally effective at priming \x93garden chair\x94 . \n\t', '\n\t\t Gagné concluded that when the prime and target share the same modifier , relation priming increases the availability of a selected relation within the modifier\x92s relational distribution . \n\t', '\n\t\t We replicate Gagné\x92s study in French in order to determine whether the same effect will be observed . \n\t', '\n\t\t While conceptual combination in the English language involves the straightforward juxtaposition of two nouns , combinations in French are made up of three separate elements , namely the head , the modifier and a linking preposition . \n\t', '\n\t\t The preposition gives some indication of the relation between the two concepts as different prepositions are used with different relations . \n\t', '\n\t\t The three French prepositions typically used are \x93de\x94 , \x93à\x94 and \x93en\x94 . \n\t', '\n\t\t While the use of a preposition in French can bias the selection of a particular relation , we have controlled for this by choosing materials exclusively associated with the \x93de\x94 preposition , which can be used with almost all relations . \n\t', '\n\t\t Consequently this eliminates any alternative influences on relation selection other than those exerted by the modifier and the head . \n\t', '\n\t\t 2 Empirical Study Two separate experiments were carried out . \n\t', '\n\t\t In the first , the prime had the same head as the target and in the second , the modifier was the repeated constituent . \n\t', '\n\t\t In both experiments , there were three conditions . \n\t', '\n\t\t In one condition the prime used the same relation as the target ; in another it used a different relation . \n\t', '\n\t\t There was also a neutral condition in which the target combination was preceded by a combination with no common noun constituent . \n\t', '\n\t\t The experimental design follows that of Gagné ( 2001 ) and facilitates the analysis of the relative amounts of priming derived from a combination containing the same head or the same modifier as the target . \n\t', '\n\t\t Priming was evaluated by comparing each of the first two conditions with the neutral condition and by comparing response times to target combinations in the same-relation condition with response times to target combinations in the different-relation condition . \n\t', '\n\t\t 2.1 Method Materials . \n\t', '\n\t\t In both experiments , sixty combined concepts were created as targets . \n\t', '\n\t\t For each target combination , three prime combinations were constructed . \n\t', '\n\t\t One used the same relation as the target and either the same head ( experiment 1 ) or the same modifier ( experiment 2 ) . \n\t', '\n\t\t Similarly , another combination used a different relation . \n\t', '\n\t\t The control combination shared no noun constituent with the target . \n\t', '\n\t\t Three lists of stimuli were arranged such that there was an equal number of each prime type in each list . \n\t', '\n\t\t Across all three lists , each target was seen with each type of prime combination . \n\t', '\n\t\t Our materials were controlled for plausibility and familiarity . \n\t', '\n\t\t Two raters scored the plausibility and familiarity of the referents of the prime combinations on a Likert scale from 1 to 7 . \n\t', '\n\t\t A two- sided Wilcoxon signed-ranks test revealed no reliable differences between conditions for plausibility , familiarity or average syllable length ( p > 0.05 ) . \n\t', '\n\t\t Procedure . \n\t', '\n\t\t Each participant was exposed to one of the lists and hence saw each target item only once . \n\t', '\n\t\t The pairs of prime and target items were presented in a randomised order along with 60 filler pairs and the complete set of filler pairs was presented to each individual . \n\t', '\n\t\t Participants sat in front of a computer screen and placed the index finger of their left hand on the F key of the keyboard and the index finger of their right hand on the J key . \n\t', '\n\t\t Participants were told that J corresponded to \x93Juste\x94 and F corresponded to \x93Faux\x94 . \n\t', '\n\t\t Trial presentation was self-paced . \n\t', '\n\t\t Following exposure to the prime combination , participants indicated whether it had a sensible , literal interpretation by pressing the appropriate key . \n\t', '\n\t\t Subsequently , the target combination was similarly displayed and participants made another sensibility judgment . \n\t', '\n\t\t There was nothing in the method of presentation to suggest any connection between consecutive combinations . \n\t', '\n\t\t Participants . \n\t', '\n\t\t 36 native French speakers participated , 18 in each experiment ( ages 20-31 , M = 24.2 ) . \n\t', '\n\t\t This selection consisted of students and teachers based in Ireland . \n\t', '\n\t\t 2.2 Results and Discussion 9.1 % of trials were excluded from the analysis . \n\t', '\n\t\t 0.8 % of trials were rejected because participants pressed a key other than J or F . \n\t', '\n\t\t Additionally , 4.6 % of trials were excluded in cases where the response \x93faux\x94 was incorrectly given . \n\t', '\n\t\t Responses deemed unreasonably fast ( < 400ms ; 0.2 % ) and unreasonably slow ( > 4000ms ; 0.9 % ) were also excluded . \n\t', '\n\t\t After eliminating all trials which did not meet the above criteria , any response times which were more than three standard deviations outside each participant\x92s mean were also rejected . \n\t', '\n\t\t This eliminated another 2.6 % of responses . \n\t', '\n\t\t A repeated measures ANOVA test was conducted to examine the effect of prime type on sense-nonsense judgments for each experiment . \n\t', '\n\t\t Tables 1 and 2 display the response time ( in milliseconds ) for appropriate responses to the target combinations in each of the experiments . \n\t', '\n\t\t Prime Target Response Time ( ms ) Same Head Same Same Modifier Relation / / x x x / x 994 x NA 1 999 1153 Table 1 : Response Times ( in milliseconds ) for Target Combinations in Experiment 1 Prime Target Response Time ( ms ) Same Head Same Same Modifier Relation x x / / x / x 998 x NA 1043 1062 Table 2 : Response Times ( in milliseconds ) for Target Combinations in Experiment 2 Evidence of priming . \n\t', '\n\t\t Responses to the target combination were faster when the prime and target shared a constituent noun . \n\t', '\n\t\t In the first experiment , the 159ms difference between the same-relation and neutral conditions was reliable , Fsubject(1 , 34 ) = 31.70 , p < .01 ; Fitem(1 , 118 ) = 27.30 , p < .01 . \n\t', '\n\t\t The 154ms difference between the different-relation and neutral conditions was also reliable , Fsubject(1 , 34 ) = 22.22 , p < .01 ; Fitem(1 , 118 ) = 27.309 , p < .01 . \n\t', '\n\t\t In the second experiment the 64 ms difference between the same-relation and neutral conditions was reliable , Fsu bject(2 , 34 ) = 9.248 , p < .05 ; Fitem(2 , 118 ) = 11.437 , p < .05 . \n\t', '\n\t\t However , the 19 ms difference between the different-relation and neutral conditions was not reliable , Fsubject(2 , 34 ) = .587 , p > .05 ; Fitem(2 , 118 ) = .337 , p > .05 . \n\t', '\n\t\t Relation influence . \n\t', '\n\t\t As predicted by the CARIN theory , the first experiment , in which the head was the repeated constituent , revealed no evidence of relation influence . \n\t', '\n\t\t No significant difference was found between response times to target 1 The relation of the neutral condition was considered irrelevant following Gagné\x92s ( 2001 ) finding that priming does not occur when the preceding combination does not share either of the target\x92s constituent nouns combinations in the same-relation and in the neutral conditions . \n\t', '\n\t\t The 5ms difference between the two conditions was not reliable ( Fs < 1 ) . \n\t', '\n\t\t However , in the repeated modifier experiment the target was easier to interpret when it was preceded by a combination with the same relation than when it was preceded by one with a different relation . \n\t', '\n\t\t Participants responded to targets following the same relation prime 45ms quicker than they did to targets following the different relation prime , Fsubject(2 , 34 ) = 4.349 , p < .05 ; Fitem(2 , 118 ) = 4.194 , p < .05 . \n\t', '\n\t\t These data indicate that French speakers are only sensitive to relational information associated with the modifier . \n\t', '\n\t\t Summary . \n\t', '\n\t\t The results of the two experiments show that the influence of a recently viewed combination is affected by its relation only in cases where the target shares the same modifier ( experiment 2 ) and not in cases where it shares the same head ( experiment 1 ) . \n\t', '\n\t\t Thus \x93ruisseau de montagne\x94 ( mountain stream ) was more effective than \x93chaussures de montagne\x94 ( mountain shoes ) at priming \x93glacier de montagne\x94 ( mountain glacier ) while \x93sac de voyage\x94 ( travel bag ) and \x93sac de cuir\x94 ( leather bag ) were equally effective at priming \x93sac de sport\x94 ( sports bag ) . \n\t', '\n\t\t These results are similar to those of Gagné ( 2001 ) and are thus consistent with research in the English language indicating that relational information is associated with the modifier and not with the head noun . \n\t', '\n\t\t Since these effects have been replicated in a language in which the order of the modifier and head are reversed , this suggests that modifiers and head nouns maintain the same role in the process of interpretation regardless of the order in which they are realised . \n\t', '\n\t\t Our findings confirm that relational information is a tangible feature of conceptual combinations and that the association between the modifier and the relation is an intrinsic property that is evident regardless of the order of the constituent nouns . \n\t', '\n\t\t 3 General Discussion While our results correspond with those of Gagné ( 2001 ) , we interpret them differently . \n\t', '\n\t\t In her study Gagné distinguished two priming effects , namely lexical priming and relation priming . \n\t', '\n\t\t She claimed that when the head noun was repeated , only lexical priming was observed but that when the modifier was repeated , both types of priming were evident . \n\t', '\n\t\t This distinction is not necessary . \n\t', '\n\t\t It is simpler to suppose that the repeated-modifier different-relation condition exhibits an interference effect which diminishes the effectiveness of lexical priming . \n\t', '\n\t\t Such an interference could arise for two reasons , neither of which requires an assumption of relation priming . \n\t', '\n\t\t The first possibility is that combinations using a different relation elicit no priming because a different sense of the modifier is associated with each relation . \n\t', '\n\t\t For example , the French term \x93en chocolat\x94 ( made of chocolate ) has very different connotations to \x93à chocolat\x94 ( for chocolate ) or \x93de chocolat\x94 ( of chocolate ) . \n\t', '\n\t\t While these terms employ the same modifier , they each have different meanings since the preposition immediately elucidates the modifying capacity of the noun . \n\t', '\n\t\t Though the relation associated with a modifier in English may not be expressed in the same way , the conceptual disparity is likely to persist nonetheless . \n\t', '\n\t\t It is therefore conceivable that the relation with which the modifier is associated can change its meaning and as a result , one modifier might not necessarily prime a combination using the same modifier in a different sense . \n\t', '\n\t\t A second possibility is that the availability of one meaning of a modifier is increased after encountering a prime using it with that sense . \n\t', '\n\t\t When the same modifier is encountered being used with a different sense in the target , the original sense is more accessible than the appropriate one . \n\t', '\n\t\t Hence , following the prime \x93sel de mer\x94 ( sea salt ) , participants may find it more difficult to interpret \x93mal de mer\x94 ( sea sickness ) because they are more likely to assume the \x93from the sea\x94 sense of the modifier instead of the correct \x93caused by the sea\x94 interpretation . \n\t', '\n\t\t An explanation of our results may be due to a combination of the above possibilities , both of which emphasise the co-dependence of the modifier and its associated relation . \n\t', '\n\t\t While our results have emphasised the link between modifier and relation , they do not suggest that modifier relational frequency is the only factor involved in selecting a plausible relation and it is likely that both the head and the modifier are involved in this process . \n\t', '\n\t\t In order to develop an accurate computational model of conceptual combination , future studies will need to consider the influence of other contributing factors . \n\t', '\n\t\t Certain heads and modifiers are strongly biased towards suggesting one particular relation . \n\t', '\n\t\t For instance , modifiers denoting substances are biased towards the <head MADE OF modifier> relation ( e.g. \x93plastic\x94 ) and in the same way , head nouns with a strongly associated schema , such as \x93factory\x94 , can be biased towards suggesting a certain relation . \n\t', '\n\t\t Furthermore , relation likelihood may be influenced by the presence of facilitating features ( Devereux & Costello , 2004 ) . \n\t', '\n\t\t Facilitating features are those features of a pair of concepts that are necessary for a given relation to be possible . \n\t', '\n\t\t For example a compound with the modifier \x93kitchen\x94 is unlikely to be interpreted using the <head MADE OF modifier> relation since kitchens are not a type of substance . \n\t', '\n\t\t Computational models of conceptual combination may have to account for the characteristics of heads and modifiers individually in order to simulate the ways in which each constituent influences relation selection . \n\t', '\n\t\t 4 Conclusion In summary , we investigated the influence of relation priming on the interpretation of French noun-noun compounds in order to ascertain whether the influence of the modifier observed in studies of English stems from its functional properties rather than the fact that it is encountered first . \n\t', '\n\t\t Our results showed that same and different- relation primes were equally effective when they shared the same head as the target , but that when they shared the same modifier the different-relation primes were less effective . \n\t', '\n\t\t This is consistent with findings from studies of English and suggests that the properties of the modifier and head noun remain consistent regardless of their order in a combination . \n\t', '\n\t\t While our results agree with predictions of the CARIN theory , we speculate that this effect may be due to different senses of the modifier being appropriate depending on its associated relation . \n\t', '\n\t\t Consequently modifiers using different relations are less effective at priming targets with the same relation used in a different sense . \n\t', '\n\t\t 5 Acknowledgements This research was funded by a UCD grant to the first author . \n\t', '\n\t\t We would like to thank Nicole Maguire for assistance in creating the French materials and we would also like to thank Rebecca Maguire for valuable comments and feedback . \n\t', '\n\t\t References Devereux , B. & Costello , F. J. ( 2004 ) . \n\t', '\n\t\t Learning relations between concepts : classification and conceptual combination . \n\t', '\n\t\t In Proceedings of the Twenty-Sixth Annual Conference of the Cognitive Science Society , ( Chicago ) . \n\t', '\n\t\t Hillsdale , NJ : Erlbaum . \n\t', '\n\t\t Gagné , C. L. ( 2001 ) . \n\t', '\n\t\t Relation and lexical priming during the interpretation of noun-noun combinations . \n\t', '\n\t\t Journal of Experimental Psychology : Learning , Memory and Cognition , 27 , 236-254 . \n\t', '\n\t\t Gagné , C. L. , & Shoben , E. J. ( 1997 ) . \n\t', '\n\t\t The influence of thematic relations on the comprehension of modifier-noun combinations . \n\t', '\n\t\t Journal of Experimental Psychology : Learning , Memory and Cognition , 23 , 71-87 . \n\t', '\n\t\t Corpus representativeness for syntactic information acquisition Núria BEL IULA , Universitat Pompeu Fabra La Rambla 30-32 08002 Barcelona Spain nuria.bel@upf.edu Abstract This paper refers to part of our research in the area of automatic acquisition of computational lexicon information from corpus . \n\t', '\n\t\t The present paper reports the ongoing research on corpus representativeness . \n\t', '\n\t\t For the task of inducing information out of text , we wanted to fix a certain degree of confidence on the size and composition of the collection of documents to be observed . \n\t', '\n\t\t The results show that it is possible to work with a relatively small corpus of texts if it is tuned to a particular domain . \n\t', '\n\t\t Even more , it seems that a small tuned corpus will be more informative for real parsing than a general corpus . \n\t', '\n\t\t 1 Introduction The coverage of the computational lexicon used in deep Natural Language Processing ( NLP ) is crucial for parsing success . \n\t', '\n\t\t But rather frequently , the absence of particular entries or the fact that the information encoded for these does not cover very specific syntactic contexts --as those found in technical texts\x97 make high informative grammars not suitable for real applications . \n\t', '\n\t\t Moreover , this poses a real problem when porting a particular application from domain to domain , as the lexicon has to be re-encoded in the light of the new domain . \n\t', '\n\t\t In fact , in order to minimize ambiguities and possible over-generation , application based lexicons tend to be tuned for every specific domain addressed by a particular application . \n\t', '\n\t\t Tuning of lexicons to different domains is really a delaying factor in the deployment of NLP applications , as it raises its costs , not only in terms of money , but also , and crucially , in terms of time . \n\t', '\n\t\t A desirable solution would be a \x91plug and play\x92 system that , given a collection of documents supplied by the customer , could induce a tuned lexicon . \n\t', '\n\t\t By \x91tuned\x92 we mean full coverage both in terms of : 1 ) entries : detecting new items and assigning them a syntactic behavior pattern ; and 2 ) syntactic behavior pattern : adapting the encoding of entries to the observations of the corpus , so as to assign a class that accounts for the occurrences of this particular word in that particular corpus . \n\t', '\n\t\t The question we have addressed here is to define the size and composition of the corpus we would need in order to get necessary and sufficient information for Machine Learning techniques to induce that type of information . \n\t', '\n\t\t Representativeness of a corpus is a topic largely dealt with , especially in corpus linguistics . \n\t', '\n\t\t One of the standard references is \n\t\t']",Positive
['\n\t\t The size and composition of the corpus to be observed has also been studied by general statistical NLP \n\t\t'],Positive
"['\n\t\t But most of these studies focused in having a corpus that actually models the whole language . \n\t', '\n\t\t However , we will see in section 3 that for inducing information for parsing we might want to model just a particular subset of a language , the one that corresponds to the texts that a particular application is going to parse . \n\t', '\n\t\t Thus , the research we report about here refers to aspects related to the quantity and optimal composition of a corpus that will be used for inducing syntactic information . \n\t', '\n\t\t In what follows , we first will briefly describe the observation corpus . \n\t', '\n\t\t In section 3 , we introduce the phenomena observed and the way we got an objective measure . \n\t', '\n\t\t In Section 4 , we report on experiments done in order to check the validity of this measure in relation with word frequency . \n\t', '\n\t\t In section 5 we address the issue of corpus size and how it affects this measure . \n\t', '\n\t\t 2 Experimental corpus description We have used a corpus of technical specialized texts , the CT . \n\t', '\n\t\t The CT is made of subcorpora belonging to 5 different areas or domains : Medicine , Computing , Law , Economy , Environmental sciences and what is called a General subcorpus made basically of news . \n\t', '\n\t\t The size of the subcorpora range between 1 and 3 million words per domain . \n\t', '\n\t\t The CT corpus covers 3 different languages although for the time being we have only worked on Spanish . \n\t', '\n\t\t For Spanish , the size of the subcorpora is stated in Table 1 . \n\t', '\n\t\t All texts have been processed and are annotated with morphosyntactic information . \n\t', '\n\t\t The CT corpus has been compiled as a test-bed for studying linguistic differences between general language and specialized texts . \n\t', '\n\t\t Nevertheless , for our purposes , we only considered it as documents that represent the language used in particular knowledge domains . \n\t', '\n\t\t In fact , we use them to simulate the scenario where a user supplies a collection of documents with no specific sampling methodology behind . \n\t', '\n\t\t 3 Measuring syntactic behavior : the case of adjectives We shall first motivate the statement that parsing lexicons require tuning for a full coverage of a particular domain . \n\t', '\n\t\t We use the term \x93full coverage\x94 to describe the ideal case where we would have correct information for all the words used in the ( unknown a priori ) set of texts we want a NLP application to handle . \n\t', '\n\t\t Note that full coverage implies two aspects . \n\t', '\n\t\t First , type coverage : all words that are used in a particular domain are in the lexicon . \n\t', '\n\t\t Second , that the information contained in the lexicon is the information needed by the grammar to parse every word occurrence as intended . \n\t', '\n\t\t Full coverage is not guaranteed by working with \x91general language\x92 dictionaries . \n\t', '\n\t\t Grammar developers know that the lexicon must be tuned to the application\x92s domain , because general language dictionaries either contain too much information , causing overgeneration , or do not cover every possible syntactic context , some of them because they are specific of a particular domain . \n\t', '\n\t\t The key point for us was to see whether texts belonging to a domain justify this practice . \n\t', '\n\t\t In order to obtain objective data about the differences among domains that motivate lexicon tuning , we have carried out an experiment to study the syntactic behavior ( syntactic contexts ) of a list of about 300 adjectives in technical texts of four different domains . \n\t', '\n\t\t We have chosen adjectives because their syntactic behavior is easy to be captured by bigrams , as we will see below . \n\t', '\n\t\t Nevertheless , the same methodology could have been applied to other open categories . \n\t', '\n\t\t The first part of the experiment consisted of computing different contexts for adjectives occurring in texts belonging to 4 different domains . \n\t', '\n\t\t We wanted to find out how significant could different uses be ; that is , different syntactic contexts for the same word depending on the domain . \n\t', '\n\t\t We took different parameters to characterize what we call \x91syntactic behavior\x92 . \n\t', '\n\t\t For adjectives , we defined 5 different parameters that were considered to be directly related with syntactic patterns . \n\t', '\n\t\t These were the following contexts : 1 ) pre-nominal position , e.g. \x91importante decisión\x92 ( important decision ) 2 ) post-nominal position , e.g. \x91decisión importante\x92 3 ) \x91ser\x92 copula1 predicative position , e.g. \x91la decisión es importante\x92 ( the decision is important ) 4 ) \x91estar\x92 copula predicative position , e.g. \x91la decisión está interesante/*importante\x92 ( the decision is interesting/important ) 5 ) modified by a quantity adverb , e.g. \x91muy interesante\x92 ( very interesting ) . \n\t', '\n\t\t Table 1 shows the data gathered for the adjective \x93paralelo\x94 ( parallel ) in the 4 different domain subcorpora . \n\t', '\n\t\t Note the differences in the position 3 ( \x91ser\x92 copula ) when observed in texts on computing , versus the other domains . \n\t', '\n\t\t Corpora/n.of occurrences 1 2 3 4 5 general ( 3.1 M words ) 1 61 29 3 0 computing ( 1.2 M words ) 4 30 0 0 0 medecine ( 3.7 M words ) 3 67 22 1 0 economy ( 1 M words ) 0 28 6 0 0 Table 1 : Computing syntactic contexts as behaviour The observed occurrences ( as in Table 1 ) were used as parameters for building a vector for every lemma for each subcorpus . \n\t', '\n\t\t We used cosine distance2 ( CD ) to measure differences among the occurrences in different subcorpora . \n\t', '\n\t\t The closer to 0 , the more significantly different , the closer to 1 , the more similar in their syntactic behavior in a particular subcorpus with respect to the general subcorpus . \n\t', '\n\t\t Thus , the CD values for the case of \x91paralelo\x92 seen in Table 1 are the following : Corpus Cosine Distance computing 0.7920 economy 0.9782 medecine 0.9791 Table 2 : CD for \x91paralelo\x92 compared to the general corpus 1 Copulative sentences are made of 2 different basic copulative verbs \x91ser\x92 and \x91estar\x92 . \n\t', '\n\t\t Most authors tend to express as \x91lexical idyosincracy\x92 preferences shown by particular adjectives as to go with one of them or even with both although with different meaning . \n\t', '\n\t\t 2 Cosine distance shows divergences that have to do with large differences in quantity between parameters in the same position , whether small quantities spread along the different parameters does not compute significantly . \n\t', '\n\t\t Cosine distance was also considered to be interesting because it computes relative weight of parameters within the vector . \n\t', '\n\t\t Thus we are not obliged to take into account relative frequency , which is actually different according to the different domains . \n\t', '\n\t\t What we were interested in was identifying significant divergences , like , in this case , the complete absence of predicative use of the adjective \x91paralelo\x92 in the computing corpus . \n\t', '\n\t\t The CD measure has been sensible to the fact that no predicative use has been observed in texts on computing , the CD going down to 0.7 . \n\t', '\n\t\t Cosine distance takes into account significant distances among the proportionality of the quantities in the different features of the vector . \n\t', '\n\t\t Hence we decided to use CD to measure the divergence in syntactic behavior of the observed adjectives . \n\t', '\n\t\t Figure 1 plots CD for the 4 subcorpora ( Medicine , Computing , Economy ) compared each one with the general subcorpus . \n\t', '\n\t\t It corresponds to the observations for about 300 adjectives , which were present in all the corpora . \n\t', '\n\t\t More than a half for each corpus is in fact below the 0.9 of similarity . \n\t', '\n\t\t Recall also that this mark holds for the different corpora , independently of the number of tokens ( Economy is made of 1 million words and Medicine of 3 ) . \n\t', '\n\t\t 1,2 1 0,8 0,6 0,4 0,2 0 -0,2 Figure 1 : Cosine distance for the 4 different subcorpus The data of figure 1 would allow us to conclude that for lexicon tuning , the sample has to be rich in domain dependent texts . \n\t', '\n\t\t 4 Frequency and CD measure For being sure that CD was a good measure , we checked to what extent what we called syntactic behavior differences measured by a low CD could be due to a different number of occurrences in each of the observed subcorpora . \n\t', '\n\t\t It would have been reasonable to think that when something is seen more times , more different contexts can be observed , while when something is seen only a few times , variations are not that significant . \n\t', '\n\t\t Figure 2 : Difference in n. of observations in 2 corpora and CD Figure 2 relates the obtained CD and the frequency for every adjective . \n\t', '\n\t\t For being able to do it , we took the difference of occurrences in two subcorpora as the frequency measure , that is , the number resulting of subtracting the occurrences in the computing subcorpus from the number of occurrences in the general subcorpus . \n\t', '\n\t\t It clearly shows that there is no regular relation between different number of occurrences in the two corpora and the observed divergence in syntactic behavior . \n\t', '\n\t\t Those elements that have a higher CD ( 0.9 ) range over all ranking positions : those that are 100 times more frequent in one than in other , etc. . \n\t', '\n\t\t Thus we can conclude that CD do capture syntactic behavior differences that are not motivated by frequency related issues . \n\t', '\n\t\t 5 Corpus size and syntactic behavior We also wanted to see the minimum corpus size for observing syntactic behavior differences clearly . \n\t', '\n\t\t The idea behind was to measure when CD gets stable , that is , independent of the number of occurrences observed . \n\t', '\n\t\t This measure would help us in deciding the minimum corpus size we need to have a reasonable representation for our induced lexicon . \n\t', '\n\t\t In fact our departure point was to check whether syntactic behavior could be compared with the figures related to number of types ( lemmas ) and number of tokens in a corpus . \n\t', '\n\t\t Biber 1993 , Sánchez and Cantos , 1998 , demonstrate that the number of new types does not increase proportionally to the number of words once a certain quantity of texts has been observed . \n\t', '\n\t\t In our experiment , we split the computing corpus in 3 sets of 150K , 350K and 600K words in order to compare the CD\x92s obtained . \n\t', '\n\t\t In Figure 3 , 1 represents the whole computing corpus of 1,200K for the set of 300 adjectives we had worked with before . \n\t', '\n\t\t 0 0,2 0,4 0,6 0,8 1 1,2 -500 2500 2000 1500 1000 500 0 1,2 1 0,8 0,6 0,4 0,2 105K 351 K 603K 3M GEN 0 Figure 3 : CD of 300 adjs . \n\t', '\n\t\t in different size subcorpora and general corpus As shown in Figure 3 , the results of this comparison were conclusive : for the computing corpus , with half of the corpus , that is around 600K , we already have a good representation of the whole corpus . \n\t', '\n\t\t The CD being superior to 0.9 for all adjectives ( mean is 0.97 and 0.009 of standard deviation ) . \n\t', '\n\t\t Surprisingly , the CD of the general corpus , the one that is made of 3 million words of news , is lower than the CD achieved for the smallest computing subcorpus . \n\t', '\n\t\t Table 3 shows the mean and standard deviation for all de subcorpora ( CC is Computing Corpus ) . \n\t', '\n\t\t Corpus size mean st. deviation CC 150K 0.81 0.04 CC 360K 0.93 0.01 CC 600K 0.97 0.009 CC 1.2 M 1 0 General 3M 0.75 0.03 Table 3 : Comparing corpus size and CD What Table 3 suggests is that according to CD , measured as shown here , the corpus to be used for inducing information about syntactic behavior does not need to be very large , but made of texts representative of a particular domain . \n\t', '\n\t\t It is part of our future work to confirm that Machine Learning Techniques can really induce syntactic information from such a corpus . \n\t', '\n\t\t References Biber , D. 1993 . \n\t', '\n\t\t Representativeness in corpus design . \n\t', '\n\t\t Literary and Linguistic Computing 8 : 243-257 . \n\t', '\n\t\t Lauer , M. 1995 . \n\t', '\n\t\t \x93How much is enough ? \n\t', '\n\t\t Data requirements for Statistical NLP\x94 . \n\t', '\n\t\t In 2nd . \n\t', '\n\t\t Conference of the Pacific Association for Computational Linguistics . \n\t', '\n\t\t Brisbane , Australia . \n\t', '\n\t\t Sánchez , A. & Cantos P. , 1997 , \x93Predictability of Word Forms ( Types ) and Lemmas in Linguistic Corpora , A Case Study Based on the Analysis of the CUMBRE Corpus : An 8-Million-Word Corpus of Contemporary Spanish,\x94 In International Journal of Corpus Linguistics Vol. 2 , No. 2 . \n\t', '\n\t\t Schone , P & D. Jurafsky . \n\t', '\n\t\t 2001. Language- Independent induction of part of speech class labels using only language universals . \n\t', '\n\t\t Proceedings IJCAI , 2001 . \n\t', '\n\t\t Yang , D-H and M. Song . \n\t', '\n\t\t 1999. \x93The Estimate of the Corpus Size for Solving Data Sparseness\x94 . \n\t', '\n\t\t Journal of KISS , 26(4) : 568-583 . \n\t', '\n\t\t Zernik , U. Lexical Acquisition . \n\t', '\n\t\t 1991. Exploiting On-Line Resources to Build a Lexicon . \n\t', '\n\t\t Lawrence Erlbaum Associates : 1-26 . \n\t', '\n\t\t Exploiting Unannotated Corpora for Tagging and Chunking Rie Kubota Ando IBM T.J. Watson Research Center 19 Skyline Dr. , Hawthorne , NY 10532 riel@us.ibm.com Abstract We present a method that exploits unannotated corpora for compensating the paucity of anno- tated training data on the chunking and tagging tasks . \n\t', '\n\t\t It collects and compresses feature frequencies from a large unannotated corpus for use by linear classifiers . \n\t', '\n\t\t Experiments on two tasks show that it consistently produces significant performance improvements . \n\t', '\n\t\t 1 Introduction This paper presents a method for exploiting large unannotated corpora for the tagging and chunking tasks . \n\t', '\n\t\t We report experiments on entity mention detection ) and part-of-speech ( POS ) tagging . \n\t', ""\n\t\t To apply classification tech- niques to chunking tasks , a common approach is to cast the task to that of token tagging , where token tags encode chunk information , e.g. , `13-PERSON ' ( beginning of person chunk ) , ` I-PERSON ' ( inside of person chunk ) , and ` O ' ( outside of any entity chunk ) . \n\t"", '\n\t\t The challenge for a classifier is to learn unknown relationships between token tags and features ( such as token strings and context information ) from tagged examples . \n\t', '\n\t\t To achieve reasonable performance , a sufficiently large number of representative ex- amples are required . \n\t', '\n\t\t Our goal is to compensate for the paucity of tagged examples or their differences from test data , by using untagged examples . \n\t', '\n\t\t One type of approaches to this problem involves iterative and automatic tagging of the untagged data such as bootstrapping or co- training . \n\t', '\n\t\t Expectation Maximization ( EM ) also uses untagged data for iteratively improving model parameters . \n\t', ""\n\t\t Another type uses untagged ' The task objective of entity mention detection is to detect and classify text spans that mention ( or refer to ) certain types of entities in the real world such as per sons and organizations . \n\t"", '\n\t\t We experiment with the data from the ACE ( Automatic Content Extraction ) program ( http://www.nist.gov/speech/index.htm ) . \n\t', '\n\t\t corpora for improving feature representation , e.g. \n\t\t']",Positive
"['\n\t\t We take the latter ap- proach . \n\t', '\n\t\t To see how unannotated corpora may help tagging , consider the following examples : \x95 \x95 \x95 the president W13-PERSON and \x95 \x95 \x95 \x95 \x95 \x95 our chairman/13-PERSON is \x95 \x95 \x95 Suppose that "" president "" appeared in the train- ing data , but "" chairman "" did n\'t , and that in a large corpus , both words ( "" chairman "" and "" president "" ) often appear as the subject of "" said "" , "" visited "" , etc. , and that both are often modified by "" vice "" , "" powerful "" , etc. . \n\t', '\n\t\t It is intuitive that such corpus statistics would help a classifier to tag "" chairman "" correctly even if "" chairman "" did not appear in the training data . \n\t', '\n\t\t Given some set of features designed for the task ( see Figure 1 for example ) , we count feature occurrences in all the word instances in the unannotated corpus to generate feature-byword co-occurrence frequency matrices . \n\t', '\n\t\t When we encounter a training or test instance of word w , we generate two kinds of features . \n\t', '\n\t\t One is the features observed in that instance ( as usual ) . \n\t', ""\n\t\t The other is the features derived from the columns ( corresponding to w ) of the featureby-word co-occurrence matrices \x97 collections of w 's context in the untagged corpus \x97 which we call corpus-context features . \n\t"", '\n\t\t Our experiments show that the corpus- context features consistently improve performance on the two tasks . \n\t', '\n\t\t There are two important elements for achieving such effectiveness in this simple framework . \n\t', '\n\t\t One is a high- performance linear classifier , Robust Risk Minimization ( RRM ) \n\t\t']",Positive
"['\n\t\t ( RRM learns feature weights by minimizing classifica- tion errors with regularization on the tagged training data . \n\t', ""\n\t\t ) Therefore , we take a ` feature- rich ' strategy to use a variety of types of cor- pus context information . \n\t"", '\n\t\t To enable classifier training with many types of corpus statistics , such vast amounts of information from a large corpus must be compressed . \n\t', '\n\t\t Hence , the sec- ond key element is a dimension reduction tech- nique . \n\t', '\n\t\t We adapt a variation of LSI , specifi- cally designed for feature occurrence frequen- cies \n\t\t']",Positive
"['\n\t\t As such , the objective of this paper is to show that a right combination of techniques produces a useful tool for coping with the paucity of tagged training data . \n\t', '\n\t\t 2 Method 2.1 Collecting corpus statistics From a given set of features designed for the task ( see Figure 1 and Figure 6 for example ) , we use context features only ( i.e. , excluding features that strongly depend on words 2 ) to gener- ate feature-by-word co-occurrence matrices . \n\t', ""\n\t\t We generate one matrix for each type , e.g. , a ` left adjacent word'-by-word matrix , a ` right adja- cent word'-by-word matrix , and so forth . \n\t"", '\n\t\t 2.2 Vector compression To compress feature-by-word matrices , we adapt a procedure proposed for semantic lexicon construction \n\t\t']",Positive
"[""\n\t\t That is to apply singular value decomposition ( SVD ) only to a smaller matrix consisting of several selected columns of the co-occurrence matrix and to ` fold in ' the rest of the columns to the reduced dimensions . \n\t"", '\n\t\t The choice of columns is important . \n\t', '\n\t\t The columns corresponding to the most frequent words should be selected . \n\t', '\n\t\t The intuition behind its theoretical justification \n\t\t']",Positive
"['\n\t\t Thus , we choose k most frequent words and reduce the dimensions to h . \n\t', '\n\t\t The dimensionality h should be no smaller than the number of target classes3 . \n\t', '\n\t\t We compress each of feature-by-word co- occurrence matrix independently of one another . \n\t', ""\n\t\t This is important , as it gives more freedom to 2For instance , it is useless to count ` co-occurrences ' of words and their endings . \n\t"", '\n\t\t Moreover , features that are nearly conditionally independent of words given classes are more useful for the purpose , since ultimately we want to capture correlations of words to classes ( through their co-occurrences with features ) rather than their correlations to specific features . \n\t', '\n\t\t 3Intuitively , there need at least h dimensions to express correlations to h classes . \n\t', '\n\t\t Ptoken , capitalization , POS in 3-token window Pbi-grams of adjacent words in ~-token window Pwords in the same syntactic chunk ~ Phead words in ~-chunk window Pword uni- and bi-grams based on subject-verbobject and preposition-noun constructions . \n\t', '\n\t\t Psyntactic chunk types Ptags in 2-token window to the left Ptri-grams of POS , capitalization , and word ending Ptri-gra~s of POS , capitalization , and left tag Figure 1 : Features for entity detection sophisticated classifiers to weight relevant types of features more heavily than irrelevant ones . \n\t', '\n\t\t If all are compressed together , the classifiers can not tear them apart . \n\t', '\n\t\t For efficient training , though optionally , we further reduce non-zero entries by zeroing out all but n entries that have the largest absolute values in each compressed vector . \n\t', '\n\t\t We call the entries of the resultant vec- tors ÑÍÕÓîØÖÑÍ~~~ö~ features . \n\t', ""\n\t\t For a training or test instance of word w , we have two kinds of features : features derived from the instance ( as usual ) , and the corpus-context features generated from w 's context in the corpus . \n\t"", '\n\t\t For our experiments , we set ( k , h , n ) _ ( 1000 , 50 , 6 ) using held-out data ( the development set described below ) . \n\t', '\n\t\t Performance is rel- atively insensitive to the changes of these pa- rameters4 . \n\t', '\n\t\t We use the same parameter setting for both entity mention detection and part-ofspeech tagging experiments . \n\t', '\n\t\t 3 Entity mention detection experiments 3.1 Experimental framework Entity classes and evaluation metric We experiment with 10 classes from the ACE entity classes \x97 obtained by combining five entity types ( Person , Organization , Facility , GPE , Location ) and two mention types ( Name , Nominal ) , which make 21-way classification when chunk boundary information is encoded into token tags . \n\t', '\n\t\t Proposed mention chunks are counted as correct only if both mention boundaries and classes are correct . \n\t', '\n\t\t We combine precision and recall into F-measure with equal weight . \n\t', '\n\t\t Features Figure 1 describes features used for entity mention detection experiments . \n\t', '\n\t\t We generate corpus-context features from the features ~~n the held-out data , k E [ 1000,5000 ] produced essentially similar performance , and so did h E [ 30,60 ] and n E [ 6,10 ] . \n\t', '\n\t\t 50 40 30 20 10 0 Per Name Loc nom Fac Name Loc Name Fac nom GPE nom Org nom Org Name Per nom GPE Name Ptoken , capitalization in 5-token windows ending ( length 1 to 4 ) Puni - and bi-grams of tags at the left Ptag-word bi-grams in 3-token windows Pbi-grams of adjacent words in 5-token windows Figure 6 : Features for POS tagging RRM HMM Corpus-ctx ~- with BW w/o 5K 90.2 ( + 7.4 ) 82 .8 82.1 ( +5.0 ) 77 .1 9K 92.7 ( + 5.0 ) 87 .7 84.9 ( +2.7 ) 82 .2 19K 93.7 ( + 2.8 ) 90 .9 87.1 ( +0.3 ) 86 .8 38K 94.7 ( + 1.8 ) 92 .9 89.8 ( -0.2 ) 89 .6 75K 95.2 ( + 1.6 ) 93 .6 91.2 ( -0.6 ) 91 .8 149K 95.6 ( + 0.9 ) 94 .7 92.3 ( -1.0 ) 93.3 Figure 7 : POS tagging accuracy results . \n\t', '\n\t\t ~umbers in parentheses are differences from their counterparts that do not use the untagged corpus . \n\t', '\n\t\t pus ) , indeed , compensates for the differences between tagged training data ( ACE ) and test data ( CNS ) . \n\t', '\n\t\t The other classifiers are apparently suffering from the dissimilarity . \n\t', '\n\t\t 4 POS Tagging Experiments Features Figure 6 shows the features we use for POS tagging . \n\t', '\n\t\t Among them , we use word uni- and bi-grams that do not overlap with the current word , to generate corpus-context fea- tures . \n\t', '\n\t\t Baseline As our baseline , we implement an HMM tagger with and without ~Ôî~ÖW~ðÑÙ reestimation ( EM for HMM ) . \n\t', '\n\t\t We smooth transition probabilities by deleted interpolation . \n\t', '\n\t\t For unseen and low-frequency words , word emission probabilities are estimated as \n\t\t']",Positive
"['\n\t\t We estimate these probabilities by relative frequencies in tagged training corpora , and perform 10 EM iterations using unannotated data . \n\t', '\n\t\t To avoid underestimating the baseline , we report its best performance among the iterations . \n\t', '\n\t\t POS tagging results We report results on the standard Brown corpus . \n\t', '\n\t\t The test data was fixed to arbitrarily-drawn one fifth of the corpus ( 230K words ) . \n\t', '\n\t\t We use the rest ( 930K words ) as tagged and untagged training data : all 930K words as untagged data for collecting corpus context and for the BW reestima- tion ; and arbitrarily-drawn various portions as tagged training data . \n\t', '\n\t\t Figure 7 shows accuracy ( # of correctly tagged words divided by # of words ) in relation to the number of tagged training examples . \n\t', ""\n\t\t The performance differ- ences between HMM and RRM mainly derive from the differences in the ` richness ' of information they make use of . \n\t"", '\n\t\t The additional fea- tures5 used by RRM are apparently effective for compensating for the paucity of the tagged data . \n\t', '\n\t\t Corpus-context features further improve the performance up to 7.4 % . \n\t', '\n\t\t This is in contrast to the Baum-Welch reestimation , which sometimes rather degrades performance . \n\t', '\n\t\t 5 Conclusion The method we present is intended for the chunking tagging tasks in which words serve as strongly effective features . \n\t', '\n\t\t Performance im- provements obtained by corpus-context features are especially large when tagged training is small or different from test data , which is useful for expediting the adaptation of the system to new domains . \n\t', '\n\t\t Acknowledgements This work was supported by the Advanced Re- search and Development Activity under the Novel Intelligence and Massive Data ( NIMD ) program PNWD-SW-6059 . \n\t', '\n\t\t References Rie Kubota Ando . \n\t', '\n\t\t 2004. Semantic lexicon con- struction : Learning from unlabeled data via spectral analysis . \n\t', '\n\t\t In Proceedings of CoNLL- 2004 . \n\t', '\n\t\t Hinrich Schuetze . \n\t', '\n\t\t 1992. Dimensions of mean- ing . \n\t', ""\n\t\t In Proceedings of Supercomputing'92 , pages 787-796 . \n\t"", '\n\t\t Ralph Weischedel , Marie Meteer , Richard Schwartz , Lance Ramshaw , and Jeff Pal- mucci . \n\t', '\n\t\t 1993. Coping with ambiguity and unknown words through probabilistic models . \n\t', '\n\t\t Computational Linguistics , 19(2):359-382 . \n\t', '\n\t\t Tong Zhang , Fred Damerau , and David Johnson . \n\t', '\n\t\t 2002. Text chunking based on a generalization of Winnow . \n\t', '\n\t\t Journal of Machine Learning Research , 2:615-637 . \n\t', ""\n\t\t ' As many of the features used with RRM are mutually dependent , there is no easy way to exploit them with HMM . \n\t"", '\n\t\t However , we note that when trained with over one million tagged examples , RRM ( with and without corpus context ) and HMM taggers produce essentially similar high accuracy . \n\t', '\n\t\t That is , the mutually-dependent features become redundant once sufficiently large tagged data becomes available . \n\t', '\n\t\t Improving Bitext Word Alignments via Syntax-based Reordering of English Elliott Franco Dr´abek and David Yarowsky Department of Computer Science Johns Hopkins University Baltimore , MD 21218 , USA {edrabek,yarowsky}@cs.jhu.edu Abstract We present an improved method for automated word alignment of parallel texts which takes advantage of knowledge of syntactic divergences , while avoiding the need for syntactic analysis of the less resource rich language , and retaining the robustness of syntactically agnostic approaches such as the IBM word alignment models . \n\t', '\n\t\t We achieve this by using simple , easily-elicited knowledge to produce syntax- based heuristics which transform the target language ( e.g. English ) into a form more closely resembling the source language , and then by using standard alignment methods to align the transformed bitext . \n\t', '\n\t\t We present experimental results under variable resource conditions . \n\t', '\n\t\t The method improves word alignment performance for language pairs such as English-Korean and English-Hindi , which exhibit longer-distance syntactic divergences . \n\t', '\n\t\t 1 Introduction Word-level alignment is a key infrastructural technology for multilingual processing . \n\t', '\n\t\t It is crucial for the development of translation models and translation lexica ( Tufi~s , 2002 ; Melamed , 1998 ) , as well as for translingual projection \n\t\t']",Positive
['\n\t\t It has increasingly attracted attention as a task worthy of study in its own right \n\t\t'],Positive
['\n\t\t Syntax-light alignment models such as the five IBM models \n\t\t'],Negative
"['\n\t\t However , these models have been less successful at modeling syntactic distortions with longer distance movement . \n\t', '\n\t\t In contrast , more syntactically informed approaches have been constrained by the often weak syntactic correspondences typical of real-world parallel texts , and by the difficulty of finding or inducing syntactic parsers for any but a few of the world\x92s most studied languages . \n\t', ""\n\t\t Our approach uses simple , easily-elicited knowledge of divergences to produce heuristic syntax- based transformations from English to a form ( English ' ) more closely resembling the source lan- Figure 1 : System Architecture guage , and then using standard alignment methods to align the transformed version to the target language . \n\t"", '\n\t\t This approach retains the robustness of syntactically agnostic models , while taking advantage of syntactic knowledge . \n\t', '\n\t\t Because the approach relies only on syntactic analysis of English , it can avoid the difficulty of developing a full parser for a new low-resource language . \n\t', '\n\t\t Our method is rapid and low cost . \n\t', '\n\t\t It requires only coarse-grained knowledge of basic word order , knowledge which can be rapidly found in even the briefest grammatical sketches . \n\t', '\n\t\t Because basic word order changes very slowly with time , word order of related languages tends to be very similar . \n\t', '\n\t\t For example , even if we only know that a language is of the Northern-Indian/Sanskrit family , we can easily guess with high confidence that it is systematically head-final . \n\t', '\n\t\t Because our method can be restricted to only bi-text pre-processing and post-processing , it can be used as a wrapper around any existing word-alignment tool , without modification , to provide improved performance by minimizing alignment distortion . \n\t', '\n\t\t 2 Prior Work The 2003 HLT-NAACL Workshop on Building and Using Parallel Texts \n\t\t']",Positive
"[""\n\t\t There is prior work studying systematic cross- English Source I\\I English Language- specific Heuristics Transform Retrace Traces Run GIZA++ English\x92 Source I/ I English\x92 Source plutoniyama kaa istemaala paramaanu hathiyaara banaane ke lie hotaa hai plutonium \x92s use nuclear weapons manufacture to is NP VP NP NP PP use of plutonium is to manufacture nuclear weapons the VP VP S English : Hindi : Figure 3 : Transformed Hindi-English ' sentence pair with gold-standard word-alignments . \n\t"", '\n\t\t Rotated nodes are marked with an arc . \n\t', '\n\t\t Figure 2 : Original Hindi-English sentence pair with gold-standard word-alignments . \n\t', '\n\t\t S NP VP VP NP NP English\x92 : plutonium of the use nuclear weapons manufacture to is VP PP Hindi : plutoniyama kaa istemaala paramaanu hathiyaara banaane ke lie hotaa hai plutonium \x92s use nuclear weapons manufacture to is linguistic structural divergences , such as the DUSTer system \n\t\t']",Negative
"['\n\t\t While the focus on major classes of structural variation such as manner-ofmotion verb-phrase transformations have facilitated both transfer and generation in machine translation , these divergences have not been integrated into a system that produces automatic word alignments and have tended to focus on more local phrasal variation rather than more comprehensive sentential syntactic reordering . \n\t', '\n\t\t Complementary prior work ( e.g. Wu , 1995 ) has also addressed syntactic transduction for bilingual parsing , translation , and word-alignment . \n\t', '\n\t\t Much of this work depends on high-quality parsing of both target and source sentences , which may be unavailable for many \x93lower density\x94 languages of interest . \n\t', '\n\t\t Tree-to-string models , such as \n\t\t']",Positive
"['\n\t\t By contrast , our method retains the robustness of the underlying aligner towards loose translations , and can if necessary use knowledge of syntactic divergences even in the absence of any training corpora whatsoever , using only a translation lexicon . \n\t', '\n\t\t 3 System Figure 1 shows the system architecture . \n\t', '\n\t\t We start by running the Collins parser \n\t\t']",Positive
"['\n\t\t Chinese has both prepositions and postpositions . \n\t', '\n\t\t resulting trees . \n\t', ""\n\t\t This yields English ' text , along with traces recording correspondences between English ' words and the English originals . \n\t"", '\n\t\t We use GIZA++ \n\t\t']",Positive
"['\n\t\t Finally , we use the traces to map these alignments to the original English words . \n\t', '\n\t\t Figure 2 shows an illustrative Hindi-English sentence pair , with true word alignments , and parse- tree over the English sentence . \n\t', '\n\t\t Although it is only a short sentence , the large number of crossing alignments clearly show the high-degree of reordering , and especially long-distance motion , caused by the syntactic divergences between Hindi and English . \n\t', ""\n\t\t Figure 3 shows the same sentence pair after English has been transformed into English ' by our system . \n\t"", '\n\t\t Tree nodes whose children have been reordered 55 50 45 40 35 30 25 75 70 65 60 55 50 45 40 35 E\x92 Method Drect 3 3.2 3.4 3.6 3trauung 4 4.2 4.4 4.6 4.8 log(number o sentences ) 335 4 , 4.5 5 log(number of training sentences ) Figure 4 : Hindi alignment performance E\x92 Method Direct 5 0 15 10 3 3.2 n3.4 3.6 3 $ 4 4.2 4.4 log(number of training sentences ) Figure 5 : Korean alignment performance are marked by a subtended arc . \n\t', '\n\t\t Crossings have been eliminated , and the alignment is now monotonic . \n\t', '\n\t\t Table 1 shows the basic word order of three major phrase types for each of the languages we treated . \n\t', '\n\t\t In each case , our heuristics transform the English trees to achieve these same word orders . \n\t', '\n\t\t For the Chinese case , we apply several more language-specific transformations . \n\t', '\n\t\t Because Chinese has both prepositions and postpositions , we retain the original preposition and add an additional bracketing postposition . \n\t', '\n\t\t We also move verb modifiers other than noun phrases to the left of the head verb . \n\t', ""\n\t\t 4 Experiments For each language we treated , we assembled sentence-aligned , tokenized training and test corpora , with hand-annotated gold-standard word alignments for the latter ' . \n\t"", '\n\t\t We did not apply any sort of morphological analysis beyond basic word tokenization . \n\t', '\n\t\t We measured system performance with wa eval align.pl , provided by Rada Mihalcea and Ted Pedersen . \n\t', '\n\t\t Each training set provides the aligner with information about lexical affinities and reordering patterns . \n\t', '\n\t\t For Hindi , Korean and Chinese , we also tested our system under the more difficult situation of having only a bilingual word list but no bitext available . \n\t', ""\n\t\t This is a plausible low-resource language scenario Figure 6 : Chinese alignment performance 3 3.2 3.4 3.6 3.6 4 4.2 4.4 4.6 log(number of training sentences ) Figure 7 : Romanian alignment performance # Train Sents Direct English ' P R F P R F Hindi Dict only 16.4 13.8 15.0 18.5 15.6 17.0 1000 26.8 23.0 24.8 28.4 24.4 26.2 3162 35.7 31.6 33.5 38.4 33.5 35.8 10000 46.6 42.7 44.6 50.4 45.2 47.6 31622 60.1 56.0 58.0 63.6 58.5 61.0 63095 64.7 61.7 63.2 66.3 62.2 64.2 Korean Dict only 26.6 12.3 16.9 27.5 12.9 17.6 1000 9.4 7.3 8.2 11.3 8.7 9.8 3162 13.2 10.2 11.5 16.0 12.4 14.0 10000 15.2 12.0 13.4 17.0 13.3 14.9 30199 21.5 16.9 18.9 21.9 17.2 19.3 Chinese Dict only 44.4 30.4 36.1 44.5 30.5 36.2 1000 33.0 22.2 26.5 30.8 22.6 26.1 3162 44.6 28.9 35.1 41.7 30.0 34.9 10000 51.1 34.0 40.8 50.7 35.8 42.0 31622 60.4 39.0 47.4 55.7 39.7 46.4 100000 66.0 43.7 52.6 63.7 45.4 53.0 Romanian 1000 49.6 27.7 35.6 50.1 28.0 35.9 3162 57.9 33.4 42.4 57.6 33.0 42.0 10000 72.6 45.5 55.9 71.3 45.0 55.2 48441 84.7 57.8 68.7 83.5 57.1 67.8 Table 2 : Performance in Precision , Recall , and F- measure ( per cent ) of all systems . \n\t"", ""\n\t\t 70 65 60 55 50 45 40 35 30 25 20 25 20 E\x92 Method Drect E\x92 Meth d Drect Source # Test Sents Mean Length Correlation Language Direct E ' Hindi 46 16.3 54.1 60.1 Korean 100 20.2 10.2 31.6 Chinese 88 26.5 60.2 63.7 Romanian 248 22.7 81.1 80.6 Table 3 : Test set characteristics , including number of sentence pairs , mean length of English sentences , and correlation r2 between English and source- language normalized word positions in gold-standard data , for direct and English ' situations . \n\t"", '\n\t\t and a test of the ability of the system to take sole responsibility for knowledge of reordering . \n\t', '\n\t\t Table 3 describes the test sets and shows the correlation in gold standard aligned word pairs between the position of the English word in the English sentence and the position of the source-language word in the source-language sentence ( normalizing the positions to fall between 0 and 1 ) . \n\t', ""\n\t\t The baseline ( direct ) correlations give quantitative evidence of differing degrees of syntactic divergence with English , and the English ' correlations demonstrate that our heuristics do have the effect of better fitting source language word order . \n\t"", ""\n\t\t 5 Results Figures 4 , 5 , 6 and 7 show learning curves for systems trained on parallel sentences with and without the English ' transforms . \n\t"", '\n\t\t Table 2 provides further detail , and also shows the performance of systems trained without any bitext , but only with access to a bilingual translation lexicon . \n\t', '\n\t\t Our system achieves consistent , substantial performance improvement under all situations for English-Hindi and English-Korean language pairs , which exhibit longer distance SOV->SVO syntactic divergence . \n\t', '\n\t\t For English-Romanian and English-Chinese , neither significant improvement nor degradation is seen , but these are language pairs with quite similar sentential word order to English , and hence have less opportunity to benefit from our syntactic transformations . \n\t', ""\n\t\t 6 Conclusions We have developed a system to improve the performance of bitext word alignment between English and a source language by first reordering parsed English into an order more closely resembling that ' Hindi training : news text from the LDC for the 2003 DARPA TIDES Surprise Language exercise ; Hindi testing : news text from Rebecca Hwa , then at the University of Maryland ; Hindi dictionary : The Hindi-English Dictionary , v. 2.0 from IIIT ( Hyderabad ) LTRC ; Korean training : Unbound Bible ; Korean testing : half from Penn Korean Treebank and half from Universal declaration of Human Rights , aligned by Woosung Kim at the Johns Hopkins University ; Korean dictionary : EngDic v. 4 ; Chinese training : news text from FBIS ; Chinese testing : Penn Chinese Treebank news text aligned by Rebecca Hwa , then at the University of Maryland ; Chinese dictionary : from the LDC ; Romanian training and testing : \n\t\t""]",Positive
"['\n\t\t of the source language , based only on knowledge of the coarse basic word order of the source language , such as can be obtained from any cross- linguistic survey of languages , and requiring no parsing of the source language . \n\t', '\n\t\t We applied the system to the task of aligning English with Hindi , Korean , Chinese and Romanian . \n\t', '\n\t\t Performance improvement is greatest for Hindi and Korean , which exhibit longer-distance constituent reordering with respect to English . \n\t', ""\n\t\t These properties suggest the proposed English ' word alignment method can be an effective approach for word alignment to languages with both greater cross-linguistic word-order divergence and an absence of available parsers . \n\t"", '\n\t\t References P. F. Brown , S. A. Della Pietra , V. J. Della Pietra , and R. L. Mercer . \n\t', '\n\t\t 1993. The mathematics of statistical machine translation : Parameter estimation . \n\t', '\n\t\t Computational Linguistics , 19(2):263\x96311 . \n\t', '\n\t\t M. Collins . \n\t', '\n\t\t 1999. Head-Driven Statistical Models for Natural Language Parsing . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , University of Pennsylvania . \n\t', '\n\t\t B. J. Dorr , L. Pearl , R. Hwa , and N. Habash . \n\t', '\n\t\t 2002. DUSTer : A method for unraveling cross-language divergences for statistical word-level alignment . \n\t', '\n\t\t In Proceedings of AMTA-02 , pages 31\x9643 . \n\t', '\n\t\t A. Lopez , M. Nosal , R. Hwa , and P. Resnik . \n\t', '\n\t\t 2002. Word-level alignment for multilingual resource acquisition . \n\t', '\n\t\t In Proceedings of the LREC-02 Workshop on Linguistic Knowledge Acquisition and Representation . \n\t', '\n\t\t I. D. Melamed . \n\t', '\n\t\t 1998. Empirical methods for MT lexicon development . \n\t', '\n\t\t Lecture Notes in Computer Science , 1529:18\x969999 . \n\t', '\n\t\t R. Mihalcea and T. Pedersen . \n\t', '\n\t\t 2003. An evalua- tion exercise for word alignment . \n\t', '\n\t\t In Rada Mi- halcea and Ted Pedersen , editors , Proceedings of the HLT-NAACL 2003 Workshop on Building and Using Parallel Texts , pages 1\x9610 . \n\t', '\n\t\t F. J. Och and H. Ney . \n\t', '\n\t\t 2000. A comparison of alignment models for statistical machine translation . \n\t', '\n\t\t In Proceedings of COLING-00 , pages 1086\x961090 . \n\t', '\n\t\t D. I. Tufi¸s . \n\t', '\n\t\t 2002. A cheap and fast way to build useful translation lexicons . \n\t', '\n\t\t In Proceedings of COLING-02 , pages 1030\x961036 . \n\t', '\n\t\t D. Wu . \n\t', '\n\t\t 1995. Stochastic inversion transduction grammars , with application to segmentation , bracketing , and alignment of parallel corpora . \n\t', '\n\t\t In Proceedings of IJCAI-95 , pages 1328\x961335 . \n\t', '\n\t\t K. Yamada and K. Knight . \n\t', '\n\t\t 2001. A syntax-based statistical translation model . \n\t', '\n\t\t In Proceedings of ACL-01 , pages 523\x96530 . \n\t', '\n\t\t D. Yarowsky , G. Ngai , and R. Wicentowski . \n\t', '\n\t\t 2001. Inducing multilingual text analysis tools via robust projection across aligned corpora . \n\t', '\n\t\t In Proceedings of HLT-01 , pages 161\x96168 . \n\t', '\n\t\t Hierarchy Extraction based on Inclusion of Appearance Eiko Yamamoto Kyoko Kanzaki Hitoshi Isahara Computational Linguistics Group , National Institute of Information and Communications Technology 3-5 Hikari-dai , Seika-cho , Soraku-gun , Kyoto , 619-0289 , Japan . \n\t', '\n\t\t eiko@nict.go jp kanzaki@nict.go jp isahara@nict.go jp Abstract In this paper , we propose a method of automatically extracting word hierarchies based on the inclusion relation of appearance patterns from corpora . \n\t', '\n\t\t We apply a complementary similarity measure to find a hierarchical word structure . \n\t', '\n\t\t This similarity measure was developed for the recognition of degraded machine- printed text in the field and can be applied to estimate one-to-many relations . \n\t', '\n\t\t Our purpose is to extract word hierarchies from corpora automatically . \n\t', '\n\t\t As the initial task , we attempt to extract hierarchies of abstract nouns co- occurring with adjectives in Japanese and compare with hierarchies in the EDR electronic dictionary . \n\t', '\n\t\t 1 Introduction The hierarchical relations of words are useful as language resources . \n\t', '\n\t\t Hierarchical semantic lexical databases such as WordNet \n\t\t']",Positive
"['\n\t\t In current thesauri in the form of hierarchical relations , words are categorized manually and classified in a top-down manner based on human intuition . \n\t', '\n\t\t This is a good way to make a lexical database for users having a specific purpose . \n\t', '\n\t\t However , word hierarchies based on human intuition tend to vary greatly depending on the lexicographer . \n\t', '\n\t\t In addition , hierarchical relations based on various data may be needed depending on each user . \n\t', '\n\t\t Accordingly , we try to extract a hierarchical relation of words automatically and statistically . \n\t', '\n\t\t In previous research , ways of extracting from definition sentences in dictionaries \n\t\t']",Positive
"['\n\t\t Also , there is a method that uses the dependence relation between words taken from a corpus \n\t\t']",Positive
"['\n\t\t In contrast , we propose a method based on the inclusion relation of appearance patterns from corpora . \n\t', '\n\t\t In this paper , to verify the suitability of our method , we attempt to extract hierarchies of abstract nouns co-occurring with adjectives in Japanese . \n\t', '\n\t\t We select two similarity measures to estimate the inclusion relation between word appearance patterns . \n\t', '\n\t\t One is a complementary similarity measure ; i.e. , a similarity measure developed for the recognition of degraded machine-printed text in the field \n\t\t']",Positive
['\n\t\t This measure can be used to estimate one-to-many relations such as superordinate\x96subordinate relations from appearance patterns \n\t\t'],Positive
"['\n\t\t The second similarity measure is the overlap coefficient , which is a similarity measure to calculate the rate of overlap between two binary vectors . \n\t', '\n\t\t Using each measure , we extract hierarchies from a corpus . \n\t', '\n\t\t After that , we compare these with the EDR electronic dictionary . \n\t', '\n\t\t 2 Experiment Corpus A good deal of linguistic research has focused on the syntactic and semantic functions of abstract nouns \n\t\t']",Positive
"['\n\t\t In the example , \x93Magi ( goat ) wa seishitsu ( nature ) ga otonashii ( gentle ) ( The nature of goats is gentle).\x94 , \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t In the linguistic data , there are sets of co-occurring adjectives for each abstract noun \x96 the total number of abstract noun types is 365 and the number of adjective types is 10,525 . \n\t', '\n\t\t Some examples are as follows . \n\t', '\n\t\t OMOI ( feeling ) : ureshii ( glad ) , kanashii ( sad ) , shiawasena ( happy ) , ... \n\t', '\n\t\t KANTEN ( viewpoint ) : igakutekina ( medical ) , rekishitekina ( historical ) , ... 3 Complementary Similarity Measure The complementary similarity measure ( CSM ) is used in a character recognition method for binary images which is robust against heavy noise or graphical designs \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t They estimated one-to-many relations from the inclusion relations between the appearance patterns of two words . \n\t', '\n\t\t The appearance pattern is expressed as an n- dimensional binary feature vector . \n\t', '\n\t\t Now , let F = ( f1 , f2 , ... , fn ) and T = ( t1 , t2 , ... , tn ) ( where fi , ti = 0 or 1 ) be the feature vectors of the appearance patterns for a word and another word , respectively . \n\t', '\n\t\t The CSM of F to T is defined as The CSM of F to T represents the degree to which F includes T ; that is , the inclusion relation between the appearance patterns of two words . \n\t', '\n\t\t In our experiment , each \x93word\x94 is an abstract noun . \n\t', '\n\t\t Therefore , n is the number of adjectives in the corpus , a indicates the number of adjectives co- occurring with both abstract nouns , b and c indicate the number of adjectives co-occurring with either abstract noun , and d indicates the number of adjectives co-occurring with neither abstract noun . \n\t', '\n\t\t 4 Overlap Coefficient The overlap coefficient ( OVLP ) is a similarity measure for binary vectors \n\t\t']",Positive
"['\n\t\t OVLP is essentially a measure of inclusion . \n\t', '\n\t\t It has a value of 1.0 if every dimension with a nonzero value for the first vector is also non-zero for the second vector or vice versa . \n\t', '\n\t\t In other words , the value is 1.0 when the first vector completely includes the second vector or vice versa . \n\t', '\n\t\t OVLP of F and T is defined as 5 EDR hierarchy The EDR Electronic \n\t\t']",Positive
"['\n\t\t The sub-dictionaries include a concept dictionary , word dictionaries , bilingual dictionaries , etc. . \n\t', '\n\t\t We verify and analyse the hierarchies that are extracted based on a comparison with the EDR dictionary . \n\t', '\n\t\t However , the hierarchies in EDR consist of hypernymic concepts represented by sentences . \n\t', '\n\t\t On the other hand , our extracted hierarchies consist of hypernyms such as abstract nouns . \n\t', '\n\t\t Therefore , we have to replace the concept composed of a sentence with the sequence of the words . \n\t', '\n\t\t We replace the description of concepts with entry words from the \x93Word List by Semantic Principles\x94 ( 1964 ) and add synonyms . \n\t', '\n\t\t We also add to abstract nouns in order to reduce any difference in representation . \n\t', '\n\t\t In this way , conceptual hierarchies of adjectives in the EDR dictionary are defined by the sequence of words . \n\t', '\n\t\t 6 Hierarchy Extraction Process The processes for hierarchy extraction from the corpus are as follows . \n\t', '\n\t\t \x93TH\x94 is a threshold value for each pair under consideration . \n\t', '\n\t\t If TH is low , we can obtain long hierarchies . \n\t', '\n\t\t However , if TH is too low , the number of word pairs taken into consideration increases overwhelmingly and the measurement reliability diminishes . \n\t', '\n\t\t In this experiment , we set 0.2 as TH . \n\t', '\n\t\t 1. Compute the similarity between appearance patterns for each pair of words . \n\t', '\n\t\t The hierarchical relation between the two words in a pair is determined by the similarity value . \n\t', '\n\t\t We express the pair as ( X , Y ) , where X is a hypernym of Y and Y is a hyponym of X. 2 . \n\t', '\n\t\t Sort the pairs by the normalized similarities and reduce the pairs where the similarity is less than TH . \n\t', '\n\t\t 3. For each abstract noun , A ) Choose a pair ( B , C ) where word B is the hypernym with the highest value . \n\t', '\n\t\t The hierarchy between B and C is set to the initial hierarchy . \n\t', '\n\t\t B ) Choose a pair ( C , D ) where hyponym D is not contained in the current hierarchy and has the highest value in pairs where the last word of the current hierarchy C is a hypernym . \n\t', '\n\t\t C ) Connect hyponym D with the tail of the current hierarchy . \n\t', '\n\t\t D ) While such a pair can be chosen , repeat B ) and C ) . \n\t', '\n\t\t E ) Choose a pair ( A , B ) where hypernym A is not contained in the current hierarchy and has the highest value in pairs where the first word of the current hierarchy B is a hypernym . \n\t', '\n\t\t F ) Connect hypernym A with the head of the current hierarchy . \n\t', '\n\t\t G ) While such a pair can be chosen , repeat E ) and F ) . \n\t', '\n\t\t 4. For the hierarchies that are built , A ) If a short hierarchy is included in a longer hierarchy with the order of the words preserved , the short one is dropped from the list of hierarchies . \n\t', '\n\t\t B ) If a hierarchy has only one or a few different words from another hierarchy , the two hierarchies are merged . \n\t', '\n\t\t 7 Extracted Hierarchy Some extracted hierarchies are as follows . \n\t', '\n\t\t In our experiment , we get koto ( matter ) as the common hypernym . \n\t', '\n\t\t koto ( matter ) -- joutai ( state ) -- kankei ( relation ) -- kakawari ( something to do with ) -- tsukiai ( have an acquaintance with ) koto ( matter ) -- toki ( when ) -- yousu ( aspect ) -- omomochi ( one\x92s face ) -- manazashi ( a look ) -- iro ( on one\x92s face ) -- shisen ( one\x92s eye ) 8 Comparison We analyse extracted hierarchies by using the number of nodes that agree with the EDR hierarchy . \n\t', '\n\t\t Specifically , we count the number of nodes ( nouns ) which agree with a word in the EDR hierarchy , preserving the order of each hierarchy . \n\t', '\n\t\t Here , two hierarchies are \x93A - B - C - D - E\x94 and \x93A - B - D - F - G.\x94 They have three agreement nodes ; \x93A - B-D.\x94 Table 1 shows the distribution of the depths of a CSM hierarchy , and the number of nodes that agree with the EDR hierarchy at each depth . \n\t', '\n\t\t Table 2 shows the same for an OVLP one . \n\t', '\n\t\t \x93Agreement Level\x94 is the number of agreement nodes . \n\t', '\n\t\t The bold font represents the number of hierarchies completely included in the EDR hierarchy . \n\t', '\n\t\t 8.1 Depth of Hierarchy The number of hierarchies made from the EDR dictionary ( EDR hierarchy ) is 932 and the deepest level is 14 . \n\t', '\n\t\t The number of CSM hierarchies is 105 and the depth is from 3 to 14 ( Table 1 ) . \n\t', '\n\t\t The number of OVLP hierarchies is 179 and the depth is from 2 to 9 ( Table 2 ) . \n\t', '\n\t\t These results show that CSM builds a deeper hierarchy than OVLP , though the number of hierarchies is less than OVLP . \n\t', '\n\t\t Also , the deepest level of CSM equals that of EDR . \n\t', '\n\t\t Therefore , comparison with the EDR dictionary is an appropriate way to verify the hierarchies that we have extracted . \n\t', '\n\t\t In both tables , we find most hierarchies have an agreement level from 2 to 4 . \n\t', '\n\t\t The deepest agreement level is 6 . \n\t', '\n\t\t For an agreement level of 5 or better , the OVLP hierarchy includes only two hierarchies while the CSM hierarchy includes nine hierarchies . \n\t', '\n\t\t This means CSM can extract hierarchies having more nodes which agree with the EDR hierarchy than is possible with OVLP . \n\t', '\n\t\t Depth of Agreement Level Hierarchy 1 2 3 4 5 6 3 1 4 1 4 8 6 2 5 9 8 1 6 8 9 4 1 7 2 6 1 1 8 1 5 2 2 9 3 2 3 1 10 1 2 11 4 1 12 1 1 13 1 2 14 1 Table 1 : Distribution of CSM hierarchy for each depth Depth of Agreement Level Hierarchy 1 2 3 4 5 6 2 1 3 2 8 1 4 25 9 1 5 24 13 7 6 21 31 5 7 5 12 1 1 8 3 5 2 1 9 1 3 1 Table 2 : Distribution of OVLP hierarchy for each depth Also , many abstract nouns agree with the hyperonymic concept around the top level . \n\t', '\n\t\t In current thesauri , the categorization of words is classified in a top-down manner based on human intuition . \n\t', '\n\t\t Therefore , we believe the hierarchy that we have built is consistent with human intuition , at least around the top level of hyperonymic concepts . \n\t', '\n\t\t 9 Conclusion We have proposed a method of automatically extracting hierarchies based on an inclusion relation of appearance patterns from corpora . \n\t', '\n\t\t In this paper , we attempted to extract objective hierarchies of abstract nouns co-occurring with adjectives in Japanese . \n\t', '\n\t\t In our experiment , we showed that complementary similarity measure can extract a kind of hierarchy from corpora , though it is a similarity measure developed for the recognition of degraded machine-printed text . \n\t', '\n\t\t Also , we can find interesting hierarchies which suit human intuition , though they are different from exact hierarchies . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t We can look a suitability of our result at that work . \n\t', '\n\t\t In our future work , we will use our approach for other parts of speech and other types of word . \n\t', '\n\t\t Moreover , we will compare with current alternative approaches such as those based on sentence patterns . \n\t', '\n\t\t References Berland , M. and Charniak , E. 1999 . \n\t', '\n\t\t Finding Parts in Very Large Corpora , In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics , pp.57-64 . \n\t', '\n\t\t Caraballo , S. A. 1999 . \n\t', '\n\t\t Automatic Construction of a Hypernym-labeled Noun Hierarchy from Text , In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics , pp. 120-126 . \n\t', '\n\t\t EDR Electronic Dictionary . \n\t', '\n\t\t 1995. http://www2.nict.go.jp/kk/e416/EDR/index.html Hagita , N. and Sawaki , M. 1995 . \n\t', '\n\t\t Robust Recognition of Degraded Machine-Printed Characters using Complementary Similarity Measure and Error-Correction Learning^In Proceedings of the SPIE \x96The International Society for Optical Engineering , 2442 : pp.236-244 . \n\t', '\n\t\t Kanzaki , K. , Ma , Q. , Yamamoto , E. , Murata , M. , and Isahara , H. 2003 . \n\t', '\n\t\t Adjectives and their Abstract concepts --- Toward an objective thesaurus from Semantic Map . \n\t', '\n\t\t In Proceedings of the Second International Workshop on Generative Approaches to the Lexicon , pp. 177-184 . \n\t', '\n\t\t Kanzaki , K. , Ma , Q. , Yamamoto , E. , Murata , M. , and Isahara , H. 2004 . \n\t', '\n\t\t Extraction of Hyperonymy of Adjectives from Large Corpora by using the Neural Network Model . \n\t', '\n\t\t In Proceedings of the Fourth International Conference on Language Resources and Evaluation , Volume II , pp.423- 426 . \n\t', '\n\t\t Kay , M. 1986 . \n\t', '\n\t\t Parsing in Functional Unification Grammar . \n\t', '\n\t\t In \x93Readings in Natural Language Processing\x94 , Grosz , B. J. , Spark Jones , K. and Webber , B. L. , ed. , pp.125-138 , Morgan Kaufmann Publishers , Los Altos , California . \n\t', '\n\t\t Manning , C. D. and Schutze , H. 1999 . \n\t', '\n\t\t Foundations of Statistical Natural Language Processing , The MIT Press , Cambridge MA . \n\t', '\n\t\t Matsumoto , Y. and Sudo , S. , Nakayama , T. , and Hirao , T. 1996 . \n\t', '\n\t\t Thesaurus Construction from Multiple Language Resources , In IPSJ SIG Notes NL-93 , pp.23-28 ( In Japanese ) . \n\t', '\n\t\t Miller , A. , Beckwith , R. , Fellbaum , C. , Gros , D. , Millier , K. , and Tengi , R. 1990 . \n\t', '\n\t\t Five Papers on WordNet , Technical Report CSL Report 43 , Cognitive Science Laboratory , Princeton University . \n\t', '\n\t\t Mosteller , F. and Wallace , D. 1964 . \n\t', '\n\t\t Inference and Disputed Authorship : The Federalist . \n\t', '\n\t\t Addison- Wesley , Reading , Massachusetts . \n\t', '\n\t\t Nemoto , K. 1969 . \n\t', '\n\t\t The combination of the noun with \x93ga-Case\x94 and the adjective , Language research2 for the computer , National Language Research Institute , pp.63-73 ( In Japanese ) . \n\t', '\n\t\t Shmid , H-J. 2000 . \n\t', '\n\t\t English Abstract Nouns as Conceptual Shells , Mouton de Gruyter . \n\t', '\n\t\t Shoutsu , Y. , Tokunaga , T. , and Tanaka , H. 2003 . \n\t', '\n\t\t The integration of Japanese dictionary and thesaurus , In IPSJ SIG Notes NL-153 , pp.141-146 ( In Japanese ) . \n\t', '\n\t\t Sparck Jones , K. 1972 . \n\t', '\n\t\t A statistical interpretation of term specificity and its application in retrieval . \n\t', '\n\t\t Journal of Documentation , 28(1) : pp. 11-21 . \n\t', '\n\t\t Takahashi , T. 1975 . \n\t', '\n\t\t A various phase related to the part-whole relation investigated in the sentence , Studies in the Japanese language 103 , The Society of Japanese Linguistics , pp.1-16 ( In Japanese ) . \n\t', '\n\t\t Tsurumaru , H. , Hitaka , T. , and Yoshita , S. 1986 . \n\t', '\n\t\t Automatic extraction of hierarchical relation between words , In IPSJ SIG Notes NL-83 , pp. 121- 128 ( In Japanese ) . \n\t', '\n\t\t Yamamoto , E. and Umemura , K. 2002 . \n\t', '\n\t\t A Similarity Measure for Estimation of One\x96to-Many Relationship in Corpus , In Journal of Natural Language Processing , pp.45-75 ( In Japanese ) . \n\t', '\n\t\t Word List by Semantic Principles . \n\t', '\n\t\t 1964. National Language Research Institute Publications , Shuei Shuppan ( In Japanese ) . \n\t', '\n\t\t Knowledge-intensive automatic e-mail summarization in CARPANTA Laura Alonso and Irene Castellon G RIAL General Linguistics Department Universitat de Barcelona Bernardino Casas and Lluis Padro TALP Software Department Universitat Politecnica de Catalunya Abstract We present CARPANTA , an e-mail summarization system that applies a knowledge intensive approach to obtain highly coherent summaries . \n\t', '\n\t\t Robustness and portability are guaranteed by the use of general-purpose NLP tools , but it also exploits language- and domain-dependent knowledge . \n\t', '\n\t\t The system is evaluated against a corpus of human-judged summaries , reaching satisfactory levels of performance . \n\t', '\n\t\t 1 Introduction We present CARPANTA , the e-mail summarization system within project PETRA . \n\t', '\n\t\t PETRA is related to the European project MAJORnoME - Unified Messaging System ( 0-2340 ) , whose aim is to introduce a unified messaging system that allows users to access e-mail , voice mail , and faxes from a common "" in-box "" . \n\t', '\n\t\t One of the lines of work developed within PETRA is the use of Natural Language Procesing ( NLP ) techniques for information management , namely , for text classification and summarization , as well as for information retrieval . \n\t', '\n\t\t This task includes the subgoal of text summarization , specially relevant for oral interfaces to electronic mail systems . \n\t', '\n\t\t The summarization module within PETRA is CARPANTA . \n\t', '\n\t\t It is currently working for Spanish , but portability to other languages is guaranteed by a language-independent core . \n\t', '\n\t\t The rest of the paper is structured as follows : first , NLP problems specific to e-mail summarization and our approach to them are described , comparing it with previous work . \n\t', '\n\t\t Section 3 presents the architecture of the system . \n\t', '\n\t\t The system is evaluated by comparison with a human-made gold standard , results can be seen in Section 4 . \n\t', '\n\t\t 2 Problems of e-mail summarization Besides the problems specific to automatic text summarization , e-mail summarization presents : \x95 noisy input ( headers , tags , ... ) \x95 no guarantee of linguistic well-formedness \x95 mixed properties of oral and written language \x95 multi-topic messages High-quality , general-purpose NLP tools cannot deal properly with such a bulk of asystematic differences from standard texts . \n\t', '\n\t\t This implies a barrier for approaches to text summarization that have proven successful in more standard genres , because they are crucially relying on the output of such tools . \n\t', '\n\t\t As a consequence , very little work has been done on quality e-mail summarization . \n\t', '\n\t\t \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t Considering e-mail summarization problems and the environment within PETRA project , summaries produced by CARPANTA have the following properties : oral output by telephone , indicative summaries just give a hint of the content , to meet the severe restrictions of length imposed by the oral format , coherent because the summary cannot be revised as easily as written ones , ( thus excluding list-of-words approach ) , extractive due to limitations for general- purpose NLP tools , knowledge-intensive combining analysis at different linguistic levels , IR techniques and IE strategies specific for e-mail , in order to build a robust system that is also capable of producing deep analyses . \n\t', '\n\t\t 3 Architecture of the System As can be seen in Figure 1 , CARPANTA is highly modular , which guarantees portability to other languages . \n\t', '\n\t\t The core processing stream is formed by language-independent strategies , while e-mail specific knowledge is in autonomous modules that can be updated and switched to address concrete necessities ( different languages , restricted domains ) . \n\t', '\n\t\t In addition to general-purpose NLP tools , the following e-mail specific resources were developed : \x95 a classification where each kind of e-mail is associated to its most adequate summary and summarization strategy ( language- independent ) ( seen in Table 1 ) \x95 bags of words and expressions that signal different kinds of e-mail specific contents ( language-dependent ) \x95 strategies to deal with these anchors and their associated content ( language- independent ) The process for e-mails to be summarized is described in what follows . \n\t', '\n\t\t Parse e-mail format . \n\t', '\n\t\t Messages undergo a pre-processing to identify headers , greetings , visit cards , quoted text , and the body of text , which is further analyzed . \n\t', '\n\t\t Linguistic analysis . \n\t', '\n\t\t First , the body of text is analyzed morphosyntactically \n\t\t']",Positive
"['\n\t\t Then , discourse chunks , signalled by punctuation and discourse markers , are found ( what we call segments ) . \n\t', '\n\t\t Finally , the salience of non-empty words is calculated according to the frequency of occurrence of their lemma . \n\t', '\n\t\t Textual analysis . \n\t', '\n\t\t Three different kinds of textual relevance have been distinguished : lexic , structural and subjective . \n\t', '\n\t\t For each of these three aspects of e-mails , a global reliability score is obtained , taking into account how well each kind of information distinguishes relevant and non-relevant pieces of the e-mail . \n\t', '\n\t\t Then , relevance is also calculated with respect to meaning units , basically , discourse segments . \n\t', '\n\t\t Lexic relevance of a segment is directly proportional to the amount of frequent words in the segment and inversely proportional to the length of the segment . \n\t', '\n\t\t Structural relevance is assigned as a result of the interpretation of discursive relations between segments and between a segment and the whole text , by means of the information associated to discourse markers . \n\t', '\n\t\t Finally , Figure 2 : General schema followed by classification rules . \n\t', '\n\t\t subjective relevance is found when the segment contains any of a list of lexical expressions signalling subjectivity . \n\t', '\n\t\t Documental analysis . \n\t', '\n\t\t Key words and expressions signalling information specific of email ( e.g. , appointment , list , etc. ) are detected by simple IE techniques , basically , pattern- matching . \n\t', '\n\t\t As a result of linguistic , textual and documental analysis , a set of meaning units is produced at different linguistic levels : words , chunks , segments and sentences , but also lines and paragraphs . \n\t', '\n\t\t Each unit is assigned a complex relevance score , one for each kind of information that is taken into account . \n\t', '\n\t\t Values for lexical , structural and subjective relevance are continious , ranging from 0 to 1 . \n\t', '\n\t\t Each unit is also assinged a binary relevance score for each kind of e-mail specific information , 1 if there is any clue signalling that kind of information in the unit , 0 if there is none . \n\t', '\n\t\t Classification The most adequate summarization strategy is determined by taking into account the characterizing features of each email , as provided by the analysis module . \n\t', '\n\t\t The general schema followed by classification rules can be seen in Figure 2 , Table 1 shows the relation between e-mail features and summarization strategies . \n\t', '\n\t\t Summarization Then , the chosen summary is produced . \n\t', '\n\t\t Different kinds of summaries are described in Table 1 . \n\t', '\n\t\t 4 Results and Discussion To tune and evaluate the performance of the system , the automatic summaries produced if strong genre evidence if strong linguistic evidence textual + documental else if evidence for a single genre specific strategy ( list , question , attachment ) else combination of genres else if strong textual evidence textual else lead CLASSIFICATION SUMMARIZATION length filter documental analysis ( domain dependent ) · attachment · forward · list · question · appointment · subject RELEVANCE OF MEANING UNITS morphosyntacticanalysis discourse analysis linguistic analysis ANALYSIS tokenization textual analysis ( domain independent ) · structural · lexic · subjectivity e^mail LANGUAGE^DEPENDENT MODULES BAGS OF WORDS bonus words stop words attachment subjectivity greetings forward farewells subject ANALYZERS morphological analyzer discourse chunker phrasal chunker WORKING UNITS subject sender attachment forward body body of content header greeting farewell footer quoted text discursive segments sentences blocks words lines PRE^PROCESSING mail parsing reduction summary summarization approach summary textual features documental features full mail whole e-mail text short ( <30 words ) pyramidal first compressed paragraph none is relevant none is relevant lead first compressed sentence none is relevant none is relevant subject subject of e-mail strong lexical relevance subject is relevant appointment segment stating appointment none is relevant evidence of appointment attachment segment describing attachment none is relevant evidence of attachment forward segment describing forward none is relevant evidence of forward question segment with question none is relevant question mark list segment preceeding the list none is relevant list lexic segment with most relevant lexic strong lexical relevance none is relevant structural most structurally salient segment strong structural relevance none is relevant subjective segment with subjectivity evidence strong subjective relevance none is relevant textual most relevant segment summing all textual evidence none is salient none is salient textual + documental most relevant segment summing all textual and documental evidenc none is salient none is salient Table 1 : Classification of summaries , characterizing features and summarization strategies . \n\t', '\n\t\t were compared with summaries produced for 200 e-mails by 20 potential users of the system , with a minimum of 2 different human summaries for each e-mail . \n\t', '\n\t\t Agreement between judges ranged from K = \x97.37 to K = 1 , with a mean of K = .47 , which indicates that agreement is far beyond chance , but also that the task of e-mail summarization is somewhat fuzzy for users . \n\t', '\n\t\t The goodness of automatic summaries was calculated by comparison with the corresponding human summaries , results can be seen in Figure 3 . \n\t', '\n\t\t For each e-mail , automatic summaries were obtained using all of the summarization strategies applicable , based on linguistic information ( lexical , structural , etc. ) , on email specific information ( appointment , attachment , etc. ) in both ( textual and documental ) or applying baseline strategies , like having the first line or paragraph as the summary . \n\t', '\n\t\t Human and automatic summaries were compared by K agreement and by precision at discourse unit level . \n\t', '\n\t\t Agreement between human and automatic summaries was very low in terms of K ( average K = .02 ) , but evaluation metrics more usual for summarization , like precision with respect to human summaries , reached 60 % average , which is the state of the art for automatic text summarization . \n\t', '\n\t\t Results show that simple methods , like taking the first line of the e-mail ( lead ) offer very good results , but , in general , summaries exploiting email specific knowledge ( list , appointment ) can improve on this baseline . \n\t', '\n\t\t However , these kinds of e-mail present very low coverage . \n\t', '\n\t\t The strategy combining general linguistic and e-mail specific knowledge ( textual and documental ) yields a good balance between coverage and precision . \n\t', '\n\t\t Finally , results concerning the chosen summary show that there is still room for improvement within the classification module , since most of the alternative summaries present higher precision rates than the chosen one . \n\t', '\n\t\t 5 Conclusions and Future Work We have presented CARPANTA , an e-mail summarization system that applies a knowledge- intensive approach to obtain highly coherent summaries , targeted to guarantee understandability in delivery by phone . \n\t', '\n\t\t Results indicate that the classification module has to be improved . \n\t', '\n\t\t Given the highly modular architecture of CARPANTA , adaptation to other languages has a very low cost of development , provided the required NLP tools are available . \n\t', '\n\t\t References J. Atserias , J. Carmona , S. Cervell , L. Ma~rquez , M. A. Martf , L. Padro~ , R. Placer , H. Rodrfguez , M. Taule , and J. Turmo . \n\t', '\n\t\t 1998a . \n\t', '\n\t\t An environment for morphosyn- tactic processing of unrestricted spanish text . \n\t', ""\n\t\t In First International Conference on Language Resources and Evaluation ( LREC'98 ) , Granada , Spain . \n\t"", '\n\t\t J. Atserias , I. Castello~n , and M. Civit . \n\t', '\n\t\t 1998b . \n\t', '\n\t\t Syntactic parsing of unrestricted spanish text . \n\t', '\n\t\t In First International Conference on Language Resources and Evaluation , Granada . \n\t', '\n\t\t LREC . \n\t', '\n\t\t A. Nenkova and A. Bagga . \n\t', '\n\t\t 2003 . \n\t', '\n\t\t Facilitating email thread ac- cess by extractive summary generation . \n\t', '\n\t\t In RANLP 2003 . \n\t', '\n\t\t E. Tzoukermann , S. Muresan , and Judith L. Klavans . \n\t', '\n\t\t 2001. Gist-it- Summarizing email using linguistic knowledge and machine learning . \n\t', ""\n\t\t In ACL-EACL'01 HLT/KM Workshop . \n\t"", '\n\t\t Finding Anchor Verbs for Biomedical IE Using Predicate-Argument Structures Akane YAKUSHIJIt Yuka TATEISItt Yusuke MIYAOt Jun\x92ichi TSUJIItt tDepartment of Computer Science , University of Tokyo Hongo 7-3-1 , Bunkyo-ku , Tokyo 113-0033 JAPAN $ CREST , JST ( Japan Science and Technology Agency ) Honcho 4-1-8 , Kawaguchi-shi , Saitama 332-0012 JAPAN {akane,yucca,yusuke,tsujii}@is.s.u-tokyo.ac.jp Abstract For biomedical information extraction , most systems use syntactic patterns on verbs ( anchor verbs ) and their arguments . \n\t', '\n\t\t Anchor verbs can be selected by focusing on their arguments . \n\t', '\n\t\t We propose to use predicate-argument structures ( PASs ) , which are outputs of a full parser , to obtain verbs and their arguments . \n\t', '\n\t\t In this paper , we evaluated PAS method by comparing it to a method using part of speech ( POSs ) pattern matching . \n\t', '\n\t\t POS patterns produced larger results with incorrect arguments , and the results will cause adverse effects on a phase selecting appropriate verbs . \n\t', '\n\t\t 1 Introduction Research in molecular-biology field is discovering enormous amount of new facts , and thus there is an increasing need for information extraction ( IE ) technology to support database building and to find novel knowledge in online journals . \n\t', '\n\t\t To implement IE systems , we need to construct extraction rules , i.e. , rules to extract desired information from processed resource . \n\t', '\n\t\t One subtask of the construction is defining a set of anchor verbs , which express realization of desired information in natural language text . \n\t', '\n\t\t In this paper , we propose a novel method of finding anchor verbs : extracting anchor verbs from predicate-argument structures ( PASs ) obtained by full parsing . \n\t', '\n\t\t We here discuss only finding anchor verbs , although our final purpose is construction of extraction rules . \n\t', '\n\t\t Most anchor verbs take topical nouns , i.e. , nouns describing target entities for IE , as their arguments . \n\t', '\n\t\t Thus verbs which take topical nouns can be candidates for anchor verbs . \n\t', '\n\t\t Our method collects anchor verb candidates by choosing PASs whose arguments are topical nouns . \n\t', '\n\t\t Then , semantically inappropriate verbs are filtered out . \n\t', '\n\t\t We leave this filtering phase as a future work , and discuss the acquisition of candidates . \n\t', '\n\t\t We have also investigated difference in verbs and their arguments extracted by naive POS patterns and PAS method . \n\t', '\n\t\t When anchor verbs are found based on whether their arguments are topical nouns , like in \n\t\t']",Positive
"['\n\t\t Thus , in this paper , we set our goal to obtain anchor verb candidates and their correct arguments . \n\t', '\n\t\t 2 Background There are some works on acquiring extraction rules automatically . \n\t', '\n\t\t \n\t\t']",Negative
"['\n\t\t One problem of their system is that dependency trees cannot treat non-local dependencies , and thus rules acquired from the constructions are partial . \n\t', '\n\t\t \n\t\t']",Negative
"['\n\t\t They used only POSs and word positions to detect relations between verbs and topical nouns . \n\t', '\n\t\t Their performance was 87.5 % precision and 82.4 % recall . \n\t', '\n\t\t One of the reasons of errors they reported is failures to detect verb-noun relations . \n\t', '\n\t\t To avoid these problems , we decided to use PASs obtained by full parsing to get precise relations between verbs and their arguments . \n\t', '\n\t\t The obtained precise relations will improve precision . \n\t', '\n\t\t In addition , PASs obtained by full parsing can treat non-local dependencies , thus recall will also be improved . \n\t', '\n\t\t The sentence below is an example which supports advantage of full parsing . \n\t', '\n\t\t A gerund \x93activating\x94 takes a non-local semantic subject \x93IL-4\x94 . \n\t', '\n\t\t In full parsing based on Head-Driven Phrase Structure Grammar ( HPSG ) \n\t\t']",Positive
"['\n\t\t IL-4 may mediate its biological effects by activating a tyrosine -phosphorylated DNA binding protein . \n\t', '\n\t\t It interacts with non -polymorphic regions of major histocompatibility complex class II molecules . \n\t', '\n\t\t Next , in Step 2 , we check each argument of ( a ) , ( b ) and ( c ) . \n\t', '\n\t\t ( a ) is discarded because it does not have a topical noun argument.2 ( b ) is selected because ARG1 \x93regions\x94 is a topical noun . \n\t', '\n\t\t Similarly , ( c ) is selected because of ARG1 \x93molecules\x94 . \n\t', '\n\t\t And then , in Step 3 , we check each POS of a predicate included in ( b ) and ( c ) . \n\t', '\n\t\t ( b ) is selected be- cause it has the verb \x93interacts\x94 in 1 which shares Figure 1 : PAS examples Figure 2 : Core verbs of PASs 3 Anchor Verb Finding by PASs By using PASs , we extract candidates for anchor verbs from a sentence in the following steps : 1 . \n\t', '\n\t\t Obtain all PASs of a sentence by a full parser . \n\t', '\n\t\t The PASs correspond not only to verbal phrases but also other phrases such as prepositional phrases . \n\t', '\n\t\t 2. Select PASs which take one or more topical nouns as arguments . \n\t', '\n\t\t 3. From the selected PASs in Step 2 , select PASs which include one or more verbs . \n\t', '\n\t\t 4. Extract a core verb , which is the innermost verbal predicate , from each of the chosen PASs . \n\t', '\n\t\t In Step 1 , we use a probabilistic HPSG parser developed by \n\t\t']",Positive
"['\n\t\t PASs obtained by the parser are illustrated in Figure 1.1 Bold words are predicates . \n\t', '\n\t\t Arguments of the predi- cates are described in ARGn ( n = 1 , 2 , ... ) . \n\t', '\n\t\t MOD- I FY denotes the modified PAS . \n\t', '\n\t\t Numbers in squares denote shared structures . \n\t', '\n\t\t Examples of core verbs are illustrated in Figure 2 . \n\t', '\n\t\t We regard all arguments in a PAS are arguments of the core verb . \n\t', '\n\t\t Extraction of candidates for anchor verbs from the sentence in Figure 1 is as follows . \n\t', '\n\t\t Here , \x94regions\x94 and \x94molecules\x94 are topical nouns . \n\t', '\n\t\t In Step 1 , we obtain all the PASs , ( a ) , ( b ) and ( c ) , in Figure 1. 1Here , named entities are regarded as chunked , and thus internal structures of noun phrases are not illustrated . \n\t', '\n\t\t the structure with ( a ) . \n\t', '\n\t\t ( c ) is discarded because it includes no verbs . \n\t', '\n\t\t Finally , in Step 4 , we extract a core verb from ( b ) . \n\t', '\n\t\t ( b ) includes 1 as MODIFY , and the predicate of 1 is the verb , \x93interacts\x94 . \n\t', '\n\t\t So we extract it . \n\t', '\n\t\t 4 Experiments We investigated the verbs and their arguments extracted by PAS method and POS pattern matching , which is less expressive in analyzing sentence structures but would be more robust . \n\t', '\n\t\t For topical nouns and POSs , we used the GENIA corpus \n\t\t']",Positive
"['\n\t\t We defined topical nouns as the names tagged as protein , peptide , amino acid , DNA , RNA , or nucleic acid . \n\t', '\n\t\t We chose PASs which take one or more topical nouns as an argument or arguments , and substrings matched by POS patterns which include topical nouns . \n\t', '\n\t\t All names tagged in the corpus were replaced by their head nouns in order to reduce complexity of sentences and thus reduce the task of the parser and the POS pattern matcher . \n\t', '\n\t\t 4.1 Implementation of PAS method We implemented PAS method on LiLFeS , a unification-based programming system for typed feature structures \n\t\t']",Positive
"['\n\t\t The selection in Step 2 described in Section 3 is realized by matching PASs with nine PAS templates . \n\t', '\n\t\t Four of the templates are illustrated in Figure 3. 4.2 POS Pattern Method We constructed a POS pattern matcher with a partial verb chunking function according to \n\t\t']",Positive
"['\n\t\t Because the original matcher has problems in recall ( its verb group detector has low coverage ) and precision ( it does not consider other words to detect relations between verb groups and topical nouns ) , we implemented 2(a) may be selected if the anaphora ( \x93it\x94 ) is resolved . \n\t', '\n\t\t But we regard anaphora resolving is too hard task as a subprocess of finding anchor verbs . \n\t', '\n\t\t Figure 3 : PAS templates N^VG^N N ^ VG VG ^ N N : is a topical noun VG : is a verb group which is accepted by a finite state machine described in \n\t\t']",Positive
['\n\t\t ) Figure 4 : POS patterns our POS pattern matcher as a modified version of one in \n\t\t'],Positive
"['\n\t\t Figure 4 shows patterns in our experiment . \n\t', '\n\t\t The last verb of VG is extracted if all of Ns are topical nouns . \n\t', '\n\t\t Non-topical nouns are disregarded . \n\t', '\n\t\t Adding candidates for verb groups raises recall of obtained relations of verbs and their arguments . \n\t', '\n\t\t Restriction on intervening tokens to non-nouns raises the precision , although it decreases the recall . \n\t', '\n\t\t 4.3 Experiment 1 We extracted last verbs of POS patterns and core verbs of PASs with their arguments from 100 abstracts ( 976 sentences ) of the GENIA corpus . \n\t', '\n\t\t We took up not the verbs only but tuples of the verbs and their arguments ( VAs ) , in order to estimate effect of the arguments on semantical filtering . \n\t', '\n\t\t Results The numbers of VAs extracted from the 100 abstracts using POS patterns and PASs are shown in Table 1. ( Total ^ VAs of verbs not extracted by the other method ) are not the same , because more than one VA can be extracted on a verb in a sentence . \n\t', '\n\t\t POS patterns method extracted more VAs , although POS patterns PASs Total 1127 766 VAs of verbs not extracted by the other 478 105 Table 1 : Numbers of VAs extracted from the 100 abstracts Appropriate Inappropriate Total Correct 43 12 55 Incorrect 20 23 43 Total 63 35 98 Table 2 : Numbers of VAs extracted by POS patterns ( in detail ) their correctness is not considered . \n\t', '\n\t\t 4.4 Experiment 2 For the first 10 abstracts ( 92 sentences ) , we manually investigated whether extracted VAs are syntactically or semantically correct . \n\t', '\n\t\t The investigation was based on two criteria : \x93appropriateness\x94 based on whether the extracted verb can be used for an anchor verb and \x93correctness\x94 based on whether the syntactical analysis is correct , i.e. , whether the arguments were extracted correctly . \n\t', '\n\t\t Based on human judgment , the verbs that represent interactions , events , and properties were selected as semantically appropriate for anchor verbs , and the others were treated as inappropriate . \n\t', '\n\t\t For example , \x93identified\x94 in \x93We identified ZEBRA protein.\x94 is not appropriate and discarded . \n\t', '\n\t\t We did not consider non-topical noun arguments for POS pattern method , whereas we considered them for PAS method . \n\t', '\n\t\t Thus decision on correctness is stricter for PAS method . \n\t', '\n\t\t Results The manual investigation results on extracted VAs from the 10 abstracts using POS patterns and PASs are shown in Table 2 and 3 respectively . \n\t', '\n\t\t POS patterns extracted more ( 98 ) VAs than PASs ( 75 ) , but many of the increment were from incorrect POS pattern matching . \n\t', '\n\t\t By POS patterns , 43 VAs ( 44 % ) were extracted based on incorrect analysis . \n\t', '\n\t\t On the other hand , by PASs , 20 VAs ( 27 % ) were extracted incorrectly . \n\t', '\n\t\t Thus the ratio of VAs extracted by syntactically correct analysis is larger on PAS method . \n\t', '\n\t\t POS pattern method extracted 38 VAs of verbs not extracted by PAS method and 7 of them are correct . \n\t', '\n\t\t For PAS method , correspondent numbers are Appropriate Inappropriate Total Correct 44 11 55 Incorrect 14 6 20 Total 58 17 75 Table 3 : Numbers of VAs extracted by PASs ( in detail ) 11 and 4 respectively . \n\t', '\n\t\t Thus the increments tend to be caused by incorrect analysis , and the tendency is greater in POS pattern method . \n\t', '\n\t\t Since not all of verbs that take topical nouns are appropriate for anchor verbs , automatic filtering is required . \n\t', '\n\t\t In the filtering phase that we leave as a future work , we can use semantical classes and frequencies of arguments of the verbs . \n\t', '\n\t\t The results with syntactically incorrect arguments will cause adverse effect on filtering because they express incorrect relationship between verbs and arguments . \n\t', '\n\t\t Since the numbers of extracted VAs after excluding the ones with incorrect arguments are the same ( 55 ) between PAS and POS pattern methods , it can be concluded that the precision of PAS method is higher . \n\t', '\n\t\t Although there are few ( 7 ) correct VAs which were extracted by POS pattern method but not by PAS method , we expect the number of such verbs can be reduced using a larger corpus . \n\t', '\n\t\t Examples of appropriate VAs extracted by only one method are as follows : ( A ) is correct and ( B ) incorrect , extracted by only POS pattern method , and ( C ) is correct and ( D ) incorrect , extracted by only PAS method . \n\t', '\n\t\t Bold words are extracted verbs or predicates and italic words their extracted arguments . \n\t', '\n\t\t ( A ) This delay is associated with down-regulation of many erythroid cell-speci~c genes , including alpha- and beta-globin , band 3 , band 4 . \n\t', '\n\t\t 1 , and .... ( B ) ... show that several elements in the ... region of the IL-2R alpha gene contribute to IL-1 responsiveness , .... ( C ) The CD4 coreceptor interacts with non- polymorphic regions of ... molecules on non-polymorphic cells and contributes to T cell activation . \n\t', '\n\t\t ( D ) Whereas activation of the HIV-1 enhancer following T-cell stimulation is mediated largely through binding of the ... factor NF-kappa B to two adjacent kappa B sites in .... 5 Conclusions We have proposed a method of extracting anchor verbs as elements of extraction rules for IE by using PASs obtained by full parsing . \n\t', '\n\t\t To compare our method with more naive and robust methods , we have extracted verbs and their arguments using POS patterns and PASs . \n\t', '\n\t\t POS pattern method could obtain more candidate verbs for anchor verbs , but many of them were extracted with incorrect arguments by incorrect matching . \n\t', '\n\t\t A later filtering process benefits by precise relations between verbs and their arguments which PASs obtained . \n\t', '\n\t\t The shortcoming of PAS method is expected to be reduced by using a larger corpus , because verbs to extract will appear many times in many forms . \n\t', '\n\t\t One of the future works is to extend PAS method to handle events in nominalized forms . \n\t', '\n\t\t Acknowledgements This work was partially supported by Grant-inAid for Scientific Research on Priority Areas ( C ) \x93Genome Information Science\x94 from the Ministry of Education , Culture , Sports , Science and Technology of Japan . \n\t', '\n\t\t References Vasileios Hatzivassiloglou and Wubin Weng . \n\t', '\n\t\t 2002. Learning anchor verbs for biological interaction patterns from published text articles . \n\t', '\n\t\t International Journal of Medical Informatics , 67:19\x9632 . \n\t', '\n\t\t Jin-Dong Kim , Tomoko Ohta , Yuka Teteisi , and Jun\x92ichi Tsujii . \n\t', '\n\t\t 2003. GENIA corpus \x96 a semantically annotated corpus for bio-textmining . \n\t', '\n\t\t Bioinformatics , 19(suppl . \n\t', '\n\t\t 1):i180\x96i182 . \n\t', '\n\t\t Takaki Makino , Minoru Yoshida , Kentaro Torisawa , and Jun-ichi Tsujii . \n\t', '\n\t\t 1998. LiLFeS \x97 towards a practical HPSG parser . \n\t', '\n\t\t In Proceedings of COLING-ACL\x9298 . \n\t', '\n\t\t Yusuke Miyao , Takaki Makino , Kentaro Torisawa , and Jun-ichi Tsujii . \n\t', '\n\t\t 2000. The LiLFeS abstract machine and its evaluation with the LinGO grammar . \n\t', '\n\t\t Natural Language Engineering , 6(1):47\x96 61 . \n\t', '\n\t\t Yusuke Miyao , Takashi Ninomiya , and Jun\x92ichi Tsujii . \n\t', '\n\t\t 2003. Probabilistic modeling of argument structures including non-local dependencies . \n\t', '\n\t\t In Proceedings of RANLP 2003 , pages 285\x96291 . \n\t', '\n\t\t Yusuke Miyao , Takashi Ninomiya , and Jun\x92ichi Tsujii . \n\t', '\n\t\t 2004. Corpus-oriented grammar development for acquiring a Head-driven Phrase Structure Grammar from the Penn Treebank . \n\t', '\n\t\t In Proceedings of IJCNLP-04 . \n\t', '\n\t\t Ivan A. Sag and Thomas Wasow . \n\t', '\n\t\t 1999. Syntactic Theory . \n\t', '\n\t\t CSLI publications . \n\t', '\n\t\t Kiyoshi Sudo , Satoshi Sekine , and Ralph Grishman . \n\t', '\n\t\t 2003. An improved extraction pattern representation model for automatic IE pattern acquisition . \n\t', '\n\t\t In Proceedings ofACL 2003 , pages 224\x96231 . \n\t', '\n\t\t Resource Analysis for Question Answering Lucian Vlad Lita Warren A. Hunt Eric Nyberg Carnegie Mellon University Carnegie Mellon University Carnegie Mellon University llita@cs.cmu.edu whunt@andrew.cmu.edu ehn@cs.cmu.edu Abstract This paper attempts to analyze and bound the utility of various structured and unstructured resources in Question Answering , independent of a specific system or component . \n\t', '\n\t\t We quantify the degree to which gazetteers , web resources , encyclopedia , web documents and web-based query expansion can help Question Answering in general and specific question types in particular . \n\t', '\n\t\t Depending on which resources are used , the QA task may shift from complex answer-finding mechanisms to simpler data extraction methods followed by answer re-mapping in local documents . \n\t', '\n\t\t 1 Introduction During recent years the Question Answering ( QA ) field has undergone considerable changes : question types have diversified , question complexity has increased , and evaluations have become more standardized - as reflected by the TREC QA track \n\t\t']",Positive
"['\n\t\t Some recent approaches have tapped into external data sources such as the Web , encyclopedias , databases in order to find answer candidates , which may then be located in the specific corpus being searched \n\t\t']",Positive
"['\n\t\t As systems improve , the availability of rich resources will be increasingly critical to QA performance . \n\t', '\n\t\t While on-line resources such as the Web , WordNet , gazetteers , and encyclopedias are becoming more prevalent , no system-independent study has quantified their impact on the QA task . \n\t', '\n\t\t This paper focuses on several resources and their inherent potential to provide answers , without concentrating on a particular QA system or component . \n\t', '\n\t\t The goal is to quantify and bound the potential impact of these resources on the QA process . \n\t', '\n\t\t 2 Related Work More and more QA systems are using the Web as a resource . \n\t', '\n\t\t Since the Web is orders of magnitude larger than local corpora , redundancy of answers and supporting passages allows systems to produce more correct , confident answers \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t Due to their complementary nature of these approaches , hybrid systems are likely to perform better \n\t\t']",Positive
"['\n\t\t Definitional questions ( \x93What is X?\x94 , \x93Who is X ? \n\t', '\n\t\t \x94 ) are especially compatible with structured resources such as gazetteers and encyclopedias . \n\t', '\n\t\t The top performing definitional systems \n\t\t']",Positive
"['\n\t\t com ) , a biography dictionary ( www.s9.com ) and Google ( www. google . \n\t', '\n\t\t com ) . \n\t', '\n\t\t 3 Approach For the purpose of this paper , resources consist of structured and semi-structured knowledge , such as the Web , web search engines , gazetteers , and encyclopedias . \n\t', '\n\t\t Although many QA systems incorporate or access such resources , few systems quantify individual resource impact on their performance and little work has been done to estimate bounds on resource impact to Question Answering . \n\t', '\n\t\t Independent of a specific QA system , we quantify the degree to which these resources are able to directly provide answers to questions . \n\t', '\n\t\t Experiments are performed on the 2,393 questions and the corresponding answer keys provided through NIST \n\t\t']",Positive
"['\n\t\t 4 Gazetteers Although the Web consists of mostly unstructured and loosely structured information , the available structured data is a valuable resource for question answering . \n\t', '\n\t\t Gazetteers in particular cover several frequently-asked factoid question types , such as \x94What is the population ofX?\x94 or \x94What is the capital of Y?\x94 . \n\t', '\n\t\t The CIA World Factbook is a database containing geographical , political , and economical profiles of all the countries in the world . \n\t', '\n\t\t We also analyzed two additional data sources containing astronomy information ( www.astronomy.com ) and detailed information about the fifty US states ( www. 50states . \n\t', '\n\t\t com ) . \n\t', '\n\t\t Since gazetteers provide up-to-date information , some answers will differ from answers in local corpora or the Web. . \n\t', '\n\t\t Moreover , questions requiring interval-type answers ( e.g. \x93How close is the sun?\x94 ) may not match answers from different sources which are also correct . \n\t', '\n\t\t Gazetteers offer high precision answers , but have limited recall since they only cover a limited number of questions ( See Table 1 ) . \n\t', '\n\t\t CIA All Q-Set #qtions R P R P TREC8 200 4 100 % 6 100 % TREC9 693 8 100 % 22 79 % TREC10 500 14 100 % 23 96 % TREC11 500 8 100 % 20 100 % TREC12 500 2 100 % 11 92 % Overall 2393 36 100 % 82 91 % Table 1 : Recall ( R ) : TREC questions can be directly answered directly by gazetteers - shown are results for CIA Factbook and All gazetteers combined . \n\t', '\n\t\t Our extractor precision is Precision ( P ) . \n\t', '\n\t\t 5 WordNet Wordnets and ontologies are very common resources and are employed in a wide variety of direct and indirect QA tasks , such as reasoning based on axioms extracted from WordNet \n\t\t']",Positive
"['\n\t\t Q-Set #qtions All Gloss Syns Hyper TREC 8 200 32 22 7 13 TREC 9 693 197 140 73 75 TREC 10 500 206 148 82 88 TREC 11 500 112 80 29 46 TREC 12 500 93 56 10 52 Overall 2393 641 446 201 268 Table 2 : Number of questions answerable using WordNet glosses ( Gloss ) , synonyms ( Syns ) , hypernyms and hyponyms ( Hyper ) , and all of them combined All . \n\t', '\n\t\t Table 2 shows an upper bound on how many TREC questions could be answered directly using WordNet as an answer source . \n\t', '\n\t\t Question terms and phrases were extracted and looked up in WordNet glosses , synonyms , hypernyms , and hyponyms . \n\t', '\n\t\t If the answer key matched the relevant WordNet data , then an answer was considered to be found . \n\t', '\n\t\t Since some answers might occur coincidentally , we these results to represent upper bounds on possible utility . \n\t', '\n\t\t 6 Structured Data Sources Encyclopedias , dictionaries , and other web databases are structured data sources that are often employed in answering definitional questions ( e.g. , \x93What is X ? \n\t', '\n\t\t \x94 , \x93Who is X ? \n\t', '\n\t\t \x94 ) . \n\t', '\n\t\t The top-performing definitional systems at TREC \n\t\t']",Positive
"['\n\t\t com ) , Wikipedia ( www.wikipedia.com ) , a biography dictionary ( www.s9.com ) and Google ( www.google.com ) . \n\t', '\n\t\t Table 3 shows a number of data sources and their impact on answering TREC questions . \n\t', '\n\t\t N- grams were extracted from each question and run through Wikipedia and Google\x92s define operator ( which searches specialized dictionaries , definition lists , glossaries , abbreviation lists etc ) . \n\t', '\n\t\t Table 3 show that TREC 10 and 11 questions benefit the most from the use of an encyclopedia , since they include many definitional questions . \n\t', '\n\t\t On the other hand , since TREC 12 has fewer definitional questions and more procedural questions , it does not benefit as much from Wikipedia or Google\x92s define operator . \n\t', '\n\t\t Q-Set #qtions WikiAll Wiki1st DefOp TREC 8 200 56 5 30 TREC 9 693 297 49 71 TREC 10 500 225 45 34 TREC 11 500 155 19 23 TREC 12 500 124 12 27 Overall 2393 857 130 185 Table 3 : The answer is found in a definition extracted from Wikipedia WikiAll , in the first definition extracted from Wikipedia Wiki1st , through Google\x92s define operator DefOp . \n\t', '\n\t\t 7 Answer Type Coverage To test coverage of different answer types , we employed the top level of the answer type hierarchy used by the JAVELIN system \n\t\t']",Positive
"['\n\t\t The most frequent types are : definition ( e.g. \x93What is viscosity?\x94 ) , person-bio ( e.g. \x93Who was La- can?\x94 ) , object(e.g . \n\t', '\n\t\t \x93Name the highest mountain.\x94 ) , process ( e.g. \x93How did Cleopatra die?\x94 ) , lexicon ( \x93What does CBS stand for?\x94)temporal(e.g . \n\t', '\n\t\t \x93When is the first day of summer?\x94 ) , numeric ( e.g. \x93How tall is Mount Everest?\x94 ) , location ( e.g. \x93Where is Tokyo?\x94 ) , and proper-name ( e.g. \x93Who owns the Raiders?\x94 ) . \n\t', '\n\t\t AType #qtions WikiAll DefOp Gaz WN object 1003 426 92 58 309 lexicon 50 25 3 0 26 defn 178 105 9 11 112 pers-bio 39 15 11 0 17 process 138 23 6 9 16 temporal 194 63 14 0 50 numeric 121 27 13 10 18 location 151 69 21 2 47 proper 231 76 10 0 32 Table 4 : Coverage of TREC questions divided by most common answer types . \n\t', '\n\t\t Table 4 shows TREC question coverage broken down by answer type . \n\t', '\n\t\t Due to temporal consistency , numeric questions are not covered very well . \n\t', '\n\t\t Although the process and object types are broad answer types , the coverage is still reasonably good . \n\t', '\n\t\t As expected , the definition and person-bio answer types are covered well by these resources . \n\t', '\n\t\t 8 The Web as a Resource An increasing number of QA systems are using the web as a resource . \n\t', '\n\t\t Since the Web is orders of magnitude larger than local corpora , answers occur frequently in simple contexts , which is more conducive to retrieval and extraction of correct , confident answers \n\t\t']",Positive
['\n\t\t The web has been employed for pattern acquisition \n\t\t'],Positive
"['\n\t\t Some of these approaches enhance existing QA systems , while others simplify the question answering task , allowing a less complex approach to find correct answers . \n\t', '\n\t\t 8.1 Web Documents Instead of searching a local corpus , some QA systems retrieve relevant documents from the web \n\t\t']",Positive
"['\n\t\t Since the density of relevant web documents can be higher than the density of relevant local documents , answer extraction may be more successful from the web. . \n\t', '\n\t\t For a TREC evaluation , answers found on the web must also be mapped to relevant documents in the local corpus . \n\t', '\n\t\t Web Retrieval Performance For QA 1000 Correct Doc Density First Correct Doc 800 700 600 500 400 300 200 100 00 10 20 30 40 50 60 70 80 90 100 document rank Figure 1 : Web retrieval : relevant document density and rank of first relevant document . \n\t', '\n\t\t In order to evaluate the impact of web documents on TREC questions , we performed an experiment where simple queries were submitted to a web search engine . \n\t', '\n\t\t The questions were tokenized and filtered using a standard stop word list . \n\t', '\n\t\t The resulting keyword queries were used to retrieve 100 documents through the Google API ( www.google.com/api ) . \n\t', '\n\t\t Documents containing the full question , question number , references to TREC , NIST , AQUAINT , Question Answering and similar content were filtered out . \n\t', '\n\t\t Figure 1 shows the density of documents containing a correct answer , as well as the rank of the first document containing a correct answer . \n\t', '\n\t\t The simple word query retrieves a relevant document for almost half of the questions . \n\t', '\n\t\t Note that for most systems , the retrieval performance should be superior since queries are usually more refined and additional query expansion is performed . \n\t', '\n\t\t However , this experiment provides an intuition and a very good lower bound on the precision and density of current web documents for the TREC QA task . \n\t', '\n\t\t 8.2 Web-Based Query Expansion Several QA systems participating at TREC have used search engines for query expansion \n\t\t']",Positive
['\n\t\t The basic query expansion method utilizes pseudo-relevance feedback ( PRF ) \n\t\t'],Positive
"['\n\t\t Content words are selected from questions and submitted as queries to a search engine . \n\t', '\n\t\t The top n retrieved documents are selected , and k terms or phrases are extracted according to an optimization criterion ( e.g. term frequency , n-gram frequency , average mutual information using corpus statistics , etc ) . \n\t', '\n\t\t These k items are used in the expanded query . \n\t', '\n\t\t We experimented by using the top 5 , 10 , 15 , 20 , 900 Answer frequency using PRF # PRF terms Figure 2 : Finding a correct answer in PRF expansion terms - applied to 2183 questions for witch answer keys exist . \n\t', '\n\t\t 50 , and 100 documents retrieved via the Google API for each question , and extracted the most frequent fifty n-grams ( up to trigrams ) . \n\t', '\n\t\t The goal was to determine the quality of query expansion as measured by the density of correct answers already present in the expansion terms . \n\t', '\n\t\t Even without filtering n- grams matching the expected answer type , simple PRF produces the correct answer in the top n-grams for more than half the questions . \n\t', '\n\t\t The best correct answer density is achieved using PRF with only 20 web documents . \n\t', '\n\t\t 8.3 Conclusions This paper quantifies the utility of well-known and widely-used resources such as WordNet , encyclopedias , gazetteers and the Web on question answering . \n\t', '\n\t\t The experiments presented in this paper represent loose bounds on the direct use of these resources in answering TREC questions . \n\t', '\n\t\t We reported the performance of these resources on different TREC collections and on different question types . \n\t', '\n\t\t We also quantified web retrieval performance , and confirmed that the web contains a consistently high density of relevant documents containing correct answers even when simple queries are used . \n\t', '\n\t\t The paper also shows that pseudo-relevance feedback alone using web documents for query expansions can produce a correct answer for fifty percent of the questions examined . \n\t', '\n\t\t 9 Acknowledgements This work was supported in part by the Advanced Research and Development Activity (ARDA)\x92s Advanced Question Answering for Intelligence ( AQUAINT ) Program . \n\t', '\n\t\t References C.L.A. Clarke , G.V. Cormack , and T.R. Lynam . \n\t', '\n\t\t 2001. Exploiting redundancy in question answering . \n\t', '\n\t\t SIGIR . \n\t', '\n\t\t S. Dumais , M. Banko , E. Brill , J. Lin , and A. Ng. 2002 . \n\t', '\n\t\t Web question answering : Is more always better ? \n\t', '\n\t\t SIGIR . \n\t', '\n\t\t J. Leidner , J. Bos , T. Dalmas , J. Curran , S. Clark , C. Bannard , B. Webber , and M. Steedman . \n\t', '\n\t\t 2003. Qed : The edinburgh trec-2003 question answering system . \n\t', '\n\t\t TREC . \n\t', '\n\t\t J. Lin and B. Katz . \n\t', '\n\t\t 2003. Question answering from the web using knowledge annotation and knowledge mining techniques . \n\t', '\n\t\t CIKM . \n\t', '\n\t\t J. Lin . \n\t', '\n\t\t 2002. The web as a resource for question answering : Perspectives and challenges . \n\t', '\n\t\t LREC . \n\t', '\n\t\t B. Magnini , M. Negri , R. Pervete , and H. Tanev . \n\t', '\n\t\t 2002. Is it the right answer ? \n\t', '\n\t\t exploiting web redundancy for answer validation . \n\t', '\n\t\t ACL . \n\t', '\n\t\t G.A. Miller , R. Beckwith , C. Fellbaum , D. Gross , and K. Miller . \n\t', '\n\t\t 1990. Five papers on wordnet . \n\t', '\n\t\t International Journal ofLexicography . \n\t', '\n\t\t D. Moldovan , D. Clark , S. Harabagiu , and S. Maiorano . \n\t', '\n\t\t 2003. Cogex : A logic prover for question answering . \n\t', '\n\t\t ACL . \n\t', '\n\t\t E. Nyberg , T. Mitamura , J. Callan , J. Carbonell , R. Frederking , K. Collins-Thompson , L. Hiyakumoto , Y. Huang , C. Huttenhower , S. Judy , J. Ko , A. Kupsc , L.V. . \n\t', '\n\t\t Lita , V. Pedro , D. Svoboda , and B. Vand Durme . \n\t', '\n\t\t 2003. A multi strategy approach with dynamic planning . \n\t', '\n\t\t TREC . \n\t', '\n\t\t D. Paranjpe , G. Ramakrishnan , and S. Srinivasan . \n\t', '\n\t\t 2003. Passage scoring for question answering via bayesian inference on lexical relations . \n\t', '\n\t\t TREC . \n\t', '\n\t\t D. Ravichandran , A. Ittycheriah , and S. Roukos . \n\t', '\n\t\t 2003. Automatic derivation of surface text patterns for a maximum entropy based question answering system . \n\t', '\n\t\t HLT-NAACL . \n\t', '\n\t\t E.M. Voorhees . \n\t', '\n\t\t 2003. Overview of the trec 2003 question answering track . \n\t', '\n\t\t TREC . \n\t', '\n\t\t J. Xu and W.B. Croft . \n\t', '\n\t\t 1996. Query expansion using local and global analysis . \n\t', '\n\t\t SIGIR . \n\t', '\n\t\t J. Xu , A. Licuanan , and R. Weischedel . \n\t', '\n\t\t 2003. Trec 2003 qa at bbn : Answering definitional questions . \n\t', '\n\t\t TREC . \n\t', '\n\t\t H. Yang , T.S. Chua , S. Wang , and C.K. Koh . \n\t', '\n\t\t 2003. Structured use of external knowledge for event- based open domain question answering . \n\t', '\n\t\t SIGIR . \n\t', '\n\t\t 1100 1000 900 800 700 600 500 400 300 200 100 0 5 10 15 20 25 30 35 40 45 50 Top 5 documents Top 10 documents Top 15 documents Top 20 documents Top 50 documents Top 100 documents \n\t', '\n\t\t TANGO : Bilingual Collocational Concordancer Jia-Yan Jian Department of Computer Science National Tsing Hua University 101 , Kuangfu Road , Hsinchu , Taiwan g914339@oz.nthu.edu.tw Yu-Chia Chang Inst . \n\t', '\n\t\t of Information System and Applictaion National Tsing Hua University 101 , Kuangfu Road , Hsinchu , Taiwan u881222@alumni.nthu.e du.tw Jason S. Chang Department of Computer Science National Tsing Hua University 101 , Kuangfu Road , Hsinchu , Taiwan jschang@cs.nthu.edu.tw Abstract In this paper , we describe TANGO as a collocational concordancer for looking up collocations . \n\t', '\n\t\t The system was designed to answer user\x92s query of bilingual collocational usage for nouns , verbs and adjectives . \n\t', '\n\t\t We first obtained collocations from the large monolingual British National Corpus ( BNC ) . \n\t', '\n\t\t Subsequently , we identified collocation instances and translation counterparts in the bilingual corpus such as Sinorama Parallel Corpus ( SPC ) by exploiting the word- alignment technique . \n\t', '\n\t\t The main goal of the concordancer is to provide the user with a reference tools for correct collocation use so as to assist second language learners to acquire the most eminent characteristic of native-like writing . \n\t', '\n\t\t 1 Introduction Collocations are a phenomenon of word combination occurring together relatively often . \n\t', '\n\t\t Collocations also reflect the speaker\x92s fluency of a language , and serve as a hallmark of near native- like language capability . \n\t', '\n\t\t Collocation extraction is critical to a range of studies and applications , including natural language generation , computer assisted language learning , machine translation , lexicography , word sense disambiguation , cross language information retrieval , and so on . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t The best methods for extracting collocations usually take into consideration both linguistic and statistical constraints . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t Moreover , log likelihood ratios are regarded as a more effective method to identify collocations especially when the occurrence count is very low \n\t\t']",Positive
"['\n\t\t Smadja\x92s XTRACT is the pioneering work on extracting collocation types . \n\t', '\n\t\t XTRACT employed three different statistical measures related to how associated a pair to be collocation type . \n\t', '\n\t\t It is complicated to set different thresholds for each statistical measure . \n\t', '\n\t\t We decided to research and develop a new and simple method to extract monolingual collocations . \n\t', '\n\t\t We also provide a web-based user interface capable of searching those collocations and its usage . \n\t', '\n\t\t The concordancer supports language learners to acquire the usage of collocation . \n\t', '\n\t\t In the following section , we give a brief overview of the TANGO concordancer . \n\t', '\n\t\t 2 TANGO TANGO is a concordancer capable of answering users\x92 queries on collocation use . \n\t', '\n\t\t Currently , TANGO supports two text collections : a monolingual corpus ( BNC ) and a bilingual corpus ( SPC ) . \n\t', '\n\t\t The system consists of four main parts : 2.1 Chunk and Clause Information Integrated For CoNLL-2000 shared task , chunking is considered as a process that divides a sentence into syntactically correlated parts of words . \n\t', '\n\t\t With the benefits of CoNLL training data , we built a chunker that turn sentences into smaller syntactic structure of non-recursive basic phrases to facilitate precise collocation extraction . \n\t', '\n\t\t It becomes easier to identify the argument-predicate relationship by looking at adjacent chunks . \n\t', '\n\t\t By doing so , we save time as opposed to n-gram statistics or full parsing . \n\t', '\n\t\t Take a text in CoNLL2000 for example : The words correlated with the same chunk tag can be further grouped together ( see Table 1 ) . \n\t', '\n\t\t For instance , with chunk information , we can extract Confidence/B-NP in/B-PP the/B-NP pound/I-NP is/B-VP widely/I-VP expected/I-VP to/I-VP take/I-VP another/B-NP sharp/I-NP dive/I-NP if/BSBAR trade/B-NP figures/I-NP for/B-PP September/B-NP ( Note : Every chunk type is associated with two different chunk tags : B-CHUNK for the first word of the chunk and I-CHUNK for the other words in the same chunk ) the target VN collocation \x93take dive\x94 from the example by considering the last word of two adjacent VP and NP chunks . \n\t', '\n\t\t We build a robust and efficient chunking model from training data of the CoNLL shared task , with up to 93.7 % precision and recall . \n\t', '\n\t\t Sentence chunking Features Confidence NP in PP the pound NP is expected to take VP another sharp dive NP if SBAR trade figures NP for PP September NP Table 1 : Chunked Sentence In some cases , only considering the chunk information is not enough . \n\t', '\n\t\t For example , the sentence \x93...the attitude he had towards the country is positive...\x94 may cause problem . \n\t', '\n\t\t With the chunk information , the system extracts out the type \x93have towards the country\x94 as a VPN collocation , yet that obviously cuts across two clauses and is not a valid collocation . \n\t', '\n\t\t To avoid that kind of errors , we further take the clause information into account . \n\t', '\n\t\t With the training and test data from CoNLL2001 , we built an efficient HMM model to identify clause relation between words . \n\t', '\n\t\t The language model provides sufficient information to avoid extracting wrong collocations . \n\t', '\n\t\t Examples show as follows ( additional clause tags will be attached ) : ( 1 ) ....the attitude ( S* he has *S ) toward the country ( 2 ) ( S* I think ( S* that the people are most concerned with the question of ( S* when conditions may become ripe . \n\t', '\n\t\t *S)S)S ) As a result , we can avoid combining a verb with an irrelevant noun as its collocate as \x93have toward country\x94 in ( 1 ) or \x93think ... people\x94 in ( 2 ) . \n\t', '\n\t\t When the sentences in the corpus are annotated with the chunk and clause information , we can consequently extract collocations more precisely . \n\t', '\n\t\t 2.2 Collocation Type Extraction A large set of collocation candidates can be obtained from BNC , via the process of integrating chunk and clause information . \n\t', '\n\t\t We here consider three prevalent Verb-Noun collocation structures in corpus : VP+NP , VP+PP+NP , and VP+NP+PP . \n\t', '\n\t\t Exploiting Logarithmic Likelihood Ratio ( LLR ) statistics , we can calculate the strength of association between two collocates . \n\t', '\n\t\t The collocational type with threshold higher than 7.88 ( confidence level 99.5 % ) will be kept as one entry in our collocation type list . \n\t', '\n\t\t 2.3 Collocation Instance Identification We subsequently identify collocation instances in the bilingual corpus ( SPC ) with the collocation types extracted from BNC in the previous step . \n\t', '\n\t\t Making use of the sequence of chunk types , we again single out the adjacent structures of VN , VPN , and VNP . \n\t', '\n\t\t With the help of chunk and clause information , we thus find the valid instances where the expected collocation types are located , so as to build a collocational concordance . \n\t', '\n\t\t Moreover , the quantity and quality of BNC also facilitate the collocation identification in another smaller bilingual corpus with better statistic measure . \n\t', '\n\t\t English sentence Chinese sentence If in this time no one shows concern for them , ^ ^ ^ ^ ^ ^ ^ 0000000 and directs them to ^ ^ ^ ^ ^ ^ ^ correct thinking , and ^ ^ ^ ^ ^ ^ ^ teaches them how to E-1000000 express and release ^ ^ emotions , this could very ^ ^ ^ ^ ^ easily leave them with a ^ ^ ^ ^ ^ ^ ^ terrible personality ^^ complex they can never resolve . \n\t', '\n\t\t Occasionally some ^ ^ ^ ^ ^ ^ kungfu movies may ^ ^ ^ ^ ^ ^ ^ appeal to foreign ^ ^ ^ ^ ^ ^ ^ audiences , but these too are exceptions to the rule . \n\t', '\n\t\t ^^^ ^^^ Table 2 : Examples of collocational translation memory Type Collocation types in BNC VN 631,638 VPN 15,394 VNP 14,008 Table 3 : The result of collocation types extracted from BNC and collocation instances identified in SPC 2.4 Extracting Collocational Translation Equivalents in Bilingual Corpus When accurate instances are obtained from bilingual corpus , we continue to integrate the statistical word-alignment techniques \n\t\t']",Positive
"['\n\t\t We first locate the translation of the noun . \n\t', '\n\t\t Subsequently , we locate the verb nearest to the noun translation to find the translation for the verb . \n\t', '\n\t\t We can think of collocation with corresponding translations as a kind of translation memory ( shows in Table 2).The implementation result of BNC and SPC shows in the Table 3 , 4 , and 5 . \n\t', '\n\t\t 3 Collocation Concordance With the collocation types and instances extracted from the corpus , we built an online collocational concordancer called TANGO for looking up translation memory . \n\t', '\n\t\t A user can type in any English query and select the intended part of speech of query and collocate . \n\t', '\n\t\t For example in Figure 1 , after query for the verb collocates of the noun \x93influence\x94 is submitted , the results are displayed on the return page . \n\t', '\n\t\t The user can then browse through different collocates types and also click to get to see all the instances of a certain collocation type . \n\t', '\n\t\t Noun VN types Language 320 Influence 319 Threat 222 Doubt 199 Crime 183 Phone 137 Cigarette 121 Throat 86 Living 79 Suicide 47 Table 4 : Examples of collocation types including a given noun in BNC VN type Example Exert That means they would influence already be exerting their influence by the time the microwave background was born . \n\t', '\n\t\t Exercise influence The Davies brothers , Adrian ( who scored 14 points ) and Graham ( four ) , exercised an important creative influence on Cambridge fortunes while their flankers Holmes and Pool-Jones were full of fire and tenacity in the loose . \n\t', '\n\t\t Wield influence Fortunately , George V had worked well with his father and knew the nature of the current political trends , but he did not wield the same influence internationally as his esteemed father . \n\t', '\n\t\t Table 5 : Examples of collocation instances extracted from SPC Moreover , using the technique of bilingual collocation alignment and sentence alignment , the system will display the target collocation with highlight to show translation equivalents in context . \n\t', '\n\t\t Translators or learners , through this web- based interface , can easily acquire the usage of each collocation with relevant instances . \n\t', '\n\t\t This collocational concordancer is a very useful tool for self-inductive learning tailored to intermedi-ate or advanced English learners . \n\t', '\n\t\t Users can obtain the result of the VN or AN collocations related to their query . \n\t', '\n\t\t TANGO shows the collocation types and instances with collocations and translation counterparts highlighted . \n\t', '\n\t\t The evaluation ( shows in Table 6 ) indicates an average precision of 89.3 % with regard to satisfactory . \n\t', '\n\t\t 4 Conclusion and Future Work In this paper , we describe an algorithm that employs linguistic and statistical analyses to extract instance of VN collocations from a very large corpus ; we also identify the corresponding translations in a parallel corpus . \n\t', '\n\t\t The algorithm is applicable to other types of collocations without being limited by collocation\x92s span . \n\t', '\n\t\t The main difference between our algorithm and previous work lies in that we extract valid instances instead of types , based on linguistic information of chunks and clauses . \n\t', '\n\t\t Moreover , in our research we observe Type The number of Translation Translation Memory ( * ) Precision of Precision of Translation Memory ( * ) selected Memory Translation sentences Memory VN 100 73 90 73 90 VPN 100 66 89 66 89 VNP 100 78 89 78 89 Table 6 : Experiment result of collocational translation memory from Sinorama parallel Corpus Figure 1 : The caption of the table other types related to VN such as VPN ( ie. verb + preposition + noun ) and VNP ( ie. verb + noun + preposition ) , which will also be crucial for machine translation and computer assisted language learning . \n\t', '\n\t\t In the future , we will apply our method to more types of collocations , to pave the way for more comprehensive applications . \n\t', '\n\t\t Acknowledgements This work is carried out under the project \x93CANDLE\x94 funded by National Science Council in Taiwan ( NSC92-2524-S007-002 ) . \n\t', '\n\t\t Further information about CANDLE is available at http://candle.cs.nthu.edu.tw/ . \n\t', '\n\t\t References Dunning , T ( 1993 ) Accurate methods for the statistics of surprise and coincidence , Computational Linguistics 19:1 , 61-75 . \n\t', '\n\t\t Hanks , P. and Church , K. W. . \n\t', '\n\t\t Word association norms , mutual information , and lexicography . \n\t', '\n\t\t Computational Linguistics , 1990 , 16(1) , pp. 22-29 . \n\t', '\n\t\t Melamed , I. Dan . \n\t', '\n\t\t "" A Word-to-Word Model of Translational Equivalence "" . \n\t', '\n\t\t In Procs . \n\t', '\n\t\t of the ACL97 . \n\t', '\n\t\t pp 490-497 . \n\t', '\n\t\t Madrid Spain , 1997 . \n\t', '\n\t\t Smadja , F. 1993 . \n\t', '\n\t\t Retrieving collocations from text : Xtract . \n\t', '\n\t\t Computational Linguistics , 19(1):143-177 . \n\t', '\n\t\t Graph-based Ranking Algorithms for Sentence Extraction , Applied to Text Summarization Rada Mihalcea Department of Computer Science University of North Texas rada@cs.unt.edu Abstract This paper presents an innovative unsupervised method for automatic sentence extraction using graph- based ranking algorithms . \n\t', '\n\t\t We evaluate the method in the context of a text summarization task , and show that the results obtained compare favorably with previously published results on established benchmarks . \n\t', '\n\t\t 1 Introduction Graph-based ranking algorithms , such as Kleinberg\x92s HITS algorithm \n\t\t']",Positive
"['\n\t\t In short , a graph-based ranking algorithm is a way of deciding on the importance of a vertex within a graph , by taking into account global information recursively computed from the entire graph , rather than relying only on local vertex-specific information . \n\t', '\n\t\t A similar line of thinking can be applied to lexical or semantic graphs extracted from natural language documents , resulting in a graph-based ranking model called TextRank \n\t\t']",Positive
"['\n\t\t Such text-oriented ranking methods can be applied to tasks ranging from automated extraction of keyphrases , to extractive summarization and word sense disambiguation \n\t\t']",Positive
"['\n\t\t In this paper , we investigate a range of graph- based ranking algorithms , and evaluate their application to automatic unsupervised sentence extraction in the context of a text summarization task . \n\t', '\n\t\t We show that the results obtained with this new unsupervised method are competitive with previously developed state-of-the-art systems . \n\t', '\n\t\t 2 Graph-Based Ranking Algorithms Graph-based ranking algorithms are essentially a way of deciding the importance of a vertex within a graph , based on information drawn from the graph structure . \n\t', '\n\t\t In this section , we present three graph-based ranking algorithms \x96 previously found to be successful on a range of ranking problems . \n\t', '\n\t\t We also show how these algorithms can be adapted to undirected or weighted graphs , which are particularly useful in the context of text-based ranking applications . \n\t', '\n\t\t Let G = ( V , E ) be a directed graph with the set of vertices V and set of edges E , where E is a subset of V x V . \n\t', '\n\t\t For a given vertex Vi , let In(Vi) be the set of vertices that point to it ( predecessors ) , and let Out(Vi) be the set of vertices that vertex Vi points to ( successors ) . \n\t', '\n\t\t 2.1 HITS HITS ( Hyperlinked Induced Topic Search ) \n\t\t']",Positive
"['\n\t\t The HITS algorithm makes a distinction between \x93authorities\x94 ( pages with a large number of incoming links ) and \x93hubs\x94 ( pages with a large number of outgoing links ) . \n\t', '\n\t\t For each vertex , HITS produces two sets of scores \x96 an \x93authority\x94 score , and a \x93hub\x94 score : HITS,(Vi) = E HITSH(Vj) ( 1 ) VjEIn(Vi) HITSH(Vi) = E HITS,(Vj) ( 2 ) Vj EOut(Vi) 2.2 Positional Power Function Introduced by \n\t\t']",Positive
"['\n\t\t E POSP(Vi) = ICI The counterpart of thepositional power function is the positional weakness function , defined as : POSW(Vi) = IVI 1 E ( 1+POSW(Vj)) ( 4 ) VjEIn(Vi) ( 1+POSP(Vj)) ( 3 ) Vj EOut(Vi) 2.3 PageRank PageRank \n\t\t']",Positive
"['\n\t\t Unlike other ranking algorithms , PageRank integrates the impact of both incoming and outgoing links into one single model , and therefore it produces only one set of scores : HITSWA ( Vi ) = ~ wjiHITSWH(Vj) ( 6 ) Vj EIn(Vi) HITSWH(Vi) = 11 wij HITSWA ( Vj ) ( 7 ) Vj EOut(Vi) ~ ( 1+wijPOSWP ( Vj ) ) ( 8 ) POSWP ( Vi ) = 1 |V | Vj EOut(Vi) PR(Vi) = ( 1\x97 d ) + d * E PR(Vj) ( 5 ) ~ ( 1 + wjiPOSWW ( Vj ) ) ( 9 ) VjEIn(Vi) |Out(Vj)| POSWW ( Vi ) = 1 |V | Vj EIn(Vi) where d is a parameter that is set between 0 and 1 1 . \n\t', '\n\t\t For each of these algorithms , starting from arbitrary values assigned to each node in the graph , the computation iterates until convergence below a given threshold is achieved . \n\t', '\n\t\t After running the algorithm , a score is associated with each vertex , which represents the \x93importance\x94 or \x93power\x94 of that vertex within the graph . \n\t', '\n\t\t Notice that the final values are not affected by the choice of the initial value , only the number of iterations to convergence may be different . \n\t', '\n\t\t 2.4 Undirected Graphs Although traditionally applied on directed graphs , recursive graph-based ranking algorithms can be also applied to undirected graphs , in which case the out- degree of a vertex is equal to the in-degree of the vertex . \n\t', '\n\t\t For loosely connected graphs , with the number of edges proportional with the number of vertices , undirected graphs tend to have more gradual convergence curves . \n\t', '\n\t\t As the connectivity of the graph increases ( i.e. larger number of edges ) , convergence is usually achieved after fewer iterations , and the convergence curves for directed and undirected graphs practically overlap . \n\t', '\n\t\t 2.5 Weighted Graphs In the context of Web surfing or citation analysis , it is unusual for a vertex to include multiple or partial links to another vertex , and hence the original definition for graph-based ranking algorithms is assuming unweighted graphs . \n\t', '\n\t\t However , in our TextRank model the graphs are build from natural language texts , and may include multiple or partial links between the units ( vertices ) that are extracted from text . \n\t', '\n\t\t It may be therefore useful to indicate and incorporate into the model the \x93strength\x94 of the connection between two vertices Vi and Vg as a weight wig added to the corresponding edge that connects the two vertices . \n\t', '\n\t\t Consequently , we introduce new formulae for graph-based ranking that take into account edge weights when computing the score associated with a vertex in the graph . \n\t', ""\n\t\t ' The factor d is usually set at 0.85 \n\t\t""]",Positive
"['\n\t\t PRW(Vi) = ( 1 \x97 d ) + d * E Vj EIn(Vi) While the final vertex scores ( and therefore rankings ) for weighted graphs differ significantly as compared to their unweighted alternatives , the number of iterations to convergence and the shape of the convergence curves is almost identical for weighted and unweighted graphs . \n\t', '\n\t\t 3 Sentence Extraction To enable the application of graph-based ranking algorithms to natural language texts , TextRank starts by building a graph that represents the text , and interconnects words or other text entities with meaningful relations . \n\t', '\n\t\t For the task of sentence extraction , the goal is to rank entire sentences , and therefore a vertex is added to the graph for each sentence in the text . \n\t', '\n\t\t To establish connections ( edges ) between sentences , we are defining a \x93similarity\x94 relation , where \x93similarity\x94 is measured as a function of content overlap . \n\t', '\n\t\t Such a relation between two sentences can be seen as a process of \x93recommendation\x94 : a sentence that addresses certain concepts in a text , gives the reader a \x93recommendation\x94 to refer to other sentences in the text that address the same concepts , and therefore a link can be drawn between any two such sentences that share common content . \n\t', '\n\t\t The overlap of two sentences can be determined simply as the number of common tokens between the lexical representations of the two sentences , or it can be run through syntactic filters , which only count words of a certain syntactic category . \n\t', '\n\t\t Moreover , to avoid promoting long sentences , we are using a normalization factor , and divide the content overlap of two sentences with the length of each sentence . \n\t', '\n\t\t Formally , given two sentences Si and Sg , with a sentence being represented by the set of Ni words that appear in the sentence : Si = Wi~ , Wi~ , ... , WiNi , the similarity of Si and Sg is defined as : Similarity ( Si,Sg ) = Ws(ISiI)+lo ( IsjI , ) The resulting graph is highly connected , with a weight associated with each edge , indicating the PRW ( Vj ) E wkj Vk EOut(Vj ) wji ( 10 ) strength of the connections between various sentence pairs in the text2 . \n\t', '\n\t\t The text is therefore represented as a weighted graph , and consequently we are using the weighted graph-based ranking formulae introduced in Section 2.5 . \n\t', '\n\t\t The graph can be represented as : ( a ) simple undirected graph ; ( b ) directed weighted graph with the orientation of edges set from a sentence to sentences that follow in the text ( directed forward ) ; or ( c ) directed weighted graph with the orientation of edges set from a sentence to previous sentences in the text ( directed backward ) . \n\t', '\n\t\t After the ranking algorithm is run on the graph , sentences are sorted in reversed order of their score , and the top ranked sentences are selected for inclusion in the summary . \n\t', '\n\t\t Figure 1 shows a text sample , and the associated weighted graph constructed for this text . \n\t', '\n\t\t The figure also shows sample weights attached to the edges connected to vertex 93 , and the final score computed for each vertex , using the PR formula , applied on an undirected graph . \n\t', '\n\t\t The sentences with the highest rank are selected for inclusion in the abstract . \n\t', '\n\t\t For this sample article , sentences with id-s 9 , 15 , 16 , 18 are extracted , resulting in a summary of about 100 words , which according to automatic evaluation measures , is ranked the second among summaries produced by 15 other systems ( see Section 4 for evaluation methodology ) . \n\t', '\n\t\t 4 Evaluation The TextRank sentence extraction algorithm is evaluated in the context of a single-document summarization task , using 567 news articles provided during the Document Understanding Evaluations 2002 ( DUC , 2002 ) . \n\t', '\n\t\t For each article , TextRank generates a 100-words summary \x97 the task undertaken by other systems participating in this single document summarization task . \n\t', '\n\t\t For evaluation , we are using the ROUGE evaluation toolkit , which is a method based on Ngram statistics , found to be highly correlated with human evaluations \n\t\t']",Positive
"['\n\t\t Two manually produced reference summaries are provided , and used in the evaluation process4 . \n\t', '\n\t\t 2In single documents , sentences with highly similar content are very rarely if at all encountered , and therefore sentence redundancy does not have a significant impact on the summarization of individual texts . \n\t', '\n\t\t This may not be however the case with multiple document summarization , where a redundancy removal technique \x96 such as a maximum threshold imposed on the sentence similarity \x96 needs to be implemented . \n\t', '\n\t\t 3Weights are listed to the right or above the edge they correspond to . \n\t', '\n\t\t Similar weights are computed for each edge in the graph , but are not displayed due to space restrictions . \n\t', '\n\t\t 4The evaluation is done using the Ngram(1,1) setting of ROUGE , which was found to have the highest correlation with human judgments , at a confidence level of 95 % . \n\t', '\n\t\t Only the first 100 words in each summary are considered . \n\t', '\n\t\t Figure 1 : Sample graph build for sentence extraction from a newspaper article . \n\t', '\n\t\t We evaluate the summaries produced by TextRank using each of the three graph-based ranking algorithms described in Section 2 . \n\t', '\n\t\t Table 1 shows the results obtained with each algorithm , when using graphs that are : ( a ) undirected , ( b ) directed forward , or ( c ) directed backward . \n\t', '\n\t\t For a comparative evaluation , Table 2 shows the results obtained on this data set by the top 5 ( out of 15 ) performing systems participating in the single document summarization task at DUC 2002 ( DUC , 2002 ) . \n\t', '\n\t\t It also lists the baseline performance , computed for 100-word summaries generated by taking the first sentences in each article . \n\t', '\n\t\t Discussion . \n\t', '\n\t\t The TextRank approach to sentence extraction succeeds in identifying the most important sentences in a text based on information exclusively 3 : BC^HurricaineGilbert , 09^11 339 4 : BC^Hurricaine Gilbert , 0348 5 : Hurricaine Gilbert heads toward Dominican Coast 6 : By Ruddy Gonzalez 7 : Associated Press Writer 8 : Santo Domingo , Dominican Republic ( AP ) 9 : Hurricaine Gilbert Swept towrd the Dominican Republic Sunday , and the Civil Defense alerted its heavily populated south coast to prepare for high winds , heavy rains , and high seas . \n\t', '\n\t\t 10 : The storm was approaching from the southeast with sustained winds of 75 mph gusting to 92 mph. 11 : "" There is no need for alarm , "" Civil Defense Director Eugenio Cabral said in a television alert shortly after midnight Saturday . \n\t', '\n\t\t 12 : Cabral said residents of the province of Barahona should closely follow Gilbert\x92s movement . \n\t', '\n\t\t 13 : An estimated 100,000 people live in the province , including 70,000 in the city of Barahona , about 125 miles west of Santo Domingo . \n\t', '\n\t\t 14. Tropical storm Gilbert formed in the eastern Carribean and strenghtened into a hurricaine Saturday night . \n\t', '\n\t\t 15 : The National Hurricaine Center in Miami reported its position at 2 a.m. Sunday at latitude 16.1 north , longitude 67.5 west , about 140 miles south of Ponce , Puerto Rico , and 200 miles southeast of Santo Domingo . \n\t', '\n\t\t 16 : The National Weather Service in San Juan , Puerto Rico , said Gilbert was moving westard at 15 mph with a "" broad area of cloudiness and heavy weather "" rotating around the center of the storm . \n\t', '\n\t\t 17. The weather service issued a flash flood watch for Puerto Rico and the Virgin Islands until at least 6 p.m. Sunday . \n\t', '\n\t\t 18 : Strong winds associated with the Gilbert brought coastal flooding , strong southeast winds , and up to 12 feet to Puerto Rico\x92s south coast . \n\t', '\n\t\t 19 : There were no reports on casualties . \n\t', '\n\t\t 20 : San Juan , on the north coast , had heavy rains and gusts Saturday , but they subsided during the night . \n\t', '\n\t\t 21 : On Saturday , Hurricane Florence was downgraded to a tropical storm , and its remnants pushed inland from the U.S. Gulf Coast . \n\t', '\n\t\t 22 : Residents returned home , happy to find little damage from 90 mph winds and sheets of rain . \n\t', '\n\t\t 23 : Florence , the sixth named storm of the 1988 Atlantic storm season , was the second hurricane . \n\t', '\n\t\t 24 : The first , Debby , reached minimal hurricane strength briefly before hitting the Mexican coast last month . \n\t', '\n\t\t 6 [ 0.15 ] [ 1.02 ] 21 [ 0.84 ] 20 [0.15]19 [ 1.58 ] 18 [ 0.70 ] 23 [ 0.80 ] 5 [ 1.20 ] 22 [ 0.70 ] 12 [ 0.93 ] [ 0.76 ] 7 [ 0.15 ] 8 [ 0.70 ] 0.35 9 [ 1.83 ] 0.15 0.2910 [ 0.99 ] 11 [ 0.56 ] 0.19 0.15 0.55 0.30 0.59 0.15 0.27 0.16 0.14 0.15 17 0.15 [0.50]24 4 [ 0.71 ] 15 14 13 [ 1.36 ] [ 1.09 ] 16 [ 1.65 ] Algorithm Graph Undirected Dir . \n\t', '\n\t\t forward Dir . \n\t', '\n\t\t backward HITSA 0.4912 0.4584 0.5023 HITSH 0.4912 0.5023 0.4584 POSP 0.4878 0.4538 0.3910 POSW 0.4878 0.3910 0.4538 PageRank 0.4904 0.4202 0.5008 Table 1 : Results for text summarization using Text- Rank sentence extraction . \n\t', '\n\t\t Graph-based ranking algorithms : HITS , Positional Function , PageRank . \n\t', '\n\t\t Graphs : undirected , directed forward , directed backward . \n\t', '\n\t\t Top 5 systems ( DUC , 2002 ) Baseline S27 S31 S28 S21 S29 0.5011 0.4914 0.4890 0.4869 0.4681 0.4799 Table 2 : Results for single document summarization for top 5 ( out of 15 ) DUC 2002 systems , and baseline . \n\t', '\n\t\t drawn from the text itself . \n\t', '\n\t\t Unlike other supervised systems , which attempt to learn what makes a good summary by training on collections of summaries built for other articles , TextRank is fully unsupervised , and relies only on the given text to derive an extractive summary . \n\t', '\n\t\t Among all algorithms , the HITSA and PageRank algorithms provide the best performance , at par with the best performing system from DUC 20025 . \n\t', '\n\t\t This proves that graph-based ranking algorithms , previously found successful in Web link analysis , can be turned into a state-of-the-art tool for sentence extraction when applied to graphs extracted from texts . \n\t', '\n\t\t Notice that TextRank goes beyond the sentence \x93connectivity\x94 in a text . \n\t', '\n\t\t For instance , sentence 15 in the example provided in Figure 1 would not be identified as \x93important\x94 based on the number of connections it has with other vertices in the graph6 , but it is identified as \x93important\x94 by TextRank ( and by humans \x96 according to the reference summaries for this text ) . \n\t', '\n\t\t Another important advantage of TextRank is that it gives a ranking over all sentences in a text \x96 which means that it can be easily adapted to extracting very short summaries , or longer more explicative summaries , consisting of more than 100 words . \n\t', '\n\t\t 5 Related Work Sentence extraction is considered to be an important first step for automatic text summarization . \n\t', '\n\t\t As a consequence , there is a large body of work on algorithms 5Notice that rows two and four in Table 1 are in fact redundant , since the ` hub\x94 ( ` weakness\x94 ) variations of the HITS ( Positional ) algorithms can be derived from their ` authority\x94 ( ` power\x94 ) counterparts by reversing the edge orientation in the graphs . \n\t', '\n\t\t 6Only seven edges are incident with vertex 15 , less than e.g. eleven edges incident with vertex 14 \x96 not selected as ` important\x94 by TextRank . \n\t', '\n\t\t for sentence extraction undertaken as part of the DUC evaluation exercises . \n\t', '\n\t\t Previous approaches include supervised learning \n\t\t']",Positive
['\n\t\t It is also notable the study reported in \n\t\t'],Positive
"['\n\t\t 6 Conclusions Intuitively , TextRank works well because it does not only rely on the local context of a text unit ( vertex ) , but rather it takes into account information recursively drawn from the entire text ( graph ) . \n\t', '\n\t\t Through the graphs it builds on texts , TextRank identifies connections between various entities in a text , and implements the concept of recommendation . \n\t', '\n\t\t A text unit recommends other related text units , and the strength of the recommendation is recursively computed based on the importance of the units making the recommendation . \n\t', '\n\t\t In the process of identifying important sentences in a text , a sentence recommends another sentence that addresses similar concepts as being useful for the overall understanding of the text . \n\t', '\n\t\t Sentences that are highly recommended by other sentences are likely to be more informative for the given text , and will be therefore given a higher score . \n\t', '\n\t\t An important aspect of TextRank is that it does not require deep linguistic knowledge , nor domain or language specific annotated corpora , which makes it highly portable to other domains , genres , or languages . \n\t', '\n\t\t References S. Brin and L. Page . \n\t', '\n\t\t 1998. The anatomy of a large-scale hypertextual Web search engine . \n\t', '\n\t\t Computer Networks and ISDN Systems , 30(1\x967) . \n\t', '\n\t\t DUC . \n\t', '\n\t\t 2002. Document understanding conference 2002. http://www- nlpir.nist.gov/projects/duc/ . \n\t', '\n\t\t P.J. Herings , G. van der Laan , and D. Talman . \n\t', '\n\t\t 2001. Measuring the power of nodes in digraphs . \n\t', '\n\t\t Technical report , Tinbergen Institute . \n\t', '\n\t\t J.M. Kleinberg . \n\t', '\n\t\t 1999 . \n\t', '\n\t\t Authoritative sources in a hyperlinked environ- ment.Journal of the ACM , 46(5):604\x96632 . \n\t', '\n\t\t C.Y. Lin and E.H. Hovy . \n\t', '\n\t\t 2003 a . \n\t', '\n\t\t Automatic evaluation of summaries using n-gram co-occurrence statistics . \n\t', '\n\t\t In Proceedings of Human Language Technology Conference ( HLT-NAACL 2003 ) , Edmonton , Canada , May . \n\t', '\n\t\t C.Y. Lin and E.H. Hovy . \n\t', '\n\t\t 2003b . \n\t', '\n\t\t The potential and limitations of sentence extraction for summarization . \n\t', '\n\t\t In Proceedings of the HLT/NAACL Workshop on Automatic Summarization , Edmonton , Canada , May . \n\t', '\n\t\t R. Mihalcea and P. Tarau . \n\t', '\n\t\t 2004. TextRank \x96 bringing order into texts . \n\t', '\n\t\t R. Mihalcea , P. Tarau , and E. Figa . \n\t', '\n\t\t 2004. PageRank on semantic networks , with application to word sense disambiguation . \n\t', '\n\t\t In Proceedings of the 20st International Conference on Computational Linguistics ( COLING 2004 ) , Geneva , Switzerland , August . \n\t', '\n\t\t G. Salton , A. Singhal , M. Mitra , and C. Buckley . \n\t', '\n\t\t 1997. Automatic text structuring and summarization . \n\t', '\n\t\t Information Processing and Management , 2(32) . \n\t', '\n\t\t S. Teufel and M. Moens . \n\t', '\n\t\t 1997. Sentence extraction as a classification task . \n\t', '\n\t\t In ACL/EACL workshop on \x94Intelligent and scalable Text summarization \x94 , pages 58\x9665 , Madrid , Spain . \n\t', '\n\t\t Compiling Boostexter Rules into a Finite-state Transducer Srinivas Bangalore AT&T Labs\x96Research 180 Park Avenue Florham Park , NJ 07932 Abstract A number of NLP tasks have been effectively modeled as classification tasks using a variety of classification techniques . \n\t', '\n\t\t Most of these tasks have been pursued in isolation with the classifier assuming unambiguous input . \n\t', '\n\t\t In order for these techniques to be more broadly applicable , they need to be extended to apply on weighted packed representations of ambiguous input . \n\t', '\n\t\t One approach for achieving this is to represent the classification model as a weighted finite-state transducer ( WFST ) . \n\t', '\n\t\t In this paper , we present a compilation procedure to convert the rules resulting from an AdaBoost classifier into an WFST . \n\t', '\n\t\t We validate the compilation technique by applying the resulting WFST on a call-routing application . \n\t', '\n\t\t 1 Introduction Many problems in Natural Language Processing ( NLP ) can be modeled as classification tasks either at the word or at the sentence level . \n\t', '\n\t\t For example , part-of-speech tagging , named-entity identification supertagging1 , word sense disambiguation are tasks that have been modeled as classification problems at the word level . \n\t', '\n\t\t In addition , there are problems that classify the entire sentence or document into one of a set of categories . \n\t', '\n\t\t These problems are loosely characterized as semantic classification and have been used in many practical applications including call routing and text classification . \n\t', '\n\t\t Most of these problems have been addressed in isolation assuming unambiguous ( one-best ) input . \n\t', '\n\t\t Typically , however , in NLP applications these modules are chained together with each module introducing some amount of error . \n\t', '\n\t\t In order to alleviate the errors introduced by a module , it is typical for a module to provide multiple weighted solutions ( ideally as a packed representation ) that serve as input to the next module . \n\t', '\n\t\t For example , a speech recognizer provides a lattice of possible recognition outputs that is to be annotated with part-of-speech and 1 associating each word with a label that represents the syntactic information of the word given the context of the sentence . \n\t', '\n\t\t named-entities . \n\t', '\n\t\t Thus classification approaches need to be extended to be applicable on weighted packed representations of ambiguous input represented as a weighted lattice . \n\t', '\n\t\t The research direction we adopt here is to compile the model of a classifier into a weighted finite-state transducer ( WFST ) so that it can compose with the input lattice . \n\t', '\n\t\t Finite state models have been extensively applied to many aspects of language processing including , speech recognition \n\t\t']",Positive
"['\n\t\t Finite- state models are attractive mechanisms for language processing since they ( a ) provide an efficient data structure for representing weighted ambiguous hypotheses ( b ) generally effective for decoding ( c ) associated with a calculus for composing models which allows for straightforward integration of constraints from various levels of speech and language processing.2 In this paper , we describe the compilation process for a particular classifier model into an WFST and validate the accuracy of the compilation process on a one-best input in a call-routing task . \n\t', '\n\t\t We view this as a first step toward using a classification model on a lattice input . \n\t', '\n\t\t The outline of the paper is as follows . \n\t', '\n\t\t In Section 2 , we review the classification approach to resolving ambiguity in NLP tasks and in Section 3 we discuss the boosting approach to classification . \n\t', '\n\t\t In Section 4 we describe the compilation of the boosting model into an WFST and validate the result of this compilation using a call- routing task . \n\t', '\n\t\t 2 Resolving Ambiguity by Classification In general , we can characterize all these tagging problems as search problems formulated as shown 2Furthermore , software implementing the finite-state calculus is available for research purposes . \n\t', '\n\t\t in Equation ( 1 ) . \n\t', '\n\t\t We notate to be the input vocabulary , to be the vocabulary of tags , an word input sequence as ( ) and tag sequence as ( ) . \n\t', '\n\t\t We are interested in , the most likely tag sequence out of the possible tag sequences ( ) that can be associated to . \n\t', '\n\t\t ( 1 ) Following the techniques of Hidden Markov Models ( HMM ) applied to speech recognition , these tagging problems have been previously modeled indirectly through the transformation of the Bayes rule as in Equation 2 . \n\t', '\n\t\t The problem is then approximated for sequence classification by a k -order Markov model as shown in Equation ( 3 ) . \n\t', '\n\t\t ( 3 ) Although the HMM approach to tagging can easily be represented as a WFST , it has a drawback in that the use of large contexts and richer features results in sparseness leading to unreliable estimation of the parameters of the model . \n\t', '\n\t\t An alternate approach to arriving at is to model Equation 1 directly . \n\t', '\n\t\t There are many examples in recent literature \n\t\t']",Positive
"['\n\t\t The general framework for these approaches is to learn a model from pairs of associations of the form ( ) where is a feature representation of and ( ) is one of the members of the tag set . \n\t', '\n\t\t Although these approaches have been more effective than HMMs , there have not been many attempts to represent these models as a WFST , with the exception of the work on compiling decision trees \n\t\t']",Positive
"['\n\t\t In this paper , we consider the boosting \n\t\t']",Positive
['\n\t\t 3 Boostexter Boostexter is a machine learning tool which is based on the boosting family of algorithms first proposed in \n\t\t'],Positive
"['\n\t\t The basic idea of boosting is to build a highly accurate classifier by combining many \x93weak\x94 or \x93simple\x94 base learner , each one of which may only be moderately accurate . \n\t', '\n\t\t A weak learner or a rule is a triple , which tests a predicate ( ^ ) of the input ( ) and assigns a weight ( ) for each member ( ) of if is true in and assigns a weight ( ) otherwise . \n\t', '\n\t\t It is assumed that a pool of such weak learners can be constructed easily . \n\t', '\n\t\t From the pool of weak learners , the selection the weak learner to be combined is performed iteratively . \n\t', '\n\t\t At each iteration , a weak learner is selected that minimizes a prediction error loss function on the training corpus which takes into account the weight assigned to each training example . \n\t', '\n\t\t Intuitively , the weights encode how important it is that correctly classifies each training example . \n\t', '\n\t\t Generally , the examples that were most often misclassified by the preceding base classifiers will be given the most weight so as to force the base learner to focus on the \x93hardest\x94 examples . \n\t', '\n\t\t As described in \n\t\t']",Positive
"['\n\t\t The iterative algorithm for combining weak learners stops after a pre- specified number of iterations or when the training set accuracy saturates . \n\t', '\n\t\t 3.1 Weak Learners In the case of text classification applications , the set of possible weak learners is instantiated from simple -grams of the input text ( ) . \n\t', '\n\t\t Thus , if is a function to produce all -grams up to of its argument , then the set of predicates for the weak learners is . \n\t', '\n\t\t For word-level classification problems , which take into account the left and right context , we extend the set of weak learners created from the word features with those created from the left and right context features . \n\t', '\n\t\t Thus features of the left context ( ) , features of the right context ( ) and the features of the word itself ( ) constitute the features at position . \n\t', '\n\t\t The predicates for the pool of weak learners are created from these set of features and are typically -grams on the feature representations . \n\t', '\n\t\t Thus the set of predicates resulting from the word level features is , from left context features is and from right context features is . \n\t', '\n\t\t The set of predicates for the weak learners for word level classification problems is :. 3.2 Decoding The result of training is a set of selected rules ( ) . \n\t', '\n\t\t The output of the final classifier is , i.e. the sum of confidence of all classifiers . \n\t', '\n\t\t The real-valued predictions of the final classifier can be converted ( 2 ) into probabilities by a logistic function transform ; that is ( 4 ) Thus the most likely tag sequence is deter- mined as in Equation 5 , where is computed using Equation 4. ( 5 ) To date , decoding using the boosted rule sets is restricted to cases where the test input is unambiguous such as strings or words ( not word graphs ) . \n\t', '\n\t\t By compiling these rule sets into WFSTs , we intend to extend their applicability to packed representations of ambiguous input such as word graphs . \n\t', '\n\t\t 4 Compilation We note that the weak learners selected at the end of the training process can be partitioned into one of three types based on the features that the learners test . \n\t', '\n\t\t We use the representation of context-dependent rewrite rules \n\t\t']",Positive
"['\n\t\t The ( weighted ) context-dependent rewrite rules have the general form ( 6 ) where , , and are regular expressions on the alphabet of the rules . \n\t', '\n\t\t The interpretation of these rules are as follows : Rewrite by when it is preceded by and followed by . \n\t', '\n\t\t Furthermore , can be extended to a rational power series which are weighted regular expressions where the weights encode preferences over the paths in \n\t\t']",Positive
"['\n\t\t Each weak learner can then be viewed as a set of weighted rewrite rules mapping the input word into each member ( ) with a weight when the predicate of the weak learner is true and with weight when the predicate of the weak learner is false . \n\t', '\n\t\t The translation between the three types of weak learners and the weighted context-dependency rules is shown in Table 13 . \n\t', '\n\t\t We note that these rules apply left to right on an input and do not repeatedly apply at the same point in an input since the output vocabulary would typically be disjoint from the input vocabulary . \n\t', '\n\t\t We use the technique described in \n\t\t']",Positive
"['\n\t\t The compilation is accomplished by the introduction of context symbols which are used as markers to identify locations for rewrites of with . \n\t', '\n\t\t After the rewrites , the markers are deleted . \n\t', '\n\t\t The compilation process is represented as a composition of five transducers . \n\t', '\n\t\t The WFSTs resulting from the compilation of each selected weak learner ( ) are unioned to create the WFST to be used for decoding . \n\t', '\n\t\t The weights of paths with the same input and output labels are added during the union operation . \n\t', '\n\t\t ( 7 ) We note that the due to the difference in the nature of the learning algorithm , compiling decision trees results in a composition of WFSTs representing the rules on the path from the root to a leaf node \n\t\t']",Positive
"['\n\t\t In order to apply the WFST for decoding , we simply compose the model with the input represented as an WFST ( ) and search for the best path ( if we are interested in the single best classification result ) . \n\t', '\n\t\t ( 8 ) We have compiled the rules resulting from boostexter trained on transcriptions of speech utterances from a call routing task with a vocabulary ( ) of 2912 and 40 classes ( ) . \n\t', '\n\t\t There were a total of 1800 rules comprising of 900 positive rules and their negative counterparts . \n\t', '\n\t\t The WFST resulting from compiling these rules has a 14372 states and 5.7 million arcs . \n\t', '\n\t\t The accuracy of the WFST on a random set of 7013 sentences was the same ( 85 % accuracy ) as the accuracy with the decoder that accompanies the boostexter program . \n\t', '\n\t\t This validates the compilation procedure . \n\t', '\n\t\t 5 Conclusions Classification techniques have been used to effectively resolve ambiguity in many natural language 3For ease of exposition , we show the positive and negative sides of a rule each resulting in a context dependency rule . \n\t', '\n\t\t However , we can represent them in the form of a single context dependency rule which is ommitted here due to space constraints . \n\t', '\n\t\t :test features of the word :test features of the left context :test features of the right context Type of Weak Learner Weak Learner Weighted Context Dependency Rule :if WORD== then else :if LeftContext== then else :if RightContext== then else Table 1 : Translation of the three types of weak learners into weighted context-dependency rules . \n\t', '\n\t\t processing tasks . \n\t', '\n\t\t However , most of these tasks have been solved in isolation and hence assume an unambiguous input . \n\t', '\n\t\t In this paper , we extend the utility of the classification based techniques so as to be applicable on packed representations such as word graphs . \n\t', '\n\t\t We do this by compiling the rules resulting from an AdaBoost classifier into a finite-state transducer . \n\t', '\n\t\t The resulting finite-state transducer can then be used as one part of a finite-state decoding chain . \n\t', '\n\t\t References S. Abney . \n\t', '\n\t\t 1991. Parsing by chunks . \n\t', '\n\t\t In Robert Berwick , Steven Abney , and Carol Tenny , editors , Principle-based parsing . \n\t', '\n\t\t Kluwer Academic Publishers . \n\t', '\n\t\t S. Bangalore and A. K. Joshi . \n\t', '\n\t\t 1999. Supertagging : An approach to almost parsing . \n\t', '\n\t\t Computational Linguistics , 25(2) . \n\t', '\n\t\t S. Bangalore and G. Riccardi . \n\t', '\n\t\t 2000. Stochastic finite-state models for spoken language machine translation . \n\t', '\n\t\t In Proceedings of the Workshop on Embedded Machine Translation Systems . \n\t', '\n\t\t L. Breiman , J.H. Friedman , R.A. Olshen , and C.J. Stone . \n\t', '\n\t\t 1984. Classification and Regression Trees . \n\t', '\n\t\t Wadsworth & Brooks , Pacific Grove , CA . \n\t', '\n\t\t Y. Freund and R. E. Schapire . \n\t', '\n\t\t 1996. Experiments with a new boosting alogrithm . \n\t', '\n\t\t In Machine Learning : Proceedings of the Thirteenth International Conference , pages 148\x96156 . \n\t', '\n\t\t C.D. Johnson . \n\t', '\n\t\t 1972. Formal Aspects of Phonological Description . \n\t', '\n\t\t Mouton , The Hague . \n\t', '\n\t\t R. M. Kaplan and M. Kay . \n\t', '\n\t\t 1994. Regular models of phonological rule systems . \n\t', '\n\t\t Computational Linguistics , 20(3):331\x96378 . \n\t', '\n\t\t K. K. Koskenniemi . \n\t', '\n\t\t 1984 . \n\t', '\n\t\t Two-level morphology : a general computation model for word -form recognition and production . \n\t', '\n\t\t Ph.D . \n\t', '\n\t\t thesis , University of Helsinki . \n\t', '\n\t\t J. Lafferty , A. McCallum , and F. Pereira . \n\t', '\n\t\t 2001. Conditional random fields : Probabilistic models for segmenting and labeling sequence data . \n\t', '\n\t\t In In Proceedings of ICML , San Francisco , CA . \n\t', '\n\t\t A. McCallum , D. Freitag , and F. Pereira . \n\t', '\n\t\t 2000. Maximum entropy markov models for information extraction and segmentation . \n\t', '\n\t\t In In Proceedings of ICML , Stanford , CA . \n\t', '\n\t\t M. Mohri and R. Sproat . \n\t', '\n\t\t 1996. An efficient compiler for weighted rewrite rules . \n\t', '\n\t\t In Proceedings ofACL , pages 231\x96238 . \n\t', '\n\t\t K. Oflazer . \n\t', '\n\t\t 1999. Dependency parsing with an extended finite state approach . \n\t', '\n\t\t In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics , Maryland , USA , June . \n\t', '\n\t\t F.C.N. Pereira and M.D. Riley . \n\t', '\n\t\t 1997. Speech recognition by composition of weighted finite automata . \n\t', '\n\t\t In E. Roche and Schabes Y. , editors , Finite State Devices for Natural Language Processing , pages 431\x96456 . \n\t', '\n\t\t MIT Press , Cambridge , Massachusetts . \n\t', '\n\t\t E. Roche . \n\t', '\n\t\t 1999. Finite state transducers : parsing free and frozen sentences . \n\t', '\n\t\t In Andr´as Kornai , editor , Extended Finite State Models of Language . \n\t', '\n\t\t Cambridge University Press . \n\t', '\n\t\t D. Roth . \n\t', '\n\t\t 1998. Learning to resolve natural language ambiguities : A unified approach . \n\t', '\n\t\t In Proceedings ofAAAI . \n\t', '\n\t\t R.E. Schapire and Y . \n\t', '\n\t\t Singer . \n\t', '\n\t\t 1999. Improved boosting algorithms using confidence-rated predictions . \n\t', '\n\t\t Machine Learning , 37(3):297\x96336 , December . \n\t', '\n\t\t R. Sproat and M. Riley . \n\t', '\n\t\t 1996. Compilation of weighted finite-state transducers from decision trees . \n\t', '\n\t\t In Proceedings ofACL , pages 215\x96222 . \n\t', '\n\t\t J. Vilar , V.M. Jim´enez , J. Amengual , A. Castellanos , D. Llorens , and E. Vidal . \n\t', '\n\t\t 1999. Text and speech translation by means of subsequential transducers . \n\t', '\n\t\t In Andr´as Kornai , editor , Extened Finite State Models of Language . \n\t', '\n\t\t Cambridge University Press . \n\t', '\n\t\t Combining Lexical , Syntactic , and Semantic Features with Maximum Entropy Models for Extracting Relations Nanda Kambhatla IBM T. J. Watson Research Center 1101 Kitchawan Road Route 134 Yorktown Heights , NY 10598 nanda@us.ibm.com Abstract Extracting semantic relationships between entities is challenging because of a paucity of annotated data and the errors induced by entity detection modules . \n\t', '\n\t\t We employ Maximum Entropy models to combine diverse lexical , syntactic and semantic features derived from the text . \n\t', '\n\t\t Our system obtained competitive results in the Automatic Content Extraction ( ACE ) evaluation . \n\t', '\n\t\t Here we present our general approach and describe our ACE results . \n\t', '\n\t\t 1 Introduction Extraction of semantic relationships between entities can be very useful for applications such as biography extraction and question answering , e.g. to answer queries such as \x93Where is the Taj Mahal?\x94 . \n\t', '\n\t\t Several prior approaches to relation extraction have focused on using syntactic parse trees . \n\t', '\n\t\t For the Template Relations task of MUC-7 , BBN researchers \n\t\t']",Positive
"['\n\t\t More recently , \n\t\t']",Positive
"['\n\t\t We build Maximum Entropy models for extracting relations that combine diverse lexical , syntactic and semantic features . \n\t', '\n\t\t Our results indicate that using a variety of information sources can result in improved recall and overall F measure . \n\t', '\n\t\t Our approach can easily scale to include more features from a multitude of sources\x96e.g . \n\t', '\n\t\t WordNet , gazatteers , output of other semantic taggers etc.\x96that can be brought to bear on this task . \n\t', '\n\t\t In this paper , we present our general approach , describe the features we currently use and show the results of our participation in the ACE evaluation . \n\t', '\n\t\t Automatic Content Extraction ( ACE , 2004 ) is an evaluation conducted by NIST to measure Entity Detection and Tracking ( EDT ) and relation detection and characterization ( RDC ) . \n\t', '\n\t\t The EDT task entails the detection of mentions of entities and chaining them together by identifying their coreference . \n\t', '\n\t\t In ACE vocabulary , entities are objects , mentions are references to them , and relations are explicitly or implicitly stated relationships among entities . \n\t', '\n\t\t Entities can be of five types : persons , organizations , locations , facilities , and geo-political entities ( geographically defined regions that define a political boundary , e.g. countries , cities , etc. ) . \n\t', '\n\t\t Mentions have levels : they can be names , nominal expressions or pronouns . \n\t', ""\n\t\t The RDC task detects implicit and explicit rela- tions ' between entities identified by the EDT task . \n\t"", '\n\t\t Here is an example : The American Medical Association voted yesterday to install the heir apparent as its president-elect , rejecting a strong , upstart challenge by a District doctor who argued that the nation\x92s largest physicians\x92 group needs stronger ethics and new leadership . \n\t', '\n\t\t In electing Thomas R. Reardon , an Oregon general practitioner who had been the chairman of its board , ... \n\t', '\n\t\t In this fragment , all the underlined phrases are mentions referring to the American Medical Association , or to Thomas R. Reardon or the board ( an organization ) of the American Medical Association . \n\t', '\n\t\t Moreover , there is an explicit management relation between chairman and board , which are references to Thomas R. Reardon and the board of the American Medical Association respectively . \n\t', ""\n\t\t Relation extraction is hard , since successful extraction implies correctly detecting both the argument mentions , correctly chaining these mentions to their re- ' Explict relations occur in text with explicit evidence sug- gesting the relationship . \n\t"", '\n\t\t Implicit relations need not have explicit supporting evidence in text , though they should be evident from a reading of the document . \n\t', '\n\t\t Type Subtype Count AT based-In 496 located 2879 residence 395 NEAR relative-location 288 PART other 6 part-Of 1178 subsidiary 366 ROLE affiliate -partner 219 citizen-Of 450 client 159 founder 37 general-staff 1507 management 1559 member 1404 other 174 owner 274 SOCIAL associate 119 grandparent 10 other -personal 108 other -professional 415 other-relative 86 parent 149 sibling 23 spouse 89 Table 1 : The list of relation types and subtypes used in the ACE 2003 evaluation . \n\t', '\n\t\t spective entities , and correctly determining the type of relation that holds between them . \n\t', '\n\t\t This paper focuses on the relation extraction component of our ACE system . \n\t', '\n\t\t The reader is referred to \n\t\t']",Positive
"['\n\t\t In the next section , we describe our extraction system . \n\t', '\n\t\t We present results in section 3 , and we conclude after making some general observations in section 4 . \n\t', '\n\t\t 2 Maximum Entropy models for extracting relations We built Maximum Entropy models for predicting the type of relation ( if any ) between every pair of mentions within each sentence . \n\t', '\n\t\t We only model explicit relations , because of poor inter-annotator agreement in the annotation of implicit relations . \n\t', '\n\t\t Table 1 lists the types and subtypes of relations for the ACE RDC task , along with their frequency of occurence in the ACE training data2 . \n\t', '\n\t\t Note that only 6 of these 24 relation types are symmetric : \x93relative-location\x94 , \x93associate\x94 , \x93other-relative\x94 , \x93other-professional\x94 , \x93sibling\x94 , and \x93spouse\x94 . \n\t', '\n\t\t We only model the relation subtypes , after making them unique by concatenating the type where appropriate ( e.g. \x93OTHER\x94 became \x93OTHER-PART\x94 and \x93OTHER-ROLE\x94 ) . \n\t', '\n\t\t We explicitly model the argument order of mentions . \n\t', '\n\t\t Thus , when comparing mentions and , we distinguish between the case where -citizen-Of- and -citizen-Of- . \n\t', '\n\t\t We thus model the extraction as a classification problem with 49 classes , two for each relation subtype and a \x93NONE\x94 class for the case where the two mentions are not related . \n\t', '\n\t\t For each pair of mentions , we compute several feature streams shown below . \n\t', '\n\t\t All the syntactic features are derived from the syntactic parse tree and the dependency tree that we compute using a statistical parser trained on the PennTree Bank using the Maximum Entropy framework \n\t\t']",Positive
"['\n\t\t The feature streams are : Words The words of both the mentions and all the words in between . \n\t', '\n\t\t Entity Type The entity type ( one of PERSON , ORGANIZATION , LOCATION , FACILITY , Geo-Political Entity or GPE ) of both the mentions . \n\t', '\n\t\t Mention Level The mention level ( one of NAME , NOMINAL , PRONOUN ) of both the mentions . \n\t', '\n\t\t Overlap The number of words ( if any ) separating the two mentions , the number of other mentions in between , flags indicating whether the two mentions are in the same noun phrase , verb phrase or prepositional phrase . \n\t', '\n\t\t Dependency The words and part-of-speech and chunk labels of the words on which the mentions are dependent in the dependency tree derived from the syntactic parse tree . \n\t', '\n\t\t Parse Tree The path of non-terminals ( removing duplicates ) connecting the two mentions in the parse tree , and the path annotated with head words . \n\t', '\n\t\t Here is an example . \n\t', '\n\t\t For the sentence fragment , been the chairman of its board ... the corresponding syntactic parse tree is shown in Figure 1 and the dependency tree is shown in Figure 2 . \n\t', '\n\t\t For the pair of mentions chairman and board , the feature streams are shown below . \n\t', '\n\t\t Words 2The reader is referred to \n\t\t']",Positive
"['\n\t\t Figure 1 : The syntactic parse tree for the fragment \x93chairman of its board\x94 . \n\t', '\n\t\t Figure 2 : The dependency tree for the fragment \x93chairman of its board\x94 . \n\t', '\n\t\t Entity Type ( for \x93chairman\x94 ) , ( for \x93board\x94 ) . \n\t', '\n\t\t Mention Level . \n\t', '\n\t\t Overlap one-mention-in-between ( the word \x93its\x94 ) , two-words-apart , in-same-noun-phrase . \n\t', '\n\t\t Dependency ( word on which is depedent ) , ( POS of word on which is dependent ) , ( chunk label of word on which is de- pendent ) , Parse Tree PERSON-NP-PP-ORGANIZATION , PERSON-NP-PP : of-ORGANIZATION ( both derived from the path shown in bold in Figure 1 ) . \n\t', '\n\t\t We trained Maximum Entropy models using features derived from the feature streams described above . \n\t', '\n\t\t 3 Experimental results We divided the ACE training data provided by LDC into separate training and development sets . \n\t', '\n\t\t The training set contained around 300K words , and 9752 instances of relations and the development set contained around 46K words , and 1679 instances of relations . \n\t', '\n\t\t Features P R F Value Words 81.9 17.4 28.6 8.0 + Entity Type 71.1 27.5 39.6 19.3 + Mention Level 71.6 28.6 40.9 20.2 + Overlap 61.4 38.8 47.6 34.7 + Dependency 63.4 44.3 52.1 40.2 + Parse Tree 63.5 45.2 52.8 40.9 Table 2 : The Precision , Recall , F-measure and the ACE Value on the development set with true mentions and entities . \n\t', '\n\t\t We report results in two ways . \n\t', '\n\t\t To isolate the perfomance of relation extraction , we measure the performance of relation extraction models on \x93true\x94 mentions with \x93true\x94 chaining ( i.e. as annotated by LDC annotators ) . \n\t', '\n\t\t We also measured performance of models run on the deficient output of mention detection and mention chaining modules . \n\t', ""\n\t\t We report both the F-measure ' and the ACE value of relation extraction . \n\t"", '\n\t\t The ACE value is a NIST metric that assigns 0 % value for a system which produces no output and 100 % value for a system that extracts all the relations and produces no false alarms . \n\t', '\n\t\t We count the misses ; the true relations not extracted by the system , and the false alarms ; the spurious relations extracted by the system , and obtain the ACE value by subtracting from 1.0 , the normalized weighted cost of the misses and false alarms . \n\t', '\n\t\t The ACE value counts each relation only once , even if it was expressed many times in a document in different ways . \n\t', '\n\t\t The reader is referred to the ACE web site ( ACE , 2004 ) for more details . \n\t', '\n\t\t We built several models to compare the relative utility of the feature streams described in the previous section . \n\t', '\n\t\t Table 2 shows the results we obtained when running on \x93truth\x94 for the development set and Table 3 shows the results we obtained when running on the output of mention detection and mention chaining modules . \n\t', '\n\t\t Note that a model trained with only words as features obtains a very high precision and a very low recall . \n\t', '\n\t\t For example , for the mention pair his and wife with no words in between , the lexical features together with the fact that there are no words in between is sufficient ( though not necessary ) to extract the relationship between the two entities . \n\t', '\n\t\t The addition of entity types , mention levels and especially , the word proximity features ( \x93overlap\x94 ) boosts the recall at the expense of the very 3 The F-measure is the harmonic mean of the precision , defined as the percentage of extracted relations that are valid , and the recall , defined as the percentage of valid relations that are extracted . \n\t', '\n\t\t NP NN DT NN IN PRP PP NP NP been the chairman of its board ... ... VBN DT NN IN PRP NN been the chairman of its board ... ... , , , , m1-m2-dependent-in-second-level(number of links traversed in dependency tree to go from one mention to another in Figure 2 ) . \n\t', '\n\t\t Features P R F Value Words 58.4 11.1 18.6 5.9 + Entity Type 43.6 14.0 21.1 12.5 + Mention Level 43.6 14.5 21.7 13.4 + Overlap 35.6 17.6 23.5 21.0 + Dependency 35.0 19.1 24.7 24.6 + Parse Tree 35.5 19.8 25.4 25.2 Table 3 : The Precision , Recall , F-measure , and ACE Value on the development set with system output mentions and entities . \n\t', '\n\t\t Eval Value F Value F Set ( T ) ( T ) ( S ) ( S ) Feb\x9202 31.3 52.4 17.3 24.9 Sept\x9203 39.4 55.2 18.3 23.6 Table 4 : The F-measure and ACE Value for the test sets with true ( T ) and system output ( S ) mentions and entities . \n\t', '\n\t\t high precision . \n\t', '\n\t\t Adding the parse tree and dependency tree based features gives us our best result by exploiting the consistent syntactic patterns exhibited between mentions for some relations . \n\t', '\n\t\t Note that the trends of contributions from different feature streams is consistent for the \x93truth\x94 and system output runs . \n\t', '\n\t\t As expected , the numbers are significantly lower for the system output runs due to errors made by the mention detection and mention chaining modules . \n\t', '\n\t\t We ran the best model on the official ACE Feb\x922002 and ACE Sept\x922003 evaluation sets . \n\t', '\n\t\t We obtained competitive results shown in Table 4 . \n\t', '\n\t\t The rules of the ACE evaluation prohibit us from disclosing our final ranking and the results of other participants . \n\t', '\n\t\t 4 Discussion We have presented a statistical approach for extracting relations where we combine diverse lexical , syntactic , and semantic features . \n\t', '\n\t\t We obtained competitive results on the ACE RDC task . \n\t', '\n\t\t Several previous relation extraction systems have focused almost exclusively on syntactic parse trees . \n\t', '\n\t\t We believe our approach of combining many kinds of evidence can potentially scale better to problems ( like ACE ) , where we have a lot of relation types with relatively small amounts of annotated data . \n\t', '\n\t\t Our system certainly benefits from features derived from parse trees , but it is not inextricably linked to them . \n\t', '\n\t\t Even using very simple lexical features , we obtained high precision extractors that can poten tially be used to annotate large amounts of unlabeled data for semi-supervised or unsupervised learning , without having to parse the entire data . \n\t', '\n\t\t We obtained our best results when we combined a variety of features . \n\t', '\n\t\t Acknowledgements We thank Salim Roukos for several invaluable suggestions and the entire ACE team at IBM for help with various components , feature suggestions and guidance . \n\t', '\n\t\t References ACE . \n\t', '\n\t\t 2004. The nist ace evaluation website . \n\t', '\n\t\t http://www.nist.gov/speech/tests/ace/ . \n\t', '\n\t\t Aron Culotta and Jeffrey Sorensen . \n\t', '\n\t\t 2004. Dependency tree kernels for relation extraction . \n\t', '\n\t\t In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics , Barcelona , Spain , July 21\x96July 26 . \n\t', '\n\t\t Radu Florian , Hany Hassan , Hongyan Jing , Nanda Kambhatla , Xiaqiang Luo , Nicolas Nicolov , and Salim Roukos . \n\t', '\n\t\t 2004. A statistical model for multilingual entity detection and tracking . \n\t', '\n\t\t In Proceedings of the Human Language Technologies Conference ( HLTNAACL\x9204 ) , Boston , Mass. , May 27 \x96 June 1 . \n\t', '\n\t\t Abraham Ittycheriah , Lucian Lita , Nanda Kambhatla , Nicolas Nicolov , Salim Roukos , and Margo Stys . \n\t', '\n\t\t 2003. Identifying and tracking entity mentions in a maximum entropy framework . \n\t', '\n\t\t In Proceedings of the Human Language Technologies Conference ( HLTNAACL\x9203 ) , pages 40\x9642 , Edmonton , Canada , May 27 \x96 June 1 . \n\t', '\n\t\t Xiaoqiang Luo , Abraham Ittycheriah , Hongyan Jing , Nanda Kambhatla , and Salim Roukos . \n\t', '\n\t\t 2004. A mention-synchronous coreference resolution algorithm based on the bell tree . \n\t', '\n\t\t In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics , Barcelona , Spain , July 21\x96July 26 . \n\t', '\n\t\t Scott Miller , Heidi Fox , Lance Ramshaw , and Ralph Weischedel . \n\t', '\n\t\t 2000. A novel use of statistical parsing to extract information from text . \n\t', '\n\t\t In 1 st Meeting of the North American Chapter of the Association for Computational Linguistics , pages 226\x96233 , Seattle , Washington , April 29\x96May 4 . \n\t', '\n\t\t Adwait Ratnaparkhi . \n\t', '\n\t\t 1999. Learning to parse natural language with maximum entropy . \n\t', '\n\t\t Machine Learning ( Special Issue on Natural Language Learning ) , 34(1- 3):151\x96176 . \n\t', '\n\t\t Stephanie Strassel , Alexis Mitchell , and Shudong Huang . \n\t', '\n\t\t 2003 . \n\t', '\n\t\t Multilingual resources for entity detection . \n\t', '\n\t\t In Proceedings of the ACL 2003 Workshop on Multilingual Resources for Entity Detection . \n\t', '\n\t\t Dmitry Zelenko , Chinatsu Aone , and Anthony Richardella. 2003 . \n\t', '\n\t\t Kernel methods for relation extraction . \n\t', '\n\t\t Journal of Machine Learning Research , 3:1083\x961106 . \n\t', '\n\t\t On the Equivalence of Weighted Finite-state Transducers Julien Quint National Institute of Informatics Hitotsubashi 2-1-2 Chiyoda-ku Tokyo 101-8430 Japan quint@nii.ac.jp Abstract Although they can be topologically different , two distinct transducers may actually recognize the same rational relation . \n\t', '\n\t\t Being able to test the equivalence of transducers allows to implement such operations as incremental minimization and iterative composition . \n\t', '\n\t\t This paper presents an algorithm for testing the equivalence of deterministic weighted finite-state transducers , and outlines an implementation of its applications in a prototype weighted finite-state calculus tool . \n\t', '\n\t\t Introduction The addition of weights in finite-state devices ( where transitions , initial states and final states are weighted ) introduced the need to reevaluate many of the techniques and algorithms used in classical finite-state calculus . \n\t', '\n\t\t Interesting consequences are , for instance , that not all non-deterministic weighted automata can be made deterministic \n\t\t']",Positive
"['\n\t\t A fundamental operation on finite-state transducers in equivalence testing , which leads to applications such as incremental minimization and iterative composition . \n\t', '\n\t\t Here , we present an algorithm for equivalence testing in the weighted case , and describe its application to these applications . \n\t', '\n\t\t We also describe a prototype implementation , which is demonstrated . \n\t', '\n\t\t 1 Definitions We define a weightedfinite-state automata ( WFST ) T over a set of weights K by an 8-tuple ( E , Q , Q , I , F , E , A , p ) where E and Q are two finite sets of symbols ( alphabets ) , Q is a finite set of states , I C_ Q is the set of initial states , F C_ Q is the set of final states,E C_ QxEU{^}xQU{^}xKxQ is the set of transitions , and A : I \x97* K and p : F \x97* K are the initial and final weight functions . \n\t', '\n\t\t A transition e E E has a label l(e) E EU{~} x QU { E } , a weight w(e) E K and a destination S(e) E Q. The set of weights is a semi-ring , that is a system ( K , ^ , ^ , ¯0,¯1 ) where 0¯ is the identity element for ^ , 1 is the identity element for ^ , and ^ is commutative \n\t\t']",Positive
"['\n\t\t The cost of a path in a WFST is the product ( ^ ) of the initial weight of the initial state , the weight of all the transitions , and the final weight of the final state . \n\t', '\n\t\t When several paths in the WFST match the same relation , the total cost is the sum ( ^ ) of the costs of all the paths . \n\t', '\n\t\t In NLP , the tropical semi-ring ( R+ U { oc } , min , + , oc , 0 ) is very often used : weights are added along a path , and if several paths match the same relation , the total cost is the cost of the path with minimal cost . \n\t', '\n\t\t The following discussion will apply to any semi-ring , with examples using the tropical semi-ring . \n\t', '\n\t\t 2 The Equivalence Testing Algorithm Several algorithms testing the equivalence of two states are presented in \n\t\t']",Positive
"['\n\t\t Two states are equivalent if and only if their respective right language are equivalent . \n\t', '\n\t\t The right language of a state is the set of words originating from this state . \n\t', '\n\t\t Two deterministic finite-state automata are equivalent if and only if they recognize the same language , that is , if their initial states have the same right language . \n\t', '\n\t\t Hence , it is possible to test the equivalence of two automata by applying the equivalence algorithm on their initial states . \n\t', '\n\t\t In order to test the equivalence of two WFSTs , we need to extend the state equivalence test algorithm in two ways : first , it must apply to transducers , and second , it must take weights into account . \n\t', '\n\t\t Handling transducers is easily achieved as the labels of transitions defined above are equivalent to symbols in an alphabet ( i.e. we consider the underlying automaton of the transducer ) . \n\t', '\n\t\t Taking weights into account means that for two WFSTs to be equivalent , they must recog- nize the same relation ( or their underlying automata must recognize the same language ) , with the same weights . \n\t', '\n\t\t However , as illustrated by figure 1 , two WFSTs can be equivalent but have a different weight distribution . \n\t', '\n\t\t States 1 and 5 have the same right language , but words have different costs ( for example , abad has a cost of 6 in the top automaton , and 5 in the bottom one ) . \n\t', '\n\t\t We notice however that the difference of weights between words is constant , so states 1 and 5 are really equivalent modulo a cost of 1 . \n\t', '\n\t\t Figure 1 : Two equivalent weighted finite-state transducers ( using the tropical semi-ring ) . \n\t', '\n\t\t Figure 2 shows the weighted equivalence algorithm . \n\t', '\n\t\t Given two states p and q , it returns a true value if they are equivalent , and a false value otherwise . \n\t', '\n\t\t Remainder weights are also passed as parameters wp and wQ . \n\t', '\n\t\t The last parameter is an associative array 5 that we use to keep track of states that were already visited . \n\t', '\n\t\t The algorithm works as follows : given two states , compare their signature . \n\t', '\n\t\t The signature of a state is a string encoding its class ( final or not ) and the list of labels on outgoing transition . \n\t', '\n\t\t In the case of deterministic transducers , if the signature for the two states do not match , then they cannot have the same right language and therefore cannot be equivalent . \n\t', '\n\t\t Otherwise , if the two states are final , then their weights ( taking into account the remainder weights ) must be the same ( lines 6\x967 ) . \n\t', '\n\t\t Then , all their outgoing transitions have to be checked : the states will be equivalent if matching transitions lead to equivalent states ( lines 8\x9612 ) . \n\t', '\n\t\t The destination states are recursively checked . \n\t', '\n\t\t The REMAINDER function computes the remainder weights for the destination states . \n\t', '\n\t\t Given two weights x and y , it returns { ¯1 , x ® y- 1 } if x < y , and { x-1 ® y,¯1 } otherwise . \n\t', '\n\t\t If there is a cycle , then we will see the same pair of states twice . \n\t', '\n\t\t The weight of the cycle must be the same in both transducers , so the remainder weights must be unchanged . \n\t', '\n\t\t This is tested in lines 2\x964 . \n\t', '\n\t\t The algorithm applies to deterministic WFSTs , which can have only one initial state . \n\t', '\n\t\t To test the equivalence of two WFSTs , we call EQUIV on the respective initial states of the the WFSTs with their initial weights as the remainder weights , and 5 is initially empty . \n\t', '\n\t\t 3 Incremental minimization An application of this equivalence algorithm is the incremental minimization algorithm of \n\t\t']",Positive
"['\n\t\t For every deterministic WFST T there exists at least one equivalent WFST M such that no other equivalent WFST has fewer states ( i.e. IQM I is minimal ) . \n\t', '\n\t\t In the unweighted case , this means that there cannot be two distinct states that are equivalent in the minimized transducer . \n\t', '\n\t\t It follows that a way to build this transducer M is to compare every pair of distinct states in QA and merge pairs of equivalent states until there are no two equivalent states in the transducer . \n\t', '\n\t\t An advantage of this method is that at any time of the application of the algorithm , the transducer is in a consistent state ; if the process has to finish under a certain time limit , it can simply be stopped ( the number of states will have decreased , even though the minimality of the result cannot be guaranteed then ) . \n\t', '\n\t\t In the weighted case , merging two equivalent states is not as easy because edges with the same label may have a different weight . \n\t', '\n\t\t In figure 3 , we see that states 1 and 2 are equivalent and can be merged , but outgoing transitions have different weights . \n\t', '\n\t\t The remainder weights have to be pushed to the following states , which can then be merged if they are equivalent modulo the remainder weights . \n\t', '\n\t\t This applies to states 3 and 4 here . \n\t', '\n\t\t Figure 3 : Non-minimal transducer and its minimized equivalent . \n\t', '\n\t\t 4 Generic Composition with Filter As shown previously \n\t\t']",Positive
"['\n\t\t A filter is introduced , whose role is to handle epsilon transitions on the lower side of the top transducer and the upper side of the lower transducer ( it is also useful in the unweighted case ) . \n\t', ""\n\t\t In our implementation described in section 5 we have generalized the use of this epsilon-free composition operation to handle two operations that are defined 4 c/2 5 a/2 6 b/1 0 c/1 1 a/1 b/2 2 d/2 d/0 3/0 7/0 a/2 b/0 3 c/1 5/0 a/1 1 0 b/1 2 a/1 b/0 4 c/2 6/0 b/0 a/1 0 1 b/1 a/2 2 c/1 3/0 EQUIV ( p , wp , q , wq , S ) 1 equiv +\x96 FALSE 2 if S[{p , q } ] =~ NIL 3 then { w'p , w'q } +\x96 S[{p , q } ] 4 equiv +\x96w'p=wpAw'q=wq 5 else if SIGNATURE(p) = SIGNATURE(q) 6 then if FINAL(p) 7 then equiv +\x96 wp ® ^(p) = wq ® ^(q) 8 S[{p , q } ] +\x96 { wp , wq } 9 for ep E E(p) , eq E E(q) , l(ep) = l(eq) 10 do { w'p , w'q } +\x96 REMAINDER(wp ® w(ep) , wq ® w(eq)) 11 equiv +\x96 equiv AEQUIV(^(ep) , w'p , ^(eq) , w'q , S ) 12 DELETE(S[{p , q } ] ) 13 return equiv Figure 2 : The equivalence algorithm on automata only , that is intersection and cross- product . \n\t"", '\n\t\t Intersection is a simple variant of the composition of the identity transducers corresponding to the operand automata . \n\t', '\n\t\t Cross-product uses the exact same algorithm but a different filter , shown in figure 4 . \n\t', '\n\t\t The preprocessing stage for both operand automata consists of adding a transition with a special symbol x at every final state , going to itself , and with a weight of ¯1 . \n\t', '\n\t\t This will allow to match words of different lengths , as when one of the automata is \x93exhausted,\x94 the x symbol will be added as long as the other automaton is not . \n\t', '\n\t\t After the composition , the x symbol is replaced everywhere by E. Figure 4 : Cross-product filter . \n\t', '\n\t\t The symbol \x93?\x94 matches any symbol ; \x93x\x94 is a special espilonsymbol introduced in the final states of the operand automata at preprocessing . \n\t', '\n\t\t The equivalence algorithm that is the subject of this paper is used in conjunction with composition of WFSTs in order to provide an iterative composition operator . \n\t', '\n\t\t Given two transducers A and B , it composes A with B , then composes the result with B again , and again , until a fixed-point is reached . \n\t', '\n\t\t This can be determined by testing the equivalence of the last two iterations . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t 5 A Prototype Implementation The algorithms described above have all been implemented in a prototype weighted finite-state tool , called w f s t , inspired from the Xerox tool x f s t \n\t\t']",Positive
"['\n\t\t From the former , it borrows a similar command-line interface and regular expression syntax , and from the latter , the addition of weights . \n\t', '\n\t\t The system will be demonstrated and should be available for download soon . \n\t', '\n\t\t The operations described above are all available in w f s t , in addition to classical operations like union , intersection ( only defined on automata ) , concatenation , etc. . \n\t', '\n\t\t The regular expression syntax is inspired from xfst and Perl ( the implementation language ) . \n\t', '\n\t\t For instance , the automaton of figure 3 was compiled from the regular expression ( a/1 a/2 b / 0 * c/1 ) | ( b/2 a/1 b / 0 * c/2 ) and the iterative composition of two previously defined WFSTs A and B is written $ A %+ $ B ( we chose % as the composition operator , and + refers to the Kleene plus operator ) . \n\t', '\n\t\t Conclusion We demonstrate a simple and powerful experimental weighted finite state calculus tool and have described an algorithm at the core of its operation for 7:x/0 7:7/0 1/0 7:x/0 0/0 x:7/0 x:7/0 2/0 the equivalence of weighted transducers . \n\t', '\n\t\t There are two major limitations to the weighted equivalence algorithm . \n\t', '\n\t\t The first one is that it works only on deterministic WFSTs ; however , not all WFSTs can be determinized . \n\t', '\n\t\t An algorithm with backtracking may be a solution to this problem , but its running time would increase , and it remains to be seen if such an algorithm could apply to undeterminizable transducers . \n\t', '\n\t\t The other limitation is that two transducers recognizing the same rational relation may have nonequivalent underlying automata , and some labels will not match ( e.g. { a , E}{b , c } vs. { a , c}{b , E } ) . \n\t', '\n\t\t A possible solution to this problem is to consider the shortest string on both sides and have \x93remainder strings\x94 like we have remainder weights in the weighted case . \n\t', '\n\t\t If successful , this technique could yield interesting results in determinization as well . \n\t', '\n\t\t References Kenneth R. Beesley and Lauri Karttunen . \n\t', '\n\t\t 2003. Finite State Morphology . \n\t', '\n\t\t CSLI Publications , Stanford , California . \n\t', '\n\t\t Jean Berstel and Christophe Reteunauer . \n\t', '\n\t\t 1988. Rational Series and their Languages . \n\t', '\n\t\t Springer Verlag , Berlin , Germany . \n\t', '\n\t\t Adam L. Buchsbaum , Raffaele Giancarlo , and Jeffery R. Westbrook . \n\t', '\n\t\t 2000. On the determinization of weighted finite automata . \n\t', '\n\t\t SIAM Journal on Computing , 30(5):1502\x961531 . \n\t', '\n\t\t Mehryar Mohri , Fernando C. N. Pereira , and Michael Riley . \n\t', '\n\t\t 1997. A rational design for a weighted finite-state transducer library . \n\t', '\n\t\t In Workshop on Implementing Automata , pages 144\x96158 , London , Ontario . \n\t', '\n\t\t Fernando C. N. Pereira and Michael Riley . \n\t', '\n\t\t 1997. Speech recognition by composition of weighted finite state automata . \n\t', '\n\t\t In Emmanuel Roche and Yves Schabes , editors , Finite-State Language Processing , pages 431\x96453 . \n\t', '\n\t\t MIT Press , Cambridge , Massachusetts . \n\t', '\n\t\t Emmanuel Roche and Yves Schabes . \n\t', '\n\t\t 1994. Two parsing algorithms by means of finite state transducers . \n\t', '\n\t\t In Proceedings of COLING\x9294 , pages 431\x96435 , Kyot o , Japan Bruce W. Watson and Jan Daciuk . \n\t', '\n\t\t 2003. An efficient incremental DFA minimization algorithm . \n\t', '\n\t\t Natural Language Engineering , 9(1):49\x9664 . \n\t', '\n\t\t A New Feature Selection Score for Multinomial Naive Bayes Text Classification Based on KL-Divergence Karl-Michael Schneider Department of General Linguistics University of Passau 94032 Passau , Germany schneide@phil.uni-passau.de Abstract We define a new feature selection score for text classification based on the KL-divergence between the distribution of words in training documents and their classes . \n\t', '\n\t\t The score favors words that have a similar distribution in documents of the same class but different distributions in documents of different classes . \n\t', '\n\t\t Experiments on two standard data sets indicate that the new method outperforms mutual information , especially for smaller categories . \n\t', '\n\t\t 1 Introduction Text classification is the assignment of predefined categories to text documents . \n\t', '\n\t\t Text classification has many applications in natural language processing tasks such as E-mail filtering , prediction of user preferences and organization of web content . \n\t', '\n\t\t The Naive Bayes classifier is a popular machine learning technique for text classification because it performs well in many domains , despite its simplicity \n\t\t']",Positive
"['\n\t\t Naive Bayes assumes a stochastic model of document generation . \n\t', '\n\t\t Using Bayes\x92 rule , the model is inverted in order to predict the most likely class for a new document . \n\t', '\n\t\t We assume that documents are generated according to a multinomial event model \n\t\t']",Positive
"['\n\t\t Thus a document is represented as a vector di = ( xi1 ... xil Vl ) of word counts where V is the vocabulary and each xit E { 0 , 1 , 2 , ... } indicates how often wt occurs in di . \n\t', '\n\t\t Given model parameters p(wt Icj ) and class prior probabilities p(cj ) and assuming independence of the words , the most likely class for a document di is computed as c* ( di ) = argmax p(cj)p(dIcj ) j ( 1 ) p(wt I cj)n(wt,di) where n(wt , di ) is the number of occurrences of wt in di . \n\t', '\n\t\t p(wtIcj) and p(cj) are estimated from training documents with known classes , using maximum likelihood estimation with a Laplacean prior : ( 2 ) IV I + Etvl1 Edicc , n(wt , di ) p(cj) = IcjI ( 3 ) ECl j~=1 Icj~I It is common practice to use only a subset of the words in the training documents for classification to avoid overfitting and make classification more efficient . \n\t', '\n\t\t This is usually done by assigning each word a score f ( wt ) that measures its usefulness for classification and selecting the N highest scored words . \n\t', '\n\t\t One of the best performing scoring functions for feature selection in text classification is mutual information \n\t\t']",Positive
"['\n\t\t The mutual information between two random variables , MI(X ; Y ) , measures the amount of information that the value of one variable gives about the value of the other \n\t\t']",Positive
"['\n\t\t Note that in the multinomial model , the word variable W takes on values from the vocabulary V . \n\t', '\n\t\t In order to use mutual information with a multinomial model , one defines new random variables Wt E { 0 , 1 } with p(Wt = 1 ) = p(W = wt ) \n\t\t']",Positive
"['\n\t\t Then the mutual information between a word wt and the class variable C is Lp(x , cj)log p p (()(c)) ( 4 ) where p(x , cj ) and p(x) are short for p(Wt = x , cj ) and p(Wt = x ) . \n\t', '\n\t\t p(x,cj) , p(x) and p(cj) are estimated from the training documents by counting how often wt occurs in each class . \n\t', '\n\t\t 2 Naive Bayes and KL-Divergence There is a strong connection between Naive Bayes and KL-divergence ( Kullback-Leibler divergence , relative entropy ) . \n\t', '\n\t\t KL-divergence measures how = argmax j lVl p(cj ) H t=1 p(wt I cj ) = 1 + Edicc , n(wt , di ) lCl MI(Wt ; C ) = L j=1 X=0,1 much one probability distribution is different from another \n\t\t']",Positive
"['\n\t\t It is defined ( for discrete distributions ) by L p(x) log p(x) ( 5 ) KL(p , q ) = q(x) x By viewing a document as a probability distribution over words , Naive Bayes can be interpreted in an information-theoretic framework \n\t\t']",Positive
"['\n\t\t Let p(wt Id ) = n(wt,d)/IdI . \n\t', '\n\t\t Taking logarithms and dividing by the length of d , ( 1 ) can be rewritten as c* ( d ) a higher score . \n\t', '\n\t\t By removing words with a lower score from the vocabulary , the training documents of each class become more similar to each other , and therefore , also to the class , in terms of word distribution . \n\t', '\n\t\t This leads to more homogeneous classes . \n\t', '\n\t\t Assuming that the test documents and training documents come from the same distribution , the similarity between the test documents and their respective classes will be increased as well , thus resulting in higher classification accuracy . \n\t', '\n\t\t We now make this more precise . \n\t', '\n\t\t Let S = { d1 , ... , dISI } be the set of training documents , and denote the class of di with c(di) . \n\t', '\n\t\t The average KLdivergence for a word wt between the training documents and their classes is given by = argmax log p(cj) + IVI n(wt , d ) log p(wt I cj ) j L t=1 = argmax 1 LIVI p(wt Id ) log p(wt I cj ) j IdI logp(cj) + t=1 ( 6 ) Adding the entropy of p(W I d ) , we get c* ( d ) = argmax 1 LIVI p(wt I d ) log p(wt I d ) j IdI logp(cj) \x97 t=1 p(wtIcj) = argmin 1 j KL(p(WI d),p(W Icj ) ) \x97IdI logp(cj) ( 7 ) This means that Naive Bayes assigns to a document d the class which is \x93most similar\x94 to d in terms of the distribution of words . \n\t', '\n\t\t Note also that the prior probabilities are usually dominated by document probabilities except for very short documents . \n\t', '\n\t\t 3 Feature Selection using KL-Divergence We define a new scoring function for feature selection based on the following considerations . \n\t', '\n\t\t In the previous section we have seen that Naive Bayes assigns a document d the class c* such that the \x93distance\x94 between d and c* is minimized . \n\t', '\n\t\t A classification error occurs when a test document is closer to some other class than to its true class , in terms of KL-divergence . \n\t', '\n\t\t We seek to define a scoring function such that words whose distribution in the individual training documents of a class is much different from the distribution in the class ( according to ( 2 ) ) receive a lower score , while words with a similar distribution in all training documents of the same class receive dzES ( 8 ) One problem with ( 8 ) is that in addition to the conditional probabilities p(wt I cj ) for each word and each class , the computation considers each individual document , thus resulting in a time requirement of O ( I S I ).1 In order to avoid this additional complexity , instead of KLt(S) we use an approxima- tion KLt(S) , which is based on the following two assumptions : ( i ) the number of occurrences of wt is the same in all documents that contain wt , ( ii ) all documents in the same class cj have the same length . \n\t', '\n\t\t Let Njt be the number of documents in cj that contain wt , and let \x98pd ( wt I cj ) = p(wt I cj ) Icj I ( 9 ) be the average probability of wt in those documents in cj that contain wt ( if wt does not occur in cj , set \x98pd ( wt I cj ) = 0 ) . \n\t', '\n\t\t Then KLt ( S ) reduces to 1 ICI pd ( wt I cj ) KLt(S) = ISI LNjtpd ( wt Icj)log p(wtIcj) j=1 ( 10 ) Plugging in ( 9 ) and ( 3 ) and defining q(wt I cj ) = Njt /I cj I , we get ICI ~KLt(S) = \x97 L p(cj)p(wtIcj)log q(wtIcj) . \n\t', ""\n\t\t ( 11 ) j=1 Note that computing KLt(S) only requires a statistics of the number of words and documents for each ' Note that KLt(S) cannot be computed simultaneously with p(wticj) in one pass over the documents in ( 2 ) : KLt(S) requires p(wt | cj ) when each document is considered , but computing the latter needs iterating over all documents itself . \n\t"", '\n\t\t KLt(S) = IsI L KL(p(wtIdi),p(wtIc(di))) . \n\t', '\n\t\t class , not per document . \n\t', '\n\t\t Thus ~KLt ( 5 ) can be computed in O ( I C I ) . \n\t', '\n\t\t Typically , ICI is much smaller than I5I . \n\t', '\n\t\t Another important thing to note is the following . \n\t', '\n\t\t By removing words with an uneven distribution in the documents of the same class , not only the documents in the class , but also the classes themselves may become more similar , which reduces the ability to distinguish between different classes . \n\t', ""\n\t\t Let p(wt) be the number of occurrences of wt in all training documents , divided by the total number of words , q(wt) = ff'1 Nyt/I 5I and define MI KL dKL 1 0.8 0.6 0.4 0.2 010 100 1000 10000 100000 ~Kt ( 5 ) = \x97p(wt) log q ( wt ) . \n\t"", '\n\t\t ( 12 ) ~Kt ( 5 ) can be interpreted as an approximation of the average divergence of the distribution of wt in the individual training documents from the global distribution ( averaged over all training documents in all classes ) . \n\t', '\n\t\t If wt is independent of the class , then ~Kt ( 5 ) = ~KLt(5) . \n\t', '\n\t\t The difference between the two is a measure of the increase in homogeneity of the training documents , in terms of the distribution of wt , when the documents are clustered in their true classes . \n\t', '\n\t\t It is large if the distribution of wt is similar in the training documents of the same class but dissimilar in documents of different classes . \n\t', '\n\t\t In analogy to mutual information , we define our new scoring function as the difference KL(wt) = ~Kt(5) \x97 ~KLt(5) . \n\t', '\n\t\t ( 13 ) We also use a variant of KL , denoted dKL , where p(wt) is estimated according to ( 14 ) : lCl p~ ( wt ) = L p(cy)p(wtIcy) ( 14 ) y=1 and p(wt Icy ) is estimated as in ( 2 ) . \n\t', '\n\t\t 4 Experiments We compare KL and dKL to mutual information , using two standard data sets : 20 Newsgroups2 and Reuters 21578.3 In tokenizing the data , only words consisting of alphabetic characters are used after conversion to lower case . \n\t', '\n\t\t In addition , all numbers are mapped to a special token NUM . \n\t', '\n\t\t For 20 Newsgroups we remove the newsgroup headers and use a stoplist consisting of the 100 most frequent words of 2http://www.ai.mit.edu/\x97j rennie/20Newsgroups/ 3http://www.daviddlewis.com/resources/testcollections/ reuters21578/ Vocabulary Size Figure 1 : Classification accuracy for 20 Newsgroups . \n\t', '\n\t\t The curves have small error bars . \n\t', '\n\t\t the British National Corpus.4 We use the ModApte split of Reuters 21578 ( Apt´e et al. , 1994 ) and use only the 10 largest classes . \n\t', '\n\t\t The vocabulary size is 111868 words for 20 Newsgroups and 22430 words for Reuters . \n\t', '\n\t\t Experiments with 20 Newsgroups are performed with 5-fold cross-validation , using 80 % of the data for training and 20 % for testing . \n\t', '\n\t\t We build a single classifier for the 20 classes and vary the number of selected words from 20 to 20000 . \n\t', '\n\t\t Figure 1 compares classification accuracy for the three scoring functions . \n\t', '\n\t\t dKL slightly outperforms mutual information , especially for smaller vocabulary sizes . \n\t', '\n\t\t The difference is statistically significant for 20 to 200 words at the 99 % confidence level , and for 20 to 2000 words at the 95 % confidence level , using a one-tailed paired t-test . \n\t', '\n\t\t For the Reuters dataset we build a binary classifier for each of the ten topics and set the number of positively classified documents such that precision equals recall . \n\t', '\n\t\t Precision is the percentage of positive documents among all positively classified documents . \n\t', '\n\t\t Recall is the percentage of positive documents that are classified as positive . \n\t', '\n\t\t In Figures 2 and 3 we report microaveraged and macroaveraged recall for each number of selected words . \n\t', '\n\t\t Microaveraged recall is the percentage of all positive documents ( in all topics ) that are classified as positive . \n\t', '\n\t\t Macroaveraged recall is the average of the recall values of the individual topics . \n\t', '\n\t\t Microaveraged recall gives equal weight to the documents and thus emphasizes larger topics , while macroaveraged recall gives equal weight to the topics and thus emphasizes smaller topics more than microav- 4 http : //www. itri.brighton . \n\t', '\n\t\t ac.uk/\x97Adam.Kilgarriff/bnc- readme.html Vocabulary Size Figure 2 : Microaveraged recall on Reuters at break- even point . \n\t', '\n\t\t Vocabulary Size Figure 3 : Macroaveraged recall on Reuters at break- even point . \n\t', '\n\t\t eraged recall . \n\t', '\n\t\t Both KL and dKL achieve slightly higher values for microaveraged recall than mutual information , for most vocabulary sizes ( Fig . \n\t', '\n\t\t 2 ) . \n\t', '\n\t\t KL performs best at 20000 words with 90.1 % microaveraged recall , compared to 89.3 % for mutual information . \n\t', '\n\t\t The largest improvement is found for dKL at 100 words with 88.0 % , compared to 86.5 % for mutual information . \n\t', '\n\t\t For smaller categories , the difference between the KL-divergence based scores and mutual information is larger , as indicated by the curves for macroaveraged recall ( Fig . \n\t', '\n\t\t 3 ) . \n\t', '\n\t\t KL yields the highest recall at 20000 words with 82.2 % , an increase of 3.9 % compared to mutual information with 78.3 % , whereas dKL has its largest value at 100 words with 78.8 % , compared to 76.1 % for mutual information . \n\t', '\n\t\t We find the largest improvement at 5000 words with 5.6 % for KL and 2.9 % for dKL , compared to mutual information . \n\t', '\n\t\t 5 Conclusion By interpreting Naive Bayes in an information theoretic framework , we derive a new scoring method for feature selection in text classification , based on the KL-divergence between training documents and their classes . \n\t', '\n\t\t Our experiments show that it outperforms mutual information , which was one of the best performing methods in previous studies \n\t\t']",Negative
"['\n\t\t The KL-divergence based scores are especially effective for smaller categories , but additional experiments are certainly required . \n\t', '\n\t\t In order to keep the computational cost low , we use an approximation instead of the exact KLdivergence . \n\t', '\n\t\t Assessing the error introduced by this approximation is a topic for future work . \n\t', '\n\t\t References Chidanand Apt´e , Fred Damerau , and Sholom M. Weiss . \n\t', '\n\t\t 1994. Towards language independent automated learning of text categorization models . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 17th ACM SIGIR Conference on Research and Development in Information Retrieval ( SIGIR \x9294 ) , pages 23\x9630 . \n\t', '\n\t\t Thomas M. Cover and Joy A. Thomas . \n\t', '\n\t\t 1991. Elements of Information Theory . \n\t', '\n\t\t John Wiley , New York . \n\t', '\n\t\t Inderjit S. Dhillon , Subramanyam Mallela , and Rahul Kumar . \n\t', '\n\t\t 2002. Enhanced word clustering for hierarchical text classification . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pages 191\x96 200 . \n\t', '\n\t\t Pedro Domingos and Michael Pazzani . \n\t', '\n\t\t 1997. On the optimality of the simple bayesian classifier under zero-one loss . \n\t', '\n\t\t Machine Learning , 29:103\x96 130. Andrew McCallum and Kamal Nigam . \n\t', '\n\t\t 1998. A comparison of event models for Naive Bayes text classification . \n\t', '\n\t\t In Learning for Text Categorization : Papers from the AAAI Workshop , pages 41\x96 48 . \n\t', '\n\t\t AAAI Press . \n\t', '\n\t\t Technical Report WS-98-05 . \n\t', '\n\t\t Jason D. M. Rennie . \n\t', '\n\t\t 2001. Improving multi-class text classification with Naive Bayes . \n\t', '\n\t\t Master\x92s thesis , Massachusetts Institute of Technology . \n\t', '\n\t\t Yiming Yang and Jan O. Pedersen . \n\t', '\n\t\t 1997. A comparative study on feature selection in text categorization . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 14th International Conference on Machine Learning ( ICML-97 ) , pages 412\x96 420 . \n\t', '\n\t\t 0.710 100 1000 10000 100000 MI KL dKL 1 0.95 0.75 0.9 0.85 0.8 0.710 100 1000 10000 100000 1 0.95 0.9 0.85 0.8 0.75 MI KL dKL \n\t', '\n\t\t Incorporating topic information into sentiment analysis models Tony Mullen National Institute of Informatics ( NII ) Hitotsubashi 2-1-2 , Chiyoda-ku Tokyo 101-8430 , Japan , mullen@nii.ac.jp Nigel Collier National Institute of Informatics ( NII ) Hitotsubashi 2-1-2 , Chiyoda-ku Tokyo 101-8430 , Japan , collier@nii.ac.jp Abstract This paper reports experiments in classifying texts based upon their favorability towards the subject of the text using a feature set enriched with topic information on a small dataset of music reviews hand-annotated for topic . \n\t', '\n\t\t The results of these experiments suggest ways in which incorporating topic information into such models may yield improvement over models which do not use topic information . \n\t', '\n\t\t 1 Introduction There are a number of challenging aspects in recognizing the favorability of opinion-based texts , the task known as sentiment analysis . \n\t', '\n\t\t Opinions in natural language are very often expressed in subtle and complex ways , presenting challenges which may not be easily addressed by simple text categorization approaches such as n-gram or keyword identification approaches . \n\t', '\n\t\t Although such approaches have been employed effectively \n\t\t']",Positive
"['\n\t\t Moving beyond these approaches can involve addressing the task at several levels . \n\t', '\n\t\t Negative reviews may contain many apparently positive phrases even while maintaining a strongly negative tone , and the opposite is also common . \n\t', '\n\t\t This paper attempts to address this issue using Support Vector Machines ( SVMs ) , a well-known and powerful tool for classification of vectors of real-valued features \n\t\t']",Positive
"['\n\t\t The present approach emphasizes the use of a variety of diverse information sources . \n\t', '\n\t\t In particular , several classes of features based upon the proximity of the topic with phrases which have been assigned favorability values are described in order to take advantage of situations in which the topic of the text may be explicitly identified . \n\t', '\n\t\t 2 Motivation In the past , work has been done in the area of characterizing words and phrases according to their emotive tone \n\t\t']",Positive
"['\n\t\t Pang et al . \n\t', '\n\t\t (2002)\x92s treatment of the task as analogous to topic-classification underscores the difference between the two tasks . \n\t', '\n\t\t A number of rhetorical devices , such as the drawing of contrasts between the reviewed entity and other entities or expectations , sarcasm , understatement , and digressions , all of which are used in abundance in many discourse domains , create challenges for these approaches . \n\t', '\n\t\t It is hoped that incorporating topic information along the lines suggested in this paper will be a step towards solving some of these problems . \n\t', '\n\t\t 3 Methods 3.1 Semantic orientation with PMI Here , the term semantic orientation ( SO ) \n\t\t']",Positive
"['\n\t\t In the present work , the approach taken by \n\t\t']",Positive
"['\n\t\t For the purposes of this paper , these phrases will be referred to as value phrases , since they will be the sources of SO values . \n\t', '\n\t\t Once the desired value phrases have been extracted from the text , each one is assigned an SO value . \n\t', '\n\t\t The SO of a phrase is determined based upon the phrase\x92s pointwise mutual information ( PMI ) with the words \x93excellent\x94 and \x93poor\x94 . \n\t', '\n\t\t PMI is defined by \n\t\t']",Positive
"['\n\t\t The SO for a is the difference between its PMI with the word \x93excellent\x94 and its PMI with the word \x93poor.\x94 The method used to derive these values takes advantage of the possibility of using the World Wide Web as a corpus , similarly to work such as \n\t\t']",Positive
"[""\n\t\t The probabilities are estimated by querying the AltaVista Advanced Search engine ' for counts . \n\t"", '\n\t\t The search engine\x92s \x93NEAR\x94 operator , representing occurrences of the two queried words within ten words of each other in a text , is used to define co-occurrence . \n\t', '\n\t\t The final SO equation is Intuitively , this yields values above zero for phrases with greater PMI with the word \x93excellent\x94 and below zero for greater PMI with \x93poor\x94 . \n\t', '\n\t\t A SO value of zero would indicate a completely neutral semantic orientation . \n\t', '\n\t\t 3.2 Osgood semantic differentiation with WordNet Further feature types are derived using the method of \n\t\t']",Positive
"['\n\t\t The three values correspond to the potency ( strong or weak ) , activity ( active or passive ) and the evaluative ( good or bad ) factors introduced in Charles Osgood\x92s Theory of Semantic Differentiation \n\t\t']",Positive
"['\n\t\t These values are derived by measuring the relative minimal path length ( MPL ) in WordNet between the adjective in question and the pair of words appropriate for the given factor . \n\t', '\n\t\t In the case of the evaluative factor ( EVA ) for example , the comparison is between the MPL between the adjective and \x93good\x94 and the MPL between the adjective and \x93bad\x94 . \n\t', '\n\t\t Only adjectives connected by synonymy to each of the opposites are considered . \n\t', '\n\t\t The method results in a list of 5410 adjectives , each of which is given a value for each of the three factors referred to as EVA , POT , and ACT . \n\t', '\n\t\t Each of these factors\x92 values are averaged over all the adjectives in a text , yielding three real-valued feature values for the text , which will be added to the SVM model . \n\t', '\n\t\t 3.3 Topic proximity and syntactic-relation features In some application domains , it is known in advance what the topic is toward which sentiment is to be evaluated . \n\t', '\n\t\t Incorporating this information is done by creating several classes of features based upon the semantic orientation values of phrases given their position in relation to the topic of the text . \n\t', '\n\t\t The approach allows secondary information to be incorporated where available , in this case , the primary information is the specific record being reviewed and the secondary information identified is the artist . \n\t', '\n\t\t Texts were annotated by hand using the Open Ontology Forge annotation tool \n\t\t']",Positive
"['\n\t\t In each record review , references ( including co-reference ) to the record being reviewed were tagged as THIS WORK and references to the artist under review were tagged as THIS ARTIST . \n\t', '\n\t\t With these entities tagged , a number of classes of features may be extracted , representing various relationships between topic entities and value phrases similar to those described in section 3.1 . \n\t', '\n\t\t The classes looked at in this work are as follows : Turney Value The average value of all value phrases\x92 SO values for the text . \n\t', '\n\t\t Classification by this feature alone is not the equivalent of Turney\x92s approach , since the present approach involves retraining in a supervised model . \n\t', '\n\t\t In sentence with THIS WORK The average value of all value phrases which occur in the same sentence as a reference to the work being reviewed . \n\t', '\n\t\t lwww.altavista.com Following THIS WORK The average value of all value phrases which follow a reference to the work being reviewed directly , or separated only by the copula or a preposition . \n\t', '\n\t\t Preceding THIS WORK The average value of all value phrases which precede a reference to the work being reviewed directly , or separated only by the copula or a preposition . \n\t', '\n\t\t In sentence with THIS ARTIST As above , but with reference to the artist . \n\t', '\n\t\t Following THIS ARTIST As above , but with reference to the artist . \n\t', '\n\t\t Preceding THIS ARTIST As above , but with reference to the artist . \n\t', '\n\t\t The features used which make use of adjectives with WordNet derived Osgood values include the following : Text-wide EVA The average EVA value of all adjectives in a text . \n\t', '\n\t\t Text-wide POT The average POT value of all adjectives in a text . \n\t', '\n\t\t Text-wide ACT The average ACT value of all adjectives in a text . \n\t', '\n\t\t TOPIC-sentence EVA The average EVA value of all adjectives which share a sentence with the topic of the text . \n\t', '\n\t\t TOPIC-sentence POT The average POT value of all adjectives which share a sentence with the topic of the text . \n\t', '\n\t\t TOPIC-sentence ACT The average ACT value of all adjectives which share a sentence with the topic of the text . \n\t', '\n\t\t The grouping of these classes should reflect some common degree of reliability of features within a given class , but due to data sparseness what might have been more natural class groupings\x97for example including value-phrase preposition topic-entity as a distinct class\x97often had to be conflated in order to get features with enough occurrences to be representative . \n\t', '\n\t\t 4 Experiments The dataset consists of 100 record reviews from the Pitchfork Media online record review publication,2 topic-annotated by hand . \n\t', '\n\t\t Features used include word unigrams and lemmatized unigrams3 as well as the features described in 3.3 which make use of topic information , namely the broader PMI derived SO values and the topic-sentence Osgood values . \n\t', '\n\t\t Due to the relatively small size of this dataset , test suites were created using 100 , 20 , 10 , and 5-fold cross validation , to maximize the amount of data available for training and the accuracy of the results . \n\t', '\n\t\t SVMs were built using Kudo\x92s TinySVM software implementation.4 5 Results Experimental results may be seen in figure 1 . \n\t', '\n\t\t It must be noted that this dataset is very small,and although the results are not conclusive they are promising insofar as they suggest that the use of incorporating PMI values towards the topic yields some improvement in modeling . \n\t', '\n\t\t They also suggest that the best way to incorporate such features is in the form of a separate SVM which may then be combined with the lemma-based model to create a hybrid . \n\t', '\n\t\t 2http://www.pitchforkmedia.com 3 We employ the Conexor FDG parser ( Tapanainen and J¨arvinen , 1997 ) for POS tagging and lemmatization 4http://cl.aist-nara.ac.jp/\x98taku-ku/software/TinySVM Model 5 folds 10 folds 20 folds 100 folds All ( THIS WORK and THIS ARTIST)PMI 70 % 70 % 68 % 69 % THIS WORK PMI 72 % 69 % 70 % 71 % All Osgood 64 % 64 % 65 % 64 % All PMI and Osgood 74 % 71 % 74 % 72 % Unigrams 79 % 80 % 78 % 82 % Unigrams , PMI , Osgood 81 % 80 % 82 % 82 % Lemmas 83 % 85 % 84 % 84 % Lemmas and Osgood 83 % 84 % 84 % 84 % Lemmas and Turney 84 % 85 % 84 % 84 % Lemmas , Turney , text-wide Osgood 84 % 85 % 84 % 84 % Lemmas , PMI , Osgood 84 % 85 % 84 % 86 % Lemmas and PMI 84 % 85 % 85 % 86 % Hybrid SVM ( PMI/Osgood and Lemmas ) 86 % 87 % 84 % 89 % Figure 1 : Accuracy results ( percent of texts correctly classed ) for 5 , 10 , 20 and 100-fold cross-validation tests with Pitchforkmedia.com record review data , hand-annotated for topic . \n\t', '\n\t\t 5.1 Discussion At the level of the phrasal SO assignment , it would seem that some improvement could be gained by adding domain context to the AltaVista Search . \n\t', '\n\t\t Many \x97perhaps most\x97terms\x92 favorability content depends to some extent on their context . \n\t', '\n\t\t As Turney notes , \x93unpredictable,\x94 is generally positive when describing a movie plot , and negative when describing an automobile or a politician . \n\t', '\n\t\t Likewise , such terms as \x93devastating\x94 might be generally negative , but in the context of music or art may imply an emotional engagement which is usually seen as positive . \n\t', '\n\t\t Likewise , using \x93excellent\x94 and \x93poor\x94 as the poles in assessing this value seems somewhat arbitrary , especially given the potentially misleading economic meaning of \x93poor.\x94 Nevertheless , cursory experiments in adjusting the search have not yielded improvements . \n\t', '\n\t\t One problem with limiting the domain ( such as adding \x93AND music\x94 or some disjunction of such constraints to the query ) is that the resultant hit count is greatly diminished . \n\t', '\n\t\t The data sparseness which results from added restrictions appears to cancel out any potential gain . \n\t', '\n\t\t It is to be hoped that in the future , as search engines continue to improve and the Internet continues to grow , more possibilities will open up in this regard . \n\t', '\n\t\t As it is , Google returns more hits than AltaVista , but its query syntax lacks a \x93NEAR\x94 operator , making it unsuitable for this task . \n\t', '\n\t\t As to why using \x93excellent\x94 and \x93poor\x94 works better than , for example \x93good\x94 and \x93bad,\x94 it is not entirely clear . \n\t', '\n\t\t Again , cursory investigations have thus far supported Turney\x92s conclusion that the former are the appropriate terms to use for this task . \n\t', '\n\t\t It also seems likely that the topic-relations aspect of the present research only scratches the surface of what should be possible . \n\t', '\n\t\t Although performance in the mid-80s is not bad , there is still considerable room for improvement . \n\t', '\n\t\t The present models may also be further expanded with features representing other information sources , which may include other types of semantic annotation \n\t\t']",Positive
"['\n\t\t In any case , it is hoped that the present work may help to indicate how various information sources pertinent to the task may be brought together . \n\t', '\n\t\t 6 Conclusion Further investigation using larger datasets is necessary for the purposes of fully exploiting topic information where it is available , but the present results suggest that this is a worthwhile direction to investigate . \n\t', '\n\t\t References K.W. Church and P. Hanks . \n\t', '\n\t\t 1989. Word association norms , mutual information and lexicography . \n\t', '\n\t\t In Proceedings of the 27th Annual Conference of the ACL , New Brunswick , NJ . \n\t', '\n\t\t N. Collier , K. Takeuchi , A. Kawazoe , T. Mullen , and T. Wattarujeekrit . \n\t', '\n\t\t 2003. A framework for integrat- ing deep and shallow semantic structures in text mining . \n\t', '\n\t\t In Proceedings of the Seventh International Conference on Knowledge-based Intelligent Information and Engineering Systems . \n\t', '\n\t\t Springer-Verlag . \n\t', '\n\t\t V. Hatzivassiloglou and K.R. McKeown . \n\t', '\n\t\t 2002. Predicting the semantic orientation of adjectives . \n\t', '\n\t\t In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and the 8th Conference of the European Chapter of the ACL . \n\t', '\n\t\t V. Hatzivassiloglou and J. Wiebe . \n\t', '\n\t\t 2000. Effects of adjective orientation and gradability on sentence subjectivity . \n\t', '\n\t\t Jaap Kamps , Maarten Marx , Robert J. Mokken , and Marten de Rijke . \n\t', '\n\t\t 2002. Words with attitude . \n\t', '\n\t\t In In Proceedings of the 1st International Conference on Global WordNet , Mysore , India . \n\t', '\n\t\t Frank Keller and Mirella Lapata . \n\t', '\n\t\t 2003. Using the web to obtain freqeuncies for unseen bigrams . \n\t', '\n\t\t Computational Linguistics , 29(3) . \n\t', '\n\t\t Special Issue on the Web as Corpus . \n\t', '\n\t\t Charles E. Osgood , George J. Succi , and Percy H. Tannenbaum . \n\t', '\n\t\t 1957. The Measurement of Meaning . \n\t', '\n\t\t University of Illinois . \n\t', '\n\t\t Bo Pang , Lillian Lee , and Shivakumar Vaithyanathan . \n\t', '\n\t\t 2002. Thumbs up ? \n\t', '\n\t\t Sentiment classification using machine learning techniques . \n\t', '\n\t\t In Empirical Methods in Natural Language Processing [ and Very Large Corpora ] . \n\t', '\n\t\t P. Tapanainen and T. J¨arvinen . \n\t', '\n\t\t 1997. A non-projective dependency parser . \n\t', '\n\t\t In Proceedings of the 5th Conference on Applied Natural Language Processing , Washington D.C. , Association of Computational Linguistics . \n\t', '\n\t\t P.D. Turney and M.L. Littman . \n\t', '\n\t\t 2003. Measuring praise and criticism : Inference of semantic orientation from association . \n\t', '\n\t\t ACM Transactions on Information Systems ( TOIS ) , 21(4):315\x96346 . \n\t', '\n\t\t P.D. Turney . \n\t', '\n\t\t 2002. Thumbs up or thumbs down ? \n\t', '\n\t\t semantic orientation applied to unsupervised classification of reviews . \n\t', '\n\t\t In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , Philadelphia . \n\t', '\n\t\t Vladimir Vapnik . \n\t', '\n\t\t 1998. Statistical Learning Theory . \n\t', '\n\t\t Wiley , Chichester , GB . \n\t', '\n\t\t J. Wiebe , T. Wilson , R. Bruce , M. Bell , and M. Martin . \n\t', '\n\t\t 2002. Learning subjective language . \n\t', '\n\t\t Technical Report TR-02-100 , University of Pittsburgh , Pittsburgh , PA . \n\t', '\n\t\t Janyce Wiebe . \n\t', '\n\t\t 2000. Learning subjective adjectives from corpora . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t 17th National Conference on Artificial Intelligence ( AAAI-2000 ) , Austin , Texas , July . \n\t', '\n\t\t J Wiebe . \n\t', '\n\t\t 2002 . \n\t', '\n\t\t Instructions for annotating opinions in newspaper articles . \n\t', '\n\t\t Technical Report TR-02-101 , University of Pittsburgh , Pittsburgh , PA . \n\t', '\n\t\t A Practical Solution to the Problem of Automatic Word Sense Induction Reinhard Rapp University of Mainz , FASK D-76711 Germersheim , Germany rapp@mail.fask.uni-mainz.de Abstract Recent studies in word sense induction are based on clustering global co-occurrence vectors , i.e. vectors that reflect the overall behavior of a word in a corpus . \n\t', '\n\t\t If a word is semantically ambiguous , this means that these vectors are mixtures of all its senses . \n\t', '\n\t\t Inducing a word\x92s senses therefore involves the difficult problem of recovering the sense vectors from the mixtures . \n\t', '\n\t\t In this paper we argue that the demixing problem can be avoided since the contextual behavior of the senses is directly observable in the form of the local contexts of a word . \n\t', '\n\t\t From human disambiguation performance we know that the context of a word is usually sufficient to determine its sense . \n\t', '\n\t\t Based on this observation we describe an algorithm that discovers the different senses of an ambiguous word by clustering its contexts . \n\t', '\n\t\t The main difficulty with this approach , namely the problem of data sparseness , could be minimized by looking at only the three main dimensions of the context matrices . \n\t', '\n\t\t 1 Introduction The topic of this paper is word sense induction , that is the automatic discovery of the possible senses of a word . \n\t', '\n\t\t A related problem is word sense disambiguation : Here the senses are assumed to be known and the task is to choose the correct one when given an ambiguous word in context . \n\t', '\n\t\t Whereas until recently the focus of research had been on sense disambiguation , papers like Pantel & \n\t\t']",Positive
['\n\t\t In the approach by Pantel & \n\t\t'],Positive
"['\n\t\t This is called global clustering . \n\t', '\n\t\t Since ( by looking at differential vectors ) their algorithm allows a word to belong to more than one cluster , each cluster a word is assigned to can be considered as one of its senses . \n\t', '\n\t\t A problem that we see with this approach is that it allows only as many senses as clusters , thereby limiting the granularity of the meaning space . \n\t', '\n\t\t This problem is avoided by \n\t\t']",Positive
"['\n\t\t This means , to find the senses of a given word only its close associations are clustered , that is for each word new clusters will be found . \n\t', '\n\t\t Despite many differences , to our knowledge almost all approaches to sense induction that have been published so far have a common limitation : They rely on global co-occurrence vectors , i.e. on vectors that have been derived from an entire corpus . \n\t', '\n\t\t Since most words are semantically ambiguous , this means that these vectors reflect the sum of the contextual behavior of a word\x92s underlying senses , i.e. they are mixtures of all senses occurring in the corpus . \n\t', '\n\t\t However , since reconstructing the sense vectors from the mixtures is difficult , the question is if we really need to base our work on mixtures or if there is some way to directly observe the contextual behavior of the senses thereby avoiding the mixing beforehand . \n\t', '\n\t\t In this paper we suggest to look at local instead of global co-occurrence vectors . \n\t', '\n\t\t As can be seen from human performance , in almost all cases the local context of an ambiguous word is sufficient to disambiguate its sense . \n\t', '\n\t\t This means that the local context of a word usually carries no ambiguities . \n\t', '\n\t\t The aim of this paper is to show how this observation whose application tends to severely suffer from the sparse-data problem can be successfully exploited for word sense induction . \n\t', '\n\t\t 2 Approach The basic idea is that we do not cluster the global co-occurrence vectors of the words ( based on an entire corpus ) but local ones which are derived from the contexts of a single word . \n\t', '\n\t\t That is , our computations are based on the concordance of a word . \n\t', '\n\t\t Also , we do not consider a term/term but a term/context matrix . \n\t', '\n\t\t This means , for each word that we want to analyze we get an entire matrix . \n\t', '\n\t\t Let us exemplify this using the ambiguous word palm with its tree and hand senses . \n\t', '\n\t\t If we assume that our corpus has six occurrences of palm , i.e. there are six local contexts , then we can derive six local co-occurrence vectors for palm . \n\t', '\n\t\t Considering only strong associations to palm , these vectors could , for example , look as shown in table 1 . \n\t', '\n\t\t The dots in the matrix indicate if the respective word occurs in a context or not . \n\t', '\n\t\t We use binary vectors since we assume short contexts where words usually occur only once . \n\t', '\n\t\t By looking at the matrix it is easy to see that contexts c1 , c3 , and c6 seem to relate to the hand sense of palm , whereas contexts c2 , c4 , and c5 relate to its tree sense . \n\t', '\n\t\t Our intuitions can be resembled by using a method for computing vector similarities , for example the cosine coefficient or the ( binary ) Jaccard-measure . \n\t', '\n\t\t If we then apply an appropriate clustering algorithm to the context vectors , we should obtain the two expected clusters . \n\t', '\n\t\t Each of the two clusters corresponds to one of the senses of palm , and the words closest to the geometric centers of the clusters should be good descriptors of each sense . \n\t', '\n\t\t However , as matrices of the above type can be extremely sparse , clustering is a difficult task , and common algorithms often deliver sub-optimal results . \n\t', '\n\t\t Fortunately , the problem of matrix sparseness can be minimized by reducing the dimensionality of the matrix . \n\t', '\n\t\t An appropriate algebraic method that has the capability to reduce the dimensionality of a rectangular or square matrix in an optimal way is singular value decomposition ( SVD ) . \n\t', '\n\t\t As shown by Schütze ( 1997 ) by reducing the dimensionality a generalization effect can be achieved that often improves the results . \n\t', '\n\t\t The approach that we suggest in this paper involves reducing the number of columns ( contexts ) and then applying a clustering algorithm to the row vectors ( words ) of the resulting matrix . \n\t', '\n\t\t This works well since it is a strength of SVD to reduce the effects of sampling errors and to close gaps in the data . \n\t', '\n\t\t c1 c2 c3 c4 c5 c6 arm \x95 \x95 beach \x95 \x95 coconut \x95 \x95 \x95 finger \x95 \x95 hand \x95 \x95 \x95 shoulder \x95 \x95 tree \x95 \x95 Table 1 : Term/context matrix for the word palm . \n\t', '\n\t\t 3 Algorithm As in previous work \n\t\t']",Positive
['\n\t\t Starting from the list of 12 ambiguous words provided by \n\t\t'],Positive
"['\n\t\t From the concordances we computed 12 term/context-matrices ( analogous to table 1 ) whose binary entries indicate if a word occurs in a particular context or not . \n\t', '\n\t\t Assuming that the amount of information that a context word pro vides depends on its association strength to the ambiguous word , in each matrix we removed all words that are not among the top 30 first order associations to the ambiguous word . \n\t', '\n\t\t These top 30 associations were computed fully automatically based on the log-likelihood ratio . \n\t', '\n\t\t We used the procedure described in \n\t\t']",Positive
"['\n\t\t This way preference is given to words that are in the middle of the frequency range . \n\t', '\n\t\t Figures 1 to 3 are based on the association lists for the words palm and poach . \n\t', '\n\t\t Given that our term/context matrices are very sparse with each of their individual entries seeming somewhat arbitrary , it is necessary to detect the regularities in the patterns . \n\t', '\n\t\t For this purpose we applied the SVD to each of the matrices , thereby reducing their number of columns to the three main dimensions . \n\t', '\n\t\t This number of dimensions may seem low . \n\t', '\n\t\t However , it turned out that with our relatively small matrices ( matrix size is the occurrence frequency of a word times the number of associations considered ) it was sometimes not possible to compute more than three singular values , as there are dependencies in the data . \n\t', '\n\t\t Therefore , we decided to use three dimensions for all matrices . \n\t', '\n\t\t The last step in our procedure involves applying a clustering algorithm to the 30 words in each matrix . \n\t', '\n\t\t For our condensed matrices of 3 rows and 30 columns this is a rather simple task . \n\t', '\n\t\t We decided to use the hierarchical clustering algorithm readily available in the MATLAB ( MATrix LABoratory ) programming language . \n\t', '\n\t\t After some testing with various similarity functions and linkage types , we finally opted for the cosine coefficient and single linkage which is the combination that apparently gave the best results . \n\t', '\n\t\t axes : grid/tools bass : fish/music crane : bird/machine drug : medicine/narcotic duty : tax/obligation motion : legal/physical palm : tree/hand plant : living/factory poach : steal/boil sake : benefit/drink space : volume/outer tank : vehicle/container Table 2 : Ambiguous words and their senses . \n\t', '\n\t\t 4 Results Before we proceed to a quantitative evaluation , by looking at a few examples let us first give a qualitative impression of some results and consider the contribution of SVD to the performance of our algorithm . \n\t', '\n\t\t Figure 1 shows a dendrogram for the word palm ( corpus frequency in the lemmatized BNC : 2054 ) as obtained after applying the algo- rithm described in the previous section , with the only modification that the SVD step was omitted , i.e. no dimensionality reduction was performed . \n\t', '\n\t\t The horizontal axes in the dendrogram is dissimilarity ( 1 \x96 cosine ) , i.e. 0 means identical items and 1 means no similarity . \n\t', '\n\t\t The vertical axes has no special meaning . \n\t', '\n\t\t Only the order of the words is chosen in such a way that line crossings are avoided when connecting clusters . \n\t', '\n\t\t As we can see , the dissimilarities among the top 30 associations to palm are all in the upper half of the scale and not very distinct . \n\t', '\n\t\t The two expected clusters for palm , one relating to its hand and the other to its tree sense , have essentially been found . \n\t', '\n\t\t According to our judgment , all words in the upper branch of the hierarchical tree are related to the hand sense of palm , and all other words are related to its tree sense . \n\t', '\n\t\t However , it is somewhat unsatisfactory that the word frond seems equally similar to both senses , whereas intuitively we would clearly put it in the tree section . \n\t', '\n\t\t Let us now compare figure 1 to figure 2 which has been generated using exactly the same procedure with the only difference that the SVD step ( reduction to 3 dimensions ) has been conducted in this case . \n\t', '\n\t\t In figure 2 the similarities are generally at a higher level ( dissimilarities lower ) , the relative differences are bigger , and the two expected clusters are much more salient . \n\t', '\n\t\t Also , the word frond is now well within the tree cluster . \n\t', '\n\t\t Obviously , figure 2 reflects human intuitions better than figure 1 , and we can conclude that SVD was able to find the right generalizations . \n\t', '\n\t\t Although space constraints prevent us from showing similar comparative diagrams for other words , we hope that this novel way of comparing dendrograms makes it clearer what the virtues of SVD are , and that it is more than just another method for smoothing . \n\t', '\n\t\t Our next example ( figure 3 ) is the dendrogram for poach ( corpus frequency : 458 ) . \n\t', '\n\t\t It is also based on a matrix that had been reduced to 3 dimensions . \n\t', '\n\t\t The two main clusters nicely distinguish between the two senses of poach , namely boil and steal . \n\t', '\n\t\t The upper branch of the hierarchical tree consists of words related to cooking , the lower one mainly contains words related to the unauthorized killing of wildlife in Africa which apparently is an important topic in the BNC. . \n\t', '\n\t\t Figure 3 nicely demonstrates what distinguishes the clustering of local contexts from the clustering of global co-occurrence vectors . \n\t', '\n\t\t To see this , let us bring our attention to the various species of animals that are among the top 30 associations to poach . \n\t', '\n\t\t Some of them seem more often affected by cooking ( pheasant , chicken , salmon ) , others by poaching ( elephant , tiger , rhino ) . \n\t', '\n\t\t According to the diagram only the rabbit is equally suitable for both activities , although fortunately its affinity to cooking is lower than it is for the chicken , and to poaching it is lower than it is for the rhino . \n\t', '\n\t\t That is , by clustering local contexts our algorithm was able to separate the different kinds of animals according to their relationship to poach . \n\t', '\n\t\t If we instead clustered global vectors , it would most likely be impossible to obtain this separation , as from a global perspective all animals have most properties ( context words ) in common , so they are likely to end up in a single cluster . \n\t', '\n\t\t Note that what we exemplified here for animals applies to all linkage decisions made by the algorithm , i.e. all decisions must be seen from the perspective of the ambiguous word . \n\t', '\n\t\t This implies that often the clustering may be counterintuitive from the global perspective that as humans we tend to have when looking at isolated words . \n\t', '\n\t\t That is , the clusters shown in figures 2 and 3 can only be understood if the ambiguous words they are derived from are known . \n\t', '\n\t\t However , this is exactly what we want in sense induction . \n\t', '\n\t\t In an attempt to provide a quantitative evaluation of our results , for each of the 12 ambiguous words shown in table 1 we manually assigned the top 30 first-order associations to one of the two senses provided by \n\t\t']",Positive
"['\n\t\t We then looked at the first split in our hierarchical trees and assigned each of the two clusters to one of the given senses . \n\t', '\n\t\t In no case was there any doubt on which way round to assign the two clusters to the two given senses . \n\t', '\n\t\t Finally , we checked if there were any misclassified items in the clusters . \n\t', '\n\t\t According to this judgment , on average 25.7 of the 30 items were correctly classified , and 4.3 items were misclassified . \n\t', '\n\t\t This gives an overall accuracy of 85.6 % . \n\t', '\n\t\t Reasons for misclassifications include the following : Some of the top 30 associations are more or less neutral towards the senses , so even for us it was not always possible to clearly assign them to one of the two senses . \n\t', '\n\t\t In other cases , outliers led to a poor first split , like if in figure 1 the first split would be located between frond and the rest of the vocabulary . \n\t', '\n\t\t In the case of sake the beverage sense is extremely rare in the BNC and therefore was not represented among the top 30 associations . \n\t', '\n\t\t For this reason the clustering algorithm had no chance to find the expected clusters . \n\t', '\n\t\t 5 Conclusions and prospects From the observations described above we conclude that avoiding the mixture of senses , i.e. clustering local context vectors instead of global co-occurrence vectors , is a good way to deal with the problem of word sense induction . \n\t', '\n\t\t However , there is a pitfall , as the matrices of local vectors are extremely sparse . \n\t', '\n\t\t Fortunately , our simulations suggest that computing the main dimensions of a matrix through SVD solves the problem of sparseness and greatly improves clustering results . \n\t', '\n\t\t Although the results that we presented in this paper seem useful even for practical purposes , we can not claim that our algorithm is capable of finding all the fine grained distinctions that are listed in manually created dictionaries such as the Longman Dictionary of Contemporary English ( LDOCE ) , or in lexical databases such as WordNet . \n\t', '\n\t\t For future improvement of the algorithm we see two main possibilities : 1 ) Considering all context words instead of only the top 30 associations would further reduce the sparse data problem . \n\t', '\n\t\t However , this requires finding an appropriate association function . \n\t', '\n\t\t This is difficult , as for example the log-likelihood ratio , although delivering almost perfect rankings , has an inappropriate value characteristic : The increase in computed strengths is over-proportional for stronger associations . \n\t', '\n\t\t This prevents the SVD from finding optimal dimensions . \n\t', '\n\t\t 2 ) The principle of avoiding mixtures can be applied more consequently if not only local instead of global vectors are used , but if also the parts of speech of the context words are considered . \n\t', '\n\t\t By operating on a part-of-speech tagged corpus those sense distinctions that have an effect on part of speech can be taken into account . \n\t', '\n\t\t Acknowledgements I would like to thank Manfred Wettler , Robert Dale , Hinrich Schütze , and Raz Tamir for help and discussions , and the DFG for financial support . \n\t', '\n\t\t References Neill , D. B. ( 2002 ) . \n\t', '\n\t\t Fully Automatic Word Sense Induction by Semantic Clustering . \n\t', '\n\t\t Cambridge University , Master\x92s Thesis , M.Phil . \n\t', '\n\t\t in Computer Speech . \n\t', '\n\t\t Pantel , P. ; Lin , D. ( 2002 ) . \n\t', '\n\t\t Discovering word senses from text . \n\t', '\n\t\t In : Proceedings of ACM SIGKDD , Edmonton , 613\x96619 . \n\t', '\n\t\t Rapp , R. ( 2002 ) . \n\t', '\n\t\t The computation of word associations : comparing syntagmatic and paradigmatic approaches . \n\t', '\n\t\t Proc . \n\t', '\n\t\t of 19th COLING , Taipei , ROC , Vol. 2 , 821\x96827 . \n\t', '\n\t\t Rapp , R. ( 2003 ) . \n\t', '\n\t\t Word sense discovery based on sense descriptor dissimilarity . \n\t', '\n\t\t In : Ninth Machine Translation Summit , New Orleans , 315\x96322 . \n\t', '\n\t\t Schütze , H. ( 1997 ) . \n\t', '\n\t\t Ambiguity Resolution in Language Learning : Computational and Cognitive Models . \n\t', '\n\t\t Stanford : CSLI Publications . \n\t', '\n\t\t Yarowsky , D. ( 1995 ) . \n\t', '\n\t\t Unsupervised word sense disambiguation rivaling supervised methods . \n\t', '\n\t\t In : Proc . \n\t', '\n\t\t of 33rd ACL , Cambridge , MA , 189\x96196 . \n\t', '\n\t\t Figure 1 : Clustering results for palm without SVD . \n\t', '\n\t\t Figure 2 : Clustering results for palm with SVD . \n\t', '\n\t\t Figure 3 : Clustering results for poach with SVD . \n\t', '\n\t\t Automatic clustering of collocation for detecting practical sense boundary Saim Shin Key-Sun Choi KAIST KAIST KorTerm KorTerm BOLA BOLA miror@world.kaist.ac.kr kschoi@world.kaist.ac.kr Abstract This paper talks about the deciding practical sense boundary of homonymous words . \n\t', '\n\t\t The important problem in dictionaries or thesauri is the confusion of the sense boundary by each resource . \n\t', '\n\t\t This also becomes a bottleneck in the practical language processing systems . \n\t', '\n\t\t This paper proposes the method about discovering sense boundary using the collocation from the large corpora and the clustering methods . \n\t', '\n\t\t In the experiments , the proposed methods show the similar results with the sense boundary from a corpus-based dictionary and sense-tagged corpus . \n\t', '\n\t\t 1 Introduction There are three types of sense boundary confusion for the homonyms in the existing dictionaries . \n\t', '\n\t\t One is sense boundaries\x92 overlapping : two senses are overlapped from some semantic features . \n\t', '\n\t\t Second , some senses in the dictionary are null ( or non-existing ) in the used corpora . \n\t', '\n\t\t Conversely , we have to generate more senses depending on the corpora , and we define these senses with practical senses . \n\t', '\n\t\t Our goal in this study is to revise sense boundary in the existing dictionaries with practical senses from the large- scaled corpus . \n\t', '\n\t\t The collocation from the large-scaled corpus contains semantic information . \n\t', '\n\t\t The collocation for ambiguous words also contains semantic information about multiple senses for this ambiguous word . \n\t', '\n\t\t This paper uses the ambiguity of collocation for the homonyms . \n\t', '\n\t\t With the clustering algorithms , we extract practical sense boundary from the collocations . \n\t', '\n\t\t This paper explains the collocation ambiguity in chapter 2 , defines the extracted collocation and proposes the used clustering methods and the labeling algorithms in chapter 3 . \n\t', '\n\t\t After explaining the experimental results in chapter 4 , this paper comes to the conclusion in chapter 5. 2 Collocation and Senses 2.1 Impractical senses in dictionary In \n\t\t']",Positive
"['\n\t\t Some senses in the manual dictionary don\x92t appear in the corpus . \n\t', '\n\t\t This situation means that there exist differences between the senses in the manual dictionaries and practical senses from corpus . \n\t', '\n\t\t These differences make problems in developing word sense disambiguation systems and applying semantic information to language processing applications . \n\t', '\n\t\t The senses in the corpus are continuously changed . \n\t', '\n\t\t In order to reflect these changes , we must analyze corpus continuously . \n\t', '\n\t\t This paper discusses about the analyzing method in order to detect practical senses using the collocation . \n\t', '\n\t\t 2.2 Homonymous collocation The words in the collocation also have their collocation . \n\t', '\n\t\t A target word for collocation is called the \x91central word\x92 , and a word in a collocation is referred to as the \x91contextual word\x92 . \n\t', '\n\t\t \x91Surrounding words\x92 mean the collocation for all contextual words . \n\t', '\n\t\t The assumption for extracting sense boundary is like this : the contextual words used in the same sense of the central word show the similar pattern of context . \n\t', '\n\t\t If collocation patterns between contextual words are similar , it means that the contextual words are used in a similar context - where used and interrelated in same sense of the central word - in the sentence . \n\t', '\n\t\t If contextual words are clustered according to the similarity in collocations , contextual words for homonymous central words can be classified according to the senses of the central words . \n\t', '\n\t\t \n\t\t']",Positive
"['\n\t\t A collocation of the central word x , window size w and corpus c is expressed with function f : V N C 4 2PC/ V . \n\t', '\n\t\t In this formula , V means a set of vocabulary , N is the size of the contextual window that is an integer , and C means a set of corpus . \n\t', '\n\t\t In this paper , vocabulary refers to all content words in the corpus . \n\t', '\n\t\t Function f shows all collocations . \n\t', '\n\t\t C/V means that C is limited to V as well as that all vocabularies are selected from a given corpus and 2PC/VP is all sets of C/V . \n\t', '\n\t\t In the equation ( 1 ) , the frequency of x is m in c . \n\t', '\n\t\t We can also express m=|c/x| . \n\t', '\n\t\t The window size of a collocation is 2w+1 . \n\t', '\n\t\t g(x)={(x,;),;^ Ix } is a word sense assignment function that gives the word senses numbered i of the word x. Ix is the word sense indexing function of x that gives an index to each sense of the word x . \n\t', '\n\t\t All contextual words x;±j of a central word x have their own contextual words in their collocation , and they also have multiple senses . \n\t', '\n\t\t This problem is expressed by the combination of g and f as follows : h4 . \n\t', '\n\t\t ( go In this paper , the problem is that the collocation of the central word is ordered according to word senses . \n\t', '\n\t\t Figure 1 show the overall process for this purpose . \n\t', '\n\t\t Figure 1 Processing for detecting sense boundary hd ; ( ( , ) ) m num x d = ; D d d d = { , ... , , ... , 0 ; n Sx = { sx0,sx1 , ... , xxm 3 Automatic clusteringofcollocation Forextracting practical senses , the contextual words foracentral word are clustered byanalyzing the pattern ofthe surroundingwords . \n\t', '\n\t\t Withthis method , we cangetthe collocationwithoutsense ambiguity , andalso discoverthe practical sense boundary . \n\t', '\n\t\t Inorderto extractthe correct sense boundary fromthe clustering phase , itneeds to remove the noise andtrivial collocation . \n\t', '\n\t\t We call this process normalization , anditis specifically provided as [ 8 ] . \n\t', '\n\t\t The statistically unrelatedwords can be said that the words with highfrequency appearregardless of theirsemantic features . \n\t', '\n\t\t Afterdeciding the statistically unrelatedwords by calculating tf·idf values , we filteredthemfromthe ori ginal surrounding words . \n\t', '\n\t\t The second normalization is usingLSI(Latent Semantic Indexing ) . \n\t', '\n\t\t Throughout the LSI transformation , we can remove the dimension ofthe contextvectoran d express the hidden features into the surface of the context vector . \n\t', '\n\t\t 3.1 Discovering senseboundary We discoveredthe senses ofthe homonyms with clustering the normalizedcollocation . \n\t', '\n\t\t The clustering classifies the contextual words having similarcontext \x96 the contextual words having similarpatternofsurroundingwords - into same cluster . \n\t', '\n\t\t Extracted clusters throughout the clustering symbolize the senses forthe central words and theircollocation . \n\t', '\n\t\t In orderto extractclusters , we used several clustering algorithms . \n\t', '\n\t\t Followings are the usedclustering methods : \x95 K-means clustering ( K ) \n\t\t']",Positive
"['\n\t\t In all clustering methods , usedsimilaritymeasure is the cosine similaritybetween two sense vectors for each contextual word . \n\t', '\n\t\t We extracted clusters with these clustering methods , tried to compare theirdiscoveredsenses andthe manually distributed senses . \n\t', '\n\t\t 3.2 Decidingfinalsense boundary Afterclustering the normalized collocation , we combined all clusteri ng results and decided the optimal sense boundary for a central word . \n\t', '\n\t\t ( g f(x,w,c))=Sxd={h l d1,...,h m d}(2) Inequation(2) , we define equation ( 1 ) as Sxd ; , this means extractedsense boundary foracentral word xwith d ; . \n\t', '\n\t\t The elements ofDare the applied clustering methods , andSxis the final combination results ofall clusteri ng methods for x. 1 M1 and M2 have differenttranslatingmethods between contextan d graph . \n\t', '\n\t\t 2 F1and F2 are different methods deciding initial centers . \n\t', '\n\t\t g0 x 1(x,1),gK),...g( , 1 h w ) , ... , ( ) , ( , ) , ( ) , ... , ( g x x I g x g x ^ + + 1 1 w h x h h m m m gx^ ( h m f(x , w,0 ) ^ ^ ^ ^ ^ ( 1 ) ^ ^ ^ ^ ^ } } This paper proposes the voting of applied clustering methods when decides final sense boundary like equation ( 3 ) . \n\t', '\n\t\t Num(x) = max { num(w , d ; ) } = Sx ( 3 ) d;^D We determined the number of the final sense boundary for each central word with the number of clusters that the most clustering algorithms were extracted . \n\t', '\n\t\t After deciding the final number of senses , we mapped clusters between clustering methods . \n\t', '\n\t\t By comparing the agreement , the pairs of the maximum agreement are looked upon the same clusters expressing the same sense , and agreement is calculated like equation ( 4 ) , which is the agreement between k-th cluster with ;-th clustering method and l-th cluster with j-th clustering method for central word x . \n\t', '\n\t\t The final step is the assigning elements into the final clusters . \n\t', '\n\t\t In equation ( 5 ) , all contextual words w are classified into the maximum results of clustering methods . \n\t', '\n\t\t New centers of each cluster are recalculated with the equation ( 6 ) based on the final clusters and their elements . \n\t', '\n\t\t Figure 2 represents the clustering result for the central word \x91chair\x92 . \n\t', '\n\t\t The pink box shows the central word \x91chair\x92 and the white boxes show the selected contextual words . \n\t', '\n\t\t The white and blue area means the each clusters separated by the clustering methods . \n\t', '\n\t\t The central word \x91chair\x92 finally makes two clusters . \n\t', '\n\t\t The one located in blue area contains the collocation for the sense about \x91the position of professor\x92 . \n\t', '\n\t\t Another cluster in the white area is the cluster for the sense about \x91furniture\x92 . \n\t', '\n\t\t The words in each cluster are the representative contextual words which similarity is included in ranking 10 . \n\t', '\n\t\t 4 Experimental results We extracted sense clusters with the proposed methods from the large-scaled corpus , and compared the results with the sense distribution of the existing thesaurus . \n\t', '\n\t\t Applied corpus for the experiments for English and Korean is Penn tree bank3 corpus and KAIST4 corpus . \n\t', ""\n\t\t 3 http://www.cis.upenn.edu/\x97treebank/home.html 4 http://kibs.kaist.ac.kr Figure 2 The clustering example for ' chair ' For evaluation , we try to compare clustering results and sense distribution of dictionary . \n\t"", '\n\t\t In case of English , used dictionary is WordNet 1.75 - Fine- grained ( WF ) and coarse-grained distribution ( WC ) . \n\t', '\n\t\t The coarse-grained senses in WordNet are adjusted sense based on corpus for SENSEVAL task . \n\t', '\n\t\t In order to evaluate the practical word sense disambiguation systems , the senses in the WordNet 1.7 are adjusted by the analyzing the appearing senses from the Semcor . \n\t', '\n\t\t For the evaluation of Korean we used Korean Unabridged Dictionary ( KD ) for fine-grained senses and Yonsei Dictionary ( YD ) for corpus-based senses . \n\t', '\n\t\t Table 1 shows the clustering results by each clustering algorithms . \n\t', '\n\t\t The used central words are 786 target homonyms for the English lexical samples in SENSEVAL26 . \n\t', '\n\t\t The numbers in Table 1 shows the average number of clusters with each clustering method shown chapter 3 by the part of speech . \n\t', '\n\t\t WC and WF are the average number of senses by the part of speech . \n\t', '\n\t\t In Table 1 and 2 , the most clustering methods show the similar results . \n\t', '\n\t\t But , CBC extracts more clusters comparing other clustering methods . \n\t', '\n\t\t Except CBC other methods extract similar sense distribution with the Coarse-grained WordNet ( WC ) . \n\t', '\n\t\t Nouns Adjectives Verbs All K 3 3.046 3.039 3.027 B 3.258 3.218 3.286 3.266 CBC 6.998 3.228 5.008 5.052 F 1 3.917 2.294 3.645 3.515 F2 4.038 5.046 3.656 4.013 Final 3.141 3.08 3.114 3.13 WC 3.261 2.887 3.366 3.252 WF 8.935 8.603 9.422 9.129 Table 1 The results of English { }U{hldj} Vot(Sx,w)max{hd;(go f ( x , w , c ^V d;^D 5 http://www.cogsci.princeton.edu/\x97wn/ 6 http://www.cs.unt.edu/\x97rada/senseval/ ) ) } xk { }I{hIx} h kd h kd agreement ( 4 ) ( 5 ) Nn 1zrSx=(^wa1,1 N ^wa2,...,1N^ Wan )(6) N n n Nouns Nouns K B C 5.5 F1 F2 M1 2.917 2.917 M2 2.833 2.583 4.083 KD YD 3.833 11.25 3.333 Table 2 The results of Korean Table 3 is the evaluating the correctness of the elements of cluster . \n\t', '\n\t\t Using the sense-tagged collocation from English test suit in SENSEVAL27 , we calculated the average agreement for all central words by each clustering algorithms . \n\t', '\n\t\t K B C F1 F2 98.666 98.578 90.91 97.316 88.333 Table 3 The average agreement by clustering methods As shown in Table 3 , overall clustering methods record high agreement . \n\t', '\n\t\t Among the various clustering algorithms , the results of K-means and buckshot are higher than other algorithms . \n\t', '\n\t\t In the K-means and fuzzy clustering , the deciding random initial shows higher agreements . \n\t', '\n\t\t But , clustering time in hierarchical deciding is faster than random deciding 5 Conclusion This paper proposes the method for boundary discovery of homonymous senses . \n\t', '\n\t\t In order to extract practical senses from corpus , we use the collocation from the large corpora and the clustering methods . \n\t', '\n\t\t In these experiments , the results of the proposed methods are different from the fine-grained sense distribution - manually analyzed by the experts . \n\t', '\n\t\t But the results are similar to the coarse-grained results \x96 corpus-based sense distribution . \n\t', '\n\t\t Therefore , these experimental results prove that we can extract practical sense distribution using the proposed methods . \n\t', '\n\t\t For the conclusion , the proposed methods show the similar results with the corpus-based sense boundary . \n\t', '\n\t\t For the future works , using this result , it\x92ll be possible to combine these results with the practical thesaurus automatically . \n\t', '\n\t\t The proposed method can apply in the evaluation and tuning process for existing senses . \n\t', '\n\t\t So , if overall research is successfully processed , we can get a automatic mechanism about adjusting and constructing knowledge base like thesaurus which is practical and containing enough knowledge from corpus . \n\t', '\n\t\t There are some related works about this research . \n\t', '\n\t\t Wortchartz is the collocation dictionary with the assumption that Collocation of a word expresses English lexical sample for the same central words he meaning of the word \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t 6 Acknowledgements This work has been supported by Ministry of Science and Technology in Korea . \n\t', '\n\t\t The result of this work is enhanced and distributed through Bank of Language Resources supported by grant No . \n\t', '\n\t\t R21-2003-000-10042-0 from Korea Science & Technology Foundation . \n\t', '\n\t\t References Ray S. and Turi R.H. 1999 . \n\t', '\n\t\t Determination of Number of Clusters in K-means Clustering and Application in Colour Image Segmentation , In \x93The 4th International Conference on Advances in Pattern Recognition and Digital Techniques\x94 , Calcuta . \n\t', '\n\t\t Heyer G. , Quasthoff U. and Wolff C. 2001 . \n\t', '\n\t\t Information Extraction from Text Corpora , In \x93IEEE Intelligent Systems and Their Applications\x94 , Volume 16 , No. 2 . \n\t', '\n\t\t Patrick Pantel and Dekang Lin . \n\t', '\n\t\t 2002 . \n\t', '\n\t\t Discovering Word Senses from Text , In \x93ACM Conference on Knowledge Discovery and Data Mining\x94 , pages 613\x96619 , Edmonton . \n\t', '\n\t\t Hyungsuk Ji , Sabine Ploux and Eric Wehrli . \n\t', '\n\t\t 2003 , Lexical Knowledge Representation with Contexonyms , In \x93The 9th Machine Translation\x94 , pages 194-201 , New Orleans Eric C.Jensen , Steven M.Beitzel , Angelo J.Pilotto , Nazli Goharian , Ophir Frieder . \n\t', '\n\t\t 2002 , Parallelizing the Buckshot Algorithm for Efficient Document Clustering , In \x93The 2002 ACM International Conference on Information and Knowledge Management , pages 04-09 , McLean , Virginia , USA . \n\t', '\n\t\t Stijn van Dongen . \n\t', '\n\t\t 2000 , A cluster algorithm for graphs , In \x93Technical Report INS-R0010\x94 , National Research Institute for Mathematics and Computer Science in the Netherlands . \n\t', '\n\t\t Song D. , Cao G. , and Bruza P.D. 2003 , Fuzzy K- means Clustering in Information Retrieval , In \x93DSTC Technical Report\x94 . \n\t', '\n\t\t 7 Saim Shin and Key-Sun Choi . \n\t', '\n\t\t 2004 , Automatic Word Sense Clustering using Collocation for Sense Adaptation , In \x93Global WordNet conference\x94 , pages 320-325 , Brno , Czech . \n\t', '\n\t\t Co-training for Predicting Emotions with Spoken Dialogue Data Beatriz Maeireizo and Diane Litman and Rebecca Hwa Department of Computer Science University of Pittsburgh Pittsburgh , PA 15260 , U.S.A. beamt@cs.pitt.edu , litman@cs.pitt.edu , hwa@cs.pitt.edu Abstract Natural Language Processing applications often require large amounts of annotated training data , which are expensive to obtain . \n\t', '\n\t\t In this paper we investigate the applicability of Co-training to train classifiers that predict emotions in spoken dialogues . \n\t', '\n\t\t In order to do so , we have first applied the wrapper approach with Forward Selection and Naïve Bayes , to reduce the dimensionality of our feature set . \n\t', '\n\t\t Our results show that Co-training can be highly effective when a good set of features are chosen . \n\t', '\n\t\t 1 Introduction In this paper we investigate the automatic labeling of spoken dialogue data , in order to train a classifier that predicts students\x92 emotional states in a human-human speech-based tutoring corpus . \n\t', '\n\t\t Supervised training of classifiers requires annotated data , which demands costly efforts from human annotators . \n\t', '\n\t\t One approach to minimize this effort is to use Co-training \n\t\t']",Positive
"['\n\t\t The main focus of this paper is to explore how Co- training can be applied to annotate spoken dialogues . \n\t', '\n\t\t A major challenge to address is in reducing the dimensionality of the many features available to the learners . \n\t', '\n\t\t The motivation for our research arises from the need to annotate a human-human speech corpus for the ITSPOKE ( Intelligent Tutoring SPOKEn dialogue System ) project \n\t\t']",Positive
"['\n\t\t Ongoing research in ITSPOKE aims to recognize emotional states of students in order to build a spoken dialogue tutoring system that automatically predicts and adapts to the student\x92s emotions . \n\t', '\n\t\t ITSPOKE uses supervised learning to predict emotions with spoken dialogue data . \n\t', '\n\t\t Although a large set of dialogues have been collected , only 8 % of them have been annotated ( 10 dialogues with a total of 350 utterances ) , due to the laborious annotation process . \n\t', '\n\t\t We believe that increasing the size of the training set with more annotated examples will increase the accuracy of the system\x92s predictions . \n\t', '\n\t\t Therefore , we are looking for a less labour-intensive approach to data annotation . \n\t', '\n\t\t 2 Data Our data consists of the student turns in a set of 10 spoken dialogues randomly selected from a corpus of 128 qualitative physics tutoring dialogues between a human tutor and University of Pittsburgh undergraduates . \n\t', '\n\t\t Prior to our study , the 453 student turns in these 10 dialogues were manually labeled by two annotators as either "" Emotional "" or "" Non-Emotional "" \n\t\t']",Positive
"['\n\t\t Perceived student emotions ( e.g. confidence , confusion , boredom , irritation , etc. ) were coded based on both what the student said and how he or she said it . \n\t', '\n\t\t For this study , we use only the 350 turns where both annotators agreed on the emotion label . \n\t', '\n\t\t 51.71 % of these turns were labeled as Non-Emotional and the rest as Emotional . \n\t', '\n\t\t Also prior to our study , each annotated turn was represented as a vector of 449 features hypothesized to be relevant for emotion prediction \n\t\t']",Positive
"['\n\t\t The features represent acoustic-prosodic ( pitch , amplitude , temporal ) , lexical , and other linguistic characteristics of both the turn and its local and global dialogue context . \n\t', '\n\t\t 3 Machine Learning Techniques In this section , we will briefly describe the machine learning techniques used by our system . \n\t', '\n\t\t 3.1 Co-training To address the challenge of training classifiers when only a small set of labeled examples is available , \n\t\t']",Positive
"['\n\t\t Under this framework , two ( or more ) learners are trained iteratively in tandem . \n\t', '\n\t\t In each iteration , the learners classify more unlabeled data to increase the training data for each other . \n\t', '\n\t\t In theory , the learners must have distinct views of the data ( i.e. , their features are conditionally independent given the label example ) , but some studies suggest that Co- training can still be helpful even when the independence assumption does not hold \n\t\t']",Positive
"['\n\t\t To apply Co-training to our task , we develop two high-precision learners : Emotional and Non- Emotional . \n\t', '\n\t\t The learners use different features because each is maximizing the precision of its label ( possibly with low recall ) . \n\t', '\n\t\t While we have not proved these two learners are conditionally independent , this division of expertise ensures that the learners are different . \n\t', '\n\t\t The algorithm for our Co-training system is shown in Figure 1 . \n\t', '\n\t\t Each learner selects the examples whose predicted labeled corresponds to its expertise class with the highest confidence . \n\t', '\n\t\t The maximum number of iterations and the number of examples added per iteration are parameters of the system . \n\t', '\n\t\t While iteration < MAXITERATION Emo_Learner.Train(train) NE_Learner.Train(train) emo_Predictions = Emo_Learner.Predict(predict) ne_Predictions = NE_Learner.Predict(predict) emo_ sorted _Predictions = Sort_by_confidence( emo_Predictions ) ne_sorted_Predictions = Sort_by_confidence( ne_Predictions ) best_emo = Emo_Learner.select_best( emo_sorted_Predictions , NUM _SAMPLES_TO_ADD ) best_ne = NE_Learner.select_best( ne_ sorted _Predictions , NUM _SAMPLES_TO_ADD ) train = train U best_emo U best_ne predict = predict \x96 best_emo \x96- best_ne end Figure 1. Algorithm for Co-training System 3.2 Wrapper Approach with Forward Selection As described in Section 2 , 449 features have been currently extracted from each utterance of the ITSPOKE corpus ( where an utterance is a student\x92s turn in a dialogue ) . \n\t', '\n\t\t Unfortunately , high dimensionality , i.e. large amount of input features , may lead to a large variance of estimates , noise , overfitting , and in general , higher complexity and inefficiencies in the learners . \n\t', '\n\t\t Different approaches have been proposed to address this problem . \n\t', '\n\t\t In this work , we have used the Wrapper Approach with Forward Selection . \n\t', '\n\t\t The Wrapper Approach , introduced by \n\t\t']",Positive
"['\n\t\t We can apply different search algorithms to find this set of features . \n\t', '\n\t\t Forward Selection is a greedy search algorithm that begins with an empty set of features , and greedily adds features to the set . \n\t', '\n\t\t Figure 2 shows our algorithm implemented for the forward wrapper approach . \n\t', '\n\t\t bestFeatures = [ ] while dim(bestFeatures) < MINFEATURES for iterations = 1 : MAXITERATIONS split train into training/development parameters = computeParameters(training) for feature = 1:MAXFEATURES evaluate(parameters,development , [ bestFeatures + feature ] ) keep validation performance end end average_performance and keep average_performance end B = best average_performance bestFeatures F B U bestFeatures end Figure 2 . \n\t', '\n\t\t Implemented algorithm for forward wrapper approach . \n\t', '\n\t\t The variables underlined are the ones whose parameters we have changed in order to test and improve the performance . \n\t', '\n\t\t We can use different criteria to select the feature to add , depending on the object of optimization . \n\t', '\n\t\t Earlier , we have explained the basis of the Co- training system . \n\t', '\n\t\t When developing an expert learner in one class , we want it to be correct most of the time when it guesses that class . \n\t', '\n\t\t That is , we want the classifier to have high precision ( possibly at the cost of lower overall accuracy ) . \n\t', '\n\t\t Therefore , we are interested in finding the best set of features for precision in each class . \n\t', '\n\t\t In this case , we are focusing on Emotional and Non-Emotional classifiers . \n\t', '\n\t\t Figure 3 shows the formulas used for the optimization criterion on each class . \n\t', '\n\t\t For the Emotional Class , our optimization criterion was to maximize the PPV ( Positive Predictive Value ) , and for the Non-Emotional Class our optimization criterion was to maximize the NPV ( Negative Predictive Value ) . \n\t', '\n\t\t Figure 3 . \n\t', '\n\t\t Confusion Matrix , Positive Predictive Value ( Precision for Emotional ) and Negative Predictive Value ( Precision for Non-Emotional ) 4 Experiments For the following experiments , we fixed the size of our training set to 175 examples ( 50 % ) , and the size of our test set to 140 examples ( 40 % ) . \n\t', '\n\t\t The remaining 10 % has been saved for later experiments . \n\t', '\n\t\t 4.1 Selecting the features The first task was to reduce the dimensionality and find the best set of features for maximizing the PPV for Emotional class and NPV for Non- Emotional class . \n\t', '\n\t\t We applied the Wrapper Approach with Forward Selection as described in section 3.2 , using Naïve Bayes to evaluate each subset of features . \n\t', '\n\t\t We have used 175 examples for the training set ( used to select the best features ) and 140 for the test set ( used to measure the performance ) . \n\t', '\n\t\t The training set is randomly divided into two sets in each iteration of the algorithm : One for training and the other for development ( 65 % and 35 % respectively ) . \n\t', '\n\t\t We train the learners with the training set and we evaluate the performance to pick the best feature with the development set . \n\t', '\n\t\t Number of Features Naïve Bayes AdaBoost -j48 Decision Trees All Features 74.5 % 83.1 % 3 best for PPV 92.9 % 92.9 % Table 1 . \n\t', '\n\t\t Precision of Emotional with all features and 3 best features for PPV using Naïve Bayes ( used for Feature Selection ) and AdaBoost -j48 Decision Trees ( used for Co-training ) The selected features that gave the best PPV for Emotional Class are 2 lexical features and one acoustic-prosodic feature . \n\t', '\n\t\t By using them we increased the precision of Naïve Bayes from 74.5 % ( using all 449 features ) to 92.9 % , and of AdaBoost -j48 Decision Trees from 83.1 % to 92.9 % ( see Table 1 ) . \n\t', '\n\t\t Number of Features Naïve Bayes AdaBoost -j48 Decision Trees All Features 74.2 % 90.7 % 1 best for NPV 100.0 % 100.0 % Table 2 . \n\t', '\n\t\t Precision of Non-Emotional with all features and best feature for NPV using Naïve Bayes ( used for Feature Selection ) and AdaBoost- j48 Decision Trees ( used for Co-training ) For the Non-Emotional Class , we increased the NPV of Naïve Bayes from 74.2 % ( with all features ) to 100 % just by using one lexical feature , and the NPV of AdaBoost -j48 Decision Trees from 90.7 % to 100 % . \n\t', '\n\t\t This precision remained the same with the set of 3 best features , one lexical and two non-acoustic prosodic features ( see Table 2 ) . \n\t', '\n\t\t These two set of features for each learner are disjoint . \n\t', '\n\t\t 4.2 Co-training experiments The two learners are initialized with only 6 labeled examples in the training set . \n\t', '\n\t\t The Co- training system added examples from the 140 \x93pseudo-labeled\x94 examples1 in the Prediction Set . \n\t', '\n\t\t The size of the training set increased in each iteration by adding the 2 best examples ( those with the highest confidence scores ) labeled by the two learners . \n\t', '\n\t\t The Emotional learner and the Non- Emotional learner were set to work with the set of features selected by the wrapper approach to optimize the precision ( PPV and NPV ) as described in section 4.1 . \n\t', '\n\t\t We have applied Weka\x92s \n\t\t']",Positive
"['\n\t\t Figure 4 illustrates the learning curve of the accuracy on the test set , taking the union of the set of features selected to label the examples . \n\t', '\n\t\t We used the 3 best features for PPV for the Emotional Learner and the best feature for NPV for the Non- Emotional Learner ( see Section 4.1 ) . \n\t', '\n\t\t The x-axis shows the number of training examples added ; the y-axis shows the accuracy of the classifier on test instances . \n\t', '\n\t\t We compare the learning curve from Co-training with a baseline of majority class and an upper-bound , in which the classifiers are trained on human-annotated data . \n\t', '\n\t\t Post-hoc analyses reveal that four incorrectly labeled examples were added to the training set : example numbers 21 , 22 , 45 , and 51 ( see the x-axis ) . \n\t', '\n\t\t Shortly after the inclusion of example 21 , the Co-training learning curve diverges from the upper-bound . \n\t', '\n\t\t All of them correspond to Non-Emotional examples that were labeled as Emotional by the Emotional learner with the highest confidence . \n\t', '\n\t\t The Co-training system stopped after adding 58 examples to the initial 6 in the training set because the remaining data cannot be labeled by the learners with high precision . \n\t', '\n\t\t However , as we can see , the training set generated by the Co-training technique can perform almost as well as the upper- bound , even if incorrectly labeled examples are included in the training set . \n\t', '\n\t\t 1 This means that although the example has been labeled , the label remains unseen to the learners . \n\t', '\n\t\t Figure 4 . \n\t', '\n\t\t Learning Curve of Accuracy using best features for Precision of Emotional/Non-Emotional Learning Curve - Accuracy ( features for Emotional/Non-Emotional Precision ) 1 7 13 19 25 31 37 43 49 55 61 67 73 79 85 91 97 103 109 115 121 127 133 139 145 151 157 163 169 175 Majority Class Cotrain Upper-bound 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 5 Conclusion We have shown Co-training to be a promising approach for predicting emotions with spoken dialogue data . \n\t', '\n\t\t We have given an algorithm that increased the size of the training set producing even better accuracy than the manually labeled training set , until it fell behind due to its inability to add more than 58 examples . \n\t', '\n\t\t We have shown the positive effect of selecting a good set of features optimizing precision for each learner and we have shown that the features can be identified with the Wrapper Approach . \n\t', '\n\t\t In the future , we will verify the generalization of our results to other partitions of our data . \n\t', '\n\t\t We will also try to address the limitation of noise in our Co-training System , and generalize our solution to a corresponding corpus of human- computer data \n\t\t']",Positive
"['\n\t\t We will also conduct experiments comparing Co- training with other semi-supervised approaches such as self-training and Active learning . \n\t', '\n\t\t 6 Acknowledgements Thanks to R. Pelikan , T. Singliar and M. Hauskrecht for their contribution with Feature Selection , and to the NLP group at University of Pittsburgh for their helpful comments . \n\t', '\n\t\t This research is partially supported by NSF Grant No. 0328431 . \n\t', '\n\t\t References A. Blum and T. Mitchell . \n\t', '\n\t\t 1998. Combining Labeled and Unlabeled Data with Co-training . \n\t', '\n\t\t Proceedings of the 11th Annual Conference on Computational Learning Theory : 92-100 . \n\t', '\n\t\t K. Forbes-Riley and D. Litman . \n\t', '\n\t\t 2004. Predicting Emotion in Spoken Dialogue from Multiple Knowledge Sources . \n\t', '\n\t\t Proceedings of Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics ( HLT/NAACL ) . \n\t', '\n\t\t S. Goldman and Y. Zhou . \n\t', '\n\t\t 2000. Enhancing Supervised Learning with Unlabeled Data . \n\t', '\n\t\t International Joint Conference on Machine Learning , 2000 . \n\t', '\n\t\t G. H. John , R. Kohavi and K. Pleger . \n\t', '\n\t\t 1994. Irrelevant Features and the Subset Selection Problem . \n\t', '\n\t\t Machine Learning : Proceedings of 11th International Conference : 121-129 , Morgan Kaufmann Publishers , San Francisco , CA . \n\t', '\n\t\t R. Kohavi and G. H. John . \n\t', '\n\t\t 1997. Wrappers for Feature Subset Selection . \n\t', '\n\t\t Artificial Intelligence , Volume 97 , Issue 1-2 . \n\t', '\n\t\t D. J. Litman and K. Forbes-Riley , 2004 . \n\t', '\n\t\t Annotating Student Emotional States in Spoken Tutoring Dialogues . \n\t', '\n\t\t Proc . \n\t', '\n\t\t 5th Special Interest Group on Discourse and Dialogue Workshop on Discourse and Dialogue ( SIGdial ) . \n\t', '\n\t\t D. J. Litman and S. Silliman , 2004 . \n\t', '\n\t\t ITSPOKE : An Intelligent Tutoring Spoken Dialogue System . \n\t', '\n\t\t Companion Proceedings of Human Language Technology conf . \n\t', '\n\t\t of the North American Chapter of the Association for Computational Linguistics ( HLT/NAACL ) . \n\t', '\n\t\t I. H. Witten and E. Frank . \n\t', '\n\t\t 2000. Data Mining : Practical Machine Learning Tools and Techniques with Java implementations . \n\t', '\n\t\t Morgan Kaufmann , San Francisco . \n\t', '\n\t\t Multimodal Database Access on Handheld Devices Elsa Pecourt and Norbert Reithinger DFKI GmbH Stuhlsatzenhausenweg3 D-66123 Saarbr¨ucken , Germany {pecourt,reithinger}@dfki.de Abstract We present the final MIAMM system , a multimodal dialogue system that employs speech , haptic interaction and novel techniques of information visualization to allow a natural and fast access to large multimedia databases on small handheld devices . \n\t', '\n\t\t 1 Introduction Navigation in large , complex and multidimensional information spaces is still a challenging task . \n\t', '\n\t\t The search is even more difficult in small devices such as MP3 players , which only have a reduced screen and lack of a proper keyboard . \n\t', ""\n\t\t In the MIAMM project ' we have developed a multimodal dialogue system that uses speech , haptic interaction and advanced techniques for information visualization to allow a natural and fast access to music databases on small scale devices . \n\t"", '\n\t\t The user can pose queries in natural language , using different dimensions , e.g. release year , genre , artist , or mood . \n\t', '\n\t\t The retrieved data are presented along this dimensions using various visualization metaphors . \n\t', '\n\t\t Haptic feedback allows the user to feel the size , density and structure of the visualized data to facilitate the navigation . \n\t', '\n\t\t All modalities are available for the user to access and navigate through the database , and to select titles to be played . \n\t', '\n\t\t The envisioned end-user device is a handheld Personal Digital Assistant ( PDA , see figure 1 ) that provides an interface to a music database . \n\t', '\n\t\t The device includes a screen where data and system messages are visualized , three force-feedback buttons on the left side and one combined scroll wheel/button on the upper right side , that can be used to navigate on the visualized data , as well as to perform actions on the data items ( e.g. play or select a song ) , a microphone to capture spoken input , and speakers to give audio output . \n\t', '\n\t\t Since we do not develop the hardware , we simulate the PDA using a 3D model on a computer screen , and the buttons 1http://www.miamm.org Figure 1 : The PDA simulator with the terrain visualization of the database by means of Phantom devices that allow the user to touch and manipulate virtual objects . \n\t', '\n\t\t In the rest of this paper , we will first give an overview of the visualization metaphors , the MIAMM architecture , and a short description of its interface language . \n\t', '\n\t\t Then we will demonstrate its functionality using an example dialogue . \n\t', '\n\t\t For more details on the MIAMM system and its components see \n\t\t']",Positive
"['\n\t\t 2 Visualization metaphors The information from the database is presented on the device using metaphors of real world objects ( cf. conceptual spaces ( G¨ardenfors , 2000 ) ) so as to provide an intuitive handling of abstract concepts . \n\t', '\n\t\t The lexicon metaphor , shown in figure 2 to the left , presents the items alphabetically ordered in a rotary card file . \n\t', '\n\t\t Each card represents one album and contains detailed background information . \n\t', '\n\t\t The time- 2http://www. sensable.com Figure 2 : Visualizations line visualization shows the items in chronological order , on a \x93rubber\x94 band that can be stretched to get a more detailed view . \n\t', '\n\t\t The wheel metaphor presents the items as a list on a conveyor belt , which can be easily and quickly rotated . \n\t', '\n\t\t Finally , the terrain metaphor ( see figure 1 ) visualizes the entire database . \n\t', '\n\t\t The rendering is based on a three layer type hierarchy , with genre , sub-genre and title layers . \n\t', '\n\t\t Each node of the hierarchy is represented as a circle containing its daughter nodes . \n\t', '\n\t\t Similarities between the items are computed from the genre and mood information in the database and mapped to interaction forces in a physical model that groups similar items together on the terrain . \n\t', '\n\t\t Since usually albums are assigned more than one genre , they can be contained in different circles and therefore be redundantly represented on the terrain . \n\t', '\n\t\t This redundancy is made clear by lines connecting the different instances of the same item . \n\t', '\n\t\t 3 The MIAMM prototype The MIAMM system uses the standard architecture for dialogue systems with analysis and generation layers , interaction management and application interface ( see figure 3 ) . \n\t', '\n\t\t To minimize the reaction delay of haptic feedback , the visual-haptic interaction component is decoupled from other more time-consuming reasoning processes . \n\t', '\n\t\t The German experimental prototype3 incorporates the following 3There are also French and English versions of the system . \n\t', '\n\t\t The modular architecture facilitates the replacement of the language dependent modules . \n\t', '\n\t\t components , some of which were reused from other projects ( semantic parser and action planning ) : a speaker independent , continuous speech recognizer converts the spoken input in a word lattice ; it uses a 500 word vocabulary , and was trained on a automatically generated corpus . \n\t', '\n\t\t A template based semantic parser for German , see \n\t\t']",Positive
"['\n\t\t The multimodal fusion module maintains the dialogue history and handles anaphoric expressions and quantification . \n\t', '\n\t\t The action planner , an adapted and enhanced version of ( L¨ockelt , 2004 ) , uses non-linear regression planning and the notion of communicative games to trigger and control system actions . \n\t', '\n\t\t The visualhaptic interaction manager selects the appropriate visualization metaphor based on data characteristics , and maintains the visualization history . \n\t', '\n\t\t Finally , the domain model provides access to the MYSQL database , which contains 7257 records with 85722 songs by 667 artists . \n\t', '\n\t\t Speech output is done by speech prompts , both for spoken and for written output . \n\t', '\n\t\t The prototype also includes a MP3 Player to play the music and speech output files . \n\t', '\n\t\t The demonstration system requires a Linux based PC for the major parts of the modules written in Java and C++ , and a Windows NT computer for visualization and haptics . \n\t', '\n\t\t The integration environment is based on the standard Simple Object Access Protocol SOAP4 for information exchange in a distributed environment . \n\t', '\n\t\t The communication between the modules uses a declarative , XML-schema based representation lan- 4http://www.w3.org/TR/SOAP/ Audio Output MP3 Player Speech prompts Music files Microphone Speaker Speech Generation Request Player Request Visualization Request Visualization Status DIALOGUE MANAGER Multimodal Fusion Goal Representation Action Planner Audio Input Continuous Speech Semantic Interpretation Semantic Representation Recognizer Display Haptic Device Visual^Haptic Interaction Haptic Processor Visualization Visual^Haptic Interpretation Visual^Haptic Generation Domain Model Domain Model Query Response Database Response Query Database Figure 3 : MIAMM architecture guage called MMIL \n\t\t']",Positive
"['\n\t\t This interface specification accounts for the incremental integration of multimodal data to achieve a full understanding of the multimodal acts within the system . \n\t', '\n\t\t Therefore , it is flexible enough to handle the various types of information processed and generated by the different modules . \n\t', '\n\t\t It is also independent from any theoretical framework , and extensible so that further developments can be incorporated . \n\t', '\n\t\t Furthermore it is compatible with existing standardization initiatives so that it can be the source of future standardizing activities in the fields . \n\t', '\n\t\t Figure 4 shows a sample of MMIL representing the output of the speech interpretation module for the user\x92s utterance \x93Give me rock\x94 . \n\t', '\n\t\t 4 An example To sketch the functionality of the running prototype we will use a sample interaction , showing the user\x92s actions , the system\x92s textual feedback on the screen and finally the displayed information . \n\t', '\n\t\t Some of the dialogue capabilities of the MIAMM system in this example are , e.g. search history ( S2 ) , relaxation of queries ( S3b ) , and anaphora resolution ( S5 ) . \n\t', '\n\t\t At any moment of the interaction the user is allowed to navigate on the visualized items , zoom in and out for details , or change the visualization metaphor . \n\t', '\n\t\t U1 : Give me rock S1a : I am looking for rock S1b : displays a terrain with rock albums U2 : I want something calm S2a : I am looking for calm rock S2b : displays list of calm rock albums U3 : I want something from the 30\x92s S3a : I am looking for calm rock 5The data categories are expressed in a RDF format compatible with ISO 11179-3 1930-1939 S3b : I could only find albums of the adjacent years displays list of calm rock albums of the 40\x92s U4 : What about the S0\x92s S4a : I am looking for calm rock 1950-1959 S4b : displays a map with rock albums U5 : selects ALBUM with the haptic buttons Play this one S5a : Playing ALBUM S5b : MP3 player starts We will show the processing details on the basis of the first utterance in the sample interaction Give me rock . \n\t', '\n\t\t The speech recognizer converts the spoken input in a word graph in MPEG7 . \n\t', '\n\t\t The semantic parser analyzes this graph and interprets it semantically . \n\t', '\n\t\t The semantic representation consists , in this example , of a speak and a display event , with two participants , the user and music with constraints on its genre ( see figure 4 ) . \n\t', '\n\t\t The multimodal fusion module receives this representation , updates the dialogue context , and passes it on to the action planner , which defines the next goal on the basis of the propositional content of the top event ( in the example event i d 1 ) and its object ( in the example participant id3 ) . \n\t', '\n\t\t In this case the user\x92s goal cannot be directly achieved because the object to display is still unresolved . \n\t', '\n\t\t The action planner has to initiate a database query to acquire the required information . \n\t', '\n\t\t It uses the constraint on the genre of the requested object to produce a database query for the domain model and a feedback request for the visual-haptic interaction module . \n\t', '\n\t\t This feedback message ( S1a in the example ) is sent to the user while the database query is being done , providing thus implicit grounding . \n\t', '\n\t\t The do- <component> <event id=""id0""> <evtType>speak</evtType> <speaker>user</speaker> <addressee>system</addressee> <dialogueAct>request</dialogueAct> </event> <event id=""id1""> <evtType>display</evtType> </event> <participant id=""id2""> <objType>user</objType> <refType>1PPDeixis</refType> <refStatus>pending</refStatus> </participant> <participant id=""id3""> <objType>music</objType> <genre>rock</genre> <refType>indefinite</refType> <refStatus>pending</refStatus> </participant> <relation source=""id3"" target=""id1"" type=""object""/> <relation source=""id1"" target=""id0"" type=""propContent""/> </component> Figure 4 : MMIL sample main model sends the result back to the action planner who inserts the data in a visualization request . \n\t', '\n\t\t The visual-haptic interaction module computes the most suitable visualization for this data set , and sends the request to the visualization module to render it . \n\t', '\n\t\t This component also reports the actual visualization status to the multimodal fusion module . \n\t', '\n\t\t This report is used to update the dialogue context , that is needed for reference resolution . \n\t', '\n\t\t The user can now use the haptic buttons to navigate on the search results , select a title to be played or continue searching . \n\t', '\n\t\t 5 Conclusions The MIAMM final prototype combines speech with new techniques for haptic interaction and data visualization to facilitate access to multimedia databases on small handheld devices . \n\t', '\n\t\t The final evaluation of the system supports our initial hypothesis that users prefer language to select information and hap- tics to navigate in the search space . \n\t', '\n\t\t The visualizations proved to be intuitive ( van Esch and Cremers , 2004 ) . \n\t', '\n\t\t Acknowledgments This work was sponsored by the European Union ( IST-2000-29487 ) . \n\t', '\n\t\t Thanks are due to our project partners : Loria ( F ) , Sony Europe ( D ) , Canon ( UK ) , and TNO ( NL ) . \n\t', '\n\t\t References Ralf Engel . \n\t', '\n\t\t 2004. Natural language understanding . \n\t', '\n\t\t In Wolfgang Wahlster , editor , SmartKom - Foundations of Multi-modal Dialogue Systems , Cognitive Technologies . \n\t', '\n\t\t Springer Verlag ( in Press ) . \n\t', '\n\t\t Peter G¨ardenfors . \n\t', '\n\t\t 2000. Conceptual Spaces . \n\t', '\n\t\t MIT Press . \n\t', '\n\t\t Markus L¨ockelt . \n\t', '\n\t\t 2004. Action planning . \n\t', '\n\t\t In Wolfgang Wahlster , editor , SmartKom - Foundations of Multi-modal Dialogue Systems , Cognitive Technologies . \n\t', '\n\t\t Springer Verlag ( in Press ) . \n\t', '\n\t\t Norbert Reithinger , Dirk Fedeler , Ashwani Kumar , Christoph Lauer , Elsa Pecourt , and Laurent Romary . \n\t', '\n\t\t 2004. Miamm - a multimodal dialogue system using haptics . \n\t', '\n\t\t In Jan van Kuppevelt , Laila Dybkjaer , and Niels Ole Bersen , editors , Natural , Intelligent and Effective Interaction in Multi- modal Dialogue Systems . \n\t', '\n\t\t Kluwer Academic Publications . \n\t', '\n\t\t Laurent Romary and Harry Bunt . \n\t', '\n\t\t 2002. Towards multimodal content representation . \n\t', '\n\t\t In Proceedings of LREC 2002 , Workshop on International Standards of Terminology and Linguistic Resources Management , Las Palmas . \n\t', '\n\t\t Myra P. van Esch and Anita H. M. Cremers . \n\t', '\n\t\t 2004. User evaluation . \n\t', '\n\t\t MIAMM Deliverable D 1.6 . \n\t', '\n\t\t WYSiwYm with wider coverage Richard Power and Roger Evans Information Technology Research Institute University of Brighton Lewes Road Brighton BN2 4AT , UK Firstname.Lastname@itri.bton.ac.uk Abstract We describe an extension of the WYSiwYm technology for knowledge editing through natural language feedback . \n\t', '\n\t\t Previous applications have addressed relatively simple tasks requiring a very limited range of nominal and clause patterns . \n\t', '\n\t\t We show that by adding a further editing operation called reconfiguration , the technology can achieve a far wider coverage more in line with other general-purpose generators . \n\t', '\n\t\t The extension will be included in a Java-based library package for producing WYSiwYm applications . \n\t', '\n\t\t 1 Introduction WYSiwYm ( What You See Is What You Meant ) is a user-interface technology through which a domain expert can formally encode knowledge by structured editing of an automatically generated feedback text \n\t\t']",Positive
"['\n\t\t The technology has hitherto addressed two practical contexts : the automatic production of multilingual technical documentation , and the formulation of queries to a database or expert system . \n\t', '\n\t\t In the first case , WYSiwYm editing encodes the desired content of the document in an interlingua , from which versions can be generated in mutliple languages ; in the second case , it yields a query encoded in a formal query language such as SQL . \n\t', '\n\t\t The benefit is the same in either context : since editing is mediated through a presentation in natural language , there is no need for the user to be acquainted with the formal details of knowledge representation or query languages . \n\t', '\n\t\t Elsewhere \n\t\t']",Positive
['\n\t\t This package was a consolidation of work carried out in a series of early applications \n\t\t'],Positive
['\n\t\t We present here an extension to this library which allows a coverage more in line with general-purpose generators like FUF/SURGE \n\t\t'],Positive
"['\n\t\t The extension is based on two new ideas : first , a change to the underlying semantic model , replacing atomic entity types with feature structures ; secondly , a corresponding change in the user interface , which now offers an extra editing operation ( called reconfiguration ) through which complex entity types may be modified . \n\t', '\n\t\t The purpose of this paper ( and the accompanying demonstration ) is to describe these novelties . \n\t', '\n\t\t 2 Editing with simple types Figure 1 : A-box with simple types In early WYSiwYm applications , the editing process served to build an A-box like that shown in figure 1 , comprising a set of entities ( represented by rectangles ) , each entity having a simple type ( represented by labels within rectangles ) and a set of relationships ( represented by labelled arcs ) . \n\t', '\n\t\t The graph in this figure is rooted in a take entity , denoting a taking event , the participants being a patient entity ( the taker ) and an an aspirin entity ( the takee ) . \n\t', '\n\t\t The intended meaning of the graph is expressed by the English sentence \x91the patient takes an aspirin\x92 . \n\t', '\n\t\t The construction of the graph through WYSiwYm editing proceeds as follows . \n\t', '\n\t\t The starting point is an empty A-box , which consists only in a constraint on the root entity \x97 for in- ARG-1 take ARG-2 patient aspirin stance , the requirement that it should be some kind of event . \n\t', '\n\t\t This unpromising A-box is supplied as input to a natural language generator with two special features : ( a ) it can generate texts from an A-box in any state of completion ( even empty ) ; ( b ) it can generate menus opening on anchors within the text , in addition to the text itself . \n\t', '\n\t\t The resulting feedback text is presented to the user through a special interface in which some spans are mouse-sensitive anchors , marking points where a new entity may be added to the A-box . \n\t', '\n\t\t Anchors are normally shown through a colour code ; here we will employ square brackets : [ Some event ] . \n\t', '\n\t\t When the user mouse-clicks on an anchor , a menu pops up listing all entity types allowed in the relevant context \x97 in this case , all event types . \n\t', '\n\t\t After the user chooses one of these options , such as \x91take\x92 , a new entity of the specified type is created , and added to the A-box at the current location ( in this case , the root of the graph ) . \n\t', '\n\t\t Assuming the ontology decrees that a take event has two participants , a person and an object , the new A-box will include two anchors allowing these entities to be defined : [ Some person ] takes [ some object ] . \n\t', '\n\t\t Opening the anchor \x91some person\x92 will yield a list of options including \x91patient\x92 ; opening \x91some object\x92 will yield options including \x91an aspirin\x92 ; in this way two more entities can be introduced , so obtaining the complete graph in figure 1. 3 Limitations in coverage For some applications , the above procedure works well , but it allows far too few variations to cope with real documents or queries of normal linguistic complexity . \n\t', '\n\t\t A single choice of event type ( \x91take\x92 ) is assumed by default to imply just one out of the thousands of possible clause patterns that could be obtained by varying mood , tense , polarity , modality , etc. , or by adding adverbial modifiers : FORCE does the patient take an aspirin ? \n\t', '\n\t\t take an aspirin TIME the patient took an aspirin the patient will take an aspirin POLARITY the patient does not take an aspirin MODALITY the patient may take an aspirin the patient must take an aspirin the patient might take an aspirin the patient should take an aspirin MODIFIER the patient takes an aspirin [ at some time ] the patient takes an aspirin [ somewhere ] the patient takes an aspirin [ in some manner ] the patient takes an aspirin [ with some frequency ] By combining just the above features , we obtain over 300 combinations ; these would multiply further if we included the semantic features controlling perfective , progressive , voice , and wh-questions . \n\t', '\n\t\t Such a large set of options challenges the feasibility of WYsIwYM , or indeed any other approach to knowledge editing by domain experts . \n\t', '\n\t\t 4 Editing with complex types Our favoured ( indeed , only ) proposal for embracing these variations is based on an analogy with a drawing tool . \n\t', '\n\t\t In WYsIwYM , choosing take from a menu of event types introduces an event entity , implicitly defaulted to present time , positive polarity , and so forth . \n\t', '\n\t\t In a drawing tool , choosing the rectangle icon from a palette of shapes introduces a rectangle entity , implicitly defaulted to a certain size , colour , and border ( to name just three features ) . \n\t', '\n\t\t Having introduced a rectangle entity , however , the user can reconfigure it by changing these features one at a time . \n\t', '\n\t\t Why should an equivalent operation not be provided for the semantic features underlying a clause ? \n\t', '\n\t\t Figure 2 : A-box with complex types arrive breathe . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t take . \n\t', '\n\t\t . \n\t', '\n\t\t . \n\t', '\n\t\t patient IDENTIFIABILITY identifiable MULTIPLICITY single aspirin IDENTIFIABILITY unidentifiable MULTIPLICITY single ARG-1 take POLARITY positive TIME present MODALITY undef ARG-2 To add this extra editing operation we must replace the simple entity types employed in early WysiwyM systems by complex types , as illustrated in figure 2 ( to simplify , just a few of the possible features are shown ) . \n\t', '\n\t\t To reconfigure an entity , the user selects the corresponding span in the feedback text ( all such spans will be mouse-sensitive ) , and chooses from a menu of options , each corresponding to a change in just one feature . \n\t', '\n\t\t With this potentially huge increase in the number of editing operations for a given feedback text , the idea of precomputing all possible menus and popping one up on demand becomes less attractive , both computationally and to the user . \n\t', '\n\t\t Instead , when the user selects a span of text , the menu of reconfigurations for that span is computed on the fly , and displayed in a static menu pane adjacent to the main text pane , which can be browsed and searched - see figure 3 . \n\t', '\n\t\t At every stage during the interaction , the user sees a feedback text ( right pane ) , with one span highlighted through a colour code , and a list of options for reconfiguring the currently selected unit ( left pane ) . \n\t', '\n\t\t If the selected unit happens to be an anchor ( square brackets ) , the operation will be one of choosing an initial entity type rather than reconfiguring an existing one , but the appearance of the interface will be the same . \n\t', '\n\t\t The user can continue the interaction in two ways : either by choosing an option from the menu pane , or by selecting a different current unit by mouse-clicking within the feedback text pane . \n\t', '\n\t\t To illustrate , we will suppose that the current A-box is as depicted in figure 2 , and that the \x91patient\x92 entity is currently selected . \n\t', '\n\t\t Highlighting the selected span in bold face rather than a colour code , the feedback text and the menu of reconfiguration options might be as follows : The patient takes an aspirin . \n\t', '\n\t\t iDENTiFiABiLiTy A patient MuLTipLiciTy The patients The labels ( iDENTiFiABiLiTy etc. ) could of course be replaced by more familiar words ( e.g. , article , number ) . \n\t', '\n\t\t Assuming that the user is happy with the subject of the sentence , he/she will ignore the reconfiguration options and instead click around the word \x91takes\x92 in the feedback text , so selecting the whole event entity : The patient takes an aspirin . \n\t', '\n\t\t poLARiTy The patient does not take an aspirin . \n\t', '\n\t\t TiME The patient took an aspirin . \n\t', '\n\t\t The patient will take an aspirin . \n\t', '\n\t\t MoDALiTy The patient must take an aspirin . \n\t', '\n\t\t The patient may take an aspirin . \n\t', '\n\t\t The patient might take an aspirin . \n\t', '\n\t\t If the first reconfiguration option is chosen , setting poLARiTy to negative , the revised options will conserve this new value throughout , except for the new polarity option , which will now be to change the value back to positive : The patient does not take an aspirin . \n\t', '\n\t\t poLARiTy The patient takes an aspirin . \n\t', '\n\t\t TiME The patient did not take an aspirin . \n\t', '\n\t\t The patient will not take an aspirin . \n\t', '\n\t\t MoDALiTy The patient must not take an aspirin . \n\t', '\n\t\t The patient may not take an aspirin . \n\t', '\n\t\t The patient might not take an aspirin . \n\t', '\n\t\t Figure 3 also shows the use of tags in the feedback text , such as Leaflet , Section , Paragraph . \n\t', '\n\t\t These provide anchor points to select and reconfigure linguistic units which have no exclusive text of their own . \n\t', '\n\t\t Such tags would not form part of the final output text in a document authoring scenario . \n\t', '\n\t\t 5 Benefits of the approach These techniques make it possible to construct complex , fluent and expressive texts using a point-and-click interface , with no typing of text . \n\t', '\n\t\t The benefits of previous WysiwyM systems are also retained here : the text is guaranteed to have a coherent internal representation which can be constrained to conform to a controlled language or house style specification , or generated ( and edited ) in a different language . \n\t', '\n\t\t The internal representation can be used to monitor the document content , for example to provide authoring support , or it can be transformed into an alternative representation for further processing . \n\t', '\n\t\t Although the motivation for this extension was to provide effective support for document authoring , the underlying model offers additional functionality in other knowledge creation scenarios as well . \n\t', '\n\t\t The examples in this paper use the complex types of the knowledge objects to represent linguistic variation , but might just Figure 3 : Snapshot of application as easily represent other kinds of semantic detail , for example in an object-oriented program specifciation scenario . \n\t', '\n\t\t 6 Conclusion In this paper we have described an extension to our earlier WysiwyM approach which supports more sophisticated interactions with the underlying knowledge base , allowing a far wider range of linguistic expressions to be constructed . \n\t', '\n\t\t This makes the system more suitable for real authoring tasks , particularly in controlled language or multilingual contexts , while also enhancing its potential for constructing and editing other kinds of complex knowledge . \n\t', '\n\t\t The system has been implemented as an extension to our WysiwyM library \n\t\t']",Positive
"['\n\t\t The demonstration requires a PC with Java and Sicstus Prolog . \n\t', '\n\t\t References John A. Bateman . \n\t', '\n\t\t 1996. KPML : The koMETPenman ( Multilingual ) Development Environment . \n\t', '\n\t\t Technical report , Institut f¨ur Integrierte Publikations- und Informationssysteme ( IPSI ) , GMD , Darmstadt , March . \n\t', '\n\t\t Release 0.9 . \n\t', '\n\t\t Nadjet Bouayad-Agha , Richard Power , Donia Scott , and Anja Belz . \n\t', '\n\t\t 2002. PILLS : Multilingual generation of medical information documents with overlapping content . \n\t', '\n\t\t In Proceedings of the Third International Conference on Language Resoures and Evaluation ( LREC 2002 ) , pages 2111\x962114 , Las Palmas . \n\t', '\n\t\t Christy Doran , Dania Egedi , Beth Ann Hockey , B. Srinivas , and Martin Zaidel . \n\t', '\n\t\t 1994. XTAG system - a wide coverage grammar for english . \n\t', '\n\t\t In Proceedings of the 15th International Conference on Computational Linguistics ( COLING 94 ) , pages 922\x96928 , Kyoto , Japan . \n\t', '\n\t\t Michael Elhadad and Jacques Robin . \n\t', '\n\t\t 1992. Controlling content realization with functional unification grammars . \n\t', '\n\t\t In Aspects of Automated Natural Language Generation , pages 89\x96104 . \n\t', '\n\t\t Springer Verlag . \n\t', '\n\t\t Roger Evans and Richard Power . \n\t', '\n\t\t 2003. Wysiwym : Building user interfaces with natural language feedback . \n\t', '\n\t\t In Research notes and demonstration papers at EACL-03 , pages 203\x96206 , Budapest , Hungary . \n\t', '\n\t\t B. Lavoie and O. Rambow . \n\t', '\n\t\t 1997. RealPro : A fast , portable sentence realizer . \n\t', '\n\t\t In Proceedings of the Conference on Applied Natural Language Processing ( ANLP\x9297 ) , Washington , DC . \n\t', '\n\t\t Paul Piwek , Roger Evans , Lynne Cahill , and Neil Tipper . \n\t', '\n\t\t 2000. Natural language generation in the mile system . \n\t', '\n\t\t In Proceedings of the IMPACTS in NLG Workshop , pages 33\x9642 , Schloss Dagstuhl , Germany . \n\t', '\n\t\t R. Power and D. Scott . \n\t', '\n\t\t 1998 . \n\t', '\n\t\t Multilingual authoring using feedback texts . \n\t', '\n\t\t In Proceedings of the 17th International Conference on Computational Linguistics and 36th Annual Meeting of the Association for Computational Linguistics , pages 1053\x961059 , Montreal , Canada . \n\t', '\n\t\t NLTK : The Natural Language Toolkit Steven Bird Department of Computer Science and Software Engineering University of Melbourne Victoria 3010 , Australia sb@csse.unimelb.edu.au Edward Loper Department of Computer and Information Science University of Pennsylvania Philadelphia PA 19104-6389 , USA edloper@gradient.cis.upenn.edu Abstract The Natural Language Toolkit is a suite of program modules , data sets , tutorials and exercises , covering symbolic and statistical natural language processing . \n\t', '\n\t\t NLTK is written in Python and distributed under the GPL open source license . \n\t', '\n\t\t Over the past three years , NLTK has become popular in teaching and research . \n\t', '\n\t\t We describe the toolkit and report on its current state of development . \n\t', '\n\t\t 1 Introduction The Natural Language Toolkit ( NLTK ) was developed in conjunction with a computational linguistics course at the University of Pennsylvania in 2001 \n\t\t']",Positive
"['\n\t\t It was designed with three pedagogical applications in mind : assignments , demonstrations , and projects . \n\t', '\n\t\t Assignments . \n\t', '\n\t\t NLTK supports assignments of varying difficulty and scope . \n\t', '\n\t\t In the simplest assignments , students experiment with existing components to perform a wide variety of NLP tasks . \n\t', '\n\t\t As students become more familiar with the toolkit , they can be asked to modify existing components , or to create complete systems out of existing components . \n\t', '\n\t\t Demonstrations . \n\t', '\n\t\t NLTK\x92s interactive graphical demonstrations have proven to be very useful for students learning NLP concepts . \n\t', '\n\t\t The demonstrations give a step-by-step execution of important algorithms , displaying the current state of key data structures . \n\t', '\n\t\t A screenshot of the chart parsing demonstration is shown in Figure 1 . \n\t', '\n\t\t Projects . \n\t', '\n\t\t NLTK provides students with a flexible framework for advanced projects . \n\t', '\n\t\t Typical projects might involve implementing a new algorithm , developing a new component , or implementing a new task . \n\t', '\n\t\t We chose Python because it has a shallow learning curve , its syntax and semantics are transparent , and it has good string-handling functionality . \n\t', '\n\t\t As an interpreted language , Python facilitates interactive exploration . \n\t', '\n\t\t As an object-oriented language , Python permits data and methods to be encapsulated and re-used easily . \n\t', '\n\t\t Python comes with an extensive standard library , including tools for graphical programming and numerical processing . \n\t', '\n\t\t The recently added generator syntax makes it easy to create interactive implementations of algorithms \n\t\t']",Positive
"['\n\t\t Figure 1 : Interactive Chart Parsing Demonstration 2 Design NLTK is implemented as a large collection of minimally interdependent modules , organized into a shallow hierarchy . \n\t', '\n\t\t A set of core modules defines basic data types that are used throughout the toolkit . \n\t', '\n\t\t The remaining modules are task modules , each devoted to an individual natural language processing task . \n\t', '\n\t\t For example , the nltk.parser module encompasses to the task of parsing , or deriving the syntactic structure of a sentence ; and the nltk.tokenizer module is devoted to the task of tokenizing , or dividing a text into its constituent parts . \n\t', '\n\t\t 2.1 Tokens and other core data types To maximize interoperability between modules , we use a single class to encode information about natural language texts \x96 the Token class . \n\t', '\n\t\t Each Token instance represents a unit of text such as a word , sentence , or document , and is defined by a ( partial ) mapping from property names to values . \n\t', '\n\t\t For example , the TEXT property is used to encode a token\x92s text content:1 >>> from nltk.token import * >>> Token(TEXT=""Hello World ! \n\t', '\n\t\t "" ) <Hello World!> The TAG property is used to encode a token\x92s partof-speech tag : >>> Token(TEXT=""python"" , TAG=""NN"" ) <python/NN> The SUBTOKENS property is used to store a tokenized text : >>> from nltk.tokenizer import * >>> tok = Token(TEXT=""Hello World ! \n\t', '\n\t\t "" ) >>> WhitespaceTokenizer().tokenize(tok) >>> print tok[\x92SUBTOKENS\x92] ) [ <Hello> , <World!> ] In a similar fashion , other language processing tasks such as word-sense disambiguation , chunking and parsing all add properties to the Token data structure . \n\t', '\n\t\t In general , language processing tasks are formulated as annotations and transformations involving Tokens . \n\t', '\n\t\t In particular , each processing task takes a token and extends it to include new information . \n\t', '\n\t\t These modifications are typically monotonic ; new information is added but existing information is not deleted or modified . \n\t', '\n\t\t Thus , tokens serve as a blackboard , where information about a piece of text is collated . \n\t', '\n\t\t This architecture contrasts with the more typical pipeline architecture where each processing task\x92s output discards its input information . \n\t', '\n\t\t We chose the blackboard approach over the pipeline approach because it allows more flexibility when combining tasks into a single system . \n\t', '\n\t\t In addition to the Token class and its derivatives , NLTK defines a variety of other data types . \n\t', '\n\t\t For instance , the probability module defines classes for probability distributions and statistical smoothing techniques ; and the cfg module defines classes for encoding context free grammars and probabilistic context free grammars . \n\t', ""\n\t\t ' Some code samples are specific to NLTK version 1.4. 2.2 The corpus module Many language processing tasks must be developed and tested using annotated data sets or corpora . \n\t"", '\n\t\t Several such corpora are distributed with NLTK , as listed in Table 1 . \n\t', '\n\t\t The corpus module defines classes for reading and processing many of these corpora . \n\t', '\n\t\t The following code fragment illustrates how the Brown Corpus is accessed . \n\t', '\n\t\t >>> from nltk.corpus import brown >>> brown.groups() [ \x92skill and hobbies\x92 , \x92popular lore\x92 , \x92humor\x92 , \x92fiction : mystery\x92 , ... ] >>> brown.items(\x92humor\x92) ( \x92cr01\x92 , \x92cr02\x92 , \x92cr03\x92 , \x92cr04\x92 , \x92cr05\x92 , \x92cr06\x92 , \x92cr07\x92 , \x92cr08\x92 , \x92cr09\x92 ) >>> brown.tokenize(\x92cr01\x92) <[<It/pps> , <was/bedz> , <among/in> , <these/dts> , <that/cs> , <Hinkle/np> , <identified/vbd> , <a/at> , ...]> A selection of 5 % of the Penn Treebank corpus is included with NLTK , and it is accessed as follows : >>> from nltk.corpus import treebank >>> treebank.groups() ( \x92raw\x92 , \x92tagged\x92 , \x92parsed\x92 , \x92merged\x92 ) >>> treebank.items(\x92parsed\x92) [ \x92wsj_0001.prd\x92 , \x92wsj_0002.prd\x92 , ... ] >>> item = \x92parsed/wsj_0001.prd\x92 >>> sentences = treebank.tokenize(item) >>> for sent in sentences[\x92SUBTOKENS\x92] : ... print sent.pp() # pretty-print ( S : ( NP-SBJ : ( NP : <Pierre> <Vinken> ) ( ADJP : ( NP : <61> <years> ) <old> ) ... 2.3 Processing modules Each language processing algorithm is implemented as a class . \n\t', '\n\t\t For example , the ChartParser and Recurs iveDescentParser classes each define a single algorithm for parsing a text . \n\t', '\n\t\t We implement language processing algorithms using classes instead of functions for three reasons . \n\t', '\n\t\t First , all algorithm-specific options can be passed to the constructor , allowing a consistent interface for applying the algorithms . \n\t', '\n\t\t Second , a number of algorithms need to have their state initialized before they can be used . \n\t', '\n\t\t For example , the NthOrderTagger class Corpus Contents and Wordcount Example Application 20 Newsgroups ( selection ) 3 newsgroups , 4000 posts , 780kw text classification Brown Corpus 15 genres , 1.15Mw , tagged training & testing taggers , text classification CoNLL 2000 Chunking Data 270kw , tagged and chunked training & testing chunk parsers Project Gutenberg ( selection ) 14 texts , 1.7Mw text classification , language modelling NIST 1999 IEER ( selection ) 63kw , named-entity markup training & testing named-entity recognizers Levin Verb Index 3k verbs with Levin classes parser development Names Corpus 8k male & female names text classification PP Attachment Corpus 28k prepositional phrases , tagged parser development Roget\x92s Thesaurus 200kw , formatted text word-sense disambiguation SEMCOR 880kw , POS & sense tagged word-sense disambiguation SENSEVAL 2 Corpus 600kw , POS & sense tagged word-sense disambiguation Stopwords Corpus 2,400 stopwords for 11 lgs text retrieval Penn Treebank ( sample ) 40kw , tagged & parsed parser development Wordnet 1.7 180kw in a semantic network WSD , NL understanding Wordlist Corpus 960kw and 20k affixes for 8lgs spell checking Table 1 : Corpora and Corpus Samples Distributed with NLTK must be initialized by training on a tagged corpus before it can be used . \n\t', '\n\t\t Third , subclassing can be used to create specialized versions of a given algorithm . \n\t', '\n\t\t Each processing module defines an interface for its task . \n\t', '\n\t\t Interface classes are distinguished by naming them with a trailing capital \x93I,\x94 such as Pa r s e r I . \n\t', '\n\t\t Each interface defines a single action method which performs the task defined by the interface . \n\t', '\n\t\t For example , the P a r s e r I interface defines the parse method and the Tokenizer interface defines the tokenize method . \n\t', '\n\t\t When appropriate , an interface defines extended action methods , which provide variations on the basic action method . \n\t', '\n\t\t For example , the Pa r s e r I interface defines the parse n method which finds at most n parses for a given sentence ; and the Tokenizer I interface defines the x t o ke n i z e method , which outputs an iterator over subtokens instead of a list of subtokens . \n\t', '\n\t\t NLTK includes the following modules : cfg , corpus , draw ( cfg , chart , corpus , featurestruct , fsa , graph , plot , rdparser , srparser , tree ) , eval , featurestruct , parser ( chart , chunk , probabilistic ) , probability , sense , set , stemmer ( porter ) , tagger , test , token , tokenizer , tree , and util . \n\t', '\n\t\t Please see the online documentation for details . \n\t', '\n\t\t 2.4 Documentation Three different types of documentation are available . \n\t', '\n\t\t Tutorials explain how to use the toolkit , with detailed worked examples . \n\t', '\n\t\t The API documentation describes every module , interface , class , method , function , and variable in the toolkit . \n\t', '\n\t\t Technical reports explain and justify the toolkit\x92s design and implementation . \n\t', '\n\t\t All are available from http:// nltk.sf.net/docs.html . \n\t', '\n\t\t 3 Installing NLTK NLTK is available from nltk.sf.net , and is packaged for easy installation under Unix , Mac OS X and Windows . \n\t', '\n\t\t The full distribution consists of four packages : the Python source code ( nltk ) ; the corpora ( nltk-data ) ; the documentation ( nltk-docs ) ; and third-party contributions ( nltk-contrib ) . \n\t', '\n\t\t Before installing NLTK , it is necessary to install Python version 2.3 or later , available from www. python . \n\t', '\n\t\t org . \n\t', '\n\t\t Full installation instructions and a quick start guide are available from the NLTK homepage . \n\t', '\n\t\t As soon as NLTK is installed , users can run the demonstrations . \n\t', '\n\t\t On Windows , the demonstrations can be run by double-clicking on their Python source files . \n\t', '\n\t\t Alternatively , from the Python interpreter , this can be done as follows : >>> import nltk.draw.rdparser >>> nltk.draw.rdparser.demo() >>> nltk.draw.srparser.demo() >>> nltk.draw.chart.demo() 4 Using and contributing to NLTK NLTK has been used at the University of Pennsylvania since 2001 , and has subsequently been adopted by several NLP courses at other universities , including those listed in Table 2 . \n\t', '\n\t\t Third party contributions to NLTK include : Brill tagger ( Chris Maloof ) , hidden Markov model tagger ( Trevor Cohn , Phil Blunsom ) , GPSG-style feature-based grammar and parser ( Rob Speer , Bob Berwick ) , finite-state morphological analyzer ( Carl de Marcken , Beracah Yankama , Bob Berwick ) , decision list and decision tree classifiers ( Trevor Cohn ) , and Discourse Representation Theory implementation ( Edward Ivanovic ) . \n\t', '\n\t\t NLTK is an open source project , and we welcome any contributions . \n\t', '\n\t\t There are several ways to contribute : users can report bugs , suggest features , or contribute patches on Sourceforge ; users can participate in discussions on the NLTK-Devel mailing list2 or in the NLTK public forums ; and users can submit their own NLTK-based projects for inclusion in the nltk contrib directory . \n\t', '\n\t\t New code modules that are relevant , substantial , original and well-documented will be considered for inclusion in NLTK proper . \n\t', '\n\t\t All source code is distributed under the GNU General Public License , and all documentation is distributed under a Creative Commons non-commercial license . \n\t', '\n\t\t Thus , potential contributors can be confident that their work will remain freely available to all . \n\t', '\n\t\t Further information about contributing to NLTK is available at http://nltk.sf.net/contrib.html . \n\t', '\n\t\t 5 Conclusion NLTK is a broad-coverage natural language toolkit that provides a simple , extensible , uniform framework for assignments , demonstrations and projects . \n\t', '\n\t\t It is thoroughly documented , easy to learn , and simple to use . \n\t', '\n\t\t NLTK is now widely used in research and teaching . \n\t', '\n\t\t Readers who would like to receive occasional announcements about NLTK are encouraged to sign up for the low-volume , moderated mailing list NLTK-Announce.3 6 Acknowledgements We are indebted to our students and colleagues for feedback on the toolkit , and to many contributors listed on the NLTK website . \n\t', '\n\t\t 2http://lists.sourceforge.net/ lists/listinfo/nltk-devel 3http://lists.sourceforge.net/ lists/listinfo/nltk-announce Graz University of Technology , Austria Information Search and Retrieval Macquarie University , Australia Intelligent Text Processing Massachusetts Institute of Technology , USA Natural Language Processing National Autonomous University of Mexico , Mexico Introduction to Natural Language Processing in Python Ohio State University , USA Statistical Natural Language Processing University of Amsterdam , Netherlands Language Processing and Information Access University of Colorado , USA Natural Language Processing University of Edinburgh , UK Introduction to Computational Linguistics University of Magdeburg , Germany Natural Language Systems University of Malta , Malta Natural Language Algorithms University of Melbourne , Australia Human Language Technology University of Pennsylvania , USA Introduction to Computational Linguistics University of Pittsburgh , USA Artificial Intelligence Application Development Simon Fraser University , Canada Computational Linguistics Table 2 : University Courses using NLTK References Edward Loper and Steven Bird . \n\t', '\n\t\t 2002. NLTK : The Natural Language Toolkit . \n\t', '\n\t\t In Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics , pages 62\x9669 . \n\t', '\n\t\t Somerset , NJ : Association for Computational Linguistics . \n\t', '\n\t\t http://arXiv.org/abs/ cs/0205028 . \n\t', '\n\t\t Edward Loper . \n\t', '\n\t\t 2004. NLTK : Building a pedagogical toolkit in Python . \n\t', '\n\t\t In PyCon DC 2004 . \n\t', '\n\t\t Python Software Foundation . \n\t', '\n\t\t http : //www. python . \n\t', '\n\t\t org/pycon/dc2004/papers/ . \n\t', '\n\t\t Guido Van Rossum . \n\t', '\n\t\t 2003a . \n\t', '\n\t\t An Introduction to Python . \n\t', '\n\t\t Network Theory Ltd. . \n\t', '\n\t\t Guido Van Rossum . \n\t', '\n\t\t 2003b . \n\t', '\n\t\t The Python Language Reference . \n\t', '\n\t\t Network Theory Ltd. \n\t', '\n\t\t Dyna : A Declarative Language for Implementing Dynamic Programs^ Jason Eisner and Eric Goldlust and Noah A. Smith Department of Computer Science , Johns Hopkins University Baltimore , MD 21218 U.S.A. {jason,eerat,nasmith}@cs.jhu.edu Abstract We present the first version of a new declarative programming language . \n\t', '\n\t\t Dyna has many uses but was designed especially for rapid development of new statistical NLP systems . \n\t', '\n\t\t A Dyna program is a small set of equations , resembling Prolog inference rules , that specify the abstract structure of a dynamic programming algorithm . \n\t', '\n\t\t It compiles into efficient , portable , C++ classes that can be easily invoked from a larger application . \n\t', '\n\t\t By default , these classes run a generalization of agenda- based parsing , prioritizing the partial parses by some figure of merit . \n\t', '\n\t\t The classes can also perform an exact backward ( outside ) pass in the service ofparameter training . \n\t', '\n\t\t The compiler already knows several implementation tricks , algorithmic transforms , and numerical optimization techniques . \n\t', '\n\t\t It will acquire more over time : we intend for it to generalize and encapsulate best practices , and serve as a testbed for new practices . \n\t', '\n\t\t Dyna is now being used for parsing , machine translation , morphological analysis , grammar induction , and finite-state modeling . \n\t', '\n\t\t 1 Introduction Computational linguistics has become a more experimental science . \n\t', '\n\t\t One often uses real-world data to test one\x92s formal models ( grammatical , statistical , or both ) . \n\t', '\n\t\t Unfortunately , as in other experimental sciences , testing each new hypothesis requires much tedious lab work : writing and tuning code until parameter estimation ( \x93training\x94 ) and inference over unknown variables ( \x93decoding\x94 ) are bug-free and tolerably fast . \n\t', '\n\t\t This is intensive work , given complex models or a large search space ( as in modern statistical parsing and machine translation ) . \n\t', '\n\t\t It is a major effort to break into the field with a new system , and modifying existing systems\x97even in a conceptually simple way\x97can require significant reengineering . \n\t', '\n\t\t Such \x93lab work\x94 mainly consists of reusing or reinventing various dynamic programming architectures . \n\t', '\n\t\t We propose that it is time to jump up a level of abstraction . \n\t', '\n\t\t We offer a new programming language , Dyna , that allows one to quickly and easily specify a model\x92s combinatorial structure . \n\t', '\n\t\t We also offer a compiler , dynac , that translates from Dyna into C++ classes . \n\t', '\n\t\t The compiler does all the tedious work of writing the training and decoding code . \n\t', '\n\t\t It is intended to do as good a job as a clever graduate student who already knows the tricks of the trade ( and is willing to maintain hand-tuned C++ ) . \n\t', '\n\t\t * We would like to thank Joshua Goodman , David McAllester , and Paul Ruhlen for useful early discussions , and pioneer users Markus Dreyer , David Smith , and Roy Tromble for their feedback and input . \n\t', '\n\t\t This work was supported by NSF ITR grant IIS-0313193 to the first author , by a Fannie & John Hertz Foundation fellowship to the third author , and by ONR MURI grant N00014-01-1-0685 . \n\t', '\n\t\t The views expressed are not necessarily endorsed by the sponsors . \n\t', '\n\t\t 2 A Basic Example : PCFG Parsing We believe Dyna is a flexible and intuitive specification language for dynamic programs . \n\t', '\n\t\t Such a program specifies how to combine partial solutions until a complete solution is reached . \n\t', '\n\t\t 2.1 The Inside Algorithm , in Dyna Fig . \n\t', '\n\t\t 1 shows a simple Dyna program that corresponds to the inside algorithm for PCFGs ( i.e. , the probabilistic generalization of CKY parsing ) . \n\t', '\n\t\t It may be regarded as a system of equations over an arbitrary number of unknowns , which have structured names such as constit(s,0,3) . \n\t', '\n\t\t These unknowns are called items . \n\t', '\n\t\t They resemble variables in a C program , but we use variable instead to refer to the capitalized identifiers X , I , K , ... in lines 2\x964.1 At runtime , a user must provide an input sentence and grammar by asserting values for certain items . \n\t', '\n\t\t If the input is John loves Mary , the user should assert values of 1 for word(John,0,1) , word(loves,1,2) , word(Mary,2,3) , and end(3) . \n\t', '\n\t\t If the PCFG contains a rewrite rule np ^ Mary with probability p(Mary I np ) = 0.003 , the user should assert that rewrite(np,Mary) has value 0.003 . \n\t', '\n\t\t Given these base cases , the equations in Fig . \n\t', '\n\t\t 1 enable Dyna to deduce values for other items . \n\t', '\n\t\t The deduced value of constit(s,0,3) will be the inside probability Os ( 0 , 3),2 and the deduced value of goal will be the total probability of all parses of the input . \n\t', '\n\t\t Lines 2\x964 are equational schemas that specify how to compute the value of items such as constit(s,0,3) from the values of other items . \n\t', '\n\t\t By using the summation operator += , lines 2\x963 jointly say that for any X , I , and K , constit(X,I,K) is defined by summation over the remaining variables , as PW rewrite(X,W)*word(W,I,K) + EY,Z,J rewrite(X,Y,Z)*constit(Y,I,J)*constit(Z,J,K) . \n\t', '\n\t\t For example , constit(s,0,3) is a sum of quantities such as rewrite(s,np,vp)*constit(np,0,1)*constit(vp,1,3) . \n\t', '\n\t\t 2.2 The Execution Model Dyna\x92s declarative semantics state only that it will find values such that all the equations hold .3 Our implementation\x92s default strategy is to propagate updates from an equation\x92s right-hand to its left-hand side , until the system converges . \n\t', '\n\t\t Thus , by default , Fig . \n\t', '\n\t\t 1 yields a bottom- up or data-driven parser . \n\t', '\n\t\t 1Much of our terminology ( item , chart , agenda ) is inherited from the parsing literature . \n\t', '\n\t\t Other terminology ( variable , term , inference rule , antecedent/consequent , assert/retract , chaining ) comes from logic programming . \n\t', '\n\t\t Dyna\x92s syntax borrows from both Prolog and C. 2That is , the probability that s would stochastically rewrite to the first three words of the input . \n\t', '\n\t\t If this can happen in more than one way , the probability sums over multiple derivations . \n\t', '\n\t\t 3Thus , future versions of the compiler are free to mix any efficient strategies , even calling numerical equation solvers . \n\t', '\n\t\t 1. :- valtype(term , real ) . \n\t', '\n\t\t % declares that all item values are real numbers 2. constit(X,I,K) += rewrite(X,W) * word(W,I,K) . \n\t', '\n\t\t % a constituent is either a word ... 3. constit(X,I,K) += rewrite(X,Y,Z) * constit(Y,I,J) * constit(Z,J,K) . \n\t', '\n\t\t % ... or a combination of two adjacent subconstituents 4. goal += constit(s,0,N) * end(N) . \n\t', '\n\t\t % a parse is any s constituent that covers the input string Figure 1 : A probabilistic CKY parser written in Dyna . \n\t', '\n\t\t Dyna may be seen as a new kind of tabled logic programming language in which theorems are not just proved , but carry values . \n\t', '\n\t\t This suggests some terminology . \n\t', '\n\t\t Lines 2\x964 of Fig . \n\t', '\n\t\t 1 are called inference rules . \n\t', '\n\t\t The items on the right-hand side are antecedents , and the item on the left-hand side is their consequent . \n\t', '\n\t\t Assertions can be regarded as axioms . \n\t', '\n\t\t And the default strategy ( unlike Prolog\x92s ) is forward chaining from the axioms , as in some theorem provers . \n\t', '\n\t\t Suppose constit(verb,1,2) increases by A . \n\t', '\n\t\t Then the program in Fig . \n\t', '\n\t\t 1 must find all the instantiated rules that have constit(verb,1,2) as an antecedent , and must update their consequents . \n\t', '\n\t\t For example , since line 3 can be instantiated as constit(vp,1,3) += rewrite(vp,verb,np)*constit(verb,1,2)*constit(np,2,3) , then constit(vp,1,3) must be increased by rewrite(vp,verb,np) * 0 * constit(np,2,3) . \n\t', '\n\t\t Line 3 actually requires infinitely many such updates , corresponding to all rule instantiations of the form constit(X,1,K) += rewrite(X,verb,Z)*constit(verb,1,2)*constit(Z,2,K) .4 However , most of these updates would have no effect . \n\t', '\n\t\t We only need to consider the finitely many instantiations where rewrite(X,verb,Z) and constit(Z,2,K) have nonzero values ( because they have been asserted or updated in the past ) . \n\t', '\n\t\t The compiled Dyna program rapidly computes this set of needed updates and adds them to a worklist of pending updates , the agenda . \n\t', '\n\t\t Updates from the agenda are processed in some prioritized order ( which can strongly affect the speed of the program ) . \n\t', '\n\t\t When an update is carried out ( e.g. , constit(vp,1,3) is increased ) , any further updates that it triggers ( e.g. , to constit(s,0,3)) are placed back on the agenda in the same way . \n\t', '\n\t\t Multiple updates to the same item are consolidated on the agenda . \n\t', '\n\t\t This cascading update process begins with axiom assertions , which are treated like other updates . \n\t', '\n\t\t 2.3 Closely Related Algorithms We now give some examples of variant algorithms . \n\t', '\n\t\t Fig . \n\t', '\n\t\t 1 provides lattice parsing for free . \n\t', '\n\t\t Instead of being integer positions in an string , I , J and K can be symbols denoting states in a finite-state automaton . \n\t', '\n\t\t The code does not have to change , only the input . \n\t', '\n\t\t Axioms should now correspond to weighted lattice arcs , e.g. , word(loves,q,r) with value p(portion of speech signal loves ) . \n\t', '\n\t\t To find the probability of the best parse instead of the total probability of all parses , simply change the value type : replace real with viterbi in line 1 . \n\t', '\n\t\t If a and b are viterbi values , a+b is implemented as max(a , b).5 4As well as instantiations constit(X,I,2) += rewrite(X,Y , verb)*constit(Y,I,1)*constit(verb,1,2) . \n\t', '\n\t\t 5Also , a*b is implemented as a + b , as viterbi values actually represent log probabilities ( for speed and dynamic range ) . \n\t', '\n\t\t Similarly , replacing real with boolean obtains an unweighted parser , in which a constituent is either derived ( true value ) or not ( false value ) Then a*b is implemented as a A b , and a+b as a V b . \n\t', '\n\t\t The Dyna programmer can declare the agenda discipline\x97i.e. , the order in which updates are processed\x97to obtain variant algorithms . \n\t', '\n\t\t Although Dyna supports stack and queue ( LIFO and FIFO ) disciplines , its default is to use a priority queue prioritized by the size of the update . \n\t', '\n\t\t When parsing with real values , this quickly accumulates a good approximation of the inside probabilities , which permits heuristic early stopping before the agenda is empty . \n\t', '\n\t\t With viterbi values , it amounts to uniform-cost search for the best parse , and an item\x92s value is guaranteed not to change once it is nonzero . \n\t', '\n\t\t Dyna will soon allow user-defined priority functions ( themselves dynamic programs ) , which can greatly speed up parsing \n\t\t']",Positive
"['\n\t\t 2.4 Parameter Training Dyna provides facilities for training parameters . \n\t', '\n\t\t For example , from Fig . \n\t', '\n\t\t 1 , it automatically derives the inside- outside ( EM ) algorithm for training PCFGs . \n\t', '\n\t\t How is this possible ? \n\t', '\n\t\t Once the program of Fig . \n\t', '\n\t\t 1 has run , goal\x92s value is the probability of the input sentence under the grammar . \n\t', '\n\t\t This is a continuous function of the axiom values , which correspond to PCFG parameters ( e.g. , the weight of rewrite(np,Mary)) . \n\t', '\n\t\t The function could be written out explicitly as a sum of products of sums of products of ... of axiom values , with the details depending on the sentence and grammar . \n\t', '\n\t\t Thus , Dyna can be regarded as computing a function F(~^) , where ^~ is a vector of axiom values and F(~^) is an objective function such as the probability of one\x92s training data . \n\t', '\n\t\t In learning , one wishes to repeatedly adjust ^~ so as to increase F( ~^ ) . \n\t', '\n\t\t Dyna can be told to evaluate the gradient of the function with respect to the current parameters ~^ : e.g. , if rewrite(vp,verb,np) were increased by e , what would happen to goal ? \n\t', '\n\t\t Then any gradient-based optimization method can be applied , using Dyna to evaluate both F(~^) and its gradient vector . \n\t', '\n\t\t Also , EM can be applied where appropriate , since it can be shown that EM\x92s E counts can be derived from the gradient . \n\t', '\n\t\t Dyna\x92s strategy for computing the gradient is automatic differentiation in the reverse mode \n\t\t']",Positive
"['\n\t\t Dyna comes with a constrained optimization module , DynaMITE,6 that can locally optimize F(~^) . \n\t', '\n\t\t At present , DynaMITE provides the conjugate gradient and variable metric methods , using the Toolkit for Advanced Optimization \n\t\t']",Positive
"['\n\t\t technique to enforce sum-to-one constraints . \n\t', '\n\t\t It supports maximum-entropy training and the EM algorithm.7 DynaMITE provides an object-oriented API that allows independent variation of such diverse elements of training as the model parameterization , optimization algorithm , smoothing techniques , priors , and datasets . \n\t', '\n\t\t How about supervised or partly supervised training ? \n\t', '\n\t\t The role of supervision is to permit some constituents to be built but not others \n\t\t']",Positive
"['\n\t\t Lines 2\x963 of Fig . \n\t', '\n\t\t 1 can simply be extended with an additional antecedent permitted(X,I,K) , which must be either asserted or derived for constit(X,I,K) to be derived . \n\t', '\n\t\t In \x93soft\x94 supervision , the permitted axioms may have values between 0 and 1.8 3 C++ Interface and Implementation A Dyna program compiles to a set of portable C++ classes that manage the items and perform inference . \n\t', '\n\t\t These classes can be used in a larger C++ application.9 This strategy keeps Dyna both small and convenient . \n\t', '\n\t\t A C++ chart object supports the computation of item values and gradients . \n\t', '\n\t\t It keeps track of built items , their values , and their derivations , which form a proof forest . \n\t', '\n\t\t It also holds an ordered agenda of pending updates . \n\t', '\n\t\t Some built items may be \x93transient,\x94 meaning that they are not actually stored in the chart at the moment but will be transparently recomputed upon demand . \n\t', '\n\t\t The Dyna compiler generates a hard-coded decision tree that analyzes the structure of each item popped from the agenda to decide which inference rules apply to it . \n\t', '\n\t\t To enable fast lookup of the other items that participate in these inference rules , it generates code to maintain appropriate indices on the chart . \n\t', '\n\t\t Objects such as constit(vp,1,3) are called terms and may be recursively nested to any depth . \n\t', '\n\t\t ( Items are just terms with values . \n\t', '\n\t\t ) Dyna has a full first-order type system for terms , including primitive and disjunctive types , and permitting compile-time type inference . \n\t', '\n\t\t These types are compiled into C++ classes that support constructors and accessors , garbage-collection , subterm sharing ( which may lead to asymptotic speedups , as in CCG parsing \n\t\t']",Positive
"['\n\t\t 10 Dyna can import new primitive term types and value types from C++ , as well as C++ functions to combine values and to user-define the weights of certain terms . \n\t', '\n\t\t In the current implementation , every rule must have the restricted form c += a1*a2* \x95 \x95 \x95 *ak ( where each ai is an item or side condition and ( X , + , * ) is a semiring of values ) . \n\t', '\n\t\t The design for Dyna\x92s next version lifts this restriction to allow arbitrary , type-heterogeneous expressions on the right-hand side of an inference rule.11 7It will eventually offer additional methods , such as deterministic annealing , simulated annealing , and iterative scaling . \n\t', '\n\t\t 8 Such item values are not probabilities . \n\t', '\n\t\t We are generally interested in log-linear models for parsing \n\t\t']",Positive
"['\n\t\t 9We are also now developing a default application : a visual debugger that allows a user to assert axioms and explore the proof forest created during inference . \n\t', '\n\t\t 10Interned values are hashed so that equal values are represented by equal pointers . \n\t', '\n\t\t It is very fast to compare and hash such representations . \n\t', '\n\t\t 11 That will make Dyna useful for a wider variety of non-NLP algo- 4 Some Further Applications Dyna is useful for any problem where partial hypotheses are assembled , or where consistency has to be maintained . \n\t', '\n\t\t It is already being used for parsing , syntax-based machine translation , morphological analysis , grammar induction , and finite-state operations . \n\t', '\n\t\t It is well known that various parsing algorithms for CFG and other formalisms can be simply written in terms of inference rules . \n\t', '\n\t\t Fig . \n\t', '\n\t\t 2 renders one such example in Dyna , namely Earley\x92s algorithm . \n\t', '\n\t\t Two features are worth noting : the use of recursively nested subterms such as lists , and the SIDE function , which evaluates to 1 or 0 according to whether its argument has a defined value yet . \n\t', '\n\t\t These side conditions are used here to prevent hypothesizing a constituent until there is a possible left context that calls for it . \n\t', '\n\t\t Several recent syntax-directed statistical machine translation models are easy to build in Dyna . \n\t', '\n\t\t The simplest \n\t\t']",Positive
"['\n\t\t When training or decoding , the hypotheses of better-trained monolingual parsers can provide either hard or soft partial supervision ( section 2.4 ) . \n\t', '\n\t\t Dyna can manipulate finite-state transducers . \n\t', '\n\t\t For instance , the weighted arcs of the composed FST M1 o M2 can be deduced from the arcs of M1 and M2 . \n\t', '\n\t\t Training M1 o M2 back-propagates to train the original weights in M1 and M2 , as in \n\t\t']",Positive
"['\n\t\t 5 Speed and Code Size One of our future priorities is speed . \n\t', '\n\t\t Comparing informally to the best hand-written C++ code we found online for inside-outside and Dijkstra\x92s algorithms , Dyna ( like Java ) currently runs up to 5 times slower . \n\t', '\n\t\t We mainly understand the reasons ( memory layout and overreliance on hashing ) and are working actively to close the gap . \n\t', '\n\t\t 12 Programmer time is also worth considering . \n\t', '\n\t\t Our inside-outside and Dijkstra\x92s algorithms are each about 5 lines of Dyna code ( plus a short C driver program ) , but were compared in the previous paragraph against efficient C++ implementations of 5500 and 900 lines . \n\t', '\n\t\t 13 Our colleague Markus Dreyer , as his first Dyna program , decided to replicate the Collins parser ( 3400 lines of C ) . \n\t', '\n\t\t His implementation used under 40 lines of Dyna code , plus a 300-line C++ driver program that mostly dealt with I/O . \n\t', '\n\t\t One of us ( Smith ) has written substantially more complex Dyna programs ( e.g. , 56 types + 46 inference rules ) , enabling research that he would not have been willing to undertake in another language . \n\t', '\n\t\t 6 Related Work This project tries to synthesize much folk wisdom . \n\t', '\n\t\t For NLP algorithms , three excellent longer papers have at- rithms ( e.g. , neural networks , constraint programming , clustering , and dynamic graph algorithms ) . \n\t', '\n\t\t However , it introduces several interesting design complications in the Dyna language and the implementation . \n\t', '\n\t\t 12Dyna spends most of its time manipulating hash tables and the priority queue . \n\t', '\n\t\t Inference is very fast because it is compiled . \n\t', '\n\t\t 13 The code size comparisons are rough ones , because of mismatches between the programs being compared . \n\t', '\n\t\t 1. need(s,0) = 1 . \n\t', '\n\t\t % begin by looking for an s that starts at position 0 2 . \n\t', '\n\t\t constit(Nonterm/Needed,I,I) += SIDE(need(Nonterm,I)) * rewrite(Nonterm,Needed) . \n\t', '\n\t\t % traditional predict step 3. constit(Nonterm/Needed,I,K) += constit(Nonterm/cons(W,Needed),I,J) * word(W,J,K) . \n\t', '\n\t\t %traditional scan step 4. constit(Nonterm/Needed,I,K) += constit(Nonterm,cons(X,Needed),I,J) * constit(X/nil,J,K) . \n\t', '\n\t\t % traditional complete step 5. goal += constit(s/nil,0,N) * end(N) . \n\t', '\n\t\t % we want a complete s constituent covering the sentence 6. need(Nonterm,J) += constit( /cons(Nonterm , ) , , J ) . \n\t', '\n\t\t %Note : underscore matches anything ( anonymous wildcard ) Figure 2 : An Earley parser in Dyna . \n\t', '\n\t\t np/Needed is syntactic sugar for slash(np,Needed) , which is the label of a partial np constituent that is still missing the list of subconstituents in Needed . \n\t', '\n\t\t In particular , np/nil is a complete np . \n\t', '\n\t\t ( A list [ n,pp ] is encoded here as cons(n,cons(pp,nil)) , although syntactic sugar for lists is also available . \n\t', '\n\t\t ) need(np,3) is derived if some partial constituent seeks an np subconstituent starting at position 3 . \n\t', '\n\t\t As usual , probabilistic , agenda-based lattice parsing comes for free , as does training . \n\t', '\n\t\t tempted similar syntheses ( though without covering variant search and storage strategies , which Dyna handles ) . \n\t', '\n\t\t \n\t\t']",Positive
['\n\t\t \n\t\t'],Positive
['\n\t\t \n\t\t'],Positive
"['\n\t\t Our approach extends this to a wider variety of processing orders , and in particular shows how to use a prioritized agenda in the general case , using novel algorithms . \n\t', '\n\t\t We also extend to a wider class of formulas ( e.g. , neural networks ) . \n\t', '\n\t\t The closest implemented work we have found is PRISM \n\t\t']",Negative
"['\n\t\t It is interesting because it inherits expressive power from Prolog . \n\t', '\n\t\t On the other hand , its rigid probabilistic framework does not permit side conditions ( Fig . \n\t', '\n\t\t 2 ) , general semirings ( Goodman ) , or general formulas ( Dyna ) . \n\t', '\n\t\t PRISM does not currently seem practical for statistical NLP research : in CKY parsing tests , it was only able to handle a small fraction of the Penn Treebank ruleset ( 2400 high- probability rules ) and tended to crash on sentences over 50 words . \n\t', '\n\t\t Dyna , by contrast , is designed for real-world use : it consistently parses over 10x faster than PRISM , scales to full-sized problems , and attempts to cover real- world necessities such as prioritization , bottom-up inference , pruning , smoothing , underflow avoidance , maxent , non-EM optimization techniques , etc. 7 Conclusions Dyna is a declarative programming language for building efficient systems quickly . \n\t', '\n\t\t As a language , it is inspired by previous work in deductive parsing , adding weights in a particularly general way . \n\t', '\n\t\t Dyna\x92s compiler has been designed with an eye toward low-level issues ( indexing , structure-sharing , garbage collection , etc. ) so that the cost of this abstraction is minimized . \n\t', '\n\t\t The goal of Dyna is to facilitate experimentation : a new model or algorithm automatically gets a new mem ory layout , indexing , and training code . \n\t', '\n\t\t We hope this will lower the barrier to entry in the field , in both research and education . \n\t', '\n\t\t In Dyna we seek to exploit as many algorithmic tricks as we can , generalizing them to as many problems as possible on behalf of future Dyna programs . \n\t', '\n\t\t In turn the body of old programs can provide a unified testbed for new training and decoding techniques . \n\t', '\n\t\t Our broader vision is to unify a problem\x92s possible algorithms by automatically deriving all of them and their possible training procedures from a single high-level Dyna program , using source-to-source program transformations and compiler directives . \n\t', '\n\t\t We plan to choose automatically among these variants by machine learning over runs on typical data . \n\t', '\n\t\t This involves , for example , automatically learning a figure of merit to guide decoding . \n\t', '\n\t\t The Dyna compiler , documentation , and examples can be found at www. dyna . \n\t', '\n\t\t org . \n\t', '\n\t\t The compiler is available under an open-source license . \n\t', '\n\t\t The commented C++ code that it generates is free to modify . \n\t', '\n\t\t References S. Benson , L. C. McInnes , and J. J. Mor´e . \n\t', '\n\t\t 2000. TAO users manual . \n\t', '\n\t\t Tech Rpt ANL/MCS-TM-242 , Argonne Nat . \n\t', '\n\t\t Lab . \n\t', '\n\t\t S. A. Caraballo , E. Charniak . \n\t', '\n\t\t 1998. New figures of merit for best-first probabilistic chart parsing . \n\t', '\n\t\t Comp . \n\t', '\n\t\t Ling. , 24(2) . \n\t', '\n\t\t Jason Eisner . \n\t', '\n\t\t 2002. Parameter estimation for probabilistic finite-state transducers . \n\t', '\n\t\t In Proc . \n\t', '\n\t\t ofACL . \n\t', '\n\t\t Joshua Goodman . \n\t', '\n\t\t 1999. Semiring parsing . \n\t', '\n\t\t Comp . \n\t', '\n\t\t Ling , 25(4) . \n\t', '\n\t\t Andreas Griewank and George Corliss , editors . \n\t', '\n\t\t 1991. Automatic Differentiation ofAlgorithms . \n\t', '\n\t\t SIAM . \n\t', '\n\t\t Dan Klein and Christopher D. Manning . \n\t', '\n\t\t 2003. A* parsing : Fast exact Viterbi parse selection . \n\t', '\n\t\t Proc . \n\t', '\n\t\t ofHLT-NAACL . \n\t', '\n\t\t David McAllester . \n\t', '\n\t\t 1999. On the complexity analysis of static analyses . \n\t', '\n\t\t 6th Intl . \n\t', '\n\t\t Static Analysis Symposium . \n\t', '\n\t\t F. Pereira and Y. Schabes . \n\t', '\n\t\t 1992. Inside-outside reestimation from partially bracketed corpora . \n\t', '\n\t\t Proc . \n\t', '\n\t\t ofACL . \n\t', '\n\t\t S. Riezler , D. Prescher , J. Kuhn , M. Johnson . \n\t', '\n\t\t 2000. Lexicalized stochastic modeling of constraint-based grammars using log-linear measures and EM training . \n\t', '\n\t\t Proc . \n\t', '\n\t\t ofACL . \n\t', '\n\t\t Stuart M. Shieber , Yves Schabes , and Fernando Pereira . \n\t', '\n\t\t 1995. Principles and implementation of deductive parsing . \n\t', '\n\t\t Journal ofLogic Programming . \n\t', '\n\t\t K. Vijay-Shanker and D. Weir . \n\t', '\n\t\t 1990. Polynomial-time parsing of combinatory categorial grammars . \n\t', '\n\t\t Proc . \n\t', '\n\t\t ofACL . \n\t', '\n\t\t Dekai Wu . \n\t', '\n\t\t 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora . \n\t', '\n\t\t Computational Linguistics , 23(3):377\x96404 . \n\t', '\n\t\t N.-F. Zhou and T. Sato . \n\t', '\n\t\t 2003. Toward a high-performance system for symbolic and statistical modeling . \n\t', '\n\t\t IJCAI-03 Workshop on Learning Statistical Models from Relational Data . \n\t', '\n\t\t MATCHKiosk : A Multimodal Interactive City Guide Michael Johnston AT&T Research 180 Park Avenue Florham Park , NJ 07932 johnston@research.att.com Srinivas Bangalore AT&T Research 180 Park Avenue Florham Park , NJ 07932 srini@research.att.com Abstract Multimodal interfaces provide more flexible and compelling interaction and can enable public information kiosks to support more complex tasks for a broader community of users . \n\t', '\n\t\t MATCHKiosk is a multimodal interactive city guide which provides users with the freedom to interact using speech , pen , touch or multimodal inputs . \n\t', '\n\t\t The system responds by generating multimodal presentations that synchronize synthetic speech with a life-like virtual agent and dynamically generated graphics . \n\t', '\n\t\t 1 Introduction Since the introduction of automated teller machines in the late 1970s , public kiosks have been introduced to provide users with automated access to a broad range of information , assistance , and services . \n\t', '\n\t\t These include self check-in at airports , ticket machines in railway and bus stations , directions and maps in car rental offices , interactive tourist and visitor guides in tourist offices and museums , and more recently , automated check-out in retail stores . \n\t', '\n\t\t The majority of these systems provide a rigid structured graphical interface and user input by only touch or keypad , and as a result can only support a small number of simple tasks . \n\t', '\n\t\t As automated kiosks become more commonplace and have to support more complex tasks for a broader community of users , they will need to provide a more flexible and compelling user interface . \n\t', '\n\t\t One major motivation for developing multimodal interfaces for mobile devices is the lack of a keyboard or mouse \n\t\t']",Positive
"['\n\t\t This limitation is also true of many different kinds of public information kiosks where security , hygiene , or space concerns make a physical keyboard or mouse impractical . \n\t', '\n\t\t Also , mobile users interacting with kiosks are often encumbered with briefcases , phones , or other equipment , leaving only one hand free for interaction . \n\t', '\n\t\t Kiosks often provide a touchscreen for input , opening up the possibility of an onscreen keyboard , but these can be awkward to use and occupy a considerable amount of screen real estate , generally leading to a more moded and cumbersome graphical interface . \n\t', '\n\t\t A number of experimental systems have investigated adding speech input to interactive graphical kiosks \n\t\t']",Positive
['\n\t\t Other work has investigated adding both speech and gesture input ( using computer vision ) in an interactive kiosk \n\t\t'],Positive
"['\n\t\t We describe MATCHKiosk , ( Multimodal Access To City Help Kiosk ) an interactive public information kiosk with a multimodal interface which provides users with the flexibility to provide input using speech , handwriting , touch , or composite multimodal commands combining multiple different modes . \n\t', '\n\t\t The system responds to the user by generating multimodal presentations which combine spoken output , a life-like graphical talking head , and dynamic graphical displays . \n\t', '\n\t\t MATCHKiosk provides an interactive city guide for New York and Washington D.C. , including information about restaurants and directions on the subway or metro . \n\t', '\n\t\t It develops on our previous work on a multimodal city guide on a mobile tablet ( MATCH ) \n\t\t']",Positive
"['\n\t\t The system has been deployed for testing and data collection in an AT&T facility in Washington , D.C. where it provides visitors with information about places to eat , points of interest , and getting around on the DC Metro . \n\t', '\n\t\t 2 The MATCHKiosk The MATCHKiosk runs on a Windows PC mounted in a rugged cabinet ( Figure 1 ) . \n\t', '\n\t\t It has a touch screen which supports both touch and pen input , and also contains a printer , whose output emerges from a slot below the screen . \n\t', '\n\t\t The cabinet also contains speakers and an array microphone is mounted above the screen . \n\t', '\n\t\t There are three main components to the graphical user interface ( Figure 2 ) . \n\t', '\n\t\t On the right , there is a panel with a dynamic map display , a click-to-speak button , and a window for feedback on speech recognition . \n\t', '\n\t\t As the user interacts with the system the map display dynamically pans and zooms and the locations of restaurants and other points of interest , graphical callouts with information , and subway route segments are displayed . \n\t', '\n\t\t In Figure 1 : Kiosk Hardware the top left there is a photo-realistic virtual agent \n\t\t']",Positive
"['\n\t\t Below the agent , there is a panel with large buttons which enable easy access to help and common functions . \n\t', '\n\t\t The buttons presented are context sensitive and change over the course of interaction . \n\t', '\n\t\t Figure 2 : Kiosk Interface The basic functions of the system are to enable users to locate restaurants and other points of interest based on attributes such as price , location , and food type , to request information about them such as phone numbers , addresses , and reviews , and to provide directions on the subway or metro between locations . \n\t', '\n\t\t There are also commands for panning and zooming the map . \n\t', '\n\t\t The system provides users with a high degree of flexibility in the inputs they use in accessing these functions . \n\t', '\n\t\t For example , when looking for restaurants the user can employ speech e.g. find me moderately priced italian restaurants in Alexandria , a multimodal combination of speech and pen , e.g. moderate italian restaurants in this area and circling Alexandria on the map , or solely pen , e.g. user writes moderate italian and alexandria . \n\t', '\n\t\t Similarly , when requesting directions they can use speech , e.g. . \n\t', '\n\t\t How do I get to the Smithsonian ? \n\t', '\n\t\t , multimodal , e.g. . \n\t', '\n\t\t How do I get from here to here ? \n\t', '\n\t\t and circling or touching two locations on the map , or pen , e.g. in Figure 2 the user has circled a location on the map and handwritten the word route . \n\t', '\n\t\t System output consists of coordinated presentations combining synthetic speech with graphical actions on the map . \n\t', '\n\t\t For example , when showing a subway route , as the virtual agent speaks each instruction in turn , the map display zooms and shows the corresponding route segment graphically . \n\t', '\n\t\t The kiosk system also has a print capability . \n\t', '\n\t\t When a route has been presented , one of the context sensitive buttons changes to Print Directions . \n\t', '\n\t\t When this is pressed the system generates an XHTML document containing a map with step by step textual directions and this is sent to the printer using an XHTML-print capability . \n\t', '\n\t\t If the system has low confidence in a user input , based on the ASR or pen recognition score , it requests confirmation from the user . \n\t', '\n\t\t The user can confirm using speech , pen , or by touching on a checkmark or cross mark which appear in the bottom right of the screen . \n\t', '\n\t\t Context-sensitive graphical widgets are also used for resolving ambiguity and vagueness in the user inputs . \n\t', '\n\t\t For example , if the user asks for the Smithsonian Museum a small menu appears in the bottom right of the map enabling them to select between the different museum sites . \n\t', '\n\t\t If the user asks to see restaurants near a particular location , e.g. show restaurants near the white house , a graphical slider appears enabling the user to fine tune just how near . \n\t', '\n\t\t The system also features a context-sensitive multimodal help mechanism \n\t\t']",Positive
"['\n\t\t The help system is triggered by spoken or written requests for help , by touching the help buttons on the left , or when the user has made several unsuccessful inputs . \n\t', '\n\t\t The type of help is chosen based on the current dialog state and the state of the visual interface . \n\t', '\n\t\t If more than one type of help is applicable a graphical menu appears . \n\t', '\n\t\t Help messages consist of multimodal presentations combining spoken output with ink drawn on the display by the system . \n\t', '\n\t\t For example , if the user has just requested to see restaurants and they are now clearly visible on the display , the system will provide help on getting information about them . \n\t', '\n\t\t 3 Multimodal Kiosk Architecture The underlying architecture of MATCHKiosk consists of a series of re-usable components which communicate using XML messages sent over sockets through a facilitator ( MCUBE ) ( Figure 3 ) . \n\t', '\n\t\t Users interact with the system through the Multimodal UI displayed on the touchscreen . \n\t', '\n\t\t Their speech and ink are processed by speech recognition ( ASR ) and handwriting/gesture recognition ( GESTURE , HW RECO ) components respectively . \n\t', '\n\t\t These recognition processes result in lattices of potential words and gestures/handwriting . \n\t', '\n\t\t These are then combined and assigned a meaning representation using a multimodal language processing architecture based on finite-state techniques ( MMFST ) \n\t\t']",Positive
"['\n\t\t This provides as output a lattice encoding all of the potential meaning representations assigned to the user inputs . \n\t', '\n\t\t This lattice is flattened to an N-best list and passed to a multimodal dialog manager ( MDM ) \n\t\t']",Positive
"['\n\t\t If additional information or confirmation is required , the MDM uses the virtual agent to enter into a short information gathering dialogue with the user . \n\t', '\n\t\t Once a command or query is complete , it is passed to the multimodal generation component ( MMGEN ) , which builds a multimodal score indicating a coordinated sequence of graphical actions and TTS prompts . \n\t', '\n\t\t This score is passed back to the Multimodal UI . \n\t', '\n\t\t The Multi- modal UI passes prompts to a visual text-to-speech component \n\t\t']",Positive
"['\n\t\t As prompts are realized the Multi- modal UI receives notifications and presents coordinated graphical actions . \n\t', '\n\t\t The subway route server is an application server which identifies the best route between any two locations . \n\t', '\n\t\t Figure 3 : Multimodal Kiosk Architecture 4 Discussion and Related Work A number of design issues arose in the development of the kiosk , many of which highlight differences between multimodal interfaces for kiosks and those for mobile systems . \n\t', '\n\t\t Array Microphone While on a mobile device a close-talking headset or on-device microphone can be used , we found that a single microphone had very poor performance on the kiosk . \n\t', '\n\t\t Users stand in different positions with respect to the display and there may be more than one person standing in front . \n\t', '\n\t\t To overcome this problem we mounted an array microphone above the touchscreen which tracks the location of the talker . \n\t', '\n\t\t Robust Recognition and Understanding is particularly important for kiosks since they have so many first-time users . \n\t', '\n\t\t We utilize the techniques for robust language modelling and multimodal understanding described in \n\t\t']",Positive
"['\n\t\t Social Interaction For mobile multimodal interfaces , even those with graphical embodiment , we found there to be little or no need to support social greetings and small talk . \n\t', '\n\t\t However , for a public kiosk which different unknown users will approach those capabilities are important . \n\t', '\n\t\t We added basic support for social interaction to the language understanding and dialog components . \n\t', '\n\t\t The system is able to respond to inputs such as Hello , How are you ? \n\t', '\n\t\t , Would you like to join us for lunch ? \n\t', '\n\t\t and so on . \n\t', '\n\t\t Context-sensitive GUI Compared to mobile systems , on palmtops , phones , and tablets , kiosks can offer more screen real estate for graphical interaction . \n\t', '\n\t\t This allowed for large easy to read buttons for accessing help and other functions . \n\t', '\n\t\t The system alters these as the dialog progresses . \n\t', '\n\t\t These buttons enable the system to support a kind of mixed- initiative in multimodal interaction where the user can take initiative in the spoken and handwritten modes while the system is also able to provide a more system-oriented initiative in the graphical mode . \n\t', '\n\t\t Printing Kiosks can make use of printed output as a modality . \n\t', '\n\t\t One of the issues that arises is that it is frequently the case that printed outputs such as directions should take a very different style and format from onscreen presentations . \n\t', '\n\t\t In previous work , a number of different multi- modal kiosk systems supporting different sets of input and output modalities have been developed . \n\t', '\n\t\t The Touch-N-Speak kiosk \n\t\t']",Positive
['\n\t\t The August system \n\t\t'],Positive
"['\n\t\t It supported spoken input from users and multi- modal output with a talking head , text to speech , and two graphical displays . \n\t', '\n\t\t The system was deployed in a cultural center in Stockholm , enabling collection of realistic data from the general public . \n\t', '\n\t\t SmartKom-Public \n\t\t']",Positive
"['\n\t\t The system uses a number of cameras and a video projector for the display . \n\t', '\n\t\t The MASK kiosk \n\t\t']",Positive
['\n\t\t The mVPQ kiosk system \n\t\t'],Positive
"['\n\t\t Users can provide input by either speech or touching options presented on a graphical display . \n\t', '\n\t\t MACK , the Media Lab Autonomous Conversational Kiosk , \n\t\t']",Positive
"['\n\t\t Users interact using speech and gestures on a paper map that sits between the user and an embodied agent . \n\t', '\n\t\t In contrast to August and mVPQ , MATCHKiosk supports composite multimodal input combining speech with pen drawings and touch . \n\t', '\n\t\t The SmartKom-Public kiosk supports composite input , but differs in that it uses free hand gesture for pointing while MATCH utilizes pen input and touch . \n\t', '\n\t\t August , SmartKom-Public , and MATCHKiosk all employ graphical embodiments . \n\t', '\n\t\t SmartKom uses an animated character , August a model-based talking head , and MATCHKiosk a sample-based video- realistic talking head . \n\t', '\n\t\t MACK uses articulated graphical embodiment with ability to gesture . \n\t', '\n\t\t In Touch-N-Speak a number of different techniques using time and pressure are examined for enabling selection of areas on a map using touch input . \n\t', '\n\t\t In MATCHKiosk , this issue does not arise since areas can be selected precisely by drawing with the pen . \n\t', '\n\t\t 5 Conclusion We have presented a multimodal public information kiosk , MATCHKiosk , which supports complex unstructured tasks such as browsing for restaurants and subway directions . \n\t', '\n\t\t Users have the flexibility to interact using speech , pen/touch , or multimodal inputs . \n\t', '\n\t\t The system responds with multimodal presentations which coordinate synthetic speech , a virtual agent , graphical displays , and system use of electronic ink . \n\t', '\n\t\t Acknowledgements Thanks to Eric Cosatto , Hans Peter Graf , and Joern Ostermann for their help with integrating the talking head . \n\t', '\n\t\t Thanks also to Patrick Ehlen , Amanda Stent , Helen Hastie , Guna Vasireddy , Mazin Rahim , Candy Kamm , Marilyn Walker , Steve Whittaker , and Preetam Maloor for their contributions to the MATCH project . \n\t', '\n\t\t Thanks to Paul Burke for his assistance with XHTML-print . \n\t', '\n\t\t References S. Bangalore and M. Johnston . \n\t', '\n\t\t 2004 . \n\t', '\n\t\t Balancing Data-driven and Rule-based Approaches in the Context of a Multimodal Conversational System . \n\t', '\n\t\t In Proceedings ofHLT-NAACL , Boston , MA . \n\t', '\n\t\t M. Beutnagel , A. Conkie , J. Schroeter , Y. Stylianou , and A. Syrdal . \n\t', '\n\t\t 1999. The AT&T Next- Generation TTS . \n\t', '\n\t\t In In Joint Meeting of ASA ; EAA and DA GA . \n\t', '\n\t\t J. Cassell , T. Stocky , T. Bickmore , Y. Gao , Y. Nakano , K. Ryokai , D. Tversky , C. Vaucelle , and H. Vilhjalmsson . \n\t', '\n\t\t 2002. MACK : Media lab autonomous conversational kiosk . \n\t', '\n\t\t In Proceedings ofIMAGINA02 , Monte Carlo . \n\t', '\n\t\t E. Cosatto and H. P. Graf . \n\t', '\n\t\t 2000. Photo-realistic Talking-heads from Image Samples . \n\t', '\n\t\t IEEE Transactions on Multimedia , 2(3):152\x96163 . \n\t', '\n\t\t J. Gustafson , N. Lindberg , and M. Lundeberg . \n\t', '\n\t\t 1999. The August spoken dialogue system . \n\t', '\n\t\t In Proceedings of Eurospeech 99 , pages 1151\x96 1154 . \n\t', '\n\t\t H. Hastie , M. Johnston , and P. Ehlen . \n\t', '\n\t\t 2002. Context-sensitive Help for Multimodal Dialogue . \n\t', '\n\t\t In Proceedings of the 4th IEEE International Conference on Multimodal Interfaces , pages 93\x96 98 , Pittsburgh , PA . \n\t', '\n\t\t M. Johnston and S. Bangalore . \n\t', '\n\t\t 2000. Finite- state Multimodal Parsing and Understanding . \n\t', '\n\t\t In Proceedings of COLING 2000 , pages 369\x96375 , Saarbr¨ucken , Germany . \n\t', '\n\t\t M. Johnston , S. Bangalore , and G. Vasireddy . \n\t', '\n\t\t 2001. MATCH : Multimodal Access To City Help . \n\t', '\n\t\t In Workshop on Automatic Speech Recognition and Understanding , Madonna di Campiglio , Italy . \n\t', '\n\t\t M. Johnston , S. Bangalore , A. Stent , G. Vasireddy , and P. Ehlen . \n\t', '\n\t\t 2002a . \n\t', '\n\t\t Multimodal Language Processing for Mobile Information Access . \n\t', '\n\t\t In Proceedings ofICSLP 2002 , pages 2237\x962240 . \n\t', '\n\t\t M. Johnston , S. Bangalore , G. Vasireddy , A. Stent , P. Ehlen , M. Walker , S. Whittaker , and P. Maloor . \n\t', '\n\t\t 2002b . \n\t', '\n\t\t MATCH : An Architecture for Multimodal Dialog Systems . \n\t', '\n\t\t In Proceedings ofACL02 , pages 376\x96383 . \n\t', '\n\t\t L. Lamel , S. Bennacef , J. L. Gauvain , H. Dartigues , and J. N. Temem . \n\t', '\n\t\t 2002. User Evaluation of the MASK Kiosk . \n\t', '\n\t\t Speech Communication , 38(1- 2):131\x96139 . \n\t', '\n\t\t S. Narayanan , G. DiFabbrizio , C. Kamm , J. Hubbell , B. Buntschuh , P. Ruscitti , and J. Wright . \n\t', '\n\t\t 2000. Effects of Dialog Initiative and Multi-modal Presentation Strategies on Large Directory Information Access . \n\t', '\n\t\t In Proceedings of ICSLP 2000 , pages 636\x96639 . \n\t', '\n\t\t S. Oviatt and P. Cohen . \n\t', '\n\t\t 2000. Multimodal Interfaces That Process What Comes Naturally . \n\t', '\n\t\t Communications of the ACM , 43(3):45\x9653 . \n\t', '\n\t\t R. Raisamo . \n\t', '\n\t\t 1998. A Multimodal User Interface for Public Information Kiosks . \n\t', '\n\t\t In Proceedings of PUI Workshop , San Francisco . \n\t', '\n\t\t W. Wahlster . \n\t', '\n\t\t 2003. SmartKom : Symmetric Multi- modality in an Adaptive and Reusable Dialogue Shell . \n\t', '\n\t\t In R. Krahl and D. Gunther , editors , Proceedings oftheHuman Computer Interaction Status Conference 2003 , pages 47\x9662 . \n\t', '\n\t\t Fragments and Text Categorization Jan Bla^tik and Eva Mrikovi and Lubo^s Popelinsk´y Knowledge Discovery Lab Faculty of Informatics , Masaryk University 602 00 Brno , Czech Republic xblatak , glum , popel @fi.muni.cz Abstract We introduce two novel methods of text categorization in which documents are split into fragments . \n\t', '\n\t\t We conducted experiments on English , French and Czech . \n\t', '\n\t\t In all cases , the problems referred to a binary document classification . \n\t', '\n\t\t We find that both methods increase the accuracy of text categorization . \n\t', '\n\t\t For the Naive Bayes classifier this increase is significant . \n\t', '\n\t\t 1 Motivation In the process of automatic classifying documents into several predefined classes \x96 text categorization \n\t\t']",Positive
"['\n\t\t In this paper we describe a novel approach to text categorization in which each documents is first split into subparts , called fragments . \n\t', '\n\t\t Each fragment is consequently seen as a new document which shares the same label with its source document . \n\t', '\n\t\t We introduce two variants of this approach \x96 skip - tail and fragments . \n\t', '\n\t\t Both of these methods are briefly described below . \n\t', '\n\t\t We demonstrate the increased accuracy that we observed . \n\t', '\n\t\t 1.1 Skipping the tail of a document The first method uses only the first sentences of a document and is henceforth referred to as skip - tail . \n\t', '\n\t\t The idea behind this approach is that the beginning of each document contains enough information for the classification . \n\t', '\n\t\t In the process of learning , each document is first replaced by its initial part . \n\t', '\n\t\t The learning algorithm then uses only these initial fragments as learning ( test ) examples . \n\t', '\n\t\t We also sought the minimum length of initial fragments that preserve the accuracy of the classification . \n\t', '\n\t\t 1.2 Splitting a document into fragments The second method splits the documents into fragments which are classified independently of each others . \n\t', '\n\t\t This method is henceforth referred to as fragments . \n\t', '\n\t\t Initially , the classifier is used to generate a model from these fragments . \n\t', '\n\t\t Subsequently , the model is utilized to classify unseen documents ( test set ) which have also been split into fragments . \n\t', '\n\t\t 2 Data We conducted experiments using English , French and Czech documents . \n\t', '\n\t\t In all cases , the problems referred to a binary document classification . \n\t', '\n\t\t The main characteristics of the data are in Table 1 . \n\t', '\n\t\t Three kinds of English documents were used : 20 Newsgroups1 ( 202 randomly chosen documents from each class were used . \n\t', '\n\t\t The mail header was removed so that the text contained only the body of the message and in some cases , replies ) Reuters-21578 , Distribution 1.02 ( only documents from money- fx , money- supply , trade classified into a single class were chosen ) . \n\t', '\n\t\t All documents marked as BRIEF and UNPROC were removed . \n\t', '\n\t\t The classification tasks involved money- f x+money -supply vs. trade , money- fx vs. money-supply , money- fx vs. trade and money- supply vs. trade . \n\t', '\n\t\t MEDLINE data3 ( 235 abstracts of medical papers that concerned gynecology and assisted reproduction ) n docs ave sdev 20 Newsgroups 138 4040 15.79 5.99 Reuters-21578 4 1022 11.03 2.02 Medline 1 235 12.54 0.22 French cooking 36 1370 9.41 1.24 Czech newspaper 15 2545 22.04 4.22 Table 1 : Data ( n=number of classification tasks , docs=number of documents , ave =average number of sentences per document , sdev =standard deviation ) 1http://www.ai.mit.edu/-jrennie/ 20Newsgroups/ 2http://www.research.att.com/-lewis 3http://www.fi.muni.cz/-zizka/medocs The French documents contained French recipes . \n\t', '\n\t\t Examples of the classification tasks are Accompagnements vs. Cremes , Cremes vs. Pates-PainsCrepes , Desserts vs. Douceurs , Entrees vs. PlatsChauds and Pates-Pains-Crepes vs. . \n\t', '\n\t\t Sauces , among others . \n\t', '\n\t\t We also used both methods for classifying Czech documents . \n\t', '\n\t\t The data involved fifteen classification tasks . \n\t', '\n\t\t The articles used had been taken from Czech newspapers . \n\t', '\n\t\t Six tasks concerned authorship recognition , the other seven to find a document source \x96 either a newspaper or a particular page ( or column ) . \n\t', '\n\t\t Topic recognition was the goal of two tasks . \n\t', '\n\t\t The structure of the rest of this paper is as follows . \n\t', '\n\t\t The method for computing the classification of the whole document from classifying fragments ( fragments method ) is described in Section 3 . \n\t', '\n\t\t Experimental settings are introduced in Section 4 . \n\t', '\n\t\t Section 5 presents the main results . \n\t', '\n\t\t We conclude with an overview of related works and with directions for potential future research in Sections 6 and 7 . \n\t', '\n\t\t 3 Classification by means of fragments of documents The class of the whole document is determined as follows . \n\t', '\n\t\t Let us take a document which consists of fragments , ... , such that and . \n\t', '\n\t\t The value of depends on the length of the document and on the number of sentences in the fragments . \n\t', '\n\t\t Let , and denotes the set of possible classes . \n\t', '\n\t\t We than use the learned model to assign a class to each of the fragments . \n\t', '\n\t\t Let be the confidence of the classification fragment into the class . \n\t', '\n\t\t This confidence measure is computed as an estimated probability of the predicted class . \n\t', '\n\t\t Then for each fragment classified to the class we define . \n\t', '\n\t\t The confidence of the classification of the whole document into is computed as follows Finally , the class which is assigned to a docu- ment is computed according to the following definition : In other words , a document is classified to a , which was assigned to the most fragments from ( the most frequent class ) . \n\t', '\n\t\t If there are two classes with the same cardinality , the confidence measure is employed . \n\t', '\n\t\t We also tested another method that exploited the confidence of classification but the results were not satisfactory . \n\t', '\n\t\t 4 Experiments For feature ( i.e. significant word ) selection , we tested four methods \n\t\t']",Positive
"['\n\t\t Eventually , we chose ig because it yielded the best results . \n\t', '\n\t\t We utilized three learning algorithms from the Weka4 system \x96 the decision tree learner J48 , the Naive Bayes , the SVM Sequential Minimal Optimization ( SMO ) . \n\t', '\n\t\t All the algorithms were used with default settings . \n\t', '\n\t\t The entire documents have been split to fragments containing 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 20 , 25 , 30 , and 40 sentences . \n\t', '\n\t\t For the skip - tail classification which uses only the beginnings of documents we also employed these values . \n\t', '\n\t\t As an evaluation criterion we used the accuracy defined as the percentage of correctly classified documents from the test set . \n\t', '\n\t\t All the results have been obtained by a 10-fold cross validation . \n\t', '\n\t\t 5 Results 5.1 General We observed that for both skip-tail and fragments there is always a consistent size of fragments for which the accuracy increased . \n\t', '\n\t\t It is the most important result . \n\t', '\n\t\t More details can be found in the next two paragraphs . \n\t', '\n\t\t Among the learning algorithms , the highest accuracy was achieved for all the three languages with the Naive Bayes . \n\t', '\n\t\t It is surprising because for full versions of documents it was the SMO algorithm that was even slightly better than the Naive Bayes in terms of accuracy . \n\t', '\n\t\t On the other hand , the highest impact was observed for J48 . \n\t', '\n\t\t Thus , for instance for Czech , it was observed for fragments that the accuracy was higher for 14 out of 15 tasks when J48 had been used , and for 12 out of 15 in the case of the Naive Bayes and the Support Vector Machines . \n\t', '\n\t\t However , the performance of J48 was far inferior to that of the other algorithms . \n\t', '\n\t\t In only three tasks J48 for and . \n\t', '\n\t\t 4http://www.cs.waikato.ac.nz/ml/weka resulted in a higher accuracy than the Naive Bayes and the Support Vector Machines . \n\t', '\n\t\t The similar situation appeared for English and French . \n\t', '\n\t\t 5.2 skip-tail skip-tail method was successful for all the three languages ( see Table 2 ) . \n\t', '\n\t\t It results in increased accuracy even for a very small initial fragment . \n\t', '\n\t\t In Figure 1 there are results for skip- tail and initial fragments of the length from 40 % up to 100 % of the average length of documents in the learning set . \n\t', '\n\t\t 92.5 92 91.5 91 skip-tail(fr) full(fr) skip-tail(eng) full(eng) 40 50 60 70 80 90 100 lentgh of the fragment n NB stail lngth incr English 143 90.96 92.04 1.3 ++105 French 36 92.04 92.56 0.9 +25 Czech 15 79.51 81.13 0.9 +12 Table 2 : Results for skip-tail and the Naive Bayes ( n=number of classification tasks , NB=average of error rates for full documents , stail=average of error rates for skip-tail , lngth=optimal length of the fragment , incr=number of tasks with the increase of accuracy : + , ++ means significant on level 95 % resp 99 % , the sign test . \n\t', '\n\t\t ) For example , for English , taking only the first 40 % of sentences in a document results in a slightly increased accuracy . \n\t', '\n\t\t Figure 2 displays the relative increase of accuracy for fragments of the length up to 40 sentences for different learning algorithms for English . \n\t', '\n\t\t It is important to stress that even for the initial fragment of the length of 5 sentences , the accuracy is the same as for full documents . \n\t', '\n\t\t When the initial fragment is longer the classification accuracy further increase until the length of 12 sentences . \n\t', '\n\t\t We observed similar behaviour for skip-tail when employed on other languages , and also for the fragments method . \n\t', '\n\t\t 5.3 fragments This method was successful for classifying English and Czech documents ( significant on level 99 % for English and 95 % for Czech ) . \n\t', '\n\t\t In the case of French cooking recipes , a small , but not significant impact has been observed , too . \n\t', '\n\t\t This may have been caused by the special format of recipes . \n\t', '\n\t\t n NB frag lngth incr English 143 91.12 93.21 1.1 ++96 French 36 92.04 92.27 1.0 19 Czech 15 82.36 84.07 1.0 +12 Table 3 : Results for fragments ( for the description see Table 2 ) Figure 1 : skip-tail , Naive Bayes . \n\t', '\n\t\t ( lentgh of the fragment = percentage of the average document length ) 5 0 -5 -10 -15 -20 -25 -30 -35 NaiveBayes-bm SMO-bm J48-bm 0 5 10 15 20 25 30 35 40 no . \n\t', '\n\t\t of senteces Figure 2 : Relative increase of accuracy : English , skip-tail 5.4 Optimal length of fragments We also looked for the optimal length of fragments . \n\t', '\n\t\t We found that for the lengths of fragments for the range about the average document length ( in the learning set ) , the accuracy increased for the significant number of the data sets ( the sign test 95 % ) . \n\t', '\n\t\t It holds for skip-tail and for all languages . \n\t', '\n\t\t and for English and Czech in the case of fragments . \n\t', '\n\t\t However , an increase of accuracy is observed even for 60 % of the average length ( see Fig . \n\t', '\n\t\t 1 ) . \n\t', '\n\t\t Moreover , for the average length this increase is significant for Czech at a level 95 % ( t-test ) . \n\t', '\n\t\t 6 Discussion and related work Two possible reasons may result in an accuracy increase for skip- tail . \n\t', '\n\t\t As a rule , the beginning of a document contains the most relevant information . \n\t', '\n\t\t The concluding part , on the other hand , often includes the author\x92s interpretation and cross- reference to other documents which can cause confusion . \n\t', '\n\t\t However , these statements are yet to be verified . \n\t', '\n\t\t Additional information , namely lexical or syntactic , may result in even higher accuracy of classification . \n\t', '\n\t\t We performed several experiments for Czech . \n\t', '\n\t\t We observed that adding noun , verb and prepositional phrases led to a small increase in the accuracy but that increase was not significant . \n\t', '\n\t\t Other kinds of fragments should be checked , for instance intersecting fragments or sliding fragments . \n\t', '\n\t\t So far we have ignored the structure of the documents ( titles , splitting into paragraphs ) and focused only on plain text . \n\t', '\n\t\t In the next stage , we will apply these methods to classifying HTML and XML documents . \n\t', '\n\t\t Larkey \n\t\t']",Positive
"['\n\t\t He exploited the structure of documents \x96 the title , the abstract , and the first twenty lines of the summary \x96 assigning different weights to each part . \n\t', '\n\t\t We showed that this approach can be used even for non-structured texts like newspaper articles . \n\t', '\n\t\t Tombros et al . \n\t', '\n\t\t \n\t\t']",Positive
